% This file was created with JabRef 2.8.1.
% Encoding: MacRoman

%----- 2022 -----%

@ARTICLE{kukreja2020lcc,
  author = {Navjot Kukreja and Jan Hueckelheim and Mathias Louboutin and John Washbourne and Paul H. J. Kelly and Gerard J. Gorman},
  title = {Lossy Checkpoint Compression in Full Waveform Inversion},
  year = {2022},
  month = {5},
  volume = {15},
  number = {9},
  pages = {3815-3829},
  abstract = {This paper proposes a new method that combines check-pointing
methods with error-controlled lossy compression for large-scale
high-performance Full-Waveform Inversion (FWI), an inverse problem commonly
used in geophysical exploration. This combination can significantly reduce
data movement, allowing a reduction in run time as well as peak memory. In
the Exascale computing era, frequent data transfer (e.g., memory bandwidth,
PCIe bandwidth for GPUs, or network) is the performance bottleneck rather
than the peak FLOPS of the processing unit. Like many other adjoint-based
optimization problems, FWI is costly in terms of the number of floating-point
operations, large memory footprint during backpropagation, and data transfer
overheads. Past work for adjoint methods has developed checkpointing methods
that reduce the peak memory requirements during backpropagation at the cost
of additional floating-point computations. Combining this traditional
checkpointing with error-controlled lossy compression, we explore the
three-way tradeoff between memory, precision, and time to solution. We
investigate how approximation errors introduced by lossy compression of the
forward solution impact the objective function gradient and final inverted
solution. Empirical results from these numerical experiments indicate that
high lossy-compression rates (compression factors ranging up to 100) have a
relatively minor impact on convergence rates and the quality of the final
solution.},
  keywords = {GMD, Lossy compression, Full waveform inversion, checkpointing, memory},
  note = {(Geoscientific Model Development)},
  doi = {10.5194/gmd-15-3815-2022},
  url = {https://arxiv.org/pdf/2009.12623.pdf}
}

@ARTICLE{siahkoohi2021dbif,
  author = {Ali Siahkoohi and Gabrio Rizzuti and Felix J. Herrmann},
  title = {Deep Bayesian inference for seismic imaging with tasks},
  journal = {Geophysics},
  year = {2022},
  month = {06},
  abstract = {We propose to use techniques from Bayesian inference and deep
neural networks to translate uncertainty in seismic imaging to uncertainty in
tasks performed on the image, such as horizon tracking. Seismic imaging is an
ill-posed inverse problem because of unavoidable bandwidth and aperture
limitations, which that is hampered by the presence of noise and
linearization errors. Many regularization methods, such as transform-domain
sparsity promotion, have been designed to deal with the adverse effects of
these errors, however, these methods run the risk of biasing the solution and
do not provide information on uncertainty in the image space and how this
uncertainty impacts certain tasks on the image. A systematic approach is
proposed to translate uncertainty due to noise in the data to confidence
intervals of automatically tracked horizons in the image. The uncertainty is
characterized by a convolutional neural network (CNN) and to assess these
uncertainties, samples are drawn from the posterior distribution of the CNN
weights, used to parameterize the image. Compared to traditional priors, in
the literature it is argued that these CNNs introduce a flexible inductive
bias that is a surprisingly good fit for many diverse domains in imaging. The
method of stochastic gradient Langevin dynamics is employed to sample from
the posterior distribution. This method is designed to handle large scale
Bayesian inference problems with computationally expensive forward operators
as in seismic imaging. Aside from offering a robust alternative to maximum a
posteriori estimate that is prone to overfitting, access to these samples
allow us to translate uncertainty in the image, due to noise in the data, to
uncertainty on the tracked horizons. For instance, it admits estimates for
the pointwise standard deviation on the image and for confidence intervals on
its automatically tracked horizons.},
  keywords = {deep priors, seismic imaging, uncertainty quantification, horizon tracking},
  note = {(Just accepted in Geophysics)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2022/siahkoohi2021dbif/paper.html}
}

%----- 2021 -----%

@ARTICLE{sanavi2021tad,
  author = {Hamideh Sanavi and Peyman P. Moghaddam and Felix J. Herrmann},
  title = {True amplitude depth migration using curvelets},
  journal = {Geophysics},
  year = {2021},
  month = {07},
  volume = {86},
  number = {4},
  pages = {S299-S310},
  abstract = {We have developed a true amplitude solution to the seismic imaging problem. We derive a diagonal scaling approach for the normal operator approximation in the curvelet domain. This is based on the theorem that states that curvelets remain approximately invariant under the action of the normal operator. We use curvelets as essential tools for approximation and inversion. We also exploit the theorem that states that the curvelet-domain approximation should be smooth in phase space by enforcing the smoothness of curvelet coefficients in the angle and space domains. We analyze our method using a reverse time migration-demigration code, simulating the acoustic wave equation on different synthetic models. Our method produces a good resolution with reflecting dips, reproduces the true amplitude reflectors, and compensates for incomplete illumination in seismic images.},
  keywords = {imaging, depth migration, inversion, amplitude, lsrtm},
  note = {(Geophysics)},
  doi = {10.1190/geo2019-0307.1}
}

@ARTICLE{rizzuti2020dfw,
  author = {Gabrio Rizzuti and Mathias Louboutin and Rongrong Wang and Felix J. Herrmann},
  title = {A dual formulation of wavefield reconstruction inversion for
large-scale seismic inversion},
  journal = {Geophysics},
  year = {2021},
  month = {10},
  volume = {86},
  number = {6},
  pages = {1ND-Z3},
  abstract = {Many of the seismic inversion techniques currently proposed that focus on robustness with respect to the background model choice are not apt to large-scale 3D applications, and the methods that are computationally feasible for industrial problems, such as full waveform inversion, are notoriously limited by convergence stagnation and require adequate starting models. We propose a novel solution that is both scalable and less sensitive to starting models or inaccurate parameters (such as anisotropy) that are typically kept fixed during inversion. It is based on a dual reformulation of the classical wavefield reconstruction inversion, whose empirical robustness with respect to these issues is well documented in the literature. While the classical version is not suited to 3D, as it leverages expensive frequency-domain solvers for the wave equation, our proposal allows the deployment of state-of-the-art time-domain finite-difference methods, and is potentially mature for industrial-scale problems.},
  keywords = {3D, Full-waveform inversion, Wave equation, Finite-difference},
  note = {(Geophysics)},
  doi = {10.1190/geo2020-0743.1},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2021/rizzuti2020dfw/rizzuti2020dfw.html}
}

@ARTICLE{yang2020lrpo,
  author = {Mengmeng Yang and Marie Graff and Rajiv Kumar and Felix J. Herrmann},
  title = {Low-rank representation of omnidirectional subsurface extended image
volumes},
  journal = {Geophysics},
  year = {2021},
  month = {01},
  volume = {86},
  number = {3},
  pages = {1MJ-WA152},
  abstract = {Subsurface-offset gathers play an increasingly important role in seismic
imaging. These gathers are used during velocity model building and inversion
of rock properties from amplitude variations. While powerful, these gathers
come with high computational and storage demands to form and manipulate these
high dimensional objects. This explains why only limited numbers of image
gathers are computed over a limited offset range. We avoid these high costs
by working with highly compressed low-rank factorizations. We arrive at these
factorizations via a combination of probings with the double two-way wave
equation and randomized singular value decompositions. In turn, the resulting
factorizations give us access to all subsurface offsets without having to
form the full extended image volumes. The latter is computationally
prohibitive because extended image volumes are quadratic in image size. As a
result, we can easily handle situations where conventional horizontal offset
gathers are no longer focused. More importantly, the factorization also
provides a mechanism to use the invariance relation of extended image volumes
for velocity continuation. With this technique, extended image volumes for
one background velocity model can directly be mapped to those of another
background velocity model. Our low-rank factorization inherits this
invariance property so we incur factorization costs only once when examining
different imaging scenarios. Because all imaging experiments only involve the
factors, they are computationally cheap with costs that scale with the rank
of the factorization. We validate our methodology on 2D synthetics including
a challenging imaging example with salt. Our experiments show that our
low-rank factorization parameterizes extended image volumes naturally.
Instead of brute force explicit cross-correlations between shifted source and receiver
wavefields, our approach relies on the underlying linear-algebra structure
that enables us to work with these objects without incurring unfeasible
demands on computation and storage.},
  keywords = {extended image volumes, low rank, randomized linear algebra, power
schemes, invariance relationship},
  note = {(Geophysics)},
  doi = {10.1190/geo2020-0152.1},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2021/yang2020lrpo/Paper_final.html}
}

%----- 2020 -----%

@ARTICLE{yang2020tdsp,
  author = {Mengmeng Yang and Zhilong Fang and Philipp A. Witte and Felix J. Herrmann},
  title = {Time-domain sparsity promoting least-squares reverse time migration
with source estimation},
  journal = {Geophysical Prospecting},
  year = {2020},
  month = {08},
  volume = {68},
  number = {9},
  pages = {2697-2711},
  abstract = {Least-squares reverse time migration is well-known for its capability to generate artifact-free true-amplitude subsurface images through fitting observed data in the least-squares sense. However, when applied to realistic imaging problems, this approach is faced with issues related to overfitting and excessive computational costs induced by many wave-equation solves. The fact that the source function is unknown complicates this situation even further. Motivated by recent results in stochastic optimization and transform-domain sparsity-promotion, we demonstrate that the computational costs of inversion can be reduced significantly while avoiding imaging artifacts and restoring amplitudes. While powerfull, these new approaches do require accurate information on the source-time function, which is often lacking. Without this information, the imaging quality deteriorates rapidly. We address this issue by presenting an approach where the source-time function is estimated on the fly through a technique known as variable projection. Aside from introducing negligible computational overhead, the proposed method is shown to perform well on imaging problems with noisy data and problems that involve complex settings such as salt. In either case, the presented method produces high resolution high-amplitude fidelity images including an estimates for the source-time function. In addition, due to its use of stochastic optimization, we arrive at these images at roughly one to two times the cost of conventional reverse time migration involving all data.},
  keywords = {sparsity inversion, source estimation, penalty},
  note = {(Geophysical Prospecting)},
  doi = {10.1111/1365-2478.13021},
  url = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalProspecting/2020/yang2020tdsp/yang2020tdsp.html}
}

@ARTICLE{daskalakis2019SIAMJISasr,
  author = {Emmanouil Daskalakis and Felix J. Herrmann and Rachel Kuske},
  title = {Accelerating Sparse Recovery by Reducing Chatter},
  journal = {SIAM Journal on Imaging Sciences},
  year = {2020},
  month = {07},
  volume = {13},
  number = {3},
  pages = {1211–1239},
  abstract = {Compressive Sensing has driven a resurgence of sparse recovery algorithms with
  l_1-norm minimization. While these minimizations are relatively well understood for small underdetermined, possibly inconsistent systems, their behavior for large over-determined and inconsistent systems has received much less attention. Specifically, we focus on large systems where computational restrictions call for algorithms that use randomized subsets of rows that are touched a limited number of times. In that regime, l_1-norm minimization algorithms exhibit unwanted fluctuations near the desired solution, and the Linear Bregman iterations are no exception. We explain this observed lack of performance in terms of chatter, a well-known phenomena observed in non-smooth dynamical systems, where intermediate solutions wander between different states stifling convergence. By identifying chatter as the culprit, we modify the Bregman iterations with chatter reducing adaptive element-wise step lengths in combination with potential support detection via threshold crossing. We demonstrate the performance of our algorithm on carefully selected stylized examples and a realistic seismic imaging problem involving millions of unknowns and matrix-free matrix-vector products that involve expensive wave-equation solves.},
  keywords = {sparsity promotion, inconsistent linear systems, Kacmarz, linearized Bregman
dynamical systems, non-smooth dynamics, chatter},
  note = {(SIAM Journal on Imaging Sciences)},
  doi = {10.1137/19M129111X},
  url = {https://slim.gatech.edu/Publications/Public/Journals/SIAMJournalOnImagingSciences/2020/daskalakis2019SIAMJISasr/daskalakis2019SIAMJISasr.pdf}
}

@ARTICLE{luporini2018aap,
  author = {Fabio Luporini and Mathias Louboutin and Michael Lange and Navjot Kukreja and Philipp A. Witte and Jan Huckelheim and Charles Yount and Paul H. J. Kelly and Felix J. Herrmann and Gerard J. Gorman},
  title = {Architecture and performance of Devito, a system for automated stencil computation},
  journal = {ACM Trans. Math. Softw.},
  year = {2020},
  month = {04},
  volume = {46},
  number = {1},
  abstract = {Stencil computations are a key part of many high-performance computing applications, such as image processing, convolutional neural networks, and finite-difference solvers for partial differential equations. Devito is a framework capable of generating highly-optimized code given symbolic equations expressed in Python, specialized in, but not limited to, affine (stencil) codes. The lowering process – from mathematical equations down to C++ code – is
performed by the Devito compiler through a series of intermediate representations. Several performance optimizations are introduced, including advanced common sub-expressions elimination, tiling and parallelization. Some of these are obtained through well-established stencil optimizers, integrated in the back-end of the Devito compiler. The architecture of the Devito compiler, as well as the performance optimizations that are applied when generating code, are presented. The effectiveness of such performance optimizations is demonstrated using operators drawn from seismic imaging applications.},
  keywords = {Stencil, finite difference method, symbolic processing, structured grid, compiler, performance optimization},
  note = {(ACM Trans. Math. Softw.)},
  doi = {10.1145/3374916},
  url = {https://slim.gatech.edu/Publications/Public/Journals/ACMTOMS/2020/luporini2018aap/luporini2018aap.pdf}
}

@ARTICLE{witte2019TPDedas,
  author = {Philipp A. Witte and Mathias Louboutin and Henryk Modzelewski and Charles Jones and James Selvage and Felix J. Herrmann},
  title = {An Event-Driven Approach to Serverless Seismic Imaging in the Cloud},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  year = {2020},
  month = {03},
  volume = {31},
  number = {9},
  pages={2032-2049},
  abstract = {Adapting the cloud for high-performance computing (HPC) is a
challenging task, as software for HPC applications hinges on fast network
connections and is sensitive to hardware failures. Using cloud infrastructure
to recreate conventional HPC clusters is therefore in many cases an
infeasible solution for migrating HPC applications to the cloud. As an
alternative to the generic lift and shift approach, we consider the specific
application of seismic imaging and demonstrate a serverless and event-driven
pproach for running large-scale instances of this problem in the cloud.
Instead of permanently running compute instances, our workflow is based on a
serverless architecture with high throughput batch computing and event-driven
computations, in which computational resources are only running as long as
they are utilized. We demonstrate that this approach is very flexible and
allows for resilient and nested levels of parallelization, including domain
decomposition for solving the underlying partial differential equations.
While the event-driven approach introduces some overhead as computational
resources are repeatedly restarted, it inherently provides resilience to
instance shut-downs and allows a significant reduction of cost by avoiding
idle instances, thus making the cloud a viable alternative to on-premise
clusters for large-scale seismic imaging.},
  keywords = {cloud, imaging, serverless, event-driven, lsrtm},
  note = {(IEEE Transactions on Parallel and Distributed Systems)},
  doi={10.1109/TPDS.2020.2982626},
  url = {https://slim.gatech.edu/Publications/Public/Journals/IEEETPDS/2020/witte2019TPDedas/witte2019TPDedas.html}
}

%----- 2019 -----%

@ARTICLE{siahkoohi2019itl,
  author = {Ali Siahkoohi and Mathias Louboutin and Felix J. Herrmann},
  title = {The importance of transfer learning in seismic modeling and imaging},
  journal = {Geophysics},
  year = {2019},
  abstract = {Accurate forward modeling is essential for solving inverse problems
in exploration seismology. Unfortunately, it is often not possible to afford being
physically or numerically accurate. To overcome this conundrum, we make use of raw and processed data from nearby surveys. We propose to use this data, consisting of shot records or velocity models, to pre-train a neural network to
correct for the effects of, for instance, the free surface or numerical dispersion, both of which can be considered as proxies for incomplete or inaccurate physics. Given this pre-trained neural network, we apply transfer learning to finetune this pre-trained neural network so it performs well on its task of mapping low-cost, but low-fidelity, solutions to high-fidelity solutions for the current survey. As long as we can limit ourselves during finetuning
to using only a small fraction of high-fidelity data, we gain processing the current survey while using information from nearby surveys. We demonstrate this principle by removing surface-related multiples and ghosts from shot records and the effects of numerical dispersion from migrated images and wave simulations},
  keywords = {deep learning, transfer learning, modeling, imaging, SRME},
  doi = {10.1190/geo2019-0056.1},
  note = {(Accepted in GEOPHYSICS)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2019/siahkoohi2019itl/siahkoohi2019itl.html}
}

@ARTICLE{kukreja2019PASCccd,
  author = {Navjot Kukreja and Jan Huckelheim and Mathias Louboutin and Paul Hovland and Gerard Gorman},
  title = {Combining checkpointing and data compression to accelerate adjoint-based optimization problems},
  journal = {Euro-Par 2019: Parallel Processing},
  year = {2019},
  pages = {87-100},
  publisher = {Springer International Publishing},
  abstract = {Seismic inversion and imaging are adjoint-based optimization problems that processes up to terabytes of data, regularly exceeding the memory capacity of available computers. Data compression is an effective strategy to reduce this memory requirement by a certain factor, particularly if some loss in accuracy is acceptable. A popular alternative is checkpointing, where data is stored at selected points in time, and values at other times are recomputed as needed from the last stored state. This allows arbitrarily large adjoint computations with limited memory, at the cost of additional recomputations. In this paper we combine compression and checkpointing for the first time to compute a realistic seismic inversion. The combination of checkpointing and compression allows larger adjoint computations compared to using only compression, and reduces the recomputation overhead significantly compared to using only checkpointing.},
  keywords = {Adjoint-state, FD, checkpointing, compression, HPC, inverse problems},
  doi = {10.1007/978-3-030-29400-7_7},
  note = {(Euro-Par 2019: Parallel Processing)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/PASC/2019/kukreja2019PASCccd/kukreja2019PASCccd.pdf}
}

@ARTICLE{peters2019aos,
  author = {Bas Peters and Felix J. Herrmann},
  title = {Algorithms and software for projections onto intersections of convex
               and non-convex sets with applications to inverse problems},
  journal = {arXiv e-prints},
  year = {2019},
  month = {03},
  abstract = {We propose algorithms and software for computing projections onto the intersection of multiple convex and non-convex constraint sets. The software package, called SetIntersectionProjection, is intended for the regularization of inverse problems in physical parameter estimation and image processing. The primary design criterion is working with multiple sets, which allows us to solve inverse problems with multiple pieces of prior knowledge. Our algorithms outperform the well known Dykstra's algorithm when individual sets are not easy to project onto because we exploit similarities between constraint sets. Other design choices that make the software fast and practical to use, include recently developed automatic selection methods for auxiliary algorithm parameters, fine and coarse grained parallelism, and a multilevel acceleration scheme. We provide implementation details and examples that show how the software can be used to regularize inverse problems. Results show that we benefit from working with all available prior information and are not limited to one or two regularizers because of algorithmic, computational, or hyper-parameter selection issues.},
  keywords = {software, algorithm},
  eprint={1902.09699},
  note = {(arXiv)},
  url = {http://arxiv.org/abs/1902.09699}
}

@ARTICLE{peters2019gms,
  author = {Bas Peters and Felix J. Herrmann},
  title = {Generalized Minkowski sets for the regularization of inverse problems},
  journal = {arXiv e-prints},
  year = {2019},
  month = {03},
  abstract = {Many works on inverse problems in the imaging sciences consider regularization via one or more penalty
functions or constraint sets. When the models/images are not easily described using one or a few penalty
functions/constraints, additive model descriptions for regularization lead to better imaging results. These
include cartoon-texture decomposition, morphological component analysis, and robust principal component
analysis; methods that typically rely on penalty functions. We propose a regularization framework, based
on the Minkowski set, that merges the strengths of additive models and constrained formulations. We
generalize the Minkowski set, such that the model parameters are the sum of two components, each of
which is constrained to an intersection of sets. Furthermore, the sum of the components is also an element
of another intersection of sets. These generalizations allow us to include multiple pieces of prior knowledge
on each of the components, as well as on the sum of components, which is necessary to ensure physical
feasibility of partial-differential-equation based parameters estimation problems. We derive the projection
operation onto the generalized Minkowski sets and construct an algorithm based on the alternating
direction method of multipliers. We illustrate how we benefit from using more prior knowledge in the
form of the generalized Minkowski set using seismic waveform inversion and video background-anomaly
separation.},
  keywords = {inverse},
  eprint={1903.03942},
  note = {(arXiv)},
  url = {http://arxiv.org/abs/1903.03942}
}

@ARTICLE{witte2018alf,
  author = {Philipp A. Witte and Mathias Louboutin and Navjot Kukreja and Fabio Luporini and Michael Lange and Gerard J. Gorman and Felix J. Herrmann},
  title = {A large-scale framework for symbolic implementations of seismic inversion algorithms in Julia},
  journal = {Geophysics},
  volume = {84},
  number = {3},
  pages = {F57-F71},
  year = {2019},
  month = {03},
  abstract = {Writing software packages for seismic inversion is a very challenging task, since problems such as full-waveform inversion or least-squares imaging are both algorithmically and computationally demanding due to the large number of unknown parameters and the fact that we are propagating waves over many wavelengths. Software frameworks therefore need to combine both versatility and performance to provide geophysicists with the means and flexibility to implement complex algorithms that scale to exceedingly large 3D problems. Following these principles, we introduce the Julia Devito Inversion framework, an open-source software package in Julia for large-scale seismic modeling and inversion based on Devito, a domain-specific language compiler for automatic code generation. The framework consists of matrix-free linear operators for implementing seismic inversion algorithms that closely resembles the mathematical notation, a flexible resilient parallelization and an interface to Devito for generating optimized stencil code to solve the underlying wave equations. In comparison to many manually optimized industry codes written in low-level languages, our software is built on the idea of independent layers of abstractions and user interfaces with symbolic operators, making it possible to manage both the complexity of algorithms and performance optimizations, while preserving modularity, which allows for a level of expressiveness needed to formulate a broad range of wave-equation-based inversion problems. Through a series of numerical examples, we demonstrate that this allows users to implement algorithms for waveform inversion and imaging as simple Julia scripts that scale to large-scale 3D problems; thus providing a truly performant research and production framework.},
  keywords = {FWI, LSRTM, modeling, inversion, software},
  doi = {10.1190/geo2018-0174.1},
  note = {(Geophysics)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2019/witte2018alf/witte2018alf.pdf}
}

@ARTICLE{witte2018cls,
  author = {Philipp A. Witte and Mathias Louboutin and Fabio Luporini and Gerard J. Gorman and Felix J. Herrmann},
  title = {Compressive least-squares migration with on-the-fly Fourier transforms},
  journal = {Geophysics},
  volume = {84},
  number = {5},
  pages = {R655-R672},
  year = {2019},
  month = {08},
  abstract = {Least-squares reverse-time migration is a powerful approach for true amplitude seismic imaging of complex geological structures, but the successful application of this method is currently hindered by its enormous computational cost, as well as high memory requirements for computing the gradient of the objective function. We tackle these problems by introducing an algorithm for low-cost sparsity-promoting least-squares migration using on-the-fly Fourier transforms. We formulate the least-squares migration objective function in the frequency domain and compute gradients for randomized subsets of shot records and frequencies, thus significantly reducing data movement and the number of overall wave equations solves. By using on-the-fly Fourier transforms, we can compute an arbitrary number of monochromatic frequency-domain wavefields with a time-domain modeling code, instead of having to solve individual Helmholtz equations for each frequency, which quickly becomes computationally infeasible when moving to high frequencies. Our numerical examples demonstrate that compressive imaging with on-the-fly Fourier transforms provides a fast and memory-efficient alternative to time-domain imaging with optimal checkpointing, whose memory requirements for a fixed background model and source wavelet is independent of the number of time steps. Instead, memory and additional computational cost grow with the number of frequencies and determine the amount of subsampling artifacts and crosstalk. In contrast to optimal checkpointing, this offers the possibility to trade both memory and computational cost for image quality or a larger number of iterations and is advantageous in new computing environments such as the cloud, where compute is often cheaper than memory and data movement.},
  keywords = {least squares migration, Fourier, sparsity-promotion},
  doi = {10.1190/geo2018-0490.1},
  note = {(Geophysics)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2019/witte2018cls/witte2018cls.pdf}
}

@ARTICLE{witte2019ecl,
  author = {Philipp A. Witte and Mathias Louboutin and Navjot Kukreja and Fabio Luporini and Michael Lange and Gerard J. Gorman and Felix J. Herrmann},
  title = {Geophysics Bright Spots: Efficient coding of large-scale seismic
inversion algorithms},
  journal = {The Leading Edge},
  volume = {38},
  number = {6},
  pages = {482-484},
  year = {2019},
  abstract = {In “A large-scale framework for symbolic implementations of seismic inversion algorithms in Julia,” Witte et al. describe new developments in how to code complex geophysical algorithms in a concise way. Subsurface seismic imaging and parameter estimation are among the most computationally challenging problems in the scientific community. Codes for solving seismic inverse problems, such as FWI or least-squares reverse time migration (LS-RTM), need to be highly optimized, but at the same time, facilitate the implementation of complex optimization algorithms. Traditionally, production-level codes in the oil and gas industry were exclusively written in low-level languages, such as C or Fortran, with extensive amounts of manual performance optimizations, thus making code maintenance, debugging, and adoption of new algorithms prohibitively challenging. Witte et al. present a paradigm of software engineering for seismic inverse problems based on symbolic user interfaces and code generation with automated performance optimization. Inspired by recent deep learning frameworks, the Julia Devito inversion framework (JUDI; an open-source software package) combines high-level abstractions for expressing seismic inversion algorithms with a domain-specific language compiler called Devito for solving the underlying wave equations. Devito's generated code is compiled just in time and outperforms codes with manual performance optimizations. JUDI utilizes Julia's high-level parallelization, making the software easily adaptable to a variety of computing environments such as densely connected HPC clusters or the cloud. The numerical examples (Figure 3) demonstrate the ability to implement a variety of complex algorithms for FWI and LS-RTM in a few lines of Julia code and run it on large-scale 3D models. The paper concludes that abstractions and performance are not mutually exclusive, and use of symbolic user interfaces can facilitate the implementation of new and innovative seismic inversion algorithms.},
  keywords = {bright spots, inversion, large-scale, julia},
  doi = {10.1190/tle38060482.1},
  note = {(The Leading Edge)},
  url = {https://library.seg.org/doi/10.1190/tle38060482.1}
}

@ARTICLE{kumar2018toi,
  author = {Rajiv Kumar and Marie Graff-Kray and Ivan Vasconcelos and Felix J. Herrmann},
  title = {Target-oriented imaging using extended image volumes—a low-rank factorization approach},
  journal = {Geophysical Prospecting},
  volume = {67},
  number = {5},
  pages = {1312-1328},
  year = {2019},
  abstract = {Imaging in geological challenging environments has led to new developments, including the idea of generating reflection responses by means of interferometric redatuming at a given target datum in the subsurface, when the target datum lies beneath a complex overburden. One way to perform this redatuming is via conventional model-based wave-equation techniques. But those techniques can be computationally expensive for large-scale seismic
problems since the number of wave-equation solves is equal to two-times the number of
sources involved during seismic data acquisition. Also conventional shot-profile
techniques require lots of memory to save full subsurface extended image volumes. Therefore,
they only form subsurface image volumes in either horizontal or vertical directions. We now present a
randomized singular value decomposition based approach built upon the matrix
probing scheme, which takes advantage of the algebraic structure of the extended
imaging system. This low-rank representation enables us to overcome both the computational
cost associated with the number of wave-equation solutions and memory usage due to explicit
storage of full subsurface extended image volumes employed by conventional migration
methods. Experimental results on complex geological models demonstrate the efficacy of
the proposed methodology and allow practical reflection-based extended imaging for large-scale 5D seismic data.},
  keywords = {randomized linear algebra, extended image volumes, target-oriented imaging},
  doi = {10.1111/1365-2478.12779},
  note = {(Geophysical Prospecting)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalProspecting/2019/kumar2018toi/kumar2018toi.html}
}

@ARTICLE{louboutin2018dae,
  author = {Mathias Louboutin and Michael Lange and Fabio Luporini and Navjot Kukreja and Philipp A. Witte and Felix J. Herrmann and Paulius Velesko and Gerard J. Gorman},
  title = {Devito (v3.1.0): an embedded domain-specific language for finite differences and geophysical exploration},
  journal = {Geoscientific Model Development},
  year = {2019},
  abstract = {We introduce Devito, a new domain-specific language for implementing
high-performance finite difference partial differential equation solvers.
The motivating application is exploration seismology where methods such as
Full-Waveform Inversion and Reverse-Time Migration are used to
invert terabytes of seismic data to create images of the earth's
subsurface. Even using modern supercomputers, it can take weeks to process a
single seismic survey and create a useful subsurface image. The computational cost is dominated by the numerical solution of wave equations and their corresponding adjoints. Therefore, a great deal of effort is invested in aggressively optimizing the performance of these wave-equation propagators for different computer architectures. Additionally, the actual set of partial differential equations being solved and their numerical discretization is under constant innovation as increasingly realistic representations of the physics are developed, further ratcheting up the cost of practical solvers. By embedding a domain-specific language within Python and making heavy use of SymPy, a symbolic mathematics library, we make it possible to develop finite difference simulators quickly using a
syntax that strongly resembles the mathematics. The Devito compiler reads this code and applies a wide range of analysis to generate highly optimized and parallel code. This approach can reduce the development time of a verified and optimized solver from months to days.},
  keywords = {wave-equation, modeling, finite-differences, HPC},
  doi = {10.5194/gmd-12-1165-2019},
  note = {(Geoscientific Model Development)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/GMD/2019/louboutin2018dae/louboutin2018dae.pdf}
}

@ARTICLE{silva2018alr,
  author = {Curt Da Silva and Yiming Zhang and Rajiv Kumar and Felix J. Herrmann},
  title = {Applications of low-rank compressed seismic data to full waveform inversion and extended image volumes},
  journal = {Geophysics},
  year = {2019},
  abstract = { Conventional oil and gas fields are increasingly difficult to explore and image, resulting in the call for more complex wave-equation based inversion algorithms that require dense long-offset samplings. Consequently, there is an exponential growth in the size of data volumes and prohibitive demands on computational resources. In this work, we propose a method to compress and process seismic data directly in a low-rank tensor format, which drastically reduces the amount of storage required to represent the data. We first outline how seismic data exhibits low-rank structure in a particular transform-domain, which can be exploited to compress the dense data in one extremely storage-efficient tensor format when the data is fully sampled. In the more realistic case of missing data, we can use efficient interpolation techniques to approximate the fully sampled volume in compressed form. In either case, once we have our data represented in its compressed tensor form, we design an algorithm to extract source or receiver gathers directly from the compressed parameters. This extraction process can be done on-the-fly directly on the compressed data, in a full waveform inversion context, and does not require scanning through the entire dataset in order to form shot gathers. To the best of our knowledge, this work is one of the first major contributions to working with seismic data applications directly in the compressed domain without reconstructing the entire data volume. We use a stochastic inversion approach, which works with small subsets of source experiments at each iteration, further reducing the computational and memory costs of full waveform inversion. We also demonstrate how this data compression and extraction technique can be applied to forming full subsurface image gathers through probing techniques.},
  keywords = {low rank, tensor, common shot/receiver gather, full waveform inversion, extended image volume},
  doi = {10.1190/geo2018-0116.1},
  note = {Accepted on January 22, 2019.},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2019/silva2018alr/silva2018alr.pdf}
}

@ARTICLE{dasilva2017uls,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {A unified {2D/3D} large scale software environment for nonlinear inverse problems},
  journal = {ACM Transactions on Mathematical Software},
  year = {2019},
  abstract = {Large scale parameter estimation problems are some of
                  the most computationally demanding problems. An
                  academic researcher's domain-specific knowledge
                  often precludes that of software design, which
                  results in software frameworks for inversion that
                  are technically correct, but not scalable to
                  realistically-sized problems. On the other hand, the
                  computational demands of the problem for realistic
                  problems result in industrial codebases that are
                  geared solely for performance, rather than
                  comprehensibility or flexibility. We propose a new
                  software design that bridges the gap between these
                  two seemingly disparate worlds. A hierarchical and
                  modular design allows a user to delve into as much
                  detail as she desires, while using high performance
                  primitives at the lower levels. Our code has the
                  added benefit of actually reflecting the underlying
                  mathematics of the problem, which lowers the
                  cognitive load on user using it and reduces the
                  initial startup period before a researcher can be
                  fully productive. We also introduce a new
                  preconditioner for the Helmholtz equation that is
                  suitable for fault-tolerant distributed
                  systems. Numerical experiments on a variety of 2D
                  and 3D test problems demonstrate the effectiveness
                  of this approach on scaling algorithms from small to
                  large scale problems with minimal code changes.},
  keywords = {optimization, PDE-constrained inversion, large scale, matlab},
  note = {Accepted on January 27, 2019.},
  url = {https://slim.gatech.edu/Publications/Public/Journals/ACMTOMS/2019/dasilva2017uls/dasilva2017uls.html}
}

%----- 2018 -----%

@ARTICLE{liu2018ssi,
  author = {Michelle Liu and Rajiv Kumar and Eldad Haber and Aleksandr Y. Aravkin},
  title = {Simultaneous-shot inversion for PDE-constrained optimization problems with missing data},
  journal = {Inverse Problems},
  volume = {35},
  number = {2},
  pages = {025003},
  year = {2018},
  abstract = {Stochastic optimization is key to efficient inversion in
PDE-constrained optimization. Using `simultaneous shots', or random
superposition of source terms, works very well in simple acquisition
geometries where all sources see all receivers, but this rarely occurs in practice. We
develop an approach that interpolates data to an ideal acquisition geometry
while solving the inverse problem using simultaneous shots. The approach is
formulated as a joint inverse problem, combining ideas from low-rank
interpolation with full-waveform inversion. Results using synthetic
experiments illustrate the flexibility and efficiency of the approach.},
  keywords = {Optimization, low-rank interpolation, full-waveform inversion},
  doi = {10.1088/1361-6420/aaf317},
  note = {(Inverse Problems)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/InverseProblems/2018/liu2018ssi/liu2018ssi.pdf}
}


@ARTICLE{fang2017sewri,
  author = {Zhilong Fang and Rongrong Wang and Felix J. Herrmann},
  title = {Source estimation for wavefield-reconstruction inversion},
  journal = {Geophysics},
  volume = {83},
  number = {4},
  pages = {R345-R359},
  year = {2018},
  abstract = {Source estimation is essential to all the wave-equation-based seismic inversions, including full-waveform inversion and the recently proposed wavefield-reconstruction inversion. When the source estimation is inaccurate, errors will propagate into the predicted data and introduce additional data misfit. As a consequence, inversion results that minimize this data misfit may become erroneous. To mitigate the errors introduced by the incorrect and pre-estimated sources, an embedded procedure that updates sources along with medium parameters is necessary for the inversion. So far, such a procedure is still missing in the context of wavefield-reconstruction inversion, a method that is, in many situations, less prone to local minima related to the so-called cycle skipping, compared to full-waveform inversion through exact data-fitting. While wavefield-reconstruction inversion indeed helps to mitigate issues related to cycle skipping by extending the search space with wavefields as auxiliary variables, it relies on having access to the correct source functions. In this paper, we remove the requirement of having the accurate source functions by proposing a source estimation technique specifically designed for wavefield-reconstruction inversion. To achieve this task, we consider the source functions as unknown variables and arrive at an objective function that depends on the medium parameters, wavefields, and source functions. During each iteration, we apply the so-called variable projection method to simultaneously project out the source functions and wavefields. After the projection, we obtain a reduced objective function that only depends on the medium parameters and invert for the unknown medium parameters by minimizing this reduced objective. Numerical experiments illustrate that this approach can produce accurate estimates of the unknown medium parameters without any prior information of the source functions.},
  keywords = {WRI, source, estimation, variable projection, FWI},
  doi = {10.1190/geo2017-0700.1},
  note = {(Geophysics)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2018/fang2017sewri/fang2017sewri.html}
}

@ARTICLE{fang2017uqfip,
  author = {Zhilong Fang and Curt Da Silva and Rachel Kuske and Felix J. Herrmann},
  title = {Uncertainty quantification for inverse problems with weak partial-differential-equation constraints},
  journal = {Geophysics},
  volume = {83},
  number = {6},
  pages = {R629-R647},
  year = {2018},
  abstract = { In a statistical inverse problem, the objective is a complete
              statistical description of unknown parameters from noisy observations in order to quantify uncertainties of the parameters of interest. We consider inverse problems with partial-differential-equation-constraints, which are applicable to
              a variety of seismic problems. Bayesian inference is one of the most widely-used approaches to precisely quantify statistics through a posterior distribution, incorporating uncertainties in observed data, modeling kernel, and prior knowledge of the parameters. Typically when formulating the posterior distribution, the partial-differential-equation-constraints are required to be exactly satisfied, resulting in a highly nonlinear forward map and a posterior distribution with many local maxima. These drawbacks make it difficult to find an appropriate approximation for the posterior distribution. Another complicating factor is that traditional Markov chain Monte Carlo methods are known to converge slowly for realistically sized problems. In this work, we relax the partial-differential-equation-constraints by introducing an auxiliary variable, which allows for Gaussian deviations in the partial-differential-equations. Thus, we obtain a new bilinear posterior distribution consisting of both data and partial-differential-equation misfit terms. We illustrate that for a particular range of variance choices for the partial-differential-equation misfit term, the new posterior distribution has fewer modes and can be well-approximated by a Gaussian distribution, which can then be sampled in a straightforward manner. Since it is prohibitively expensive to explicitly construct the dense covariance matrix of the Gaussian approximation for intermediate to large-scale problems, we present a method to implicitly construct it, which enables efficient sampling. We apply this framework to two-dimensional seismic inverse problems with 1,800 and 92,455 unknown parameters. The results illustrate that our framework can produce comparable statistical quantities to those produced by conventional Markov chain Monte Carlo type methods while requiring far fewer partial-differential-equation solves, which are the main computational bottlenecks in these problems.},
  keywords = {UQ, FWI, acoustic, weak-constraint},
  doi = {10.1190/geo2017-0824.1},
  note = {(Geophysics)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2018/fang2017uqfip/fang2017uqfip.html}
}

@ARTICLE{peters2018pmf,
  author = {Bas Peters and Brendan R. Smithyman and Felix J. Herrmann},
  title = {Projection methods and applications for seismic nonlinear inverse problems with multiple constraints},
  journal = {Geophysics},
  volume = {84},
  number = {2},
  pages = {R251-R269},
  year = {2018},
  month = {02},
  abstract = {Nonlinear inverse problems are often hampered by non-uniqueness and local minima because of missing low frequencies and far offsets in the data, lack of access to good starting models, noise, and modeling errors. A well-known approach to counter these deficiencies is to include prior information on the unknown model, which regularizes the inverse problem. While conventional regularization methods have resulted in enormous progress in ill-posed (geophysical) inverse problems, challenges remain when the prior information consists of multiple pieces. To handle this situation, we propose an optimization framework that allows us to add multiple pieces of prior information in the form of constraints. Compared to additive regularization penalties, constraints have a number of advantages making them more suitable for inverse problems such as full-waveform inversion. The proposed framework is rigorous because it offers assurances that multiple constraints are imposed uniquely at each iteration, irrespective of the order in which they are invoked. To project onto the intersection of multiple sets uniquely, we employ Dykstra’s algorithm that scales to large problems and does not rely on trade-off parameters. In that sense, our approach differs substantially from approaches such as Tikhonov regularization, penalty methods, and gradient filtering. None of these offer assurances, which makes them less suitable to full-waveform inversion where unrealistic intermediate results effectively derail the iterative inversion process. By working with intersections of sets, we keep expensive objective and gradient calculations unaltered, separate from projections, and we also avoid trade-off parameters. These features allow for easy integration into existing code bases. In addition to more predictable behavior, working with constraints also allows for heuristics where we built up the complexity of the model gradually by relaxing the constraints. This strategy helps to avoid convergence to local minima that represent unrealistic models. We illustrate this unique feature with examples of varying complexity.},
  keywords = {full-waveform inversion, optimization, constraints, regularization, projection, intersection},
  doi = {10.1190/geo2018-0192.1},
  note = {(Geophysics)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2018/peters2018pmf/peters2018pmf.html}
}

@ARTICLE{sharan2018fsp,
  author = {Shashin Sharan and Rongrong Wang and Felix J. Herrmann},
  title = {Fast sparsity-promoting microseismic source estimation},
  journal = {Geophysical Journal International},
  volume = {216},
  number = {1},
  pages = {164-181},
  year = {2019},
  month = {01},
  abstract = {Microseismic events are generated during hydraulic fracturing of unconventional reservoirs and carry information on fracture locations and the origin times associated with these microseismic events. For drilling purposes and to prevent hazardous situations, we need to have accurate knowledge on the fracture locations as well as on their size, and their density. Because microseismic waves can travel far distances, microseismic data collected at the surface and or in boreholes can help us to monitor hydraulic fracturing. While so-called back propagation or time-reversal methods are able to focus recorded energy back onto the sources when a reasonable velocity model is available, these methods suffer from blurring especially in situations where the data acquisition suffers from lack of aperture, sparse sampling, and noise. As a result, these methods typically cannot resolve sources in close proximity, a desired feature since we need this information if we want to follow the fracture evolution in space and time. In that situation, we need to estimate the locations and the associated source-time functions for closely spaced microseismic sources along the active fractures. To overcome the limitations of time-reversal methods, we propose a wave-equation based inversion approach where we invert for the complete source wavefield in both space and time. By promoting sparsity on the source wavefield in space, we negate the effects of non-radiating sources during the inversion and obtain high-resolution intensity plots and high-fidelity estimates for the source-time functions. We obtain these results relatively fast by accelerating the linearized Bregman method with a dual formulation. Through experiments, we demonstrate that our method is computationally feasible, robust to noise, and works for closely spaced sources with overlapping source-time functions in complex geological settings.},
  keywords = {Waveform inversion, Joint inversion, induced seismicity},
  doi = {10.1093/gji/ggy415},
  note = {(Geophysical Journal International)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalJournalInternational/2018/sharan2018fsp/sharan2018fsp.pdf}
}

@ARTICLE{witte2018fwip3,
  author = {Philipp A. Witte and Mathias Louboutin and Keegan Lensink and Michael Lange and Navjot Kukreja and Fabio Luporini and Gerard Gorman and Felix J. Herrmann},
  title = {Full-Waveform Inversion - Part 3: optimization},
  journal = {The  Leading  Edge},
  volume = {37},
  number = {2},
  pages = {142-145},
  year = {2018},
  month = {01},
  abstract = {This tutorial is the third part of a full-waveform inversion (FWI) tutorial series with a step-by-step walkthrough of setting up forward and adjoint wave equations and building a basic FWI inversion framework. For discretizing and solving wave equations, we use Devito, a Python-based domain-specific language for automated generation of finite-difference code (Lange et al., 2016). The first two parts of this tutorial (Louboutin et al., 2017, 2018) demonstrated how to solve the acoustic wave equation for modeling seismic shot records and how to compute the gradient of the FWI objective function using the adjoint-state method. With these two key ingredients, we will now build an inversion framework that can be used to minimize the FWI least-squares objective function.},
  keywords = {devito, finite-differences, FWI, Modeling, tutorial, inversion},
  doi  = {10.1190/tle37020142.1},
  note = {(The Leading Edge)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/TheLeadingEdge/2018/witte2018fwip3/witte2018fwip3.html}
}

%----- 2017 -----%

@ARTICLE{kumar2017hrc,
  author = {Rajiv Kumar and Haneet Wason and Shashin Sharan and Felix J. Herrmann},
  title = {Highly repeatable {3D} compressive full-azimuth towed-streamer time-lapse acquisition --- a numerical feasibility study at scale},
  journal = {The  Leading  Edge},
  volume = {36},
  number = {8},
  pages = {677-687},
  year = {2017},
  month = {08},
  abstract = {Most conventional 3D time-lapse (or 4D) acquisitions are
                  ocean-bottom cable (OBC) or ocean-bottom node (OBN)
                  surveys since these surveys are relatively easy to
                  replicate compared to towed-streamer surveys. To
                  attain high degrees of repeatability, survey
                  replicability and dense periodic sampling has become
                  the norm for 4D surveys that renders this technology
                  expensive. Conventional towed-streamer acquisitions
                  suffer from limited illumination of subsurface due
                  to narrow azimuth. Although, acquisition techniques
                  such as multi-azimuth, wide-azimuth, rich-azimuth
                  acquisition, etc., have been developed to illuminate
                  the subsurface from all possible angles, these
                  techniques can be prohibitively expensive for
                  densely sampled surveys. This leads to uneven
                  sampling, i.e., dense receiver and coarse source
                  sampling or vice-versa, in order to make these
                  acquisitions more affordable. Motivated by the
                  design principles of Compressive Sensing (CS), we
                  acquire economic, randomly subsampled (or
                  compressive) and simultaneous towed-streamer
                  time-lapse data without the need of replicating the
                  surveys. We recover densely sampled time-lapse data
                  on one and the same periodic grid by using a
                  joint-recovery model (JRM) that exploits shared
                  information among different time-lapse recordings,
                  coupled with a computationally cheap and scalable
                  rank-minimization technique. The acquisition is low
                  cost since we have subsampled measurements (about
                  70\% subsampled), simulated with a simultaneous
                  long-offset acquisition configuration of two source
                  vessels travelling across a survey area at random
                  azimuths. We analyze the performance of our proposed
                  compressive acquisition and subsequent recovery
                  strategy by conducting a synthetic, at scale,
                  seismic experiment on a 3D time-lapse model
                  containing geological features such as channel
                  systems, dipping and faulted beds, unconformities
                  and a gas cloud. Our findings indicate that the
                  insistence on replicability between surveys and the
                  need for OBC/OBN 4D surveys can, perhaps, be
                  relaxed. Moreover, this is a natural next step
                  beyond the successful CS acquisition examples
                  discussed in this special issue.},
  keywords = {time-lapse seismic, marine, 3D, simultaneous long offset, CS, rank minimization},
  doi = {10.1190/tle36080677.1},
  note = {(The Leading Edge)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/TheLeadingEdge/2017/kumar2017hrc/kumar2017hrc.html}
}

@ARTICLE{louboutin2017fwi,
  author = {Mathias Louboutin and Philipp A. Witte and Michael Lange and Navjot Kukreja and Fabio Luporini and Gerard Gorman and Felix J. Herrmann},
  title = {Full-Waveform Inversion - Part 1: forward modeling},
  journal = {The  Leading  Edge},
  volume = {36},
  number = {12},
  pages = {1033-1036},
  year = {2017},
  month = {12},
  abstract = {Since its re-introduction by Pratt (1999), full-waveform inversion (FWI) has gained a lot of attention in geophysical exploration because of its ability to build high resolution velocity models more or less automatically in areas of complex geology. While there is an extensive and growing literature on the topic, publications focus mostly on technical aspects, making this topic inaccessible for a broader audience due to the lack of simple introductory resources for newcomers to geophysics. We will accomplish this by providing a hands-on walkthrough of FWI using Devito (Lange et al. 2016), a system based on domain-specific languages that automatically generates code for time-domainfinite-differences.},
  keywords = {finite-differences, devito, tutorial, FWI, modeling},
  doi  = {10.1190/tle36121033.1},
  note = {(The Leading Edge)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/TheLeadingEdge/2017/louboutin2017fwi/louboutin2017fwi.html}
}

@ARTICLE{louboutin2017fwip2,
  author = {Mathias Louboutin and Philipp A. Witte and Michael Lange and Navjot Kukreja and Fabio Luporini and Gerard Gorman and Felix J. Herrmann},
  title = {Full-Waveform Inversion - Part 2: adjoint modeling},
  journal = {The  Leading  Edge},
  volume = {37},
  number = {1},
  pages = {69-72},
  year = {2018},
  month = {01},
  abstract = {This tutorial is the second part of a three part tutorial series on full-waveform inversion (FWI), in which we provide a step by step walk through of setting up forward and adjoint wave equation solvers and an optimization framework for inversion. In part 1 (Louboutin et al., 2017), we demonstrated how to discretize the acoustic wave equation and how to set up a basic forward modeling scheme using Devito, a domain-specific language (DSL) in Python for automated finite-difference (FD) computations (Lange et al., 2016). Devito allows us to define wave equations as symbolic Python expressions (Meurer et al., 2017), from which optimized FD stencil code is automatically generated at run time. In part 1, we show how we can use Devito to set up and solve acoustic wave equations with (impulsive) seismic sources and sample wavefields at the receiver locations to model shot records.},
  keywords = {tutorial, devito, finite-difference, acoustic, FWI},
  doi  = {10.1190/tle37010069.1},
  note = {(The Leading Edge)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/TheLeadingEdge/2018/louboutin2017fwip2/louboutin2017fwip2.html}
}

@ARTICLE{oghenekohwo2017hrt,
  author = {Felix Oghenekohwo and Felix J. Herrmann},
  title = {Highly repeatable time-lapse seismic with distributed {Compressive} {Sensing}---mitigating effects of calibration errors},
  journal = {The  Leading  Edge},
  year = {2017},
  month = {08},
  volume = {36},
  number = {8},
  pages = {688-694},
  abstract = {Recently, we demonstrated that combining joint recovery
                  with low-cost non-replicated randomized sampling
                  tailored to time-lapse seismic can give us access to
                  high fidelity, highly repeatable, dense prestack
                  vintages, and high-grade time-lapse. To arrive at
                  this result, we assumed well-calibrated
                  surveys---i.e., we presumed accurate post-plot
                  source/receiver positions. Unfortunately, in
                  practice seismic surveys are prone to calibration
                  errors, which are unknown deviations between actual
                  and post-plot acquisition geometry. By means of
                  synthetic experiments, we analyze the possible
                  impact of these errors on vintages and on time-lapse
                  data obtained with our joint recovery model from
                  compressively sampled surveys. Supported by these
                  experiments, we demonstrate that highly repeatable
                  time-lapse vintages are attainable despite the
                  presence of unknown calibration errors in the
                  positions of the shots. We assess the repeatability
                  quantitatively for two scenarios by studying the
                  impact of calibration errors on conventional dense
                  but irregularly sampled surveys and on low-cost
                  compressed surveys. To separate time-lapse effects
                  from calibration issues, we consider the idealized
                  case where the subsurface remains unchanged and the
                  practical situation where time-lapse changes are
                  restricted to a subset of the data. In both cases,
                  the quality of the recovered vintages and time-lapse
                  decreases gracefully for low-cost compressed surveys
                  with increasing calibration errors. Conversely, the
                  quality of vintages from expensive densely
                  periodically sampled surveys decreases more rapidly
                  as unknown and difficult to control calibration
                  errors increase.},
  keywords = {time-lapse seismic, marine, random sampling, calibration errors, joint-recovery method},
  doi  = {10.1190/tle36080688.1},
  note = {(The Leading Edge)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/TheLeadingEdge/2017/oghenekohwo2017hrt/oghenekohwo2017hrt.html}
}

@ARTICLE{kumar2016bls,
  author = {Rajiv Kumar and Oscar Lopez and Damek Davis and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Beating level-set methods for {5D} seismic data interpolation: a primal-dual alternating approach},
  journal = {IEEE Transactions on Computational Imaging},
  year = {2017},
  month = {04},
  abstract = {Acquisition cost is a crucial bottleneck for seismic
                  workflows, and low-rank formulations for data
                  interpolation allow practitioners to 'fill in' data
                  volumes from critically subsampled data acquired in
                  the field. Tremendous size of seismic data volumes
                  required for seismic processing remains a major
                  challenge for these techniques. Residual-constrained
                  formulations require less parameter tuning when the
                  target noise floor is known. We propose a new
                  approach to solve residual constrained formulations
                  for interpolation. We represent the data volume in a
                  compressed manner using low-rank matrix factors, and
                  build a block-coordinate algorithm with constrained
                  convex subproblems that are solved with a
                  primal-dual splitting scheme. The develop
                  optimization framework works on the whole seismic
                  temporal frequency slices and does not require
                  windowing or non-trivial sorting of seismic
                  data. The new approach is competitive with state of
                  the art level-set algorithms that interchange the
                  role of objectives with constraints. We use the new
                  algorithm to successfully interpolate a large scale
                  5D seismic data volume (upto 1010 data-points),
                  generated from the geologically complex synthetic 3D
                  Compass velocity model, where 80\% of the data has
                  been removed. We also develop a robust extension of
                  the primal-dual approach to deal with the outliers
                  (or noise) in the data.},
  keywords = {matrix completion, seismic data, seismic trace interpolation, alternating minimization, primal-dual splitting},
  note = {(published online in IEEE Transactions on Computational Imaging)},
  doi = {10.1109/TCI.2017.2693966},
  url = {https://slim.gatech.edu/Publications/Public/Journals/IEEETransComputationalImaging/2017/kumar2016bls/kumar2016bls.pdf}
}


@ARTICLE{louboutin2016ppf,
  author = {Mathias Louboutin and Michael Lange and Felix J. Herrmann and Navjot Kukreja and Gerard Gorman},
  title = {Performance prediction of finite-difference solvers for different computer architectures},
  journal = {Computers \& Geosciences},
  year = {2017},
  month = {08},
  volume = {105},
  pages = {148-157},
  abstract = {The life-cycle of a partial differential equation (PDE)
                  solver is often characterized by three development
                  phases: the development of a stable numerical
                  discretization; development of a correct (verified)
                  implementation; and the optimization of the
                  implementation for different computer
                  architectures. Often it is only after significant
                  time and effort has been invested that the
                  performance bottlenecks of a PDE solver are fully
                  understood, and the precise details varies between
                  different computer architectures. One way to
                  mitigate this issue is to establish a reliable
                  performance model that allows a numerical analyst to
                  make reliable predictions of how well a numerical
                  method would perform on a given computer
                  architecture, before embarking upon potentially long
                  and expensive implementation and optimization
                  phases. The availability of a reliable performance
                  model also saves developer effort as it both informs
                  the developer on what kind of optimisations are
                  beneficial, and when the maximum expected
                  performance has been reached and optimisation work
                  should stop. We show how discretization of a
                  wave-equation can be theoretically studied to
                  understand the performance limitations of the method
                  on modern computer architectures. We focus on the
                  roofline model, now broadly used in the
                  high-performance computing community, which
                  considers the achievable performance in terms of the
                  peak memory bandwidth and peak floating point
                  performance of a computer with respect to
                  algorithmic choices. A first principles analysis of
                  operational intensity for key time-stepping
                  finite-difference algorithms is presented. With this
                  information available at the time of algorithm
                  design, the expected performance on target computer
                  systems can be used as a driver for algorithm
                  design. },
  keywords = {finite differences, performance, modeling, HPC},
  note = {(Computers \& Geosciences)},
  doi = {https://doi.org/10.1016/j.cageo.2017.04.014},
  url = {https://slim.gatech.edu/Publications/Public/Journals/ComputersAndGeosciences/2016/louboutin2016ppf/louboutin2016ppf.pdf}
}


@ARTICLE{oghenekohwo2016GEOPctl,
  author = {Felix Oghenekohwo and Haneet Wason and Ernie Esser and Felix J. Herrmann},
  title = {Low-cost time-lapse seismic with distributed compressive sensing---{Part} 1: exploiting common information among the vintages},
  journal = {Geophysics},
  year = {2017},
  month = {05},
  volume = {82},
  number = {3},
  pages = {P1-P13},
  abstract = {Time-lapse seismic is a powerful technology for
                  monitoring a variety of subsurface changes due to
                  reservoir fluid flow. However, the practice can be
                  technically challenging when one seeks to acquire
                  colocated time-lapse surveys with high degrees of
                  replicability among the shot locations. We have
                  determined that under "ideal" circumstances, in
                  which we ignore errors related to taking
                  measurements off the grid, high-quality prestack
                  data can be obtained from randomized subsampled
                  measurements that are observed from surveys in which
                  we choose not to revisit the same randomly
                  subsampled on-the-grid shot locations. Our
                  acquisition is low cost because our measurements are
                  subsampled. We have found that the recovered finely
                  sampled prestack baseline and monitor data actually
                  improve significantly when the same on-the-grid shot
                  locations are not revisited. We achieve this result
                  by using the fact that different time-lapse data
                  share information and that nonreplicated
                  (on-the-grid) acquisitions can add information when
                  prestack data are recovered jointly. Whenever the
                  time-lapse data exhibit joint structure---i.e., they
                  are compressible in some transform domain and share
                  information---sparsity-promoting recovery of the
                  "common component" and "innovations," with respect
                  to this common component, outperforms independent
                  recovery of the prestack baseline and monitor
                  data. The recovered time-lapse data are of high
                  enough quality to serve as the input to extract
                  poststack attributes used to compute time-lapse
                  differences. Without joint recovery, artifacts---due
                  to the randomized subsampling---lead to
                  deterioration of the degree of repeatability of the
                  time-lapse data. We tested this method by carrying
                  out experiments with reliable statistics from
                  thousands of repeated experiments. We also confirmed
                  that high degrees of repeatability are achievable
                  for an ocean-bottom cable survey acquired with
                  time-jittered continuous recording.},
  keywords = {acquisition, time-lapse seismic, marine, random sampling, joint recovery method},
  note = {(Geophysics)},
  doi = {10.1190/geo2016-0076.1},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2017/oghenekohwo2016GEOPctl/oghenekohwo2016GEOPctl.html}
}


@ARTICLE{peters2016cvp,
  author = {Bas Peters and Felix J. Herrmann},
  title = {Constraints versus penalties for edge-preserving full-waveform inversion},
  journal = {The Leading Edge},
  year = {2017},
  month = {01},
  volume = {36},
  number = {1},
  pages = {94-100},
  abstract = {Full-waveform inversion is challenging in complex
                  geologic areas. Even when provided with an accurate
                  starting model, the inversion algorithms often
                  struggle to update the velocity model. Compared with
                  other areas in applied geophysics, including prior
                  information in full-waveform inversion is still in
                  its relative infancy. In part, this is due to the
                  fact that it is difficult to incorporate prior
                  information that relates to geologic settings where
                  strong discontinuities in the velocity model
                  dominate, because these settings call for nonsmooth
                  regularizations. We tackle this problem by including
                  constraints on the spatial variations and value
                  ranges of the inverted velocities, as opposed to
                  adding penalties to the objective, which is more
                  customary in mainstream geophysical inversion. By
                  demonstrating the lack of predictability of
                  edge-preserving inversion when the regularization is
                  in the form of an added penalty term, we advocate
                  the inclusion of constraints instead. Our examples
                  show that the latter leads to more predictable
                  results and to significant improvements in the
                  delineation of salt bodies when these constraints
                  are relaxed gradually in combination with extending
                  the search space to approximately fit the observed
                  data but not the noise.},
  keywords = {full waveform inversion, salt dome, algorithm, noise, optimization, total variation, penalty methods},
  note = {(The Leading Edge)},
  doi = {10.1190/tle36010094.1},
  url = {https://slim.gatech.edu/Publications/Public/Journals/TheLeadingEdge/2016/peters2016cvp/peters2016cvp.html}
}


@ARTICLE{wason2016GEOPctl,
  author = {Haneet Wason and Felix Oghenekohwo and Felix J. Herrmann},
  title = {Low-cost time-lapse seismic with distributed compressive sensing---{Part} 2: impact on repeatability},
  journal = {Geophysics},
  year = {2017},
  month = {05},
  volume = {82},
  number = {3},
  pages = {P15-P30},
  abstract = {Irregular or off-the-grid spatial sampling of sources
                  and receivers is inevitable in field seismic
                  acquisitions. Consequently, time-lapse surveys
                  become particularly expensive since current
                  practices aim to replicate densely sampled surveys
                  for monitoring changes occurring in the reservoir
                  due to hydrocarbon production. We demonstrate that
                  under certain circumstances, high-quality prestack
                  data can be obtained from cheap randomized
                  subsampled measurements that are observed from
                  nonreplicated surveys. We extend our time-jittered
                  marine acquisition to time-lapse surveys by
                  designing acquisition on irregular spatial grids
                  that render simultaneous, subsampled and irregular
                  measurements. Using the fact that different
                  time-lapse data share information and that
                  nonreplicated surveys add information when prestack
                  data are recovered jointly, we recover periodic
                  densely sampled and colocated prestack data by
                  adapting the recovery method to incorporate a
                  regularization operator that maps traces from an
                  irregular spatial grid to a regular periodic
                  grid. The recovery method is, therefore, a combined
                  operation of regularization, interpolation
                  (estimating missing fine-grid traces from subsampled
                  coarse-grid data), and source separation (unraveling
                  overlapping shot records). By relaxing the
                  insistence on replicability between surveys, we find
                  that recovery of the time-lapse difference shows
                  little variability for realistic field scenarios of
                  slightly nonreplicated surveys that suffer from
                  unavoidable natural deviations in spatial sampling
                  of shots (or receivers) and pragmatic
                  compressed-sensing based nonreplicated surveys when
                  compared to the "ideal" scenario of exact
                  replicability between surveys. Moreover, the
                  recovered densely sampled prestack baseline and
                  monitor data improve significantly when the
                  acquisitions are not replicated, and hence can serve
                  as input to extract poststack attributes used to
                  compute time-lapse differences. Our observations are
                  based on experiments conducted for an ocean-bottom
                  cable survey acquired with time-jittered continuous
                  recording assuming source equalization (or same
                  source signature) for the time-lapse surveys and no
                  changes in wave heights, water column velocities or
                  temperature and salinity profiles, etc.},
  keywords = {marine acquisition, time-lapse seismic, off-the-grid recovery, random sampling, joint recovery method, optimization},
  note = {(Geophysics)},
  doi = {10.1190/geo2016-0252.1},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2017/wason2016GEOPctl/wason2016GEOPctl.html}
}


@ARTICLE{yarman2017GPmbd,
  author = {Can Evren Yarman and Rajiv Kumar and James Rickett},
  title = {A model based data driven dictionary learning for seismic data representation},
  journal = {Geophysical Prospecting},
  year = {2017},
  month = {04},
  abstract = {Planar waves events recorded in a seismic array can be
                  represented as lines in the Fourier domain. However,
                  in the real-world seismic events usually have
                  curvature or amplitude variability that means their
                  Fourier transforms are no longer strictly linear,
                  but rather occupy conic regions of the Fourier
                  domain that are narrow at low frequencies, but
                  broaden at high frequencies where the effect of
                  curvature becomes more pronounced. One can consider
                  these regions as localised "signal cones". In this
                  work, we consider a space-time variable signal cone
                  to model the seismic data. The variability of the
                  signal cone is obtained through scaling, slanting
                  and translation of the kernel for cone-limited
                  (C-limited) functions (functions whose Fourier
                  transform lives within a cone) or C-Gaussian
                  function (a multivariate function whose Fourier
                  transform decays exponentially with respect to
                  slowness and frequency) which constitutes our
                  dictionary. We find a discrete number of scaling,
                  slanting and translation parameters from a continuum
                  by optimally matching the data. This is a nonlinear
                  optimization problem which we address by a fixed
                  point method which utilizes a variable projection
                  method with e1 constraints on the linear parameters
                  and bound constraints on the nonlinear
                  parameters. We observe that slow decay and
                  oscillatory behavior of the kernel for C-limited
                  functions constitute bottlenecks for the
                  optimization problem which we partially overcome by
                  the C-Gaussian function. We demonstrate our method
                  through an interpolation example. We present the
                  interpolation result using the estimated parameters
                  obtained from the proposed method and compare it
                  with those obtained using sparsity promoting
                  curvelet decomposition, matching pursuit Fourier
                  interpolation and sparsity promoting plane wave
                  decomposition methods.},
  keywords = {dictionary learning, kernel method, reproducing kernel, variational method, nonlinear optimization, variable projection},
  note = {(published online in Geophysical Prospecting)},
  doi = {10.1111/1365-2478.12533},
  url = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalProspecting/2017/yarman2017GPmbd/yarman2017GPmbd.pdf}
}


%----- 2016 -----%

@ARTICLE{bougher2015CSEGust,
  author = {Ben B. Bougher and Felix J. Herrmann},
  title = {Using the scattering transform to predict stratigraphic units from well logs},
  journal = {CSEG Recorder},
  year = {2016},
  month = {01},
  volume = {41},
  number = {1},
  pages = {22-25},
  abstract = {Much of geophysical interpretation relies on trained
                  pattern recognition of signals and images, a
                  workflow that can be modeled by supervised machine
                  learning. A challenge of supervised learning is
                  determining a physically meaningful feature set that
                  can successfully classify the data. Defined by a
                  network of cascading wavelets, the scattering
                  transform provides a non-linear multiscale analysis
                  that has deep connections to the fractal statistics
                  of the signal. Interestingly, the scattering
                  transform takes the form of a pre-trained
                  convolutional neural network. This paper uses the
                  scattering transform to extract features from well
                  logs in order to train a classifier that can predict
                  stratigraphic units. The methodology is tested on
                  interpreted well logs from Trenton-Black River
                  project and initial results are presented.},
  keywords = {machine learning, scattering transform, well logs},
  note = {(CSEG Recorder)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/CSEGRecorder/2016/bougher2015CSEGust/bougher2015CSEGust.html},
  url2 = {http://csegrecorder.com/articles/view/using-the-scattering-transform-to-predict-strat-units-from-well-logs}
}


@ARTICLE{esser2016TLEcwi,
  author = {Ernie Esser and Llu\'{i}s Guasch and Felix J. Herrmann and Mike Warner},
  title = {Constrained waveform inversion for automatic salt flooding},
  journal = {The Leading Edge},
  year = {2016},
  month = {03},
  volume = {35},
  number = {3},
  pages = {235-239},
  abstract = {Given appropriate data acquisition, processing to remove
                  nonprimary arrivals, and use of an accurate
                  migration algorithm, it is the quality of the
                  subsurface velocity model that typically controls
                  the quality of imaging that can be obtained from
                  salt-affected seismic data. Full-waveform inversion
                  has the potential to improve the accuracy,
                  resolution, repeatability, and speed with which such
                  velocity models can be generated, but, in the
                  absence of an accurate starting model, that
                  potential is difficult to realize in
                  practice. Presented are successful inversion
                  results, obtained from synthetic subsalt models,
                  using a robust full-waveform inversion code that
                  includes constraints upon the set of allowable earth
                  models. These constraints include limitations on the
                  total variation of the velocity of the model and,
                  most significantly, on the asymmetric variation of
                  velocity with depth such that negative velocity
                  excursions are limited. During the iteration, these
                  constraints are relaxed progressively so that the
                  final model is driven principally by the seismic
                  data, but the constraints act to steer the inversion
                  path away from local minima in its early
                  stages. This methodology is applied to portions of
                  the 2004 BP benchmark and Phase I SEAM salt models,
                  recovering an accurate model of the salt body,
                  including its base and flanks, and an accurate model
                  of the subsalt velocity structure, starting from
                  one-dimensional velocity models that are severely
                  cycle skipped. This approach removes entirely the
                  requirement to pick salt boundaries from migrated
                  seismic data, and acts as a form of automatic salt
                  and sediment flooding during full-waveform
                  inversion.},
  keywords = {high-contrast and high velocity, salt, FWI, velocity-model building, total-variation minimization},
  note = {(The Leading Edge)},
  doi = {10.1190/tle35030235.1},
  url = {http://dx.doi.org/10.1190/tle35030235.1}
}


@ARTICLE{esser2016tvr,
  author = {Ernie Esser and Llu\'{i}s Guasch and Tristan van Leeuwen and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Total-variation regularization strategies in full-waveform inversion},
  journal = {SIAM Journal on Imaging Sciences},
  year = {2018},
  volume = {11},
  number = {1},
  pages = {376-406},
  abstract = {We propose an extended full-waveform inversion
                  formulation that includes general convex constraints
                  on the model. Though the full problem is highly
                  nonconvex, the overarching optimization scheme
                  arrives at geologically plausible results by solving
                  a sequence of relaxed and warm-started constrained
                  convex subproblems. The combination of box,
                  total-variation, and successively relaxed asymmetric
                  total-variation constraints allows us to steer free
                  from parasitic local minima while keeping the
                  estimated physical parameters laterally continuous
                  and in a physically realistic range. For accurate
                  starting models, numerical experiments carried out
                  on the challenging 2004 BP velocity benchmark
                  demonstrate that bound and total-variation
                  constraints improve the inversion result
                  significantly by removing inversion artifacts,
                  related to source encoding, and by clearly improved
                  delineation of top, bottom, and flanks of a
                  high-velocity high-contrast salt inclusion. The
                  experiments also show that for poor starting models
                  these two constraints by themselves are insufficient
                  to detect the bottom of high-velocity inclusions
                  such as salt. Inclusion of the one-sided asymmetric
                  total-variation constraint overcomes this issue by
                  discouraging velocity lows to buildup during the
                  early stages of the inversion. To the author's
                  knowledge the presented algorithm is the first to
                  successfully remove the imprint of local minima
                  caused by poor starting models and band-width
                  limited finite aperture data.},
  keywords = {full-waveform inversion, constrained optimization, total variation, salt, hinge loss},
  note = {(SIAM Journal on Imaging Sciences)},
  note2 = {(Computing Research Repository)},
  doi = {10.1137/17M111328X},
  url = {https://slim.gatech.edu/Publications/Public/Journals/SIAMJournalOnImagingSciences/2018/esser2016tvr/esser2016tvr.pdf},
  url2 = {https://doi.org/10.1137/17M111328X},
  url3 = {https://arxiv.org/abs/1608.06159}
}


@ARTICLE{lin2015scatterEPSI,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Estimation of primaries by sparse inversion with scattering-based multiple predictions for data with large gaps},
  journal = {Geophysics},
  year = {2016},
  month = {05},
  volume = {81},
  number = {3},
  pages = {V183-V197},
  abstract = {We propose to solve the Estimation of Primaries by
                  Sparse Inversion problem without any explicit data
                  reconstruction, from a seismic record with missing
                  near-offsets and large holes in the acquisition grid
                  which does not produce aliasing but otherwise causes
                  large errors in the multiple prediction. Exclusion
                  of the unknown data as an inversion variable from
                  the process is desirable, since it sidesteps
                  possible issues arising from fitting observations
                  that are also unknowns in the same inversion
                  problem. Instead, we simulate their multiple
                  contributions by augmenting the forward prediction
                  model for the total wavefield with a scattering
                  series that mimics the action of the free surface
                  reflector within the confines of the unobserved
                  trace locations. Each term in this scattering series
                  simply involves convolution of the predicted
                  wavefield once more with the current estimated
                  surface-free Green's function at these unobserved
                  locations. We investigate the necessary
                  modifications to the primary estimation algorithm to
                  account for the resulting nonlinearity in the
                  modeling operator, and also demonstrate that just a
                  few scattering terms are enough to satisfactorily
                  mitigate the effects of near-offset data gaps during
                  the inversion process. Numerical experiments on
                  synthetic data show that the final derived method
                  can significantly outperform explicit data
                  reconstruction for large near-offset gaps. This is
                  achieved with a similar computational cost and
                  better memory efficiency compared to explicit data
                  reconstruction. We also show on real data that our
                  scheme outperforms pre-interpolation of the
                  near-offset gap.},
  keywords = {multiples, REPSI, EPSI, scattering, sparsity, optimization},
  note = {(Geophysics)},
  doi = {10.1190/geo2015-0263.1},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2016/lin2015scatterEPSI/lin2015scatterEPSI.html}
}


@ARTICLE{li2015GEOPmgn,
  author = {Xiang Li and Ernie Esser and Felix J. Herrmann},
  title = {Modified {Gauss}-{Newton} full-waveform inversion explained---why sparsity-promoting updates do matter},
  journal = {Geophysics},
  year = {2016},
  month = {05},
  volume = {81},
  number = {3},
  pages = {R125-R138},
  abstract = {Full-waveform inversion can be formulated as a nonlinear
                  least-squares optimization problem. This non-convex
                  problem can be extremely computationally expensive
                  because it requires repeatedly solving large linear
                  systems that correspond to discretized partial
                  differential equations. Randomized subsampling
                  techniques allow us to work with small subsets of
                  (monochromatic) source experiments, reducing the
                  computational cost. However, this subsampling
                  weakens subsurface illumination and introduces
                  subsampling related incoherent artifacts. These
                  subsampling-related artifacts---in conjunction with
                  local minima that are known to plague full-waveform
                  inversion---motivate us to come up with a technique
                  to "regularize" this problem. Following earlier
                  work, we take advantage of the fact that curvelets
                  represent subsurface models and perturbations
                  parsimoniously. At first impulse promoting sparsity
                  on the model directly seems the most natural way to
                  proceed, but we will demonstrate that in certain
                  cases it can be advantageous to promote sparsity on
                  the Gauss-Newton updates instead. While constraining
                  the one-norm of the descent directions does not
                  change not change the underlying full-waveform
                  inversion objective, the constrained model updates
                  remain descent directions, remove
                  subsampling-related artifacts and improve the
                  overall inversion result. We empirically observe
                  this phenomenon in situations where the different
                  model updates occur at roughly the same locations in
                  the curvelet domain. We further investigate and
                  analyze this phenomenon, where nonlinear inversions
                  benefit from sparsity-promoting constraints on the
                  updates, by means of a set of carefully selected
                  examples including phase retrieval and full-waveform
                  inversion. In all cases, we observe a faster decay
                  of the residual and model error as a function of the
                  number of iterations.},
  keywords = {full-waveform inversion, Gauss-Newton method, sparsity promotion},
  note = {(Geophysics)},
  doi = {10.1190/geo2015-0266.1},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2016/li2015GEOPmgn/li2015GEOPmgn.pdf}
}


@ARTICLE{lopez2015IEEEogl,
  author = {Oscar Lopez and Rajiv Kumar and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Off-the-grid low-rank matrix recovery and seismic data reconstruction},
  journal = {IEEE Journal of Selected Topics in Signal Processing},
  year = {2016},
  month = {06},
  volume = {10},
  number = {4},
  pages = {658-671},
  abstract = {Matrix sensing problems capitalize on the knowledge that
                  a data matrix of interest exhibits low rank
                  properties. This low dimensional structure often
                  arises because the data matrix is obtained by
                  sampling a smooth function on a regular (or
                  structured) grid. However, in many practical
                  situations the measurements are taken on an
                  irregular grid (that is accurately known). This
                  results in an "unstructured data matrix" that is
                  less fit for the low rank model in comparison to its
                  regular counterpart and therefore subject to
                  degraded reconstruction via rank penalization
                  techniques. In this paper, we propose and analyze a
                  modified low-rank matrix recovery work-flow that
                  admits unstructured observations. By incorporating a
                  regularization operator which accurately maps
                  structured data to unstructured data, into the
                  nuclear-norm minimization problem, we are able to
                  compensate for data irregularity. Furthermore, by
                  construction our formulation yields output that is
                  supported on a structured grid. We establish
                  recovery error bounds for our methodology and offer
                  matrix sensing and matrix completion numerical
                  experiments including applications to seismic trace
                  interpolation to demonstrate the potential of the
                  approach.},
  keywords = {matrix sensing, matrix completion, nuclear-norm relaxation, nonuniform discrete Fourier transform, data regularization, seismic data, seismic trace interpolation},
  note = {(IEEE Journal of Selected Topics in Signal Processing)},
  doi = {10.1109/JSTSP.2016.2555482},
  url = {https://slim.gatech.edu/Publications/Public/Journals/IEEESignalProcessingMagazine/2016/lopez2015IEEEogl/lopez2015IEEEogl.pdf},
  url2 = {http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7454674}
}


@ARTICLE{tu2015GJIsem,
  author = {Ning Tu and Aleksandr Y. Aravkin and Tristan van Leeuwen and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Source estimation with surface-related multiples---fast ambiguity-resolved seismic imaging},
  journal = {Geophysical Journal International},
  year = {2016},
  month = {03},
  volume = {205},
  pages = {1492-1511},
  abstract = {We address the problem of obtaining a reliable seismic
                  image without prior knowledge of the source wavelet,
                  especially from data that contain strong
                  surface-related multiples. Conventional reverse-time
                  migration requires prior knowledge of the source
                  wavelet, which is either technically or
                  computationally challenging to accurately determine;
                  inaccurate estimates of the source wavelet can
                  result in seriously degraded reverse-time migrated
                  images, and therefore wrong geological
                  interpretations. To solve this problem, we present a
                  "wavelet-free" imaging procedure that simultaneously
                  inverts for the source wavelet and the seismic
                  image, by tightly integrating source estimation into
                  a fast least-squares imaging framework, namely
                  compressive imaging, given a reasonably accurate
                  background velocity model. However, this joint
                  inversion problem is difficult to solve as it is
                  plagued with local minima and the ambiguity with
                  respect to amplitude scalings because of the
                  multiplicative, and therefore nonlinear, appearance
                  of the source wavelet in the otherwise linear
                  formalism. We have found a way to solve this
                  nonlinear joint-inversion problem using a technique
                  called variable projection, and a way to overcome
                  the scaling ambiguity by including surface-related
                  multiples in our imaging procedure following recent
                  developments in surface-related multiple prediction
                  by sparse inversion. As a result, we obtain without
                  prior knowledge of the source wavelet
                  high-resolution seismic images, comparable in
                  quality to images obtained assuming the true source
                  wavelet is known. By leveraging the computationally
                  efficient compressive-imaging methodology, these
                  results are obtained at affordable computational
                  costs compared with conventional processing work
                  flows that include surface-related multiple removal
                  and reverse-time migration.},
  keywords = {inverse theory, time series analysis, computational seismology, wave propagation, free surface, multiples, seismic imaging},
  note = {(published online in Geophysical Journal International)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalJournalInternational/2016/tu2015GJIsem/tu2015GJIsem.pdf},
  url2 = {http://gji.oxfordjournals.org/content/205/3/1492.full.pdf?keytype=ref&ijkey=zFydRGQpiJzeCS9}
}


@ARTICLE{vanleeuwen2015GPWEMVA,
  author = {Tristan van Leeuwen and Rajiv Kumar and Felix J. Herrmann},
  title = {Enabling affordable omnidirectional subsurface extended image volumes via probing},
  journal = {Geophysical Prospecting},
  year = {2017},
  month = {03},
  volume = {65},
  number = {2},
  pages = {385-406},
  abstract = {Image gathers as a function of subsurface offset are an
                  important tool for the inference of rock properties
                  and velocity analysis in areas of complex
                  geology. Traditionally, these gathers are thought of
                  as multidimensional correlations of the source and
                  receiver wavefields. The bottleneck in computing
                  these gathers lies in the fact that one needs to
                  store, compute, and correlate these wavefields for
                  all shots in order to obtain the desired image
                  gathers. Therefore, the image gathers are typically
                  only computed for a limited number of subsurface
                  points and for a limited range of subsurface
                  offsets, which may cause problems in complex
                  geological areas with large geologic dips. We
                  overcome increasing computational and storage costs
                  of extended image volumes by introducing a
                  formulation that avoids explicit storage and removes
                  the customary and expensive loop over shots found in
                  conventional extended imaging. As a result, we end
                  up with a matrix–vector formulation from which
                  different image gathers can be formed and with which
                  amplitude-versus-angle and wave-equation migration
                  velocity analysis can be performed without requiring
                  prior information on the geologic dips. Aside from
                  demonstrating the formation of two-way extended
                  image gathers for different purposes and at greatly
                  reduced costs, we also present a new approach to
                  conduct automatic wave-equation-based
                  migration-velocity analysis. Instead of focusing in
                  particular offset directions and preselected subsets
                  of subsurface points, our method focuses every
                  subsurface point for all subsurface offset
                  directions using a randomized probing technique. As
                  a consequence, we obtain good velocity models at low
                  cost for complex models without the need to provide
                  information on the geologic dips.},
  keywords = {migration velocity analysis, AVA, stochastic optimization},
  note = {(Geophysical Prospecting)},
  doi = {10.1111/1365-2478.12418},
  url = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalProspecting/2016/vanleeuwen2015GPWEMVA/vanleeuwen2015GPWEMVA.pdf}
}


%----- 2015 -----%

@ARTICLE{dasilva2014htuck,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Optimization on the {Hierarchical} {Tucker} manifold - applications to tensor completion},
  journal = {Linear Algebra and its Applications},
  year = {2015},
  month = {09},
  volume = {481},
  pages = {131-173},
  abstract = {In this work, we develop an optimization framework for
                  problems whose solutions are well-approximated by
                  Hierarchical Tucker (HT) tensors, an efficient
                  structured tensor format based on recursive subspace
                  factorizations. By exploiting the smooth manifold
                  structure of these tensors, we construct standard
                  optimization algorithms such as Steepest Descent and
                  Conjugate Gradient for completing tensors from
                  missing entries. Our algorithmic framework is fast
                  and scalable to large problem sizes as we do not
                  require SVDs on the ambient tensor space, as
                  required by other methods. Moreover, we exploit the
                  structure of the Gramian matrices associated with
                  the HT format to regularize our problem, reducing
                  overfitting for high subsampling ratios. We also
                  find that the organization of the tensor can have a
                  major impact on completion from realistic seismic
                  acquisition geometries. These samplings are far from
                  idealized randomized samplings that are usually
                  considered in the literature but are realizable in
                  practical scenarios. Using these algorithms, we
                  successfully interpolate large-scale seismic data
                  sets and demonstrate the competitive computational
                  scaling of our algorithms as the problem sizes
                  grow.},
  keywords = {hierarchical tucker tensors, tensor completion, Riemannian manifold optimization, Gauss–Newton, differential geometry, low-rank tensor},
  note = {(Linear Algebra and its Applications)},
  doi = {10.1016/j.laa.2015.04.015},
  url = {https://slim.gatech.edu/Publications/Public/Journals/LinearAlgebraAndItsApplications/2015/dasilva2014htuck/dasilva2014htuck.pdf},
  url2 = {http://www.sciencedirect.com/science/article/pii/S0024379515002530}
}


@ARTICLE{kumar2014GEOPemc,
  author = {Rajiv Kumar and Curt Da Silva and Okan Akalin and Aleksandr Y. Aravkin and Hassan Mansour and Ben Recht and Felix J. Herrmann},
  title = {Efficient matrix completion for seismic data reconstruction},
  journal = {Geophysics},
  year = {2015},
  month = {09},
  volume = {80},
  number = {5},
  pages = {V97-V114},
  abstract = {Despite recent developments in improved acquisition,
                  seismic data often remain undersampled along source
                  and receiver coordinates, resulting in incomplete
                  data for key applications such as migration and
                  multiple prediction. We have interpreted the
                  missing-trace interpolation problem in the context
                  of matrix completion (MC), and developed three
                  practical principles for using low-rank optimization
                  techniques to recover seismic data. Specifically, we
                  strive for recovery scenarios wherein the original
                  signal is low rank and the subsampling scheme
                  increases the singular values of the matrix. We use
                  an optimization program that restores this low-rank
                  structure to recover the full volume. Omitting one
                  or more of these principles can lead to poor
                  interpolation results, as we found
                  experimentally. In light of this theory, we
                  compensate for the high-rank behavior of data in the
                  source-receiver domain by using the midpoint-offset
                  transformation for 2D data and a source-receiver
                  permutation for 3D data to reduce the overall
                  singular values. Simultaneously, to work with
                  computationally feasible algorithms for large-scale
                  data, we use a factorization-based approach to MC,
                  which significantly speeds up the computations
                  compared with repeated singular value decompositions
                  without reducing the recovery quality. In the
                  context of our theory and experiments, we also find
                  that windowing the data too aggressively can have
                  adverse effects on the recovery quality. To overcome
                  this problem, we carried out our interpolations for
                  each frequency independently while working with the
                  entire frequency slice. The result is a
                  computationally efficient, theoretically motivated
                  framework for interpolating missing-trace data. Our
                  tests on realistic 2D and 3D seismic data sets show
                  that our method compares favorably in terms of
                  computational speed and recovery quality with
                  existing curvelet- and tensor-based techniques.},
  keywords = {2D, 3D, algorithm, interpolation, signal processing, low-rank},
  note = {(Geophysics)},
  doi = {10.1190/geo2014-0369.1},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2015/kumar2014GEOPemc/kumar2014GEOPemc.pdf},
  url2 = {http://library.seg.org/doi/abs/10.1190/geo2014-0369.1}
}


@ARTICLE{kumar2015sss,
  author = {Rajiv Kumar and Haneet Wason and Felix J. Herrmann},
  title = {Source separation for simultaneous towed-streamer marine acquisition --- a compressed sensing approach},
  journal = {Geophysics},
  year = {2015},
  month = {11},
  volume = {80},
  number = {6},
  pages = {WD73-WD88},
  abstract = {Simultaneous marine acquisition is an economic way to
                  sample seismic data and speed up acquisition,
                  wherein single or multiple source vessels fire
                  sources at near-simultaneous or slightly random
                  times, resulting in overlapping shot records. The
                  current paradigm for simultaneous towed-streamer
                  marine acquisition incorporates “low variability” in
                  source firing times, i.e., 0 ≤ 1 or 2 s because the
                  sources and receivers are moving. This results in a
                  low degree of randomness in simultaneous data, which
                  is challenging to separate (into its constituent
                  sources) using compressed-sensing-based separation
                  techniques because randomization is key to
                  successful recovery via compressed sensing. We have
                  addressed the challenge of source separation for
                  simultaneous towed-streamer acquisitions via two
                  compressed-sensing-based approaches, i.e., sparsity
                  promotion and rank minimization. We have evaluated
                  the performance of the sparsity-promotion- and
                  rank-minimization-based techniques by simulating two
                  simultaneous towed-streamer acquisition scenarios,
                  i.e., over/under and simultaneous long offset. A
                  field data example from the Gulf of Suez for the
                  over/under acquisition scenario was also
                  developed. We observed that the proposed approaches
                  gave good and comparable recovery qualities of the
                  separated sources, but the rank-minimization
                  technique outperformed the sparsity-promoting
                  technique in terms of the computational time and
                  memory. We also compared these two techniques with
                  the normal-moveout-based median-filtering-type
                  approach, which had comparable results.},
  keywords = {acquisition, 2D, inversion, marine, source separation, optimization, sparsity, rank},
  note = {(Geophysics)},
  doi = {10.1190/geo2015-0108.1},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2015/kumar2015sss/kumar2015sss_revised.pdf},
  url2 = {http://library.seg.org/doi/abs/10.1190/geo2015-0108.1}
}


@ARTICLE{tu2014fis,
  author = {Ning Tu and Felix J. Herrmann},
  title = {Fast imaging with surface-related multiples by sparse inversion},
  journal = {Geophysical Journal International},
  year = {2015},
  month = {04},
  volume = {201},
  number = {1},
  pages = {304-317},
  abstract = {In marine exploration seismology, surface-related
                  multiples are usually treated as noise mainly
                  because subsequent processing steps, such as
                  migration velocity analysis and imaging, require
                  multiple-free data. Failure to remove these
                  wavefield components from the data may lead to
                  erroneous estimates for migration velocity or result
                  in strong coherent artefacts that interfere with the
                  imaged reflectors. However, multiples can carry
                  complementary information compared to primaries, as
                  they interact with the free surface and are
                  therefore exposed more to the subsurface. Recent
                  work has shown that when processed correctly
                  multiples can improve seismic illumination. Given a
                  sufficiently accurate background velocity model and
                  an estimate for the source signature, we propose a
                  new and computationally efficient linearized
                  inversion procedure based on two-way wave equations,
                  which produces accurate images of the subsurface
                  from the total upgoing wavefield including
                  surface-related multiples. Modelling of the
                  surface-related multiples in the proposed method
                  derives from the well-known surface-related multiple
                  elimination method. We incur a minimal overhead from
                  incorporating the multiples by having the
                  wave-equation solver carry out the multiple
                  predictions via the inclusion of an areal source
                  instead of expensive dense matrix-matrix
                  multiplications. By using subsampling techniques, we
                  obtain high-quality true-amplitude least-squares
                  migrated images at computational costs of roughly a
                  single reverse-time migration (RTM) with all the
                  data. These images are virtually free of coherent
                  artefacts from multiples. Proper inversion of the
                  multiples would be computationally infeasible
                  without using these techniques that significantly
                  bring down the cost. By promoting sparsity in the
                  curvelet domain and using rerandomization, out
                  method gains improved robustness to errors in the
                  background velocity model, and errors incurred in
                  the linearization of the wave equation with respect
                  to the model. We demonstrate the superior
                  performance of the proposed method compared to the
                  conventional RTM using realistic synthetic
                  examples.},
  keywords = {multiples, inversion, Kaczmarz, compressive sensing, curvelet, approximate message passing},
  note = {(Geophysical Journal International)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalJournalInternational/2014/tu2014fis/tu2014fis.pdf},
  url2 = {http://gji.oxfordjournals.org/cgi/content/full/ggv020?ijkey=pKzrSXbtxjlOjzd&keytype=ref}
}


@ARTICLE{tu2015TLEfls,
  author = {Ning Tu and Felix J. Herrmann},
  title = {Fast least-squares imaging with surface-related multiples: application to a {North}-{Sea} data set},
  journal = {The Leading Edge},
  year = {2015},
  month = {07},
  volume = {34},
  number = {7},
  pages = {788–794},
  abstract = {In marine seismic acquisition, surface-related multiples
                  constitute a significant portion of the acquired
                  data. Typically, multiples are removed during
                  early-stage data-processing as they can lead to
                  phantom reflectors during migration that may result
                  in erroneous geological interpretations. However, if
                  properly dealt with, multiples can provide valuable
                  extra information and complement primaries in
                  illuminating the subsurface. In this article, we
                  demonstrate the limitation of the reverse-time
                  migration in imaging these multiples, and present an
                  alternative inversion procedure that is
                  computationally efficient, that jointly maps both
                  primaries and multiples to the true reflectors, and
                  where the source function is estimated on the
                  fly. As a result, we obtain high-quality, mostly
                  artifact-free, broad-band images where the imprint
                  of the source-function are partly removed at a
                  computationally affordable expense compared to the
                  combined costs of the wave-equation based
                  surface-related multiple elimination and the
                  reverse-time migration. We achieve all this by
                  including the total downgoing wavefields as areal
                  sources in the least-squares migration in
                  combination with curvelet-domain sparsity promotion.
                  We apply the proposed method to a shallow-water
                  marine data set from the North sea, which contains
                  abundant short-period surface-related multiples, and
                  show its efficacy in eliminating coherent imaging
                  artifacts associated with these multiples. We also
                  demonstrate the benefits of joint imaging of
                  primaries and multiples compared to imaging these
                  signal components separately.},
  keywords = {migration, multiples, inversion, least squares, field data},
  note = {(The Leading Edge)},
  doi = {10.1190/tle34070788.1},
  url = {https://slim.gatech.edu/Publications/Public/Journals/TheLeadingEdge/2015/tu2015TLEfls/tu2015TLEfls.html}
}


@ARTICLE{vanleeuwen2015IPpmp,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {A penalty method for {PDE}-constrained optimization in inverse problems},
  journal = {Inverse Problems},
  year = {2015},
  month = {12},
  volume = {32},
  number = {1},
  pages = {015007},
  abstract = {Many inverse and parameter estimation problems can be
                  written as PDE-constrained optimization
                  problems. The goal is to infer the parameters,
                  typically coefficients of the PDE, from partial
                  measurements of the solutions of the PDE for several
                  right-hand sides. Such PDE-constrained problems can
                  be solved by finding a stationary point of the
                  Lagrangian, which entails simultaneously updating
                  the parameters and the (adjoint) state
                  variables. For large-scale problems, such an
                  all-at-once approach is not feasible as it requires
                  storing all the state variables. In this case one
                  usually resorts to a reduced approach where the
                  constraints are explicitly eliminated (at each
                  iteration) by solving the PDEs. These two
                  approaches, and variations thereof, are the main
                  workhorses for solving PDE-constrained optimization
                  problems arising from inverse problems. In this
                  paper, we present an alternative method that aims to
                  combine the advantages of both approaches. Our
                  method is based on a quadratic penalty formulation
                  of the constrained optimization problem. By
                  eliminating the state variable, we develop an
                  efficient algorithm that has roughly the same
                  computational complexity as the conventional reduced
                  approach while exploiting a larger search
                  space. Numerical results show that this method
                  indeed reduces some of the nonlinearity of the
                  problem and is less sensitive to the initial
                  iterate.},
  keywords = {penalty method, PDE, optimization, inverse problems},
  note = {(Inverse Problems)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/InverseProblems/2015/vanleeuwen2015IPpmp/vanleeuwen2015IPpmp.pdf},
  url2 = {http://stacks.iop.org/0266-5611/32/i=1/a=015007}
}


%----- 2014 -----%

@ARTICLE{vanLeeuwen2014SISC3Dfds,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {{3D} frequency-domain seismic inversion with controlled sloppiness},
  journal = {SIAM Journal on Scientific Computing},
  year = {2014},
  month = {10},
  volume = {36},
  number = {5},
  pages = {S192-S217},
  abstract = {Seismic waveform inversion aims at obtaining detailed
                  estimates of subsurface medium parameters, such as
                  the spatial distribution of soundspeed, from
                  multiexperiment seismic data. A formulation of this
                  inverse problem in the frequency domain leads to an
                  optimization problem constrained by a Helmholtz
                  equation with many right-hand sides. Application of
                  this technique to industry-scale problems faces
                  several challenges: First, we need to solve the
                  Helmholtz equation for high wave numbers over large
                  computational domains. Second, the data consist of
                  many independent experiments, leading to a large
                  number of PDE solves. This results in high
                  computational complexity both in terms of memory and
                  CPU time as well as input/output costs. Finally, the
                  inverse problem is highly nonlinear and a lot of art
                  goes into preprocessing and regularization. Ideally,
                  an inversion needs to be run several times with
                  different initial guesses and/or tuning
                  parameters. In this paper, we discuss the
                  requirements of the various components (PDE solver,
                  optimization method, \dots) when applied to
                  large-scale three-dimensional seismic waveform
                  inversion and combine several existing approaches
                  into a flexible inversion scheme for seismic
                  waveform inversion. The scheme is based on the idea
                  that in the early stages of the inversion we do not
                  need all the data or very accurate PDE solves. We
                  base our method on an existing preconditioned Krylov
                  solver (CARP-CG) and use ideas from stochastic
                  optimization to formulate a gradient-based
                  (quasi-Newton) optimization algorithm that works
                  with small subsets of the right-hand sides and uses
                  inexact PDE solves for the gradient calculations. We
                  propose novel heuristics to adaptively control both
                  the accuracy and the number of right-hand sides. We
                  illustrate the algorithms on synthetic benchmark
                  models for which significant computational gains can
                  be made without being sensitive to noise and without
                  losing the accuracy of the inverted model.},
  keywords = {seismic inversion, Helmholtz equation, preconditioning, Kaczmarz method, inexact gradient, block-cg},
  note = {(SISC)},
  doi = {10.1137/130918629},
  url = {http://epubs.siam.org/doi/abs/10.1137/130918629},
  url2 = {https://slim.gatech.edu/Publications/Public/Journals/SIAMJournalOnScientificComputing/2014/vanLeeuwen2014SISC3Dfds/vanLeeuwen2014SISC3Dfds.pdf}
}


@ARTICLE{aravkin2014SISCfmd,
  author = {Aleksandr Y. Aravkin and Rajiv Kumar and Hassan Mansour and Ben Recht and Felix J. Herrmann},
  title = {Fast methods for denoising matrix completion formulations, with applications to robust seismic data interpolation},
  journal = {SIAM Journal on Scientific Computing},
  year = {2014},
  month = {10},
  volume = {36},
  number = {5},
  pages = {S237-S266},
  abstract = {Recent SVD-free matrix factorization formulations have
                  enabled rank minimization for systems with millions
                  of rows and columns, paving the way for matrix
                  completion in extremely large-scale applications,
                  such as seismic data interpolation. In this paper,
                  we consider matrix completion formulations designed
                  to hit a target data-fitting error level provided by
                  the user, and we propose an algorithm called LR-BPDN
                  that is able to exploit factorized formulations to
                  solve the corresponding optimization problem. Since
                  practitioners typically have strong prior knowledge
                  about target error level, this innovation makes it
                  easy to apply the algorithm in practice, leaving
                  only the factor rank to be determined. Within the
                  established framework, we propose two extensions
                  that are highly relevant to solving practical
                  challenges of data interpolation. First, we propose
                  a weighted extension that allows known subspace
                  information to improve the results of matrix
                  completion formulations. We show how this weighting
                  can be used in the context of frequency
                  continuation, an essential aspect to seismic data
                  interpolation. Second, we propose matrix completion
                  formulations that are robust to large measurement
                  errors in the available data. We illustrate the
                  advantages of LR-BPDN on collaborative filtering
                  problems using the MovieLens 1M and 10M and Netflix
                  100M datasets. Then we use the new method, along
                  with its robust and subspace reweighted extensions,
                  to obtain high-quality reconstructions for
                  large-scale seismic interpolation problems with real
                  data, even in the presence of data contamination.},
  keywords = {matrix completion, factorized formulations, seismic trace interpolation, variational optimization, robust statistics},
  note = {(SISC)},
  doi = {10.1137/130919210},
  url = {https://slim.gatech.edu/Publications/Public/Journals/SIAMJournalOnScientificComputing/2014/aravkin2014SISCfmd/aravkin2014SISCfmd.pdf},
  url2 = {http://epubs.siam.org/doi/abs/10.1137/130919210}
}


@ARTICLE{vanLeeuwen2014GEOPcav,
  author = {Tristan van Leeuwen and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Comment on: “Application of the variable projection scheme for frequency-domain full-waveform inversion” (M. Li, J. Rickett, and A. Abubakar, Geophysics, 78, no. 6, R249–R257)},
  journal = {Geophysics},
  year = {2014},
  month = {05},
  volume = {79},
  number = {3},
  pages = {X11-X17},
  keywords = {waveform inversion, variable projection},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2014/vanLeeuwen2014GEOPcav/vanLeeuwen2014GEOPcav.pdf},
  doi = {10.1190/geo2013-0466.1},
  note = {(discussion by Tristan van Leeuwen, Aleksandr Y. Aravkin, and Felix J. Herrmann)}
}


@ARTICLE{jumah2014GPdre,
  author = {Bander Jumah and Felix J. Herrmann},
  title = {Dimensionality-reduced estimation of primaries by sparse inversion},
  journal = {Geophysical Prospecting},
  year = {2014},
  month = {09},
  volume = {62},
  number = {5},
  pages = {972-993},
  abstract = {Wave-equation based methods, such as the estimation of
                  primaries by sparse inversion, have been successful
                  in the mitigation of the adverse effects of
                  surface-related multiples on seismic imaging and
                  migration-velocity analysis. However, the reliance
                  of these methods on multidimensional convolutions
                  with fully sampled data exposes the ‘curse of
                  dimensionality’, which leads to disproportional
                  growth in computational and storage demands when
                  moving to realistic 3D field data. To remove this
                  fundamental impediment, we propose a
                  dimensionality-reduction technique where the ‘data
                  matrix’ is approximated adaptively by a randomized
                  low-rank factorization. Compared to conventional
                  methods, which need for each iteration passage
                  through all data possibly requiring on-the-fly
                  interpolation, our randomized approach has the
                  advantage that the total number of passes is reduced
                  to only one to three. In addition, the low-rank
                  matrix factorization leads to considerable
                  reductions in storage and computational costs of the
                  matrix multiplies required by the sparse
                  inversion. Application of the proposed method to
                  two-dimensional synthetic and real data shows that
                  significant performance improvements in speed and
                  memory use are achievable at a low computational
                  up-front cost required by the low-rank
                  factorization.},
  keywords = {sparse inversion, factorization, primaries},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/1365-2478.12113/abstract},
  url2 = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalProspecting/2014/jumah2014GPdre/jumah2014GPdre.pdf},
  doi = {10.1111/1365-2478.12113},
  note = {(Geophysical Prospecting)}
}


@ARTICLE{ghadermarzy2013ncs,
  author = {Navid Ghadermarzy and Hassan Mansour and Ozgur Yilmaz},
  title = {Non-convex compressed sensing using partial support information},
  journal = {Journal of Sampling Theory in Signal and Image Processing},
  year = {2014},
  volume = {13},
  number = {3},
  pages = {249-270},
  abstract = {In this paper we address the recovery conditions of
                  weighted $\ell_p$ minimization for signal reconstruction
                  from compressed sensing measurements when partial
                  support in- formation is available. We show that
                  weighted $\ell_p$ minimization with 0 < p < 1 is stable
                  and robust under weaker sufficient conditions
                  compared to weighted $\ell_1$ minimization. Moreover, the
                  sufficient recovery conditions of weighted $\ell_p$ are
                  weaker than those of regular $\ell_p$ minimization if at
                  least 50\% of the support estimate is accurate. We
                  also review some algorithms which exist to solve the
                  non-convex $\ell_p$ problem and illustrate our results
                  with numerical experiments.},
  keywords = {Compressed sensing, weighted $\ell_p$, nonconvex optimization, sparse reconstruction},
  url = {http://arxiv.org/abs/1311.3773}
}


@ARTICLE{herrmann2013TLEffwi,
  author = {Felix J. Herrmann and Andrew J. Calvert and Ian Hanlon and Mostafa Javanmehri and Rajiv Kumar and Tristan van Leeuwen and Xiang Li and Brendan R. Smithyman and Eric Takam Takougang and Haneet Wason},
  title = {Frugal full-waveform inversion: from theory to a practical algorithm},
  journal = {The Leading Edge},
  year = {2013},
  month = {09},
  volume = {32},
  number = {9},
  pages = {1082-1092},
  abstract = {As conventional oil and gas fields are maturing, our
                  profession is challenged to come up with the
                  next-generation of more and more sophisticated
                  exploration tools. In exploration seismology this
                  trend has led to the emergence of wave-equation
                  based inversion technologies such as reverse-time
                  migration and full-waveform inversion. While
                  significant progress has been made in wave-equation
                  based inversion, major challenges remain in the
                  development of robust and computationally feasible
                  workflows that give reliable results in
                  geophysically challenging areas that may include
                  ultra-low shear velocity zones or high-velocity
                  salt. Moreover, sub-salt production carries risks
                  that needs mitigation, which raises the bar from
                  creating sub-salt images to inverting for sub-salt
                  overpressure.},
  keywords = {waveform inversion, optimization},
  url = {https://slim.gatech.edu/Publications/Public/Journals/TheLeadingEdge/2013/herrmann2013ffwi/herrmann2013ffwi.html},
  doi = {10.1190/tle32091082.1}
}


@ARTICLE{vanLeeuwen2013GJImlm,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {Mitigating local minima in full-waveform inversion by expanding the search space},
  journal = {Geophysical Journal International},
  year = {2013},
  month = {10},
  volume = {195},
  pages = {661-667},
  abstract = {Wave equation based inversions, such as full-waveform
                  inversion and reverse-time migration, are
                  challenging because of their computational costs,
                  memory requirements and reliance on accurate initial
                  models. To confront these issues, we propose a novel
                  formulation of wave equation based inversion based
                  on a penalty method. In this formulation, the
                  objective function consists of a data-misfit term
                  and a penalty term, which measures how accurately
                  the wavefields satisfy the wave equation. This new
                  approach is a major departure from current
                  formulations where forward and adjoint wavefields,
                  which both satisfy the wave equation, are correlated
                  to compute updates for the unknown model
                  parameters. Instead, we carry out the inversions
                  over two alternating steps during which we first
                  estimate the wavefield everywhere, given the current
                  model parameters, source and observed data, followed
                  by a second step during which we update the model
                  parameters, given the estimate for the wavefield
                  everywhere and the source. Because the inversion
                  involves both the synthetic wavefields and the
                  medium parameters, its search space is enlarged so
                  that it suffers less from local minima. Compared to
                  other formulations that extend the search space of
                  wave equation based inversion, our method differs in
                  several aspects, namely (i) it avoids storage and
                  updates of the synthetic wavefields because we
                  calculate these explicitly by finding solutions that
                  obey the wave equation and fit the observed data and
                  (ii) no adjoint wavefields are required to update
                  the model, instead our updates are calculated from
                  these solutions directly, which leads to significant
                  computational savings. We demonstrate the validity
                  of our approach by carefully selected examples and
                  discuss possible extensions and future research.},
  url = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalJournalInternational/2013/vanLeeuwen2013GJImlm/vanLeeuwen2013mlm.pdf},
  doi = {10.1093/gji/ggt258},
  eprint = {http://gji.oxfordjournals.org/content/early/2013/07/30/gji.ggt258.full.pdf+html}
}


@ARTICLE{mansour2013GEOPiwr,
  author = {Hassan Mansour and Felix J. Herrmann and Ozgur Yilmaz},
  title = {Improved wavefield reconstruction from randomized sampling via weighted one-norm minimization},
  journal = {Geophysics},
  year = {2013},
  month = {08},
  volume = {78},
  number = {5},
  pages = {V193-V206},
  abstract = {Missing-trace interpolation aims to recover the gaps
                  caused by physical obstacles or deliberate
                  subsampling to control acquisition costs in
                  otherwise regularly sampled seismic
                  wavefields. Although transform-domain sparsity
                  promotion has proven to be an effective tool to
                  solve this recovery problem, current recovery
                  techniques do not fully utilize a priori information
                  derived from the locations of the transform-domain
                  coefficients, especially when curvelet domain
                  sparsity is exploited. We use recovery by weighted
                  one-norm minimization, which exploits correlations
                  between the locations of significant curvelet
                  coefficients of different partitions, e.g., shot
                  records, common-offset gathers, or frequency slices
                  of the acquired data. We use these correlations to
                  define a sequence of 2D curvelet-based recovery
                  problems that exploit 3D continuity exhibited by
                  seismic wavefields without relying on the highly
                  redundant 3D curvelet transform. To test the
                  performance of our weighted algorithm, we compared
                  recoveries from different data sorting and
                  partitioning scenarios for a seismic line from the
                  Gulf of Suez. These tests demonstrated that our
                  method is superior to standard $\ell_1$ minimization
                  in terms of antialiasing capability, reconstruction
                  quality and computational memory requirements.},
  keywords = {trace interpolation, weighted one-norm minimization, compressed sensing, randomized sampling},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2013/mansour2013GEOPiwr/mansour2013GEOPiwr.pdf},
  doi = {10.1190/geo2012-0383.1}
}


@ARTICLE{lin2013GEOPrepsi,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Robust estimation of primaries by sparse inversion via one-norm minimization},
  journal = {Geophysics},
  year = {2013},
  month = {05},
  volume = {78},
  number = {3},
  pages = {R133-R150},
  abstract = {A recently proposed method called estimation of
                  primaries by sparse inversion (EPSI) avoids the need
                  for adaptive subtraction of approximate multiple
                  predictions by directly inverting for the
                  multiple-free subsurface impulse response as a
                  collection of band-limited spikes. Although it can
                  be shown that the correct primary impulse response
                  is obtained through the sparsest possible solution,
                  the original EPSI algorithm was not designed to take
                  advantage of this result, and instead it relies on a
                  multitude of inversion parameters, such as the level
                  of sparsity per gradient update. We proposed and
                  tested a new algorithm, named robust EPSI, in which
                  we make obtaining the sparsest solution an explicit
                  goal. Our approach remains a gradient-based approach
                  like the original algorithm, but it is derived from
                  a new biconvex optimization framework based on an
                  extended basis-pursuit denoising
                  formulation. Furthermore, because it is based on a
                  general framework, robust EPSI can recover the
                  impulse response in transform domains, such as
                  sparsifying curvelet-based representations, without
                  changing the underlying algorithm. We discovered
                  that the sparsity-minimizing objective of our
                  formulation enabled it to operate successfully on a
                  variety of synthetic and field marine data sets
                  without excessive tweaking of inversion
                  parameters. We also found that recovering the
                  solution in alternate sparsity domains can
                  significantly improve the quality of the directly
                  estimated primaries, especially for weaker
                  late-arrival events. In addition, we found that
                  robust EPSI produces a more artifact-free impulse
                  response compared to the original algorithm.},
  keywords = {multiples, optimization, sparsity, waveform inversion, pareto, biconvex, algorithm, EPSI},
  doi = {10.1190/geo2012-0097.1},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2013/lin2013GEOPrepsi/lin2013GEOPrepsi.pdf}
}


@ARTICLE{shahidi2013GPars,
  author = {Reza Shahidi and Gang Tang and Jianwei Ma and Felix J. Herrmann},
  title = {Application of randomized sampling schemes to curvelet-based sparsity-promoting seismic data recovery},
  journal = {Geophysical Prospecting},
  volume = {61},
  number = {5},
  pages = {973-997},
  year = {2013},
  month = {09},
  abstract = {Reconstruction of seismic data is routinely used to
                  improve the quality and resolution of seismic data
                  from incomplete acquired seismic
                  recordings. Curvelet-based Recovery by
                  Sparsity-promoting Inversion, adapted from the
                  recently-developed theory of compressive sensing, is
                  one such kind of reconstruction, especially good for
                  recovery of undersampled seismic data. Like
                  traditional Fourier-based methods, it performs best
                  when used in conjunction with randomized
                  subsampling, which converts aliases from the usual
                  regular periodic subsampling into easy-to-eliminate
                  noise. By virtue of its ability to control gap size,
                  along with the random and irregular nature of its
                  sampling pattern, jittered (sub)sampling is one
                  proven method that has been used successfully for
                  the determination of geophone positions along a
                  seismic line. In this paper, we extend jittered
                  sampling to two-dimensional acquisition design, a
                  more difficult problem, with both underlying
                  Cartesian, and hexagonal grids. We also study what
                  we term separable and non-separable two-dimensional
                  jittered samplings. We find hexagonal jittered
                  sampling performs better than Cartesian jittered
                  sampling, while fully non-separable jittered
                  sampling performs better than separable jittered
                  sampling. Two other 2D randomized sampling methods,
                  Poisson Disk sampling and Farthest Point sampling,
                  both known to possess blue-noise spectra, are also
                  shown to perform well.},
  keywords = {Geophysical Prospecting, randomized sampling, curvelets},
  url = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalProspecting/2013/shahidi2013GPars/shahidi2013GPars.pdf},
  doi = {10.1111/1365-2478.12050}
}


@ARTICLE{moghaddam2013GEOPnoa,
  author = {Peyman P. Moghaddam and Henk Keers and Felix J. Herrmann and Wim A. Mulder},
  title = {A new optimization approach for source-encoding full-waveform inversion},
  journal = {Geophysics},
  year = {2013},
  month = {05},
  volume = {78},
  number = {3},
  pages = {R125-R132},
  abstract = {Waveform inversion is the method of choice for
                  determining highly heterogeneous subsurface
                  structure. However, conventional waveform inversion
                  requires that the wavefield for each source is
                  computed separately. This makes it very expensive
                  for realistic 3D seismic surveys. Source-encoding
                  waveform inversion, in which the sources are
                  modelled simultaneously, is considerably faster than
                  conventional waveform inversion but suffers from
                  artifacts. These artifacts can partly be removed by
                  assigning random weights to the source
                  wavefields. We found that the misfit function, and
                  therefore also its gradient, for source-encoding
                  waveform inversion is an unbiased random estimation
                  of the misfit function used in conventional waveform
                  inversion. We found a new method of source-encoding
                  waveform inversion which takes into account the
                  random nature of the gradients used in the
                  optimization. In this new method, the gradient at
                  each iteration is a weighted average of past
                  gradients such that the most recent gradients have
                  the largest weights with exponential decay. This way
                  we damped the random fluctuations of the gradient by
                  incorporating information from the previous
                  iterations. We compare this new method with existing
                  source-encoding waveform inversion methods as well
                  as conventional waveform inversion and found that
                  the model misfit reduction is faster and smoother
                  than those of existing source-encoding waveform
                  inversion methods, and it approaches the model
                  misfit reduction obtained in conventional waveform
                  inversion.},
  keywords = {Geophysics, FWI, optimization, source encoding},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2013/moghaddam2013GEOPnoa/moghaddam2013GEOPnoa.pdf},
  doi = {10.1190/GEO2012-0090.1}
}


@ARTICLE{vanderneut2013GJIirs,
  author = {Joost {van der Neut} and Felix J. Herrmann},
  title = {Interferometric redatuming by sparse inversion},
  journal = {Geophysical Journal International},
  year = {2013},
  month = {02},
  volume = {192},
  pages = {666-670},
  abstract = {Assuming that exact transmission responses are known
                  between the surface and a particular depth level in
                  the subsurface, seismic sources can be effectively
                  mapped to that level by a process called
                  interferometric redatuming. After redatuming, the
                  obtained wavefields can be used for imaging below
                  this particular depth level. Interferometric
                  redatuming consists of two steps, namely (i) the
                  decomposition of the observed wavefields into up-
                  and down-going constituents and (ii) a
                  multidimensional deconvolution of the up- and
                  downgoing wavefields. While this method works in
                  theory, sensitivity to noise and artifacts due to
                  incomplete acquisition call for a different
                  formulation. In this letter, we demonstrate the
                  benefits of formulating the two steps that undergird
                  interferometric redatuming in terms of a
                  transform-domain sparsity-promoting program. By
                  exploiting compressibility of seismic wavefields in
                  the curvelet domain, we not only become robust with
                  respect to noise but we are also able to remove
                  certain artifacts while preserving the frequency
                  content. These improvements lead to a better image
                  of the target from the redatumed data.},
  keywords = {Controlled source seismology, interferometry, inverse theory},
  url = {http://gji.oxfordjournals.org/content/192/2/666}
}


@ARTICLE{berg2008SJSCpareto,
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  title = {Probing the {Pareto} frontier for basis pursuit solutions},
  journal = {SIAM Journal on Scientific Computing},
  year = {2008},
  volume = {31},
  pages = {890-912},
  number = {2},
  month = {01},
  abstract = {The basis pursuit problem seeks a minimum one-norm
                  solution of an underdetermined least-squares
                  problem. Basis pursuit denoise (BPDN) fits the
                  least-squares problem only approximately, and a
                  single parameter determines a curve that traces the
                  optimal trade-off between the least-squares fit and
                  the one-norm of the solution. We prove that this
                  curve is convex and continuously differentiable over
                  all points of interest, and show that it gives an
                  explicit relationship to two other optimization
                  problems closely related to BPDN. We describe a
                  root-finding algorithm for finding arbitrary points
                  on this curve; the algorithm is suitable for
                  problems that are large scale and for those that are
                  in the complex domain. At each iteration, a spectral
                  gradient-projection method approximately minimizes a
                  least-squares problem with an explicit one-norm
                  constraint. Only matrix-vector operations are
                  required. The primal-dual solution of this problem
                  gives function and derivative information needed for
                  the root-finding method. Numerical experiments on a
                  comprehensive set of test problems demonstrate that
                  the method scales well to large problems.},
  keywords = {basis pursuit, convex program, duality, Newton{\textquoteright}s method, one-norm regularization, projected gradient, root-finding, sparse solutions, optimization},
  doi = {10.1137/080714488},
  publisher = {SIAM},
  url = {https://slim.gatech.edu/Publications/Public/Journals/SIAMJournalOnScientificComputing/2008/vanderberg08SIAMptp/vanderberg08SIAMptp.pdf}
}


@ARTICLE{vandenberg2010IEEEter,
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  title = {Theoretical and empirical results for recovery from multiple measurements},
  journal = {IEEE Transactions on Information Theory},
  year = {2010},
  month = {05},
  volume = {56},
  number = {5},
  pages = {2516-2527},
  abstract = {The joint-sparse recovery problem aims to recover, from
                  sets of compressed measurements, unknown sparse
                  matrices with nonzero entries restricted to a subset
                  of rows. This is an extension of the
                  single-measurement-vector (SMV) problem widely
                  studied in compressed sensing. We analyze the
                  recovery properties for two types of recovery
                  algorithms. First, we show that recovery using
                  sum-of-norm minimization cannot exceed the uniform
                  recovery rate of sequential SMV using L1
                  minimization, and that there are problems that can
                  be solved with one approach but not with the
                  other. Second, we analyze the performance of the
                  ReMBo algorithm [M. Mishali and Y. Eldar, IEEE
                  Trans. Sig. Proc., 56 (2008)] in combination with L1
                  minimization, and show how recovery improves as more
                  measurements are taken. From this analysis it
                  follows that having more measurements than number of
                  nonzero rows does not improve the potential
                  theoretical recovery rate.},
  keywords = {convex optimization, joint sparsity, multiple channels, sparse recovery},
  doi = {10.1109/TIT.2010.2043876},
  url = {http://www.math.ucdavis.edu/%7Empf/2010-joint-sparsity.html}
}


@ARTICLE{vandenberg2009ACMstf,
  author = {Ewout {van den Berg} and Michael P. Friedlander and Gilles Hennenfent and Felix J. Herrmann and Rayan Saab and Ozgur Yilmaz},
  title = {Sparco: a testing framework for sparse reconstruction},
  journal = {ACM Transactions on Mathematical Software},
  year = {2009},
  month = {02},
  volume = {35},
  number = {4},
  pages = {1-16},
  abstract = {Sparco is a framework for testing and benchmarking
                  algorithms for sparse reconstruction. It includes a
                  large collection of sparse reconstruction problems
                  drawn from the imaging, compressed sensing, and
                  geophysics literature. Sparco is also a framework
                  for implementing new test problems and can be used
                  as a tool for reproducible research. Sparco is
                  implemented entirely in Matlab, and is released as
                  open-source software under the GNU Public License.},
  keywords = {compressed sensing, sparse recovery, linear operators},
  url = {http://doi.acm.org/10.1145/1462173.1462178}
}


@ARTICLE{aravkin2012IPNuisance,
  author = {Aleksandr Y. Aravkin and Tristan {van Leeuwen}},
  title = {Estimating nuisance parameters in inverse problems},
  journal = {Inverse Problems},
  year = {2012},
  volume = {28},
  number = {11},
  month = {10},
  abstract = {Many inverse problems include nuisance parameters which,
                  while not of direct interest, are required to
                  recover primary parameters. Structure present in
                  these problems allows efficient optimization
                  strategies - a well known example is variable
                  projection, where nonlinear least squares problems
                  which are linear in some parameters can be very
                  efficiently optimized. In this paper, we extend the
                  idea of projecting out a subset over the variables
                  to a broad class of maximum likelihood (ML) and
                  maximum a posteriori likelihood (MAP) problems with
                  nuisance parameters, such as variance or degrees of
                  freedom. As a result, we are able to incorporate
                  nuisance parameter estimation into large-scale
                  constrained and unconstrained inverse problem
                  formulations. We apply the approach to a variety of
                  problems, including estimation of unknown variance
                  parameters in the Gaussian model, degree of freedom
                  (d.o.f.)  parameter estimation in the context of
                  robust inverse problems, automatic calibration, and
                  optimal experimental design. Using numerical
                  examples, we demonstrate improvement in recovery of
                  primary parameters for several large- scale inverse
                  problems. The proposed approach is compatible with a
                  wide variety of algorithms and formulations, and its
                  implementation requires only minor modifications to
                  existing algorithms.},
  keywords = {full waveform inversion, students t, variance},
  doi = {10.1088/0266-5611/28/11/115016},
  url = {http://arxiv.org/abs/1206.6532},
  url2 = {https://slim.gatech.edu/Publications/Public/Journals/InverseProblems/2012/aravkin2012IPNuisance/aravkin2012IPNuisance.pdf},
  url3 = {http://iopscience.iop.org/0266-5611/28/11/115016/}
}


@ARTICLE{Aravkin11TRridr,
  author = {Aleksandr Y. Aravkin and Michael P. Friedlander and Felix J. Herrmann and Tristan van Leeuwen},
  title = {Robust inversion, dimensionality reduction, and randomized sampling},
  journal = {Mathematical Programming},
  year = {2012},
  volume = {134},
  pages = {101-125},
  number = {1},
  month = {08},
  abstract = {We consider a class of inverse problems in which the
                  forward model is the solution operator to linear
                  ODEs or PDEs. This class admits several
                  dimensionality-reduction techniques based on data
                  averaging or sampling, which are especially useful
                  for large-scale problems.  We survey these
                  approaches and their connection to stochastic
                  optimization.  The data-averaging approach is only
                  viable, however, for a least-squares misfit, which
                  is sensitive to outliers in the data and artifacts
                  unexplained by the forward model. This motivates us
                  to propose a robust formulation based on the
                  Student's t-distribution of the error. We
                  demonstrate how the corresponding penalty function,
                  together with the sampling approach, can obtain good
                  results for a large-scale seismic inverse problem
                  with 50 \% corrupted data.},
  keywords = {inverse problems, seismic inversion, stochastic optimization, robust
	estimation, optimization, FWI},
  doi = {10.1007/s10107-012-0571-6},
  url = {http://www.springerlink.com/content/35rwr101h5736340/},
  url2 = {https://slim.gatech.edu/Publications/Public/Journals/MathematicalProgramming/aravkin2012MPrid/aravkin2012MPrid.pdf}
}


@ARTICLE{bernabe2004JGRpas,
  author = {Y. Bernab{\'e} and U. Mok and B. Evans and Felix J. Herrmann},
  title = {Permeability and storativity of binary mixtures of high-and low-porosity materials},
  journal = {Journal of Geophysical Research: Solid Earth},
  year = {2004},
  volume = {109},
  pages = {B12207},
  month = {12},
  abstract = {As a first step toward determining the mixing laws for
                  the transport properties of rocks, we prepared
                  binary mixtures of high- and low-permeability
                  materials by isostatically hot-pressing mixtures of
                  fine powders of calcite and quartz. The resulting
                  rocks were marbles containing varying concentrations
                  of dispersed quartz grains. Pores were present
                  throughout the rock, but the largest ones were
                  preferentially associated with the quartz particles,
                  leading us to characterize the material as being
                  composed of two phases, one with high permeability
                  and the second with low permeability. We measured
                  the permeability and storativity of these materials
                  using the oscillating flow technique, while
                  systematically varying the effective pressure and
                  the period and amplitude of the input fluid
                  oscillation. Control measurements performed using
                  the steady state flow and pulse decay techniques
                  agreed well with the oscillating flow tests. The
                  hydraulic properties of the marbles were highly
                  sensitive to the volume fraction of the
                  high-permeability phase (directly related to the
                  quartz content). Below a critical quartz content,
                  slightly less than 20 wt \%, the high-permeability
                  volume elements were disconnected, and the overall
                  permeability was low. Above the critical quartz
                  content the high-permeability volume elements formed
                  throughgoing paths, and permeability increased
                  sharply.  We numerically simulated fluid flow
                  through binary materials and found that permeability
                  approximately obeys a percolation-based mixing law,
                  consistent with the measured permeability of the
                  calcite-quartz aggregates.},
  keywords = {permeability, porosity, SLIM, modeling},
  doi = {10.1029/2004JB003111},
  url = {https://slim.gatech.edu/Publications/Public/Journals/JournalOfGeophysicalResearch/2004/bernabe04JGRpas/bernabe04JGRpas.pdf},
  url2 = {http://onlinelibrary.wiley.com/doi/10.1029/2004JB003111/abstract}
}


@ARTICLE{mansour2012IEEETITrcs,
  author = {Michael P. Friedlander and Hassan Mansour and Rayan Saab and Ozgur Yilmaz},
  title = {Recovering compressively sampled signals using partial support information},
  journal = {IEEE Transactions on Information Theory},
  year = {2012},
  volume = {58},
  pages = {1122-1134},
  number = {2},
  month = {02},
  abstract = {We study recovery conditions of weighted $\ell_1$
                  minimization for signal reconstruction from
                  compressed sensing measurements when partial support
                  information is available. We show that if at least
                  50\% of the (partial) support information is
                  accurate, then weighted $\ell_1$ minimization is
                  stable and robust under weaker sufficient conditions
                  than the analogous conditions for standard $\ell_1$
                  minimization.  Moreover, weighted $\ell_1$
                  minimization provides better upper bounds on the
                  reconstruction error in terms of the measurement
                  noise and the compressibility of the signal to be
                  recovered. We illustrate our results with extensive
                  numerical experiments on synthetic data and real
                  audio and video signals.},
  address = {University of British Columbia, Vancouver},
  institution = {Department of Computer Science},
  keywords = {compressive sensing},
  doi = {10.1109/TIT.2011.2167214},
  url = {https://slim.gatech.edu/Publications/Public/Journals/IEEETransInformationTheory/2012/mansour2012IEEETITrcs/mansour2012IEEETITrcs.pdf}
}


@ARTICLE{vandenberg2011SIAMsol,
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  title = {Sparse optimization with least-squares constraints},
  journal = {SIAM Journal on Optimization},
  year = {2011},
  month = {11},
  volume = {21},
  number = {4},
  pages = {1201–1229},
  abstract = {The use of convex optimization for the recovery of
                  sparse signals from incomplete or compressed data is
                  now common practice. Motivated by the success of
                  basis pursuit in recovering sparse vectors, new
                  formulations have been proposed that take advantage
                  of different types of sparsity. In this paper we
                  propose an efficient algorithm for solving a general
                  class of sparsifying formulations. For several
                  common types of sparsity we provide applications,
                  along with details on how to apply the algorithm,
                  and experimental results.},
  keywords = {basis pursuit, compressed sensing, convex program,
                  duality, group sparsity, matrix completion, Newton’s
                  method, root-finding, sparse solutions},
  doi = {10.1137/100785028},
  url = {http://www.math.ucdavis.edu/%7Empf/2010-sparse-optimization-with-least-squares.html}
}


@ARTICLE{Friedlander11TRhdm,
  author = {Michael P. Friedlander and Mark Schmidt},
  title = {Hybrid deterministic-stochastic methods for data fitting},
  journal = {SIAM Journal on Scientific Computing},
  year = {2012},
  volume = {34},
  pages = {A1380-A1405},
  number = {3},
  month = {01},
  abstract = {Many structured data-fitting applications require the
                  solution of an optimization problem involving a sum
                  over a potentially large number of
                  measurements. Incremental gradient algorithms (both
                  deterministic and randomized) offer inexpensive
                  iterations by sampling only subsets of the terms in
                  the sum. These methods can make great progress
                  initially, but often slow as they approach a
                  solution. In contrast, full gradient methods achieve
                  steady convergence at the expense of evaluating the
                  full objective and gradient on each iteration. We
                  explore hybrid methods that exhibit the benefits of
                  both approaches. Rate of convergence analysis and
                  numerical experiments illustrate the potential for
                  the approach.},
  keywords = {optimization},
  doi = {10.1137/110830629}
}


@ARTICLE{friedlander2007SJOero,
  author = {Michael P. Friedlander and P. Tseng},
  title = {Exact regularization of convex programs},
  journal = {SIAM Journal on Optimization},
  year = {2007},
  volume = {18},
  pages = {1326-1350},
  number = {4},
  month = {05},
  abstract = {The regularization of a convex program is exact if all
                  solutions of the regularized problem are also
                  solutions of the original problem for all values of
                  the regularization parameter below some positive
                  threshold. For a general convex program, we show
                  that the regularization is exact if and only if a
                  certain selection problem has a Lagrange
                  multiplier. Moreover, the regularization parameter
                  threshold is inversely related to the Lagrange
                  multiplier. We use this result to generalize an
                  exact regularization result of Ferris and
                  Mangasarian [Appl. Math.  Optim., 23(1991),
                  pp. 266{\textendash}273] involving a linearized
                  selection problem. We also use it to derive
                  necessary and sufficient conditions for exact
                  penalization, similar to those obtained by Bertsekas
                  [Math. Programming, 9(1975), pp. 87{\textendash}99]
                  and by Bertsekas, Nedi , Ozdaglar [Convex Analysis
                  and Optimization, Athena Scientific, Belmont, MA,
                  2003]. When the regularization is not exact, we
                  derive error bounds on the distance from the
                  regularized solution to the original solution
                  set. We also show that existence of a
                  {\textquoteleft}{\textquoteleft}weak sharp
                  minimum{\textquoteright}{\textquoteright} is in some
                  sense close to being necessary for exact
                  regularization. We illustrate the main result with
                  numerical experiments on the l1 regularization of
                  benchmark (degenerate) linear programs and
                  semidefinite/second-order cone programs. The
                  experiments demonstrate the usefulness of l1
                  regularization in finding sparse solutions.},
  keywords = {SLIM,Optimization},
  doi = {10.1137/060675320}
}


@ARTICLE{friedlander2007TASdtd,
  author = {Michael P. Friedlander and M. A. Saunders},
  title = {Discussion: the {Dantzig} selector: statistical estimation when p is much larger than n},
  journal = {The Annals of Statistics},
  year = {2007},
  volume = {35},
  pages = {2385-2391},
  number = {6},
  month = {03},
  keywords = {dantzig, SLIM, statistics},
  doi = {10.1214/009053607000000479}
}


@ARTICLE{haber10TRemp,
  author = {Eldad Haber and Matthias Chung and Felix J. Herrmann},
  title = {An effective method for parameter estimation with {PDE} constraints with multiple right hand sides},
  journal = {SIAM Journal on Optimization},
  year = {2012},
  volume = {22},
  number = {3},
  month = {07},
  abstract = {Often, parameter estimation problems of
                  parameter-dependent PDEs involve multiple right-hand
                  sides. The computational cost and memory
                  requirements of such problems increase linearly with
                  the number of right-hand sides. For many
                  applications this is the main bottleneck of the
                  computation.  In this paper we show that problems
                  with multiple right-hand sides can be reformulated
                  as stochastic programming problems by combining the
                  right-hand sides into a few „simultaneous”
                  sources. This effectively reduces the cost of the
                  forward problem and results in problems that are
                  much cheaper to solve. We discuss two solution
                  methodologies: namely sample average approximation
                  and stochastic approximation. To illustrate the
                  effectiveness of our approach we present two model
                  problems, direct current resistivity and seismic
                  tomography.},
  keywords = {SLIM, FWI, optimization},
  url = {http://dx.doi.org/10.1137/11081126X}
}


@ARTICLE{hennenfent2008GEOPnii,
  author = {Gilles Hennenfent and Ewout {van den Berg} and Michael P. Friedlander and Felix J. Herrmann},
  title = {New insights into one-norm solvers from the {Pareto} curve},
  journal = {Geophysics},
  year = {2008},
  month = {07},
  volume = {73},
  number = {4},
  pages = {A23-A26},
  abstract = {Geophysical inverse problems typically involve a trade
                  off between data misfit and some prior. Pareto
                  curves trace the optimal trade off between these two
                  competing aims. These curves are commonly used in
                  problems with two-norm priors where they are plotted
                  on a log-log scale and are known as L-curves. For
                  other priors, such as the sparsity-promoting one
                  norm, Pareto curves remain relatively unexplored. We
                  show how these curves lead to new insights in
                  one-norm regularization. First, we confirm the
                  theoretical properties of smoothness and convexity
                  of these curves from a stylized and a geophysical
                  example. Second, we exploit these crucial properties
                  to approximate the Pareto curve for a large-scale
                  problem. Third, we show how Pareto curves provide an
                  objective criterion to gauge how different one-norm
                  solvers advance towards the solution.},
  keywords = {Pareto, SLIM, Geophysics, optimization, acquisition, processing},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2008/hennenfent08GEOnii/hennenfent08GEOnii.pdf},
  doi = {10.1190/1.2944169}
}


@ARTICLE{hennenfent2010GEOPnct,
  author = {Gilles Hennenfent and Lloyd Fenelon and Felix J. Herrmann},
  title = {Nonequispaced curvelet transform for seismic data reconstruction: a sparsity-promoting approach},
  journal = {Geophysics},
  year = {2010},
  volume = {75},
  pages = {WB203-WB210},
  number = {6},
  month = {12},
  abstract = {We extend our earlier work on the nonequispaced fast
                  discrete curvelet transform (NFDCT) and introduce a
                  second generation of the transform. This new
                  generation differs from the previous one by the
                  approach taken to compute accurate curvelet
                  coefficients from irregularly sampled data. The
                  first generation relies on accurate Fourier
                  coefficients obtained by an l2-regularized inversion
                  of the nonequispaced fast Fourier transform (FFT)
                  whereas the second is based on a direct
                  l1-regularized inversion of the operator that links
                  curvelet coefficients to irregular data. Also, by
                  construction the second generation NFDCT is lossless
                  unlike the first generation NFDCT. This property is
                  particularly attractive for processing irregularly
                  sampled seismic data in the curvelet domain and
                  bringing them back to their irregular record-ing
                  locations with high fidelity. Secondly, we combine
                  the second generation NFDCT with the standard fast
                  discrete curvelet transform (FDCT) to form a new
                  curvelet-based method, coined nonequispaced curvelet
                  reconstruction with sparsity-promoting inversion
                  (NCRSI) for the regularization and interpolation of
                  irregularly sampled data. We demonstrate that for a
                  pure regularization problem the reconstruction is
                  very accurate. The signal-to-reconstruction error
                  ratio in our example is above 40 dB. We also conduct
                  combined interpolation and regularization
                  experiments. The reconstructions for synthetic data
                  are accurate, particularly when the recording
                  locations are optimally jittered. The reconstruction
                  in our real data example shows amplitudes along the
                  main wavefronts smoothly varying with limited
                  acquisition imprint.},
  keywords = {curvelet transforms, data acquisition, geophysical techniques, seismology, SLIM, processing},
  doi = {10.1190/1.3494032},
  publisher = {SEG},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2010/hennenfent2010GEOPnct/hennenfent2010GEOPnct.pdf}
}


@ARTICLE{hennenfent2008GEOPsdw,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Simply denoise: wavefield reconstruction via jittered undersampling},
  journal = {Geophysics},
  year = {2008},
  volume = {73},
  pages = {V19-V28},
  number = {3},
  month = {05},
  abstract = {In this paper, we present a new discrete undersampling
                  scheme designed to favor wavefield reconstruction by
                  sparsity-promoting inversion with transform elements
                  that are localized in the Fourier domain. Our work
                  is motivated by empirical observations in the
                  seismic community, corroborated by recent results
                  from compressive sampling, which indicate favorable
                  (wavefield) reconstructions from random as opposed
                  to regular undersampling. As predicted by theory,
                  random undersampling renders coherent aliases into
                  harmless incoherent random noise, effectively
                  turning the interpolation problem into a much
                  simpler denoising problem. A practical requirement
                  of wavefield reconstruction with localized
                  sparsifying transforms is the control on the maximum
                  gap size. Unfortunately, random undersampling does
                  not provide such a control and the main purpose of
                  this paper is to introduce a sampling scheme, coined
                  jittered undersampling, that shares the benefits of
                  random sampling, while offering control on the
                  maximum gap size. Our contribution of jittered
                  sub-Nyquist sampling proves to be key in the
                  formulation of a versatile wavefield
                  sparsity-promoting recovery scheme that follows the
                  principles of compressive sampling. After studying
                  the behavior of the jittered undersampling scheme in
                  the Fourier domain, its performance is studied for
                  curvelet recovery by sparsity-promoting inversion
                  (CRSI). Our findings on synthetic and real seismic
                  data indicate an improvement of several decibels
                  over recovery from regularly-undersampled data for
                  the same amount of data collected.},
  html_version = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2008/hennenfent08GEOsdw/paper_html/paper.html},
  keywords = {sampling, Geophysics, SLIM, acquisition, processing, optimization, compressive sensing},
  doi = {10.1190/1.2841038},
  publisher = {SEG},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2008/hennenfent08GEOsdw/hennenfent08GEOsdw.pdf}
}


@ARTICLE{hennenfent2006CiSEsdn,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Seismic denoising with nonuniformly sampled curvelets},
  journal = {Computing in Science \& Engineering},
  year = {2006},
  volume = {8},
  number = {3},
  pages = {16-25},
  month = {05},
  abstract = {The authors present an extension of the fast discrete
                  curvelet transform (FDCT) to nonuniformly sampled
                  data. This extension not only restores curvelet
                  compression rates for nonuniformly sampled data but
                  also removes noise and maps the data to a regular
                  grid.},
  keywords = {CiSE, processing},
  doi = {10.1109/MCSE.2006.49},
  url = {https://slim.gatech.edu/Publications/Public/Journals/CiSE/2006/hennenfent06CiSEsdn/hennenfent06CiSEsdn.pdf }
}


@ARTICLE{herrmann2012IIsi,
  author = {Felix J. Herrmann},
  title = {Seismic advances},
  journal = {International Innovation},
  year = {2013},
  pages = {46-49},
  month = {01},
  abstract = {Current seismic exploration techniques are hampered by
                  bottlenecks in data sampling and processing due to
                  challenges in data collection, demand for more data
                  and the increasing need to study highly complex
                  geological settings. Professor Felix J. Herrmann's
                  group is developing novel techniques to overcome
                  these barriers which could greatly benefit the
                  hydrocarbon industry.},
  keywords = {seismic exploration techniques, compressive sensing, wave-equation-based
	data mining, dynamic nonlinear optimization},
  url = {https://slim.gatech.edu/Publications/Public/Journals/InternationalInnovation/2012/herrmann2012IIsi/herrmann2012IIsi.pdf}
}


@ARTICLE{herrmann2010GEOPrsg,
  author = {Felix J. Herrmann},
  title = {Randomized sampling and sparsity: getting more information from fewer samples},
  journal = {Geophysics},
  year = {2010},
  volume = {75},
  pages = {WB173-WB187},
  number = {6},
  month = {12},
  abstract = {Many seismic exploration techniques rely on the
                  collection of massive data volumes that are
                  subsequently mined for information during
                  processing.  Although this approach has been
                  extremely successful in the past, current efforts
                  toward higher-resolution images in increasingly
                  complicated regions of the earth continue to reveal
                  fundamental shortcomings in our workflows. Chiefly
                  among these is the so-called
                  {\textquotedblleft}curse of
                  dimensionality{\textquotedblright} exemplified by
                  Nyquist{\textquoteright}s sampling criterion, which
                  disproportionately strains current acquisition and
                  processing systems as the size and desired
                  resolution of our survey areas continue to
                  increase. We offer an alternative sampling method
                  leveraging recent insights from compressive sensing
                  toward seismic acquisition and processing for data
                  that are traditionally considered to be
                  undersampled. The main outcome of this approach is a
                  new technology where acquisition and processing
                  related costs are no longer determined by overly
                  stringent sampling criteria, such as Nyquist. At the
                  heart of our approach lies randomized incoherent
                  sampling that breaks subsampling related
                  interferences by turning them into harmless noise,
                  which we subsequently remove by promoting
                  transform-domain sparsity. Now, costs no longer grow
                  significantly with resolution and dimensionality of
                  the survey area, but instead depend only on
                  transform-domain sparsity. Our contribution is
                  twofold. First, we demonstrate by means of carefully
                  designed numerical experiments that compressive
                  sensing can successfully be adapted to seismic
                  exploration.  Second, we show that accurate recovery
                  can be accomplished for compressively sampled data
                  volumes sizes that exceed the size of conventional
                  transform-domain data volumes by only a small
                  factor. Because compressive sensing combines
                  transformation and encoding by a single linear
                  encoding step, this technology is directly
                  applicable to acquisition and to dimensionality
                  reduction during processing. In either case,
                  sampling, storage, and processing costs scale with
                  transform-domain sparsity. We illustrate this
                  principle by means of number of case studies.},
  keywords = {data acquisition, geophysical techniques, Nyquist criterion, sampling
	methods, seismology, SLIM, acquisition, compressive sensing, optimization},
  doi = {10.1190/1.3506147},
  publisher = {SEG},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2010/herrmann2010GEOPrsg/herrmann2010GEOPrsg.pdf}
}


@ARTICLE{herrmann2005ICAEsdb,
  author = {Felix J. Herrmann},
  title = {Seismic deconvolution by atomic decomposition: a parametric approach with sparseness constraints},
  journal = {Integrated Computer-Aided Engineering},
  year = {2005},
  volume = {12},
  pages = {69-90},
  number = {1},
  month = {01},
  abstract = {In this paper an alternative approach to the blind
                  seismic deconvolution problem is presented that aims
                  for two goals namely recovering the location and
                  relative strength of seismic reflectors, possibly
                  with super-localization, as well as obtaining
                  detailed parametric characterizations for the
                  reflectors. We hope to accomplish these goals by
                  decomposing seismic data into a redundant dictionary
                  of parameterized waveforms designed to closely match
                  the properties of reflection events associated with
                  sedimentary records. In particular, our method
                  allows for highly intermittent non-Gaussian records
                  yielding a reflectivity that can no longer be
                  described by a stationary random process or by a
                  spike train. Instead, we propose a reflector
                  parameterization that not only recovers the
                  reflector{\textquoteright}s location and relative
                  strength but which also captures reflector
                  attributes such as its local scaling, sharpness and
                  instantaneous phase-delay. The first set of
                  parameters delineates the stratigraphy whereas the
                  second provides information on the lithology. As a
                  consequence of the redundant parameterization,
                  finding the matching waveforms from the dictionary
                  involves the solution of an ill-posed problem. Two
                  complementary sparseness-imposing methods Matching
                  and Basis Pursuit are compared for our dictionary
                  and applied to seismic data.},
  address = {Amsterdam, The Netherlands},
  issn = {1069-2509},
  keywords = {deconvolution, SLIM, processing, modelling},
  publisher = {IOS Press},
  url = {http://dl.acm.org/citation.cfm?id=1238980.1238986},
  url2 = {https://slim.gatech.edu/Publications/Public/Journals/IntegratedComputerAidedEngineering/2005/herrmann2005ICAEsdb/herrmann2005ICAEsdb.pdf}
}


@ARTICLE{herrmann2004GJIssa,
  author = {Felix J. Herrmann and Y. Bernab\'e },
  title = {Seismic singularities at upper-mantle phase transitions: a site percolation model},
  journal = {Geophysical Journal International},
  year = {2004},
  volume = {159},
  pages = {949-960},
  number = {3},
  month = {12},
  abstract = {Mineralogical phase transitions are usually invoked to
                  account for the sharpness of globally observed
                  upper-mantle seismic discontinuities. We propose a
                  percolation-based model for the elastic properties
                  of the phase mixture in the coexistence regions
                  associated with these transitions. The major
                  consequence of the model is that the elastic moduli
                  (but not the density) display a singularity at the
                  percolation threshold of the high-pressure
                  phase. This model not only explains the sharp but
                  continuous change in seismic velocities across the
                  phase transition, but also predicts its abruptness
                  and scale invariance, which are characterized by a
                  non-integral scale exponent. Using the
                  receiver-function approach and new, powerful
                  signal-processing techniques, we quantitatively
                  determine the singularity exponent from recordings
                  of converted seismic waves at two Australian
                  stations (CAN and WRAB). Using the estimated values,
                  we construct velocity{\textendash}depth profiles
                  across the singularities and verify that the
                  calculated converted waveforms match the
                  observations under CAN. Finally, we point out a
                  series of additional predictions that may provide
                  new insights into the physics and fine structure of
                  the upper-mantle transition zone.},
  keywords = {percolation, SLIM, modelling},
  doi = {10.1111/j.1365-246X.2004.02464.x}
}


@ARTICLE{herrmann2007GJInlp,
  author = {Felix J. Herrmann and Urs Boeniger and D. J. Verschuur},
  title = {Non-linear primary-multiple separation with directional curvelet frames},
  journal = {Geophysical Journal International},
  year = {2007},
  month = {08},
  volume = {170},
  number = {2},
  pages = {781-799},
  abstract = {Predictive multiple suppression methods consist of two
                  main steps: a prediction step, during which
                  multiples are predicted from seismic data, and a
                  primary-multiple separation step, during which the
                  predicted multiples are
                  {\textquoteright}matched{\textquoteright} with the
                  true multiples in the data and subsequently
                  removed. This second separation step, which we will
                  call the estimation step, is crucial in practice: an
                  incorrect separation will cause residual multiple
                  energy in the result or may lead to a distortion of
                  the primaries, or both. To reduce these adverse
                  effects, a new transformed-domain method is proposed
                  where primaries and multiples are separated rather
                  than matched. This separation is carried out on the
                  basis of differences in the multiscale and
                  multidirectional characteristics of these two signal
                  components. Our method uses the curvelet transform,
                  which maps multidimensional data volumes into almost
                  orthogonal localized multidimensional prototype
                  waveforms that vary in directional and
                  spatio-temporal content. Primaries-only and
                  multiples-only signal components are recovered from
                  the total data volume by a non-linear optimization
                  scheme that is stable under noisy input data. During
                  the optimization, the two signal components are
                  separated by enhancing sparseness (through weighted
                  l1-norms) in the transformed domain subject to
                  fitting the observed data as the sum of the
                  separated components to within a user-defined
                  tolerance level. Whenever, during the optimization,
                  the estimates for the primaries in the transformed
                  domain correlate with the predictions for the
                  multiples, the recovery of the coefficients for the
                  estimated primaries will be suppressed while for
                  regions where the correlation is small the method
                  seeks the sparsest set of coefficients that
                  represent the estimation for the primaries. Our
                  algorithm does not seek a matched filter and as such
                  it differs fundamentally from traditional adaptive
                  subtraction methods. The method derives its
                  stability from the sparseness obtained by a
                  non-parametric (i.e. not depending on a parametrized
                  physical model) multiscale and multidirectional
                  overcomplete signal representation. This sparsity
                  serves as prior information and allows for a
                  Bayesian interpretation of our method during which
                  the log-likelihood function is minimized while the
                  two signal components are assumed to be given by a
                  superposition of prototype waveforms, drawn
                  independently from a probability function that is
                  weighted by the predicted primaries and
                  multiples. In this paper, the predictions are based
                  on the data-driven surface-related multiple
                  elimination method. Synthetic and field data
                  examples show a clean separation leading to a
                  considerable improvement in multiple suppression
                  compared to the conventional method of adaptive
                  matched filtering. This improved separation
                  translates into an improved stack.},
  keywords = {signal separation, SLIM, processing},
  doi = {10.1111/j.1365-246X.2007.03360.x},
  url = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalJournalInternational/2007/herrmann2007GJInlp/herrmann2007GJInlp.pdf}
}


@ARTICLE{herrmann2009GEOPcbm,
  author = {Felix J. Herrmann and Cody R. Brown and Yogi A. Erlangga and Peyman P. Moghaddam},
  title = {Curvelet-based migration preconditioning and scaling},
  journal = {Geophysics},
  year = {2009},
  volume = {74},
  pages = {A41-A46},
  month = {07-08},
  abstract = {The extremely large size of typical seismic imaging
                  problems has been one of the major stumbling blocks
                  for iterative techniques to attain accurate
                  migration amplitudes. These iterative methods are
                  important because they complement theoretical
                  approaches that are hampered by difficulties to
                  control problems such as finite-acquisition
                  aperture, source-receiver frequency response, and
                  directivity. To solve these problems, we apply
                  preconditioning, which significantly improves
                  convergence of least-squares migration. We discuss
                  different levels of preconditioning that range from
                  corrections for the order of the migration operator
                  to corrections for spherical spreading, and position
                  and reflector-dip dependent amplitude errors. While
                  the first two corrections correspond to simple
                  scalings in the Fourier and physical domain, the
                  third correction requires phase-space (space spanned
                  by location and dip) scaling, which we carry out
                  with curvelets. We show that our combined
                  preconditioner leads to a significant improvement of
                  the convergence of least-squares
                  {\textquoteleft}wave-equation{\textquoteright}
                  migration on a line from the SEG AA{\textquoteright}
                  salt model.},
  keywords = {migration, SLIM, imaging},
  doi = {10.1190/1.3124753},
  url = {http://geophysics.geoscienceworld.org/content/74/4/A41.abstract},
  url2 = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2009/herrmann2009GEOPcbm/herrmann2009GEOPcbm.pdf}
}


@ARTICLE{herrmann2009GEOPcsf,
  author = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title = {Compressive simultaneous full-waveform simulation},
  journal = {Geophysics},
  year = {2009},
  volume = {74},
  number = {4},
  pages = {A35-A40},
  month = {07-08},
  abstract = {The fact that computational complexity of wavefield
                  simulation is proportional to the size of the
                  discretized model and acquisition geometry, and not
                  to the complexity of the simulated wavefield, is a
                  major impediment within seismic imaging. By turning
                  simulation into a compressive sensing
                  problem{\textendash}-where simulated data is
                  recovered from a relatively small number of
                  independent simultaneous sources{\textendash}-we
                  remove this impediment by showing that compressively
                  sampling a simulation is equivalent to compressively
                  sampling the sources, followed by solving a reduced
                  system. As in compressive sensing, this allows for a
                  reduction in sampling rate and hence in simulation
                  costs. We demonstrate this principle for the
                  time-harmonic Helmholtz solver. The solution is
                  computed by inverting the reduced system, followed
                  by a recovery of the full wavefield with a sparsity
                  promoting program. Depending on the
                  wavefield{\textquoteright}s sparsity, this approach
                  can lead to significant cost reductions, in
                  particular when combined with the implicit
                  preconditioned Helmholtz solver, which is known to
                  converge even for decreasing mesh sizes and
                  increasing angular frequencies. These properties
                  make our scheme a viable alternative to explicit
                  time-domain finite-differences.},
  keywords = {full-waveform, SLIM, modelling, compressive sensing},
  doi = {10.1190/1.3115122},
  url = {http://library.seg.org/doi/abs/10.1190/1.3115122},
  url2 = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2009/herrmann2009GEOPcsf/herrmann2009GEOPcsf.pdf}
}


@ARTICLE{Herrmann11TRfcd,
  author = {Felix J. Herrmann and Michael P. Friedlander and Ozgur Yilmaz},
  title = {Fighting the curse of dimensionality: compressive sensing in exploration seismology},
  journal = {Signal Processing Magazine, IEEE},
  year = {2012},
  volume = {29},
  pages = {88-100},
  number = {3},
  month = {05},
  abstract = {Many seismic exploration techniques rely on the
                  collection of massive data volumes that are mined
                  for information during processing. This approach has
                  been extremely successful, but current efforts
                  toward higher resolution images in increasingly
                  complicated regions of Earth continue to reveal
                  fundamental shortcomings in our typical workflows.
                  The "curse" of dimensionality is the main roadblock
                  and is exemplified by Nyquist's sampling criterion,
                  which disproportionately strains current acquisition
                  and processing systems as the size and desired
                  resolution of our survey areas continues to
                  increase.},
  issn = {1053-5888},
  keywords = {Earth, Nyquist sampling criterion, dimensionality curse, higher-resolution images, massive data volumes, seismic exploration techniques, strains current acquisition system, strains current processing system, geographic information systems, seismology},
  doi = {10.1109/MSP.2012.2185859},
  url = {https://slim.gatech.edu/Publications/Public/Journals/IEEESignalProcessingMagazine/2012/Herrmann11TRfcd/Herrmann11TRfcd.pdf}
}


@ARTICLE{herrmann2008GJInps,
  author = {Felix J. Herrmann and Gilles Hennenfent},
  title = {Non-parametric seismic data recovery with curvelet frames},
  journal = {Geophysical Journal International},
  year = {2008},
  volume = {173},
  pages = {233-248},
  month = {04},
  abstract = {Seismic data recovery from data with missing traces on
                  otherwise regular acquisition grids forms a crucial
                  step in the seismic processing flow. For instance,
                  unsuccessful recovery leads to imaging artifacts and
                  to erroneous predictions for the multiples,
                  adversely affecting the performance of multiple
                  elimination. A non-parametric transform-based
                  recovery method is presented that exploits the
                  compression of seismic data volumes by recently
                  developed curvelet frames. The elements of this
                  transform are multidimensional and directional and
                  locally resemble wavefronts present in the data,
                  which leads to a compressible representation for
                  seismic data. This compression enables us to
                  formulate a new curvelet-based seismic data recovery
                  algorithm through sparsity-promoting inversion. The
                  concept of sparsity-promoting inversion is in itself
                  not new to geophysics. However, the recent insights
                  from the field of {\textquoteleft}compressed
                  sensing{\textquoteright} are new since they clearly
                  identify the three main ingredients that go into a
                  successful formulation of a recovery problem, namely
                  a sparsifying transform, a sampling strategy that
                  subdues coherent aliases and a sparsity-promoting
                  program that recovers the largest entries of the
                  curvelet-domain vector while explaining the
                  measurements. These concepts are illustrated with a
                  stylized experiment that stresses the importance of
                  the degree of compression by the sparsifying
                  transform. With these findings, a curvelet-based
                  recovery algorithms is developed, which recovers
                  seismic wavefields from seismic data volumes with
                  large percentages of traces missing. During this
                  construction, we benefit from the main three
                  ingredients of compressive sampling, namely the
                  curvelet compression of seismic data, the existence
                  of a favorable sampling scheme and the formulation
                  of a large-scale sparsity-promoting solver based on
                  a cooling method. The recovery performs well on
                  synthetic as well as real data and performs better
                  by virtue of the sparsifying property of
                  curvelets. Our results are applicable to other areas
                  such as global seismology.},
  keywords = {curvelet transform, reconstruction, SLIM, acquisition},
  url = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalJournalInternational/2008/herrmann2008GJInps.pdf},
  doi = {10.1111/j.1365-246X.2007.03698.x}
}


@ARTICLE{herrmann11GPelsqIm,
  author = {Felix J. Herrmann and Xiang Li},
  title = {Efficient least-squares imaging with sparsity promotion and compressive sensing},
  journal = {Geophysical Prospecting},
  year = {2012},
  volume = {60},
  pages = {696-712},
  number = {4},
  month = {07},
  abstract = {Seismic imaging is a linearized inversion problem
                  relying on the minimization of a least-squares
                  misfit functional as a function of the medium
                  perturbation. The success of this procedure hinges
                  on our ability to handle large systems of
                  equations---whose size grows exponentially with the
                  demand for higher resolution images in more and more
                  complicated areas---and our ability to invert these
                  systems given a limited amount of computational
                  resources. To overcome this "curse of
                  dimensionality" in problem size and computational
                  complexity, we propose a combination of randomized
                  dimensionality-reduction and divide-and-conquer
                  techniques. This approach allows us to take
                  advantage of sophisticated sparsity-promoting
                  solvers that work on a series of smaller subproblems
                  each involving a small randomized subset of
                  data. These subsets correspond to artificial
                  simultaneous-source experiments made of random
                  superpositions of sequential-source experiments. By
                  changing these subsets after each subproblem is
                  solved, we are able to attain an inversion quality
                  that is competitive while requiring fewer
                  computational, and possibly, fewer acquisition
                  resources. Application of this concept to a
                  controlled series of experiments showed the validity
                  of our approach and the relationship between its
                  efficiency---by reducing the number of sources and
                  hence the number of wave-equation solves---and the
                  image quality. Application of our
                  dimensionality-reduction methodology with sparsity
                  promotion to a complicated synthetic with well-log
                  constrained structure also yields excellent results
                  underlining the importance of sparsity promotion.},
  address = {University of British Columbia, Vancouver},
  keywords = {SLIM, imaging, optimization, compressive sensing},
  doi = {10.1111/j.1365-2478.2011.01041.x},
  url = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalProspecting/2012/herrmann11GPelsqIm/herrmann11GPelsqIm.pdf},
  url2 = {http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2478.2011.01041.x/full}
}


@ARTICLE{herrmann2008ACHAsac,
  author = {Felix J. Herrmann and Peyman P. Moghaddam and Christiaan C. Stolk},
  title = {Sparsity- and continuity-promoting seismic image recovery with curvelet frames},
  journal = {Applied and Computational Harmonic Analysis},
  year = {2008},
  volume = {24},
  pages = {150-173},
  number = {2},
  month = {03},
  abstract = {A nonlinear singularity-preserving solution to seismic
                  image recovery with sparseness and continuity
                  constraints is proposed. We observe that curvelets,
                  as a directional frame expansion, lead to sparsity
                  of seismic images and exhibit invariance under the
                  normal operator of the linearized imaging
                  problem. Based on this observation we derive a
                  method for stable recovery of the migration
                  amplitudes from noisy data. The method corrects the
                  amplitudes during a post-processing step after
                  migration, such that the main additional cost is one
                  ap- plication of the normal operator, i.e. a
                  modeling followed by a migration. Asymptotically
                  this normal operator corresponds to a
                  pseudodifferential operator, for which a convenient
                  diagonal approximation in the curvelet domain is
                  derived, including a bound for its error and a
                  method for the estimation of the diagonal from a
                  compound operator consisting of discrete
                  implementations for the scattering operator and its
                  adjoint the migration operator. The solution is
                  formulated as a nonlinear optimization problem where
                  sparsity in the curvelet domain as well as
                  continuity along the imaged reflectors are jointly
                  promoted. To enhance sparsity, the $ell_1$-norm on the
                  curvelet coefficients is minimized, while continuity
                  is promoted by minimizing an anisotropic diffusion
                  norm on the image. The performance of the recovery
                  scheme is evaluated with a time-reversed
                  {\textquoteright}wave-equation{\textquoteright}
                  migration code on synthetic datasets, including the
                  complex SEG/EAGE AA salt model.},
  keywords = {curvelet transform, imaging, SLIM, processing},
  doi = {10.1016/j.acha.2007.06.007},
  url = {https://slim.gatech.edu/Publications/Public/Journals/ACHA/2008/herrmann2008ACHAsac/herrmann2008ACHAsac.pdf}
}


@ARTICLE{herrmann2008GEOPcbs,
  author = {Felix J. Herrmann and Deli Wang and Gilles Hennenfent and Peyman P. Moghaddam},
  title = {Curvelet-based seismic data processing: a multiscale and nonlinear approach},
  journal = {Geophysics},
  year = {2008},
  volume = {73},
  pages = {A1-A5},
  number = {1},
  month = {03},
  abstract = {Mitigating missing data, multiples, and erroneous
                  migration amplitudes are key factors that determine
                  image quality. Curvelets, little
                  {\textquoteleft}{\textquoteleft}plane
                  waves,{\textquoteright}{\textquoteright} complete
                  with oscillations in one direction and smoothness in
                  the other directions, sparsify a property we
                  leverage explicitly with sparsity promotion. With
                  this principle, we recover seismic data with high
                  fidelity from a small subset (20\%) of randomly
                  selected traces. Similarly, sparsity leads to a
                  natural decorrelation and hence to a robust
                  curvelet-domain primary-multiple separation for
                  North Sea data. Finally, sparsity helps to recover
                  migration amplitudes from noisy data. With these
                  examples, we show that exploiting the
                  curvelet{\textquoteright}s ability to sparsify
                  wavefrontlike features is powerful, and our results
                  are a clear indication of the broad applicability of
                  this transform to exploration
                  seismology. {\copyright}2008 Society of Exploration
                  Geophysicists},
  keywords = {curvelet transform, SLIM, acquisition, processing},
  doi = {10.1190/1.2799517},
  publisher = {SEG},
  url = { https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2008/herrmann08GEOcbs/herrmann08GEOcbs.pdf }
}

@ARTICLE{tang2008CITEocs,
  author = {Wen Tang and Jianwei Ma and Felix J. Herrmann},
  title = {Optimized Compressed Sensing for Curvelet-based Seismic Data Reconstruction},
  journal = {CiteSeer},
  year = {2008},
  abstract = {Compressed sensing (CS) or compressive sampling provides a new sampling theory to reduce data acquisition, which says that compressible signals can be exactly reconstructed from highly incomplete sets of measurements. Very recently, the CS has been applied for seismic exploration and started to compact the traditional data acquisition. In this paper, we present an optimized sampling strategy for the CS data acquisition, which leads to better performance by the curvelet sparsity-promoting inversion in comparison with random sampling and jittered sampling scheme. One of motivation is to reduce the mutual coherence between measurement sampling schemes and curvelet sparse transform in the CS framework. The basic idea of our optimization is to directly redistribute the energy in frequency domain making original spectrum easily discriminated from the random noise induced by random undersampling, while offering control on the maximum gap size. Numerical experiments on synthetic and real seismic data show good performances of the proposed optimized CS for seismic data reconstruction.},
  keywords = {Optimized Compressive Sensing, curvelet transform},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.154.7994},
  url2 = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.154.7994&rep=rep1&type=pdf}
}

@ARTICLE{herrmann2008GEOPacd,
  author = {Felix J. Herrmann and Deli Wang and D. J. Verschuur},
  title = {Adaptive curvelet-domain primary-multiple separation},
  journal = {Geophysics},
  year = {2008},
  volume = {73},
  pages = {A17-A21},
  number = {3},
  month = {08},
  abstract = {In many exploration areas, successful separation of
                  primaries and multiples greatly determines the
                  quality of seismic imaging. Despite major advances
                  made by surface-related multiple elimination (SRME),
                  amplitude errors in the predicted multiples remain a
                  problem. When these errors vary for each type of
                  multiple in different ways (as a function of offset,
                  time, and dip), they pose a serious challenge for
                  conventional least-squares matching and for the
                  recently introduced separation by curvelet-domain
                  thresholding. We propose a data-adaptive method that
                  corrects amplitude errors, which vary smoothly as a
                  function of location, scale (frequency band), and
                  angle. With this method, the amplitudes can be
                  corrected by an elementwise curvelet-domain scaling
                  of the predicted multiples. We show that this
                  scaling leads to successful estimation of primaries,
                  despite amplitude, sign, timing, and phase errors in
                  the predicted multiples. Our results on synthetic
                  and real data show distinct improvements over
                  conventional least-squares matching in terms of
                  better suppression of multiple energy and
                  high-frequency clutter and better recovery of
                  estimated primaries. {\copyright}2008 Society of
                  Exploration Geophysicists},
  keywords = {Geophysics, SLIM, processing},
  doi = {10.1190/1.2904986},
  publisher = {SEG},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2008/herrmann08GEOacd/herrmann08GEOacd.pdf }
}


@ARTICLE{herrmann2011RECORDERcsse2,
  author = {Felix J. Herrmann and Haneet Wason and Tim T.Y. Lin},
  title = {Compressive sensing in seismic exploration: an outlook on a new paradigm},
  journal = {CSEG Recorder},
  year = {2011},
  volume = {36},
  pages = {34-39},
  number = {6},
  month = {06},
  abstract = {Many seismic exploration techniques rely on the
                  collection of massive data volumes that are
                  subsequently mined for information during
                  processing. While this approach has been extremely
                  successful in the past, current efforts toward
                  higher resolution images in increasingly complicated
                  regions of the Earth continue to reveal fundamental
                  shortcomings in our workflows. Chiefly amongst these
                  is the so-called "curse of dimensionality"
                  exemplified by Nyquist's sampling criterion, which
                  disproportionately strains current acquisition and
                  processing systems as the size and desired
                  resolution of our survey areas continues to
                  increase. We offer an alternative sampling method
                  leveraging recent insights from compressive sensing
                  towards seismic acquisition and processing for data
                  that, from a traditional point of view, are
                  considered to be undersampled. The main outcome of
                  this approach is a new technology where acquisition
                  and processing related costs are decoupled the
                  stringent Nyquist sampling criterion. At the heart
                  of our approach lies randomized incoherent sampling
                  that breaks subsampling-related interferences by
                  turning them into harmless noise, which we
                  subsequently remove by promoting sparsity in a
                  transform-domain. Acquisition schemes designed to
                  fit into this regime no longer grow significantly in
                  cost with increasing resolution and dimensionality
                  of the survey area, but instead its cost ideally
                  only depends on transform-domain sparsity of the
                  expected data. Our contribution is twofold. First,
                  we demonstrate by means of carefully designed
                  numerical experiments that ideas from compressive
                  sensing can be adapted to seismic
                  acquisition. Second, we leverage the property that
                  seismic data volumes are well approximated by a
                  small percentage of curvelet coefficients. Thus
                  curvelet-domain sparsity allows us to recover
                  conventionally-sampled seismic data volumes from
                  compressively-sampled data volumes whose size
                  exceeds this percentage by only a small
                  factor. Because compressive sensing combines
                  transformation and encoding by a single linear
                  encoding step, this technology is directly
                  applicable to seismic acquisition and therefore
                  constitutes a new paradigm where acquisitions costs
                  scale with transform-domain sparsity instead of the
                  gridsize. We illustrate this principle by showcasing
                  recovery of a real seismic line from simulated
                  compressively sampled acquisitions.},
  note = {Part 1 was published in April and Part 2 was published in June},
  url = {http://csegrecorder.com/articles/view/compressive-sensing-in-seismic-exploration-an-outlook-on-a-new-paradigm},
  url2 = {https://slim.gatech.edu/Publications/Public/Journals/CSEGRecorder/2011/herrmann2011RECORDERcsse/herrmann2011RECORDERcsse.pdf}
}


@ARTICLE{herrmann2011RECORDERcsse1,
  author = {Felix J. Herrmann and Haneet Wason and Tim T.Y. Lin},
  title = {Compressive sensing in seismic exploration: an outlook on a new paradigm},
  journal = {CSEG Recorder},
  year = {2011},
  volume = {36},
  pages = {19-33},
  number = {4},
  month = {04},
  abstract = {Many seismic exploration techniques rely on the
                  collection of massive data volumes that are
                  subsequently mined for information during
                  processing. While this approach has been extremely
                  successful in the past, current efforts toward
                  higher resolution images in increasingly complicated
                  regions of the Earth continue to reveal fundamental
                  shortcomings in our workflows. Chiefly amongst these
                  is the so-called "curse of dimensionality"
                  exemplified by Nyquist's sampling criterion, which
                  disproportionately strains current acquisition and
                  processing systems as the size and desired
                  resolution of our survey areas continues to
                  increase. We offer an alternative sampling method
                  leveraging recent insights from compressive sensing
                  towards seismic acquisition and processing for data
                  that, from a traditional point of view, are
                  considered to be undersampled. The main outcome of
                  this approach is a new technology where acquisition
                  and processing related costs are decoupled the
                  stringent Nyquist sampling criterion. At the heart
                  of our approach lies randomized incoherent sampling
                  that breaks subsampling-related interferences by
                  turning them into harmless noise, which we
                  subsequently remove by promoting sparsity in a
                  transform-domain. Acquisition schemes designed to
                  fit into this regime no longer grow significantly in
                  cost with increasing resolution and dimensionality
                  of the survey area, but instead its cost ideally
                  only depends on transform-domain sparsity of the
                  expected data. Our contribution is split into two
                  part.},
  note = {Part 1 was published in April and Part 2 was published in June},
  url = {http://csegrecorder.com/articles/view/compressive-sensing-in-seismic-exploration-outlook-on-a-new-paradigm},
  url2 = {https://slim.gatech.edu/Publications/Public/Journals/CSEGRecorder/2011/herrmann2011RECORDERcsse/herrmann2011RECORDERcsse.pdf}
}


@ARTICLE{kumar2010TNPecr,
  author = {Vishal Kumar and Jounada Oueity and Ron Clowes and Felix J. Herrmann},
  title = {Enhancing crustal reflection data through curvelet denoising},
  journal = {Technophysics},
  year = {2011},
  volume = {508},
  pages = {106-116},
  number = {1-4},
  month = {07},
  abstract = {Suppression of incoherent noise, which is present in the
                  seismic signal and may often lead to ambiguous
                  interpretation, is a key step in processing
                  associated with crustal reflection data. In this
                  paper, we make use of the parsimonious
                  representation of seismic data in the curvelet
                  domain to perform the noise attenuation while
                  preserving the coherent energy and its amplitude
                  information. Curvelets are a recently developed
                  mathematical transform that has as one of its
                  properties minimal overlap between seismic signal
                  and noise in the transform domain, thereby
                  facilitating signal-noise separation. The problem is
                  cast as an inverse problem and the results are
                  obtained by updating the solution at each
                  iteration. We demonstrate the effectiveness of this
                  procedure at removing noise on both synthetic shot
                  gathers and a synthetic stacked seismic section. We
                  then apply curvelet denoising to deep crustal
                  seismic reflection data where the signal-to-noise
                  ratio is low. The reflection data were recorded
                  along Lithoprobe's SNORCLE Line 1 across
                  Paleoproterozoic-Archean domains in Canada's
                  Northwest Territories. After initial processing, we
                  apply the iterative curvelet denoising to both
                  pre-stack shot gathers and post-stack data. Ground
                  roll, random noise and much of the anomalous
                  vertical energy is removed from the pre-stack shot
                  gathers, to the extent that crustal reflections,
                  including those from the Moho, are clearly seen on
                  individual gathers. Denoised stacked data show a
                  series of dipping reflections in the lower crust
                  that extend into the Moho. The Moho itself is
                  relatively flat and characterized by a sharp, narrow
                  band of reflections. Comparing the results for the
                  stacked data with those from F-X deconvolution,
                  curvelet denoising outperforms the latter by
                  attenuating incoherent noise with minimal harm to
                  the signal. Because curvelet denoising retains
                  amplitude information, it provides opportunities for
                  further studies of seismic sections through
                  attribute analyses. Curvelet denoising provides an
                  important new tool in the processing toolbox for
                  crustal seismic reflection data.},
  keywords = {SLIM, processing},
  doi = {10.1016/j.tecto.2010.07.01},
  url = {http://www.sciencedirect.com/science/article/pii/S0040195110003227}
}


@ARTICLE{vanLeeuwen2010IJGswi,
  author = {Tristan van Leeuwen and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Seismic waveform inversion by stochastic optimization},
  journal = {International Journal of Geophysics},
  year = {2011},
  volume = {2011},
  month = {12},
  abstract = {We explore the use of stochastic optimization methods
                  for seismic waveform inversion. The basic principle
                  of such methods is to randomly draw a batch of
                  realizations of a given misfit function and goes
                  back to the 1950s. The ultimate goal of such an
                  approach is to dramatically reduce the computational
                  cost involved in evaluating the misfit. Following
                  earlier work, we introduce the stochasticity in
                  waveform inversion problem in a rigorous way via a
                  technique called randomized trace estimation. We
                  then review theoretical results that underlie recent
                  developments in the use of stochastic methods for
                  waveform inversion. We present numerical experiments
                  to illustrate the behavior of different types of
                  stochastic optimization methods and investigate the
                  sensitivity to the batch size and the noise level in
                  the data. We find that it is possible to reproduce
                  results that are qualitatively similar to the
                  solution of the full problem with modest batch
                  sizes, even on noisy data. Each iteration of the
                  corresponding stochastic methods requires an order
                  of magnitude fewer PDE solves than a comparable
                  deterministic method applied to the full problem,
                  which may lead to an order of magnitude speedup for
                  waveform inversion in practice.},
  keywords = {SLIM, FWI, optimization},
  note = {Article ID: 689041, 18pages},
  doi = {10.1155/2011/689041},
  url = {https://slim.gatech.edu/Publications/Public/Journals/InternationJournalOfGeophysics/2011/vanLeeuwen10IJGswi/vanLeeuwen10IJGswi.pdf}
}


@ARTICLE{VanLeeuwen11TRfwiwse,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {Fast waveform inversion without source encoding},
  journal = {Geophysical Prospecting},
  year = {2013},
  month = {06},
  volume = {61},
  pages = {10-19},
  abstract = {Randomized source encoding has recently been proposed as
                  a way to dramatically reduce the costs of full
                  waveform inversion. The main idea is to replace all
                  sequential sources by a small number of simultaneous
                  sources. This introduces random crosstalk in the
                  model updates and special stochastic optimization
                  strategies are required to deal with this. Two
                  problems arise with this approach: i) source
                  encoding can only be applied to fixed-spread
                  acquisition setups, and ii) stochastic optimization
                  methods tend to converge very slowly, relying on
                  averaging to get rid of the cross-talk. Although the
                  slow convergence is partly offset by the low
                  iteration cost, we show that conventional
                  optimization strategies are bound to outperform
                  stochastic methods in the long run. In this paper we
                  argue that we don¬øt need randomized source encoding
                  to reap the benefits of stochastic optimization and
                  we review an optimization strategy that combines the
                  benefits of both conventional and stochastic
                  optimization. The method uses a gradually increasing
                  batch of sources. Thus, iterations are very cheap
                  initially and this allows the method to make fast
                  progress in the beginning. As the batch size grows,
                  the method behaves like conventional optimization,
                  allowing for fast convergence. Numerical examples
                  suggest that the stochastic and hybrid method
                  perform equally well with and without source
                  encoding and that the hybrid method outperforms both
                  conventional and stochastic optimization. The method
                  does not rely on source encoding techniques and can
                  thus be applied to non fixed-spread data.},
  keywords = {SLIM, FWI, optimization},
  doi = {10.1111/j.1365-2478.2012.01096.x},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2478.2012.01096.x/abstract},
  url2 = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalProspecting/2013/VanLeeuwen11TRfwiwse/VanLeeuwen11TRfwiwse.pdf},
  note = {Article first published online: 10 JULY 2012}
}


@ARTICLE{Li11TRfrfwi,
  author = {Xiang Li and Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Fast randomized full-waveform inversion with compressive sensing},
  journal = {Geophysics},
  year = {2012},
  volume = {77},
  pages = {A13-A17},
  number = {3},
  month = {05},
  abstract = {Wave-equation based seismic inversion can be formulated
                  as a nonlinear inverse problem where the medium
                  properties are obtained via minimization of a least-
                  squares misfit functional. The demand for higher
                  resolution models in more geologically complex areas
                  drives the need to develop techniques that explore
                  the special structure of full-waveform inversion to
                  reduce the computational burden and to regularize
                  the inverse problem. We meet these goals by using
                  ideas from compressive sensing and stochastic
                  optimization to design a novel Gauss-Newton method,
                  where the updates are computed from random subsets
                  of the data via curvelet-domain sparsity
                  promotion. Application of this idea to a realistic
                  synthetic shows improved results compared to
                  quasi-Newton methods, which require passes through
                  all data. Two different subset sampling strategies
                  are considered: randomized source encoding, and
                  drawing sequential shots firing at random source
                  locations from marine data with missing near and far
                  offsets. In both cases, we obtain excellent
                  inversion results compared to conventional methods
                  at reduced computational costs. },
  keywords = {SLIM, FWI, compressive sensing, optimization},
  doi = {10.1190/geo2011-0410.1},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2012/Li11TRfrfwi/Li11TRfrfwi.pdf}
}


@ARTICLE{lin2007GEOPcwe,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Compressed wavefield extrapolation},
  journal = {Geophysics},
  year = {2007},
  volume = {72},
  pages = {SM77-SM93},
  number = {5},
  month = {08},
  abstract = {An explicit algorithm for the extrapolation of one-way
                  wavefields is proposed that combines recent
                  developments in information theory and theoretical
                  signal processing with the physics of wave
                  propagation. Because of excessive memory
                  requirements, explicit formulations for wave
                  propagation have proven to be a challenge in 3D. By
                  using ideas from compressed sensing, we are able to
                  formulate the (inverse) wavefield extrapolation
                  problem on small subsets of the data volume, thereby
                  reducing the size of the operators. Compressed
                  sensing entails a new paradigm for signal recovery
                  that provides conditions under which signals can be
                  recovered from incomplete samplings by nonlinear
                  recovery methods that promote sparsity of the
                  to-be-recovered signal. According to this theory,
                  signals can be successfully recovered when the
                  measurement basis is incoherent with the
                  representa-tion in which the wavefield is sparse. In
                  this new approach, the eigenfunctions of the
                  Helmholtz operator are recognized as a basis that is
                  incoherent with curvelets that are known to compress
                  seismic wavefields. By casting the wavefield
                  extrapolation problem in this framework, wavefields
                  can be successfully extrapolated in the modal
                  domain, despite evanescent wave modes.  The degree
                  to which the wavefield can be recovered depends on
                  the number of missing (evanescent) wavemodes and on
                  the complexity of the wavefield. A proof of
                  principle for the compressed sensing method is given
                  for inverse wavefield extrapolation in 2D, together
                  with a pathway to 3D during which the multiscale and
                  multiangular properties of curvelets, in relation to
                  the Helmholz operator, are exploited. The results
                  show that our method is stable, has reduced dip
                  limitations, and handles evanescent waves in inverse
                  extrapolation. {\copyright}2007 Society of
                  Exploration Geophysicists},
  keywords = {SLIM, wave propagation, modelling},
  doi = {10.1190/1.2750716},
  publisher = {SEG},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2007/lin07cwe/lin07cwe.pdf}
}


@ARTICLE{Mansour11TRssma,
  author = {Hassan Mansour and Haneet Wason and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Randomized marine acquisition with compressive sampling matrices},
  journal = {Geophysical Prospecting},
  year = {2012},
  volume = {60},
  pages = {648-662},
  number = {4},
  month = {07},
  abstract = {Seismic data acquisition in marine environments is a
                  costly process that calls for the adoption of
                  simultaneous-source or randomized acquisition - an
                  emerging technology that is stimulating both
                  geophysical research and commercial
                  efforts. Simultaneous marine acquisition calls for
                  the development of a new set of design principles
                  and post-processing tools. In this paper, we discuss
                  the properties of a specific class of randomized
                  simultaneous acquisition matrices and demonstrate
                  that sparsity-promoting recovery improves the
                  quality of reconstructed seismic data volumes. We
                  propose a practical randomized marine acquisition
                  scheme where the sequential sources fire airguns at
                  only randomly time-dithered instances. We
                  demonstrate that the recovery using sparse
                  approximation from random time-dithering with a
                  single source approaches the recovery from
                  simultaneous-source acquisition with multiple
                  sources. Established findings from the field of
                  compressive sensing indicate that the choice of the
                  sparsifying transform that is incoherent with the
                  compressive sampling matrix can significantly impact
                  the reconstruction quality. Leveraging these
                  findings, we then demonstrate that the compressive
                  sampling matrix resulting from our proposed sampling
                  scheme is incoherent with the curvelet
                  transform. The combined measurement matrix exhibits
                  better isometry properties than other transform
                  bases such as a non-localized multidimensional
                  Fourier transform. We illustrate our results with
                  simulations of "ideal" simultaneous-source marine
                  acquisition, which dithers both in time and space,
                  compared with periodic and randomized
                  time-dithering.},
  keywords = {curvelet transform, Fourier, marine acquisition},
  doi = {10.1111/j.1365-2478.2012.01075.x},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2478.2012.01075.x/abstract},
  url2 = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalProspecting/2012/Mansour11TRssma/Mansour11TRssma.pdf}
}


@ARTICLE{saab2008ACHAsrb,
  author = {Rayan Saab and Ozgur Yilmaz},
  title = {Sparse recovery by non-convex optimization - instance optimality},
  journal = {Applied and Computational Harmonic Analysis},
  year = {2010},
  volume = {29},
  pages = {30-48},
  number = {1},
  month = {07},
  abstract = {In this note, we address the theoretical properties of
                  $\Delta_p$, a class of compressed sensing decoders
                  that rely on $l^p$ minimization with $p \in (0,1)$
                  to recover estimates of sparse and compressible
                  signals from incomplete and inaccurate
                  measurements. In particular, we extend the results
                  of Cand{\textquoteleft}es, Romberg and Tao [3] and
                  Wojtaszczyk [30] regarding the decoder $\Delta_1$,
                  based on $\ell^1$ minimization, to $\Delta p$ with
                  $p \in (0,1)$. Our results are two-fold. First, we
                  show that under certain sufficient conditions that
                  are weaker than the analogous sufficient conditions
                  for $\Delta_1$ the decoders $\Delta_p$ are robust to
                  noise and stable in the sense that they are $(2,p)$
                  instance optimal. Second, we extend the results of
                  Wojtaszczyk to show that, like $\Delta_1$, the
                  decoders $\Delta_p$ are (2,2) instance optimal in
                  probability provided the measurement matrix is drawn
                  from an appropriate distribution. While the
                  extension of the results of [3] to the setting where
                  $p \in (0,1)$ is straightforward, the extension of
                  the instance optimality in probability result of
                  [30] is non-trivial. In particular, we need to prove
                  that the $LQ_1$ property, introduced in [30], and
                  shown to hold for Gaussian matrices and matrices
                  whose columns are drawn uniformly from the sphere,
                  generalizes to an $LQ_p$ property for the same
                  classes of matrices. Our proof is based on a result
                  by Gordon and Kalton [18] about the Banach-Mazur
                  distances of p-convex bodies to their convex hulls.},
  keywords = {non-convex, compressive sensing},
  doi = {10.1016/j.acha.2009.08.002},
  url = {http://www.sciencedirect.com/science/article/pii/S1063520309000864},
  url2 = {https://slim.gatech.edu/Publications/Public/Journals/ACHA/2010/saab2008ACHAsrb/saab2008ACHAsrb.pdf}
}


@ARTICLE{wang2008GEOPbws,
  author = {Deli Wang and Rayan Saab and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Bayesian wavefield separation by transform-domain sparsity promotion},
  journal = {Geophysics},
  year = {2008},
  volume = {73},
  pages = {1-6},
  number = {5},
  month = {07},
  abstract = {Successful removal of coherent noise sources greatly
                  determines the quality of seismic imaging. Major
                  advances were made in this direction, e.g.,
                  Surface-Related Multiple Elimination (SRME) and
                  interferometric ground-roll removal. Still, moderate
                  phase, timing, amplitude errors and clutter in the
                  predicted signal components can be detrimental.
                  Adopting a Bayesian approach along with the
                  assumption of approximate curvelet-domain
                  independence of the to-be-separated signal
                  components, we construct an iterative algorithm that
                  takes the predictions produced by for example SRME
                  as input and separates these components in a robust
                  fashion. In addition, the proposed algorithm
                  controls the energy mismatch between the separated
                  and predicted components. Such a control, which was
                  lacking in earlier curvelet-domain formulations,
                  produces improved results for primary-multiple
                  separation on both synthetic and real data.},
  keywords = {curvelet transform, SLIM, Geophysics, processing, optimization},
  doi = {10.1190/1.2952571},
  html_version = {https://slim.gatech.edu/Publications/Public/Journals/2008/wang08TRbss/paper_html/paper.html},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2008/wang08GEObws/wang08GEObws.pdf}
}
