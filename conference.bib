% This file was created with JabRef 2.9.
% Encoding: ISO8859_1

%-----2020-----%

@CONFERENCE{rizzuti2020EAGEtwri,
  author = {Gabrio Rizzuti and Mathias Louboutin and Rongrong Wang and Felix J. Herrmann},
  title = {Time-domain wavefield reconstruction inversion for large-scale
seismics},
  year = {2020},
  month = {1},
  abstract = {Wavefield reconstruction inversion is an imaging technique akin to
full-waveform inversion, albeit based on a relaxed version of the wave
equation. This relaxation aims to beat the multimodality typical of
full-waveform inversion. However it prevents the use of time-marching solvers
for the augmented equation and, as a consequence, cannot be straightforwardly
employed to large 3D problems. In this work, we formulate a dual version of
wavefield reconstruction inversion amenable to explicit time-domain solvers,
yielding a robust and scalable inversion technique.},
  keywords = {3D, Full-waveform inversion, EAGE, Time-domain},
  note = {Accepted in EAGE},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2020/rizzuti2020EAGEtwri/rizzuti2020EAGEtwri.html}
}

@CONFERENCE{siahkoohi2020EAGEdlb,
  author = {Ali Siahkoohi and Gabrio Rizzuti and Felix J. Herrmann},
  title = {A deep-learning based Bayesian approach to seismic imaging and
uncertainty quantification},
  year = {2020},
  month = {1},
  abstract = {Uncertainty quantification is essential when dealing with ill-conditioned
inverse problems due to the inherent nonuniqueness of the solution. Bayesian
approaches allow us to determine how likely an estimation of the unknown
parameters is via formulating the posterior distribution. Unfortunately, it
is often not possible to formulate a prior distribution that precisely
encodes our prior knowledge about the unknown. Furthermore, adherence to
handcrafted priors may greatly bias the outcome of the Bayesian analysis. To
address this issue, we propose to use the functional form of a randomly
initialized convolutional neural network as an implicit structured prior,
which is shown to promote natural images and excludes images with unnatural
noise. In order to incorporate the model uncertainty into the final estimate,
we sample the posterior distribution using stochastic gradient Langevin
dynamics and perform Bayesian model averaging on the obtained samples. Our
synthetic numerical experiment verifies that deep priors combined with
Bayesian model averaging are able to partially circumvent imaging artifacts
and reduce the risk of overfitting in the presence of extreme noise. Finally,
we present pointwise variance of the estimates as a measure of uncertainty,
which coincides with regions that are difficult to image.},
  keywords = {seismic imaging, uncertainty quantification, stochastic gradient
Langevin dynamics, deep learning, EAGE},
  note = {Accepted in EAGE},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2020/siahkoohi2020EAGEdlb/siahkoohi2020EAGEdlb.html}
}

%-----2019-----%

@CONFERENCE{herrmann2019NIPSliwcuc,
  author = {Felix J. Herrmann and Ali Siahkoohi and Gabrio Rizzuti},
  title = {Learned imaging with constraints and uncertainty quantification},
  booktitle = {Neural Information Processing Systems (NeurIPS)},
  year = {2019},
  month = {12},
  abstract = {We outline new approaches to incorporate ideas from convolutional
networks into wave-based least-squares imaging. The aim is to combine
hand-crafted constraints with deep convolutional networks allowing us to
directly train a network capable of generating samples from the posterior. The main contributions include combination of weak deep priors with hard handcrafted constraints and a possible new way to sample the posterior.},
  keywords = {imaging, deep learning, uncertainty quantification, constraint},
  note = {Accepted on October 1, 2019},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/NIPS/2019/herrmann2019NIPSliwcuc/herrmann2019NIPSliwcuc.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/NIPS/2019/herrmann2019NIPSliwcuc/herrmann2019NIPSliwcuc_pres.pdf},
  url2 = {https://openreview.net/pdf?id=Hyet2Q29IS}
}

@CONFERENCE{herrmann2019EAGEHPCaii,
  author = {Felix J. Herrmann and Charles Jones and Gerard Gorman and Jan Hückelheim and Keegan Lensink and Paul Kelly and Navjot Kukreja and Henryk Modzelewski and Michael Lange and Mathias Louboutin and Fabio Luporini and James Selvages and Philipp A. Witte},
  title = {Accelerating ideation & innovation cheaply in the Cloud the power of abstraction, collaboration & reproducibility},
  booktitle = {4th EAGE Workshop on High-performance Computing},
  year = {2019},
  month = {10},
  keywords = {EAGE, HPC, Devito, JUDI, Cloud},
  note = {(EAGE HPC Workshop, Dubai)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGEHPC/2019/herrmann2019EAGEHPCaii/herrmann2019EAGEHPCaii_pres.pdf}
}

@CONFERENCE{rizzuti2019EAGElis,
  author = {Gabrio Rizzuti and Ali Siahkoohi and Felix J. Herrmann},
  title = {Learned iterative solvers for the Helmholtz equation},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2019},
  month = {06},
  abstract = {We propose a ‘learned’ iterative solver for the Helmholtz equation, by combining traditional Krylov-based solvers with machine learning. The method is, in principle, able to circumvent the shortcomings of classical iterative solvers, and has clear advantages over purely data-driven ap- proaches. We demonstrate the effectiveness of this approach under a 1.5-D assumption, when ade- quate a priori information about the velocity distribution is known.},
  keywords = {EAGE, Helmholtz, Iterative, Machine learning},
  note = {(EAGE, Copenhagen)},
  doi = {10.3997/2214-4609.201901542},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2019/rizzuti2019EAGElis/rizzuti2019EAGElis.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2019/rizzuti2019EAGElis/rizzuti2019EAGElis_pres.pdf}
}

@CONFERENCE{peters2019SEGans,
  author = {Bas Peters and Felix J. Herrmann},
  title = {A numerical solver for least-squares sub-problems in 3D wavefield reconstruction inversion and related problem formulations},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2019},
  month = {09},
  pages = {1536-1540},
  abstract = {Recent years saw a surge of interest in seismic waveform inversion approaches based on quadratic-penalty or augmented-Lagrangian methods, including Wavefield Reconstruction Inversion. These methods typically need to solve a least-squares sub-problem that contains a discretization of the Helmholtz equation. Memory requirements for direct solvers are often prohibitively large in three dimensions, and this limited the examples in the literature to two dimensions. We present an algorithm that uses iterative Helmholtz solvers as a black-box to solve the least-squares problem corresponding to 3D grids. This algorithm enables Wavefield Reconstruction Inversion and related formulations, in three dimensions. Our new algorithm also includes a root-finding method to convert a penalty into a constraint on the data-misfit without additional computational cost, by reusing precomputed quantities. Numerical experiments show that the cost of parallel communication and other computations are small compared to the main cost of solving one Helmholtz problem per source and one per receiver.},
  keywords = {SEG, Least-squares, numerical linear algebra, penalty method, quadratic constraints, full-waveform inversion, root finding},
  note = {(SEG, San Antonio)},
  doi = {10.1190/segam2019-3216638.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/peters2019SEGans/peters2019SEGans.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/peters2019SEGans/peters2019SEGans_pres.pdf}
}  

@CONFERENCE{rizzuti2019SEGadf,
  author = {Gabrio Rizzuti and Mathias Louboutin and Rongrong Wang and Emmanouil Daskalakis and Felix J. Herrmann},
  title = {A dual formulation for time-domain wavefield reconstruction inversion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2019},
  month = {09},
  pages = {1480-1485},
  abstract = {We illustrate a dual formulation for full-waveform inversion potentially apt to large 3-D problems. It is based on the optimization of the wave equation compliance, under the constraint of data misfit not exceeding a prescribed noise level. In the Lagrangian formulation, model and wavefield state variables are complemented with multipliers having the same dimension of data ("dual data" variables). Analogously to classical wavefield reconstruction inversion, the wavefield unknowns can be projected out in closed form, by solving a version of the augmented wave equation. This leads to a saddle-point problem whose variables are only model and dual data. As such, this formulation represents a model extension, and is potentially robust against local minima. The classical downsides of model extension methods and wavefield reconstruction inversion are here effectively mitigated: storage of the dual variables is affordable, the augmented wave equation is amenable to time-marching finite-difference schemes, and no continuation strategy for penalty parameters is needed, with the prospect of 3-D applications.},
  keywords = {SEG, full-waveform inversion, time-domain, 3D},
  note = {(SEG, San Antonio)},
  doi = {10.1190/segam2019-3216760.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/rizzuti2019SEGadf/rizzuti2019SEGadf.html},
  software = {https://github.com/slimgroup/Software.rizzuti2019SEGadf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/rizzuti2019SEGadf/rizzuti2019SEGadf_pres.pdf}
}

@CONFERENCE{siahkoohi2019SEGdlb,
  author = {Ali Siahkoohi and Rajiv Kumar and Felix J. Herrmann},
  title = {Deep-learning based ocean bottom seismic wavefield recovery},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2019},
  month = {09},
  pages = {2232-2237},
  abstract = {Ocean bottom surveys usually suffer from having very sparse receivers. Assuming a desirable source sampling, achievable by existing methods such as (simultaneous-source) randomized marine acquisition, we propose a deep-learning based scheme to bring the receivers to the same spatial grid as sources using a convolutional neural network. By exploiting source-receiver reciprocity, we construct training pairs by artificially subsampling the fully-sampled single-receiver frequency slices using a random training mask and later, we deploy the trained neural network to fill-in the gaps in single-source frequency slices. Our experiments show that a random training mask is essential for successful wavefield recovery, even when receivers are on a periodic gird. No external training data is required and experiments on a 3D synthetic data set demonstrate that we are able to recover receivers for up to 90 % missing receivers, missing either randomly or periodically, with a better recovery for random case, at low to midrange frequencies.},
  keywords = {SEG, obn, reconstruction, machine learning, reciprocity},
  note = {(SEG, San Antonio)},
  doi = {10.1190/segam2019-3216632.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/siahkoohi2019SEGdlb/siahkoohi2019SEGdlb.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/siahkoohi2019SEGdlb/siahkoohi2019SEGdlb_pres.pdf}
}

@CONFERENCE{siahkoohi2019SEGsrm,
  author = {Ali Siahkoohi and Dirk J. Verschuur and Felix J. Herrmann},
  title = {Surface-related multiple elimination with deep learning},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2019},
  month = {09},
  pages = {4629-4634},
  abstract = {We explore the potential of neural networks in approximating the action of the computationally expensive Estimation of Primaries by Sparse Inversion (EPSI) algorithm, applied to real data, via a supervised learning algorithm. We show that given suitable training data, consisting of a relatively cheap prediction of multiples and pairs of shot records with and without surface-related multiples, obtained via EPSI, a well-trained neural network is capable of providing an approximation to the action of the EPSI algorithm. We perform our numerical experiment on the field Nelson data set. Our results demonstrate that the quality of the multiple elimination via our neural network improves compared to the case where we only feed the network with shot records with surface-related multiples. We take these benefits by supplying the neural network with a relatively poor prediction of the multiples, e.g. obtained by a relatively cheap single step of Surface-Related Multiple Elimination.},
  keywords = {SEG, SRME, EPSI, machine learning},
  note = {(SEG, San Antonio)},
  doi = {10.1190/segam2019-3216723.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/siahkoohi2019SEGsrm/siahkoohi2019SEGsrm.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/siahkoohi2019SEGsrm/siahkoohi2019SEGsrm_pres.pdf}
}

@CONFERENCE{witte2019SEGedw,
  author = {Philipp A. Witte and Mathias Louboutin and Henryk Modzelewski and Charles Jones and James Selvage and Felix J. Herrmann},
  title = {Event-driven workflows for large-scale seismic imaging in the cloud},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2019},
  month = {09},
  pages = {3984-3988},
  abstract = {Cloud computing has seen a large rise in popularity in recent years and is becoming a cost effective alternative to on-premise computing, with theoretically unlimited scalability. However, so far little progress has been made in adapting the cloud for high performance computing (HPC) tasks, such as seismic imaging and inversion. As the cloud does not provide the same type of fast and reliable connections as conventional HPC clusters, porting legacy codes developed for HPC environments to the cloud is ineffective and misses out on an opportunity to take advantage of new technologies presented by the cloud. We present a novel approach of bringing seismic imaging and inversion workflows to the cloud, which does not rely on a traditional HPC environment, but is based on serverless and event-driven computations. Computational resources are assigned dynamically in response to events, thus minimizing idle time and providing resilience to hardware failures. We test our workflow on two large-scale imaging examples and demonstrate that cost-effective HPC in the cloud is possible, but requires careful reconsiderations of how to bring software to the cloud.},
  keywords = {SEG, cloud, large-scale, imaging, RTM, LS-RTM, workflow},
  note = {(SEG, San Antonio)},
  doi = {10.1190/segam2019-3215069.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/witte2019SEGedw/witte2019SEGedw.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/witte2019SEGedw/witte2019SEGedw_pres.pdf}
}

@CONFERENCE{witte2019RHPCssi,
  author = {Philipp A. Witte and Mathias Louboutin and Charles Jones and Felix J. Herrmann},
  title = {Serverless seismic imaging in the cloud},
  year = {2020},
  month = {03},
  abstract = {This talk presents a serverless approach to seismic imaging in the
cloud based on high-throughput containerized batch processing, event-driven
computations and a  domain-specific language compiler for solving the
underlying wave equations. A 3D case study on Azure demonstrates that this
approach allows reducing the operating cost of up to a factor of 6, making
the cloud a viable alternative to on-premise HPC clusters for seismic
imaging.},
  keywords = {cloud, hpc, reverse-time migration, serverless},
  note = {Rice Oil and Gas High Performance Computing Conference 2020},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/RHPC/2019/witte2019RHPCssi/witte2019RHPCssi.html},
  url2 = {https://www.youtube.com/watch?v=epilCpa078A&list=PLcsG4X8Zn_UA3m7tgIMbOdXD8eOEPiUYD&index=17},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/RHPC/2019/witte2019RHPCssi/witte2019RHPCssi_pres.pdf}
}

@CONFERENCE{yang2019SEGlrr,
  author = {Mengmeng Yang and Marie Graff and Rajiv Kumar and Felix J. Herrmann},
  title = {Low-rank representation of subsurface extended image volumes with power iterations},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2019},
  month = {09},
  pages = {4470-4474},
  abstract = {Full subsurface extended image volumes (EIVs) contain abundant information such as subsurface image gathers used in imaging, interpretation of rock properties or velocity analysis, but are expensive in term of computational cost and storage requirement due to their large size. Yet, due to their redundant information, monochromatic EIVs exhibit a low-rank structure that allows us to get a good approximation at a computational cost proportional to the rank k , while conventional techniques require at least the number of sources. Such low-rank factorized EIVs are computed thanks to the randomized singular values decomposition (SVD) algorithm. However, recent developments on low-rank approximations of EIVs raise two major questions. First, monochromatic EIVs rely on time-harmonic wave equation solvers, which do not scale well to realistic 3D models. Second, the rank of the monochromatic EIVs increases with the frequency, which yields increasing computational costs and storage. Here, we propose a construction approach based on a time-domain finite-difference wave-equation solver with time stepping, which is combined to power iteration schemes in the randomized SVD algorithm to accelerate the decay of the singular values. We compare the performances of simultaneous iterations, block-Krylov iterations and the original randomized SVD on the computation of the EIV for a small section of the Marmousi model. Then, we show further results on the full Marmousi model to validate our approach.},
  keywords = {SEG, full extended image volumes, time domain, randomized SVD, power iterations},
  note = {(SEG, San Antonio)},
  doi = {10.1190/segam2019-3215961.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/yang2019SEGlrr/yang2019SEGlrr.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/yang2019SEGlrr/yang2019SEGlrr_pres.pdf}
}

@CONFERENCE{zhang2019SEGhfw,
  author = {Yijun Zhang and Shashin Sharan and Felix J. Herrmann},
  title = {High-frequency wavefield recovery with weighted matrix factorizations},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2019},
  month = {09},
  pages = {3959-3963},
  abstract = {Acquired seismic data is normally not the fully sampled data we would like to work with since traces are missing due to physical constraints or budget limitations. Rank minimization is an effective way to recovering the missing trace data. Unfortunately, this technique's performance may deteriorate at higher frequency because high-frequency data can not necessarily be captured accurately by low-rank matrix factorizations albeit remedies exist such as hierarchical semi-separable matrices. As a result, recovered data often suffers from low signal to noise ratio S/Rs at the higher frequencies. To deal with this situation, we propose a weighted recovery method that improves the performance at the high frequencies by recursively using information from matrix factorizations at neighboring lower frequencies. Essentially, we include prior information from previously reconstructed frequency slices during the wavefield reconstruction. We apply our method to data collected from the Gulf of Suez, which shows that our method performs well compared to the traditional method without weighting.},
  keywords = {SEG, algorithm, data reconstruction, interpolation, processing, frequency-domain},
  note = {(SEG, San Antonio)},
  doi = {10.1190/segam2019-3215103.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/zhang2019SEGhfw/zhang2019SEGhfw.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/zhang2019SEGhfw/zhang2019SEGhfw_pres.pdf}
}

@CONFERENCE{peters2019SIAMGEOgmrip,
  author = {Bas Peters and Felix J. Herrmann},
  title = {Generalized Minkowski sets for the regularization of inverse problems},
  booktitle = {SIAM Conference on Mathematical and Computational Issues in the
Geosciences},
  year = {2019},
  month = {05},
  abstract = {We present a new algorithm to compute projections onto the intersection of constraint sets, designed particularly for multiple sets because we exploit similarities between constraint sets. When we do not know projections onto the individual sets in closed form, as is the case for total-variation constraints, our algorithm does not need other optimization algorithms to solve sub-problems. This a significant advantage in terms of computational cost and number of tuning parameters and stopping conditions, compared to classical algorithms to compute projections onto the intersection, such as Dykstra's algorithm.
  The proposed algorithm is suitable for problems with a large number of model parameters such as full-waveform inversion because it exploits coarse and fine-grained parallelism, and we also present a multilevel accelerated version. The corresponding software is open-source and implemented in Julia.
  We present strategies to use projections onto multiple constraints to regularize full-waveform inversion for models with salt domes or sedimentary geology.},
  keywords = {Projections, Intersections, Sets, Minkowski set, Inverse problems},
  note = {(SIAM)},
  url = {https://www.pathlms.com/siam/courses/11267/sections/14618/video_presentations/128671},
  url2 = {https://meetings.siam.org/sess/dsp_talk.cfm?p=97656}
}

@CONFERENCE{rizzuti2019SIAMGStwri,
  author = {Gabrio Rizzuti and Mathias Louboutin and Rongrong Wang and Emmanouil Daskalakis and Felix J. Herrmann},
  title = {A dual formulation for time-domain wavefield reconstruction inversion},
  booktitle = {SIAM Conference on Mathematical and Computational Issues in the Geosciences},
  year = {2019},
  month = {03},
  abstract = {Wavefield reconstruction inversion (WRI) is based on the combination of data misfit and PDE-residual penalty terms, and it aims at the reconstruction of the optimal property model and field pair. A full-space optimization would require the storage of field variables for every frequency or time sample (and every source). Remedies via variable projection are well studied, but they entail the solution of a modified wave equation, which is costly both in frequency and time domain. Here, we explore an efficient alternative to WRI, where properties are complemented, instead, by elements of the dual data space. The starting point of our proposal is the denoising reformulation of WRI: the PDE compliance is optimized under the constraint of a given data fit. Since the field variable can be eliminated from the associated saddle-point problem, we obtain a new objective functional of only properties and Lagrange multipliers. Its evaluation, and gradient computation, involves the solution of the original wave equation, which is amenable to time-domain discretization. The optimization scheme of choice can now leverage on affordable storage of the iterates.},
  keywords = {WRI},
  note = {(SIAM GS)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SIAMGS/2019/rizzuti2019SIAMGStwri/rizzuti2019SIAMGStwri.pdf}
}

@CONFERENCE{witte2019SIAMCSEclsmft,
  author = {Philipp A. Witte and Mathias Louboutin and Fabio Luporini and Gerard J. Gorman and Felix J. Herrmann},
  title = {Compressive least squares migration with on-the-fly Fourier transforms},
  booktitle = {SIAM Conference on Computational Science and Engineering},
  year = {2019},
  month = {03},
  abstract = {Least-squares seismic imaging is an inversion-based approach for accurately imaging the earth's subsurface. However, in the time-domain, the computational cost and memory requirements of this approach scale with the size and recording length of the seismic experiment, thus making this approach often prohibitively expensive in practice. To overcome these issues, we borrow ideas from compressive sensing and signal processing and introduce an algorithm for sparsity-promoting seismic imaging using on-the-fly Fourier transforms. By computing gradients and functions values for random subsets of source locations and frequencies, we considerably limit the number of wave equation solves, while on-the-fly Fourier transforms allow computing an arbitrary number of monochromatic frequency-domain wavefields with a time-domain modeling code and without having to solve large-scale Helmholtz equations. The memory requirements of this approach are independent of the number of time steps and solely depend on the number of frequencies, which determine the amount of crosstalk and subsampling artifacts in the image. We show the application of our approach to several large-scale open source data sets and compare the results to a conventional time-domain approach with optimal checkpointing.},
  keywords = {least-squares, migration, imaging, on-the-fly, Fourier},
  note = {(SIAM CSE)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SIAMCSE/2019/witte2019SIAMCSEclsmft/witte2019SIAMCSEclsmft.pdf}
}


%-----2018-----%

@CONFERENCE{peters2018CMSWMissrip,
  author = {Bas Peters and Felix J. Herrmann},
  title = {Intersections and sums of sets for the regularization of inverse problems},
  booktitle = {Canadian Mathematical Society Winter Meeting},
  year = {2018},
  month = {12},
  abstract = {We present new algorithms to compute projections onto the intersection of constraint sets. We focus on problems with multiple sets for which there is no simple and closed-form projection. Different from more classical methods such as Dykstra's algorithm, we do not need other algorithms to solve sub-problems.
Our algorithms are based on the alternating direction method of multipliers and apply to models/images/video on small 2D and large 3D grids because we exploit computational similarity between constraint sets, coarse and fine-grained parallelism, and we also present a multilevel accelerated version.
To obtain more flexible descriptions of prior knowledge, we introduce a formulation that allows constraint sets to be the sum of intersections of sets, which is essentially an extension of a Minkowski set. This formulation builds on the success of additive image descriptions that are usually based on penalty methods, such as cartoon-texture decomposition and robust principal component analysis.
We show applications where we use multiple constraint sets to regularize partial-differential-equation based parameter estimation problems such as seismic waveform inversion, as well as various image and video processing and segmentation tasks.},
  keywords = {Sets, Intersections, Projections, Inverse problems},
  note = {(CMSWM)},
  url = {https://cms.math.ca/Events/winter18/abs/ipi#bp}
}

@CONFERENCE{alfaraj2018SEGswi,
  author = {Ali M. Alfaraj and Rajiv Kumar and Felix J. Herrmann},
  title = {Seismic waveform inversion using decomposed one-way wavefields},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2018},
  month = {10},
  pages = {1379-1383},
  abstract = {Conventional seismic full waveform inversion (FWI) estimates a velocity model by matching the two-way predicted data with the observed data. The non-linearity of the problem and the complexity of the Earth's subsurface, which results in complex wave propagation may lead to unsatisfactory inversion results. Moreover, having inaccurate overburden models, i.e water velocity model in the ocean bottom acquisition setting, may lead to erroneous velocity models. In order to obtain better velocity models, we simplify the FWI problem by decomposing the total two-way observed and predicted wavefields into one-way wavefields at the receiver locations using acoustic wavefield decomposition. We then propose to fit the less-complex one-way wavefield to obtain a velocity model that contains valuable information about the overburden. We use this inverted velocity model as a better initial model for the inversion using the other more-complex one-way wavefield. We demonstrate our proposed algorithm on acoustic non-inverse crime synthetic data produced from the Marmousi2 model. The proposed method provides improved inversion results compared with conventional FWI due to properly accounting for separate model updates from the up- and down-going wavefields.},
  keywords = {SEG, fwi, one-way, decomposition},
  note = {(SEG, Anaheim)},
  doi = {10.1190/segam2018-2998162.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2018/alfaraj2018SEGswi/alfaraj2018SEGswi.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2018/alfaraj2018SEGswi/alfaraj2018SEGswi_pres.pdf}
}

@CONFERENCE{herrmann2018SMCtpa,
  author = {Felix J. Herrmann and Gerard J. Gorman and Jan Hückelheim and Keegan Lensink and Paul Kelly and Navjot Kukreja and Henryk Modzelewski and Michael Lange and Mathias Louboutin and Fabio Luporini and Ali Siahkoohi and Philipp A. Witte},
  title = {The power of abstraction in Computational Exploration Seismology},
  booktitle = {Smoky Mountains Computational Sciences and Engineering Conference},
  year = {2018},
  month = {08},
  abstract = {The field of Computational Exploration Seismology has over the years benefited tremendously from developments in HPC. Back in the 80’s and early 90’s, the oil & gas industry was among the main drivers of HPC technology with a resurgence about ten years ago with the advent of full-waveform inversion-i.e., an adjoint-state method for inverse problems that involve the wave equation. 
While the combination of hardware developments and tedious manual coding efforts have resulted in major improvements, the rate of innovations is slow in part because of the inherent complexities of the mostly monolithic code bases. As a result, innovations make it too slowly into these optimized codes. This stifles innovation and could prevent adaptation of important developments such as machine learning with deep convolutional neural networks into existing workflows.
In comparison, the technological advances in the academic as well as commercial machine learning communities have been much faster in part because of robust and well abstracted code bases such as PyTorch and TensorFlow. Since both machine learning and adjoint-state methods rely on back propagation strong similarities exist ready to be exploited.
During my talk, I will discuss lessons we learned in finding the appropriate abstractions, read Domain Specific Language, for time-stepping stencil-based finite differences in Devito, and how this is relevant for machine learning with deep networks. I will also share a vision on how ideas from machine learning can be merged into the development of the next-generation of wave-equation based imaging codes.
In particular, I will demonstrate the remarkable ability of deep convolutional networks to map low-fidelity dispersed numerical wave simulations to high-fidelity ones. I feel that this apparently generalizable capability of neural nets opens a complete new approach to tackle fundamental problems in computational exploration seismology.},
  keywords = {SMC},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SMC/2018/herrmann2018SMCtpa/herrmann2018SMCtpa_pres.pdf}
}

@CONFERENCE{kadu2018SEGMSFWI,
  author = {Ajinkya Kadu and Rajiv Kumar and Tristan van Leeuwen},
  title = {Full-waveform inversion with Mumford-Shah regularization},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2018},
  month = {10},
  pages = {1258-1262},
  abstract = {Full-waveform inversion (FWI) is a non-linear procedure to es- timate subsurface rock parameters from surface measurements of induced seismic waves. This procedure is ill-posed in nature and hence, requires regularization to enhance some structure depending on the prior information. Recently, Total-variation (TV) regularization has gained popularity due to its ability to produce blocky structures. Contrary to this, the earth behave more like a piecewise smooth function. TV regularization fails to enforce this prior information into FWI. We propose a Mumford-Shah functional to incorporate the piecewise smooth spatial structure in the FWI procedure. The resulting opti- mization problem is solved by a splitting method. We show the improvement in results against TV regularization on two synthetic camembert examples.},
  keywords = {SEG, FWI, Mumford-Shah, Regularization},
  note = {(SEG, Anaheim)},
  doi = {10.1190/segam2018-2997224.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2018/kadu2018SEGMSFWI/kadu2018SEGMSFWI.pdf}
}

@CONFERENCE{kumar2018SEGlreiv,
  author = {Rajiv Kumar and Marie Graff-Kray and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Low-rank representation of extended image volumes––applications to imaging and velocity continuation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2018},
  month = {10},
  pages = {4418-4422},
  abstract = {In this paper, we present a cost-effective low-rank factorized form of computing the full subsurface extended image volumes without explicitly calculating the receiver wavefields.  The propose approach is computationally feasible, which exploits the low-rank structure of full subsurface extended image volumes organized as a matrix, thus avoiding the customary loop over sources.  Using carefully selected stylized examples, we show how conventional migration images can be extracted from the low-rank factorized form. We also present a velocity continuation procedure that uses the same low-rank form and allows us to map the extended image for one velocity model to another velocity model without recomputing all the source wavefields.},
  keywords = {SEG, low-rank, image volume, velocity continuation},
  note = {(SEG, Anaheim)},
  doi = {10.1190/segam2018-2998404.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2018/kumar2018SEGlreiv/kumar2018SEGlreiv.html}
}

@CONFERENCE{louboutin2018SEGeow,
  author = {Mathias Louboutin and Philipp A. Witte and Felix J. Herrmann},
  title = {Effects of wrong adjoints for RTM in TTI media},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2018},
  month = {10},
  pages = {331-335},
  abstract = {In order to obtain accurate images of the subsurface, anisotropic modeling and imaging is necessary. However, the twenty-one parameter complete wave-equation is too computationally expensive to be of use in this case. The transverse tilted isotropic wave-equation is then the next best feasible representation of the physics to use for imaging. The main complexity arising from transverse tilted isotropic imaging is to model the receiver wavefield (back propagation of the data or data residual) for the imaging condition. Unlike the isotropic or the full physics wave-equations, the transverse tilted isotropic wave-equation is not not self-adjoint. This difference means that time-reversal will not model the correct receiver wavefield and this can lead to incorrect subsurface images. In this work, we derive and implement the adjoint wave-equation to demonstrate the necessity of exact adjoint modeling for anisotropic modeling and compare our result with adjoint-free time-reversed imaging.},
  keywords = {SEG, TTI, Imaging, RTM, Adjoint, Finite-differences, Anisotropy},
  note = {(SEG, Anaheim)},
  doi = {10.1190/segam2018-2996274.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2018/louboutin2018SEGeow/louboutin2018SEGeow.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2018/louboutin2018SEGeow/louboutin2018SEGeow_pres.pdf}
}

@CONFERENCE{moldoveanu2018SEGcsbdl,
  author = {Nick Moldoveanu and Philip Bilsby and John Quigley and Rajiv Kumar and Felix J. Herrmann},
  title = {Compressive sensing based design for land and OBS surveys: The noise issue},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2018},
  month = {10},
  pages = {102-106},
  abstract = {Two important developments in seismic acquisition in the last decade were introduction of simultaneous shooting and reconstruction via inversion of the complete seismic wavefield. As the seismic wavefield is typically under sampled in at least one of the four spatial coordinates, both developments could contribute to improve the sampling and, in addition, to increase acquisition efficiency.
Herein, we look at the impact of seismic noise when simultaneous shooting and seismic wavefield reconstruction are implemented for a land or ocean bottom seismic survey.},
  keywords = {SEG},
  note = {(SEG, Anaheim)},
  doi = {10.1190/segam2018-2997759.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2018/moldoveanu2018SEGcsbdl/moldoveanu2018SEGcsbdl.pdf}
}

@CONFERENCE{sharan2018IEEEIUSspoa,
  author = {Shashin Sharan and Rajiv Kumar and Diego S. Dumani and Mathias Louboutin and Rongrong Wang and Stanislav Emelianov and Felix J. Herrmann},
  booktitle={2018 IEEE International Ultrasonics Symposium (IUS)}, 
  title = {Sparsity-promoting photoacoustic imaging with source estimation},
  year = {2018},
  month = {10},
  pages = {206-212},
  abstract = {Photoacoustics has emerged as a high-contrast imaging modality that
provides optical absorption maps inside of tissues, therefore
complementing morphological information of conventional ultrasound. The
laser-generated photoacoustic waves are usually envelope-detected, thus
disregarding the specific waveforms generated by each photoabsorber.
Here we propose a sparsity-promoting image reconstruction method that
allows the estimation of each photoabsorber's source-time function.
Preliminary studies showed the ability to reconstruct the optical
absorption map of an in silico vessel phantom. By using a
sparsity-promoting imaging method, absorption maps and source-time
functions can still be recovered even in situations where the number of
transducers is decreased by a factor of six. Moreover, the recovery is
able to attain higher resolution than conventional beamforming methods.
Because our method recovers the source-time function of the
absorbers, it could potentially also be used to distinguish different
types of photoabsorbers, or the degree of aggregation of exogenous
agents, under the assumption that these would generate different
source-time functions at the moment of laser irradiation.},
  keywords = {IEEE, Transducers, Photoacoustic imaging, Image reconstruction, Acceleration, Estimation, Phantoms, sparsity promoting inversion, source-time function, wave equation, photoacoustic imaging},
  note = {(IEEE IUS, Kobe)},
  doi={10.1109/ULTSYM.2018.8580037}, 
  url = {https://slim.gatech.edu/Publications/Public/Conferences/IEEEIUS/2018/sharan2018IEEEIUSspoa/sharan2018IEEEIUSspoa.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IEEEIUS/2018/sharan2018IEEEIUSspoa/sharan2018IEEEIUSspoa_pres.pdf}
}

@CONFERENCE{sharan2018SEGada,
  author = {Shashin Sharan and Rajiv Kumar and Rongrong Wang and Felix J. Herrmann},
  title = {A debiasing approach to microseismic},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2018},
  month = {10},
  pages = {2942-2946},
  abstract = {Microseismic data is often used to locate fracture locations and their origin in time created by fracking. Although surface microseismic data can have large apertures and is easier to acquire than the borehole data, it often suffers from poor signal to noise ratio (S/R). Poor S/R poses a challenge in terms of estimating the correct location and source-time function of a microseismic source. In this work, we propose a denoising step in combination with a computationally cheap debiasing based approach to locate microseismic sources and to estimate their source-time functions with correct amplitude from extremely noisy data. Through numerical experiments, we demonstrate that our method can work with closely spaced microseismic sources with source-time functions of different peak amplitudes and frequencies. We have also shown the ability of our method with the smooth velocity model.},
  keywords = {SEG, full-waveform inversion, microseismic, imaging, noise, wave equation},
  note = {(SEG, Anaheim)},
  doi = {10.1190/segam2018-2997252.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2018/sharan2018SEGada/sharan2018SEGada.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2018/sharan2018SEGada/sharan2018SEGada_pres.pdf}
}

@CONFERENCE{siahkoohi2018SEGcnn,
  author = {Ali Siahkoohi and Mathias Louboutin and Rajiv Kumar and Felix J. Herrmann},
  title = {Deep Convolutional Neural Networks in prestack seismic---two exploratory examples},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2018},
  month = {10},
  pages = {2196-2200},
  abstract = {Deep convolutional neural networks are having quite an impact and have resulted in step changes in the capabilities of image and voice recognition systems and there are indications they may have a similar impact on post-stack seismic images. While these developments are certainly important, we are after all dealing with an imaging problem involving an unknown earth and not completely understood physics. When the imaging step itself is not handled properly, this may possibly offset gains offered by deep learning. Motivated by our work on deep convolutional neural networks in seismic data reconstruction, we discuss how generative networks can be employed in prestack problems ranging from the relatively mondane removal of the effects of the free surface to dealing with the complex effects of numerical dispersion in time-domain finite differences. While the results we present are preliminary, there are strong indications that generative deep convolutional neural networks can have a major impact on complex tasks in prestack seismic data processing and modeling for inversion.},
  keywords = {SEG, Machine Learning, SRME, Dispersion, Modeling, Processing},
  note = {(SEG, Anaheim)},
  doi = {10.1190/segam2018-2998599.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2018/siahkoohi2018SEGcnn/siahkoohi2018SEGcnn.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2018/siahkoohi2018SEGcnn/siahkoohi2018SEGcnn_pres.pdf}
}

@CONFERENCE{yang2018SEGrde,
  author = {Mengmeng Yang and Rajiv Kumar and Rongrong Wang and Felix J. Herrmann},
  title = {Removing density effects in LS-RTM with a low-rank matched filter},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2018},
  month = {10},
  pages = {4251-4255},
  abstract = {Least-squares reverse-time migration faces difficulties when it inverts the data containing strong components related to density variation with velocity-only Born modeling operator. The strong density perturbation will be inverted as strong dummy velocity perturbations, which influence the amplitudes and phase of the velocity perturbations in the inverted model. The traditional method is to invert the additional density variations by developing Born operator with respect to both density and velocity or modify the image condition. In this work, we develop a matched-filter based LS-RTM for velocity-only Born modeling operator, which removes the artifacts in the imaging created by the strong density variation. This method doesn't call for extra work of finite difference stencil and is more general. In the experiment part, we use a complex discontinuous layered medium with strong density variations at the ocean bottom, and show the efficacy of the propose formulation.},
  keywords = {SEG, Least square, low rank, density effect},
  note = {(SEG, Anaheim)},
  doi = {10.1190/segam2018-2997814.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2018/yang2018SEGrde/yang2018SEGrde.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2018/yang2018SEGrde/yang2018SEGrde_pres.pdf}
}

@CONFERENCE{alfaraj2018EAGEasa,
  author = {Ali M. Alfaraj and Rajiv Kumar and Felix J. Herrmann},
  title = {Automatic statics and residual statics correction with low-rank approximation},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2018},
  month = {06},
  abstract = {Seismic data, mainly on land, suffers from long-wavelength statics
due to the laterally varying and heterogeneous nature of the near-surface
weathering layers. We propose an automatic, data-driven and computationally efficient
statics correction method based on low-rank approximation to correct for such
statics. The method does not require a model to estimate static time shifts,
which is the case for other static correction methods; rather it applies the
appropriate static corrections on the data such that it becomes low rank in a
certain domain. As of now, the method is applicable to data that has been
corrected for elevation statics. Due to the near-surface irregularities and
due to approximations used by static correction methods that lead to not fully
correcting for statics, an iterative residual statics correction becomes
necessary. Our proposed method corrects for residual statics without the
necessity of the surface consistency assumption and a multi-iterate process.
Additional benefits of the method include artifacts and noise suppression. We
demonstrate the successful application of our method on several synthetic
data examples.},
  keywords = {EAGE, statics, residual statics, rank, land},
  note = {(EAGE, Copenhagen)},
  doi = {10.3997/2214-4609.201801107},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2018/alfaraj2018EAGEasa/alfaraj2018EAGEasa.html},
 presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2018/alfaraj2018EAGEasa/alfaraj2018EAGEasa_pres.pdf}
}

@CONFERENCE{kadu2018EAGEdfwi,
  author = {Ajinkya Kadu and Rajiv Kumar},
  title = {Decentralized full-waveform inversion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2018},
  month = {06},
  abstract = {With the advent of efficient seismic data acquisition, we are
having a surplus of seismic data, which is improving the imaging of the earth using
full-waveform inversion. However, such inversion suffers from many issues,
including (i) substantial network waiting time due to repeated communications of
function and gradient values in the distributed environment, and (ii)
requirement of the sophisticated optimizer to solve an optimization problem
involving non-smooth regularizers. To circumvent these issues, we propose a
decentralized full-waveform inversion, a scheme where connected agents in a
network optimize their objectives locally while being in consensus. The
proposed formulation can be solved using the ADMM method efficiently. We demonstrate
using the standard marmousi model that such scheme can decouple the
regularization from data fitting and reduce the network waiting time.},
  keywords = {EAGE},
  note = {(EAGE, Copenhagen)},
  doi = {10.3997/2214-4609.201801230},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2018/kadu2018EAGEdfwi/kadu2018EAGEdfwi.pdf}
}

@CONFERENCE{kumar2018EAGEcsb,
  author = {Rajiv Kumar and Shashin Sharan and Nick Moldoveanu and Felix J. Herrmann},
  title = {Compressed sensing based land simultaneous acquisition using encoded sweeps},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2018},
  month = {06},
  abstract = {Simultaneous shooting methods using encoded sweeps can enhance the
productivity of land acquisition in situations where deployment of many
vibrators and larger receiver spread is not possible in the field due to
obstructions or permit limitations. However, the existing framework requires
shooting the full sequence of encoded sweeps on each shot point to
reconstruct the complete frequency bandwidth Green's function. Although this simultaneous
shooting method reduces the sweeping time vs conventional sequential
shooting, the gain in efficiency is limited. To further reduce the sweeping time, we
propose to acquire randomly selected subsets of the encoded sweeps sequences
followed by a rank-minimization based joint source separation and spectral
interpolation framework to reconstruct the full bandwidth deblended Green's
function. We demonstrate the advantages of proposed sampling and
reconstruction framework using a synthetic seismic line simulated using SEG-SEAM Phase II
land velocity and density model.},
  keywords = {EAGE, compressed sensing, source separation, spectral interpolation, Land, SEAM},
  note = {(EAGE, Copenhagen)},
  doi = {10.3997/2214-4609.201800643},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2018/kumar2018EAGEcsb/kumar2018EAGEcsb.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2018/kumar2018EAGEcsb/kumar2018EAGEcsb_pres.pdf}
}

@CONFERENCE{siahkoohi2018EAGEsdr,
  author = {Ali Siahkoohi and Rajiv Kumar and Felix J. Herrmann},
  title = {Seismic data reconstruction with Generative Adversarial Networks},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2018},
  month = {06},
  abstract = {A main challenge in seismic imaging is acquiring densely sampled data. Compressed Sensing has provided theoretical foundations upon which desired sampling rate can be achieved by applying a sparsity promoting algorithm on sub-sampled data. The key point in successful recovery is to deploy a randomized sampling scheme. In this paper, we propose a novel deep learning-based method for fast and accurate reconstruction of heavily under-sampled seismic data, regardless of type of sampling. A neural network learns to do reconstruction directly from data via an adversarial process. Once trained, the reconstruction can be done by just feeding the frequency slice with missing data into the neural network. This adaptive nonlinear model makes the algorithm extremely flexible, applicable to data with arbitrarily type of sampling. With the assumption that we have access to training data, the quality of reconstructed slice is superior even for extremely low sampling rate (as low as 10%) due to the data-driven nature of the method.},
  keywords = {EAGE, data reconstruction, machine learning, generative adversarial networks},
  note = {(EAGE, Copenhagen)},
  doi = {10.3997/2214-4609.201801393},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2018/siahkoohi2018EAGEsdr/siahkoohi2018EAGEsdr.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2018/siahkoohi2018EAGEsdr/siahkoohi2018EAGEsdr_pres.pdf}
}

%-----2017-----%

@CONFERENCE{alfaraj2017SEGrsw,
  author = {Ali M. Alfaraj and Rajiv Kumar and Felix J. Herrmann},
  title = {Reconstruction of {S-waves} from low-cost randomized and simultaneous acquisition by joint sparse inversion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2017},
  month = {09},
  pages = {2533-2538},
  abstract = {Spatial Nyquist sampling rate, which is proportional to
                  the apparent subsurface velocity, is the key
                  deciding cost factor for conventional seismic
                  acquisition. Due to the lower shear wave velocity
                  compared with compressional waves, finer spatial
                  sampling is required to properly record the earlier,
                  which increases the acquisition costs. This is one
                  of the reasons why shear waves are usually not
                  considered in practice. To avoid having the Nyquist
                  criterion as the deciding cost factor and to utilize
                  multicomponent data to its available full extent, we
                  propose acquiring randomly undersampled ocean bottom
                  seismic data. Each component can then be
                  interpolated separately, followed by elastic
                  decomposition to recover up- and down-going
                  S-waves. Instead, we jointly interpolate and
                  decompose the recorded multicomponent data by
                  solving one sparsity promoting optimization
                  problem. This way we ensure that the relative
                  amplitudes of the multicomponent data is
                  preserved. We compare two sparsifying transforms:
                  the curvelet transform and the frequency-wavenumber
                  transform. Another key cost deciding factor for
                  seismic acquisition is the efficiency of acquiring
                  data. This calls for simultaneous acquisition, which
                  requires a source separation step. Similarly,
                  instead of taking a two-step approach, we perform a
                  sparsity-promoting joint source separation
                  decomposition. Results on economically and
                  efficiently acquired synthetic data of both joint
                  methods show their ability of reconstructing
                  accurate up- and down-going S-waves.},
  keywords = {SEG, elastic, shear wave, randomized acquisition, interpolation, source separation, decomposition, sparse},
  note = {(SEG, Houston)},
  doi = {10.1190/segam2017-17681376.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/alfaraj2017SEGrsw/alfaraj2017SEGrsw.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/alfaraj2017SEGrsw/alfaraj2017SEGrsw_pres.pdf}
}


@CONFERENCE{daskalakis2017SEGdds,
  author = {Emmanouil Daskalakis and Rachel Kuske and Felix J. Herrmann},
  title = {Developments in the direction of solving extremly large problems in {Geophysics}},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2017},
  month = {09},
  pages = {4375-4378},
  abstract = {Often in exploration Geophysics we are forced to work
                  with extremely large problems. Acquisitions via
                  dense grids of receivers translate into very large
                  mathematical systems. Usually, depending on the
                  acquisition, the size of the matrix of the system to
                  be solved, can be measured in the millions. The
                  proper way to address this problem is by
                  subsampling. Even though subsampling can reduce the
                  computational efforts required, it can not address
                  stability problems caused by preconditioning and/or
                  instrumental response errors. In this abstract, we
                  introduce a modification of the linearized Bregman
                  solver for these large problems that resolves
                  stability issues.},
  keywords = {SEG, linearized Bregman, cycling, weighted increment, least-squares migration},
  note = {(SEG, Houston)},
  doi = {10.1190/segam2017-17795188.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/daskalakis2017SEGdds/daskalakis2017SEGdds.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/daskalakis2017SEGdds/daskalakis2017SEGdds_pres.pdf}
}


@CONFERENCE{sharan2017SEGhrf,
  author = {Shashin Sharan and Rongrong Wang and Felix J. Herrmann},
  title = {High resolution fast microseismic source collocation and source time function estimation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2017},
  month = {09},
  pages = {2778-2783},
  abstract = {Sparsity promotion based joint microseismic source
                  collocation and source time function estimation,
                  using Linearized Bregman algorithm, although simple
                  to implement, suffers from slow convergence. This is
                  due to the fact that Linearized Bregman algorithm
                  has only first order of convergence. In this work,
                  we propose to accelerate the existing Linearized
                  Bregman algorithm using the L-BFGS
                  algorithm. Without any initial guess for the source
                  location or source time function, our method is able
                  to estimate the source location and source time
                  function for kinematically correct smooth velocity
                  model. We demonstrate the effectiveness of our
                  method for multiple sources spaced within half a
                  wavelength. We also compare our results with
                  Linearized Bregman based method in ``2.5``D instead
                  of ``2``D.},
  keywords = {SEG, algorithm, full-waveform inversion, microseismic, passive imaging, wave equation},
  note = {(SEG, Houston)},
  doi = {10.1190/segam2017-17787749.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/sharan2017SEGhrf/sharan2017SEGhrf.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/sharan2017SEGhrf/sharan2017SEGhrf_pres.pdf}
}


@CONFERENCE{wang2017SEGdff,
  author = {Rongrong Wang and Felix J. Herrmann},
  title = {A denoising formulation of full-waveform inversion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2017},
  month = {09},
  pages = {1594-1598},
  abstract = {We propose a wave-equation-based subsurface inversion
                  method that in many cases is more robust than the
                  conventional Full-Waveform Inversion. The new
                  formulation is written in a denoising form that
                  allows the synthetic data to match the observed ones
                  up to a small error. Compared to the Full-Waveform
                  Inversion, our method treats the noise arising from
                  the data measuring/recording process and that from
                  the synthetic modelling process
                  separately. Comparing to the Wavefields
                  Reconstruction Inversion, the new formulation
                  mitigates the difficulty of choosing the penalty
                  parameter $\lambda$. To solve the proposed
                  optimization problem, we develop an efficient
                  frequency domain algorithm that alternatively
                  updates the model and the data. Numerical
                  experiments confirm strong stability of the proposed
                  method by comparisons between the results of our
                  algorithm with that from both plain FWI and a
                  weighted formulation of the FWI.},
  keywords = {SEG, FWI, denoising},
  note = {(SEG, Houston)},
  doi = {10.1190/segam2017-17794690.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/wang2017SEGdff/wang2017SEGdff.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/wang2017SEGdff/wang2017SEGdff_pres.pdf}
}


@CONFERENCE{yang2017SEGfsp,
  author = {Mengmeng Yang and Emmanouil Daskalakis and Felix J. Herrmann},
  title = {Fast sparsity-promoting least-squares migration with multiples in time domain},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2017},
  month = {09},
  pages = {4828-4832},
  abstract = {Based on the latest developments of research in
                  inversion technology with optimization, researchers
                  have made significant progress in the implementation
                  of least-squares reverse-time migration (LS-RTM) of
                  primaries. In Marine data however, these
                  applications rely on the success of a pre-imaging
                  separation of primaries and multiples, which can be
                  modeled as a multi-dimensional convolution between
                  the vertical derivative of the surface-free Green’s
                  function and the down-going receiver
                  wavefield. Instead of imaging the primaries and
                  multiples separately, we implement the LS-RTM of the
                  total down-going wavefield by combining areal source
                  injection and linearized Born modelling, where
                  strong surface related multiples are generated from
                  a strong density variation at the ocean bottom. The
                  advantage including surface related multiples in
                  LS-RTM is the extra illumination we obtain from
                  these multiples without incurring additional
                  computational costs related to carrying out
                  multi-dimensional convolutions part of conventional
                  multiple prediction procedures. Even though we are
                  able to avert these computational costs, our
                  approach shares the large costs of LSRTM. We reduce
                  these costs by combining randomized source
                  subsampling with our sparsity-promoting imaging
                  technology, which produces artifact-free,
                  high-resolution images, with the surface-related
                  multiples migrated properly.},
  keywords = {SEG, sparsity, least-squares RTM, multiples, time domain},
  note = {(SEG, Houston)},
  doi = {10.1190/segam2017-17741608.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/yang2017SEGfsp/yang2017SEGfsp.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/yang2017SEGfsp/yang2017SEGfsp_pres.pdf}
}


@CONFERENCE{yang2017SEGWStds,
  author = {Mengmeng Yang and Felix J. Herrmann},
  title = {Time domain sparsity promoting LSRTM with surface-related multiples in shallow-water case},
  booktitle = {SEG Workshop on Multiples: Status, Progress, Challenges and Open issues; Houston},
  year = {2017},
  month = {09},
  abstract = {In the SRME relation, multiples are expressed as the multi- dimensional convolution between the the vertical derivative of the surface-free Green’s function and the down-going receiver wavefield. This relation leads to the methods that separate the surface-related multiples and primaries. Instead of imaging separated multiples trivially, we introduced the SRME relation into wave equation by areal source injection, which costs nothing extra to involve multiples into the forward wavefields. The related image contains the phantoms components from cross-correlation between different-orders of events. Especially in shallow water, due to the strong magnitudes of multiples and their overlap with primaries, the artifacts contain strong phantoms not only of ocean bottom but also subsurface layer interfaces. We use sparsity-promoting inversions where the nicely curved structure of the subsurface models are detected to help cleaning up the phantoms. We reduce the costs by combin- ing randomized source subsampling with linearized Bregman method and get cleaned image with one data pass.},
  keywords = {SEG, workshop, multiple},
  note = {(SEG Workshop, Houston)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/yang2017SEGWStds/yang2017SEGWStds.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/yang2017SEGWStds/yang2017SEGWStds_post.pdf}
}


@CONFERENCE{zhang2017SEGmsd,
  author = {Yiming Zhang and Curt Da Silva and Rajiv Kumar and Felix J. Herrmann},
  title = {Massive {3D} seismic data compression and inversion with hierarchical {Tucker}},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2017},
  month = {09},
  pages = {1347-1352},
  abstract = {Modern-day oil and gas exploration, especially in areas
                  of complex geology such as fault belts and sub-salt
                  areas, is an increasingly expensive and risky
                  endeavour. Typically long-offset and dense sampling
                  seismic data are required for subsequent shot based
                  processing procedures, e.g. wave-equation based
                  inversion (WEI) and surface-related multiple
                  elimination (SRME). However, these strict
                  requirements result in an exponential growth in data
                  volume size and prohibitive demands on computational
                  resources, given the multidimensional nature of the
                  data volumes. Moreover the physical constraints and
                  cost limitations impose restrictions on acquiring
                  fully sampled data. In this work, we propose to
                  invert our large-scale data from a set of subsampled
                  measurements, resulting in an estimate of the true
                  volume in a compressed low-rank tensor
                  format. Rather than expanding the data to its
                  fully-sampled form for later downstream processes,
                  we demonstrate how to use this compressed data
                  directly via on-the-fly common shot or receiver
                  gathers extraction. The combination of massive
                  compression and fast on demand data reconstruction
                  of 3D shot or receiver gathers leads to a
                  substantial reduction in memory costs but with
                  minimal effects on results in the subsequent
                  processing procedures. We demonstrate the effective
                  implementation of our proposed framework on
                  full-waveform inversion on a 3D seismic synthetic
                  data set generated from a Overthrust model.},
  keywords = {SEG, 3D, tensor algebra, FWI},
  note = {(SEG, Houston)},
  doi = {10.1190/segam2017-17742951.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/zhang2017SEGmsd/zhang2017SEGmsd.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/zhang2017SEGmsd/zhang2017SEGmsd_pres.pdf}
}


@CONFERENCE{alfaraj2017EAGEswr,
  author = {Ali M. Alfaraj and Rajiv Kumar and Felix J. Herrmann},
  title = {Shear wave reconstruction from low cost randomized acquisition},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2017},
  month = {06},
  abstract = {Shear waves travel in the subsurface at a lower speed
                  compared with compressional waves. Therefore, much
                  finer spatial sampling is required to properly
                  record the shear waves. This leads to higher
                  acquisition costs which are typically avoided by
                  designing surveys geared towards only compressional
                  waves imaging. We propose using randomly
                  under-sampled ocean bottom acquisition for recording
                  both compressional and shear waves. The recorded
                  multicomponent data is then interpolated using an
                  SVD-free low rank interpolation scheme that is
                  feasible for large scale seismic data volumes to
                  obtain finely sampled data. Following that, we
                  perform elastic wavefield decomposition at the ocean
                  bottom to recover accurate up- and dow-going
                  S-waves. Synthetic data results indicate that using
                  randomized under-sampled acquisition, we can recover
                  accurate S-waves with an economical cost compared
                  with conventional acquisition designs.},
  keywords = {EAGE, shear waves, rank minimization, interpolation, randomized acquisition},
  note = {(EAGE, Paris)},
  doi = {10.3997/2214-4609.201700594},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2017/alfaraj2017EAGEswr/alfaraj2017EAGEswr.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2017/alfaraj2017EAGEswr/alfaraj2017EAGEswr_pres.pdf}
}


@CONFERENCE{fang2017AIPepm,
  author = {Zhilong Fang and Curt Da Silva and Felix J. Herrmann},
  title = {An efficient penalty method for {PDE}-constrained optimization problem with source estimation and stochastic optimization},
  booktitle = {Applied Inverse Problems Annual Conference Proceedings},
  year = {2017},
  month = {05-06},
  pages = {40},
  abstract = {Many inverse problems in applied science and engineering
                  can be written as PDE-constrained optimization
                  problems. Typically, one is interested in estimating
                  the unknown medium parameters of the PDE describing
                  the system, given multi-source measurements of the
                  fields in the true medium. The objective function is
                  often very oscillatory, which gives rise to
                  problematic local minima, and merely evaluating one
                  or more of its derivatives requires solving multiple
                  PDEs, which are expensive. In many applications such
                  as seismic exploration, only the source location is
                  available and the source signature itself is
                  unknown, which results in erroneous estimated
                  parameters. In this work, we propose a penalty
                  method to solve the PDE-constrained optimization
                  problem, whereby we replace the PDE constraints by a
                  two-norm penalty term, which leads to a bi-linear
                  optimization problem with a much less oscillatory
                  objective function. By applying the variable
                  projection method, we develop an efficient method to
                  invert the unknown parameters and source signatures
                  simultaneously. To reduce the computational cost, we
                  randomly select a subset of all data to update the
                  parameters at each iteration. Numerical examples
                  demonstrate that this method is less prone to
                  becoming stuck in harmful local minima and is able
                  to successfully invert the unknown parameters and
                  source signature with less computational cost
                  compared to using all the data.},
  keywords = {optimization, source estimation, stochastic optimization, penalty method},
  note = {(AIP, Hangzhou)},
  url = {http://aip2017.csp.escience.cn/dct/page/1},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/AIP/2017/fang2017AIPepm/fang2017AIPepm_pres.pdf}
}


@CONFERENCE{fang2017WAVESuqf,
  author = {Zhilong Fang and Curt Da Silva and Rachel Kuske and Felix J. Herrmann},
  title = {Uncertainty quantification for inverse problems with a weak wave-equation constraint},
  booktitle = {WAVES 2017 --- 13th International Conference on Mathematical and Numerical Aspects of Wave Propagation},
  year = {2017},
  month = {05},
  pages = {127-128},
  abstract = {In this work, we present a new posterior distribution to
                  quantify uncertainties in solutions of wave-equation
                  based inverse problems. By introducing an auxiliary
                  variable for the wavefields, we weaken the strict
                  wave-equation constraint used by conventional
                  Bayesian approaches. With this weak constraint, the
                  new posterior distribution is a bi-Gaussian
                  distribution with respect to both model parameters
                  and wavefields, which can be directly sampled by the
                  Gibbs sampling method.},
  keywords = {uncertainty, wave equation, constraint, Gibbs sampling},
  note = {(WAVES, Minneapolis)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/WAVES/2017/fang2017WAVESuqf/fang2017WAVESuqf.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/WAVES/2017/fang2017WAVESuqf/fang2017WAVESuqf_pres.pdf}
}


@CONFERENCE{herrmann2017SIAMdsa,
  author = {Felix J. Herrmann and Curt Da Silva},
  title = {Domain-specific abstractions for full-waveform inversion},
  booktitle = {SIAM Conference on Computational Science and Engineering},
  year = {2017},
  month = {02-03},
  keywords = {SIAM, FWI, inverse problems, software},
  note = {(SIAM Conference on Computational Science and Engineering, Atlanta)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2017/herrmann2017SIAMdsa/herrmann2017SIAMdsa_pres.pdf}
}


@CONFERENCE{kukreja2016OGHPClsm,
  author = {Navjot Kukreja and Mathias Louboutin and Michael Lange and Fabio Luporini and Gerard Gorman},
  title = {Leveraging symbolic math for rapid development of applications for seismic modeling},
  year = {2017},
  month = {03},
  booktitle = {OGHPC},
  abstract = {Wave propagation kernels are the core of many commonly
                  used algorithms for inverse problems in exploration
                  geophysics. While they are easy to write and analyze
                  for the simplied cases, the code quickly becomes
                  complex when the physics needs to be made more
                  precise or the performance of these codes is to be
                  optimized. Signicant eort is repeated every time new
                  forms of physics need to be implemented, or a new
                  computing platform to be supported. The use of
                  symbolic mathematics as a domain specic language
                  (DSL) for input, combined with automatic generation
                  of high performance code customized for the target
                  hardware platform is a promising approach to
                  maximize code reuse. Devito is a DSL for nite
                  dierence that uses symbolic mathematics to generate
                  optimized code for wave propagation based on a
                  provided wave equation. It enables rapid application
                  development in a eld where the average time spent on
                  development has historically been in weeks and
                  months. The Devito DSL system is completely wrapped
                  within the Python programming language and the fact
                  that the running code is in C is completely
                  transparent, making it simple to include Devito as
                  part of a larger work ow including multiple
                  applications over a large cluster.},
  keywords = {OGHPC, finite differences, HPC, modelling, time domain},
  note = {(Oil and Gas HPC Conference, Rice University)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/OGHPC/2017/kukreja2016OGHPClsm/kukreja2016OGHPClsm.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/OGHPC/2017/kukreja2016OGHPClsm/kukreja2016OGHPClsm_pres.pdf}
}


@CONFERENCE{kukreja2017SIAMdsm,
  author = {Navjot Kukreja and Michael Lange and Mathias Louboutin and Fabio Luporini and Gerard Gorman},
  title = {Devito: symbolic math for automated fast finite difference computations},
  booktitle = {SIAM Conference on Computational Science and Engineering},
  year = {2017},
  month = {02-03},
  keywords = {SIAM, devito, finite differences, high performance computing, modelling, acoustic, compiler, stencil},
  note = {(SIAM Conference on Computational Science and Engineering, Atlanta)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2017/kukreja2017SIAMdsm/kukreja2017SIAMdsm_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2017/kukreja2017SIAMdsm/kukreja2017SIAMdsm_pres.mov}
}


@CONFERENCE{kumar2017EAGEdha,
  author = {Rajiv Kumar and Nick Moldoveanu and Felix J. Herrmann},
  title = {Denoising high-amplitude cross-flow noise using curvelet-based stable principle component pursuit},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2017},
  month = {06},
  abstract = {Removal of high-amplitude cross-flow noise in marine
                  towed-streamer acquisition is of great interest
                  because cross-flow noise hinders the success of
                  subsequent processing (e.g. EPSI) and
                  migration. However, the removal of cross-flow noise
                  is a challenging process because cross-flow noise
                  dominates steep angles and low-frequency components
                  of the signal. As a result, applying a simple
                  high-pass filter can result in a loss of coherent
                  diving waves and reflected energy. We propose a
                  stable curvelet-based principle-component pursuit
                  approach that does not suffer from this shortcoming
                  because it uses angle- and scale-adaptivity of the
                  curvelet transform in combination with the low-rank
                  property of cross-flow noise. As long as the
                  cross-flow noise exhibits low-rank in the curvelet
                  domain, our method successfully separates this
                  signal component from the diving waves and seismic
                  reflectivity, which is well-know to be sparse in the
                  curvelet domain. Experimental results on a
                  common-shot gather extracted from a coil shooting
                  survey in the Gulf of Mexico shows the potential of
                  our approach.},
  keywords = {EAGE, denoising, coil data, cross-flow noise, curvelet, SPCP},
  note = {(EAGE, Paris)},
  doi = {10.3997/2214-4609.201701055},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2017/kumar2017EAGEdha/kumar2017EAGEdha.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2017/kumar2017EAGEdha/kumar2017EAGEdha_poster.pdf}
}


@CONFERENCE{lange2017SCIPYofd,
  author = {Michael Lange and Navjot Kukreja and Fabio Luporini and Mathias Louboutin and Charles Yount and Jan H\"uckelheim and Gerard Gorman},
  title = {Optimised finite difference computation from symbolic equations},
  booktitle = {Python in Science Conference Proceedings},
  year = {2017},
  month = {07},
  pages = {89-96},
  abstract = {Domain-specific high-productivity environments are
                  playing an increasingly important role in scientific
                  computing due to the levels of abstraction and
                  automation they provide. In this paper we introduce
                  Devito, an open-source domain-specific framework for
                  solving partial differential equations from symbolic
                  problem definitions by the finite difference
                  method. We highlight the generation and automated
                  execution of highly optimized stencil code from only
                  a few lines of high-level symbolic Python for a set
                  of scientific equations, before exploring the use of
                  Devito operators in seismic inversion problems.},
  keywords = {finite difference, domain-specific languages, symbolic Python},
  note = {(SciPy, Texas)},
  url = {http://conference.scipy.org/proceedings/scipy2017/michael_lange.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SciPy/2017/lange2017SCIPYofd/lange2017SCIPYofd_pres.pdf}
}


@CONFERENCE{louboutin2017EAGEdns,
  author = {Mathias Louboutin and Llu\'{i}s Guasch and Felix J. Herrmann},
  title = {Data normalization strategies for full-waveform inversion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2017},
  month = {06},
  abstract = {Amplitude mismatch is an inherent problem in seismic
                  inversion. Most of the source estimation techniques
                  are associated with amplitude uncertainty due to
                  incomplete representation of the physics or
                  estimation method parameters. Rewriting the
                  inversion problem in an amplitude free formulation
                  allows to mitigate the amplitude ambiguity and help
                  the inversion process to converge. We present in
                  this work two different strategies to lessen
                  amplitude effects in seismic inversion, derive the
                  corresponding update directions and show how we
                  handle scaling error correctly in both the objective
                  function and the gradient.},
  keywords = {EAGE, FWI, normalization, inversion, nonlinear},
  note = {(EAGE, Paris)},
  doi = {10.3997/2214-4609.201700720},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2017/louboutin2017EAGEdns/louboutin2017EAGEdns.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2017/louboutin2017EAGEdns/louboutin2017EAGEdns_poster.pdf}
}


@CONFERENCE{louboutin2017EAGEess,
  author = {Mathias Louboutin and Felix J. Herrmann},
  title = {Extending the search space of time-domain adjoint-state {FWI} with randomized implicit time shifts},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2017},
  month = {06},
  abstract = {Adjoint-state full-waveform inversion aims to obtain
                  subsurface properties such as velocity, density or
                  anisotropy parameters, from surface recorded
                  data. As with any (non-stochastic) gradient based
                  optimization procedure, the solution of this
                  inversion procedure is to a large extend determined
                  by the quality of the starting model. If this
                  starting model is too far from the true model, these
                  derivative-based optimizations will likely end up in
                  local minima and erroneous inversion results. In
                  certain cases, extension of the search space,
                  e.g. by making the wavefields or focused matched
                  sources additional unknowns, has removed some of
                  these non-uniqueness issues but these rely on
                  time-harmonic formulations. Here, we follow a
                  different approach by combining an implicit
                  extension of the velocity model, time compression
                  techniques and recent results on stochastic sampling
                  in non-smooth/non-convex optimization},
  keywords = {EAGE, time domain, FWI, cycle skipping, inversion, nonconvex},
  note = {(EAGE, Paris)},
  doi = {10.3997/2214-4609.201700831},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2017/louboutin2017EAGEess/louboutin2017EAGEess.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2017/louboutin2017EAGEess/louboutin2017EAGEess_pres.pdf}
}


@CONFERENCE{louboutin2017SIAMras,
  author = {Mathias Louboutin and Michael Lange and Navjot Kukreja and Fabio Luporini and Felix J. Herrmann and Gerard Gorman},
  title = {Raising the abstraction to separate concerns: enabling different physics for geophysical exploration},
  booktitle = {SIAM Conference on Computational Science and Engineering},
  year = {2017},
  month = {02-03},
  abstract = {Full-waveform inversion is a PDE-constrained
                  optimisation problem involving massive amounts of
                  data (petabytes) and large numbers of unknowns
                  (O(10^9)). This well known compute-intensive and
                  data-intensive is extremely challenging for several
                  reasons. First, there is the complexity of having to
                  handle extremely large data volumes with metadata
                  related to experimental details in the field, and
                  the discretization of the unknown earth parameters
                  and approximate physics. Second, reduced or
                  adjoint-state methods call for computationally
                  intensive PDE solves for each source experiment (of
                  which there are thousands) for each iteration of a
                  gradient-based optimization scheme. The talks will
                  give an overview how carefully chosen layers of
                  abstraction can help manage both the complexity and
                  scale of inversion while still achieving the high
                  degree of computational performance required to make
                  full-waveform a practical tool. Specifically, the
                  presentations will focus on domain specific stencil
                  language for time-stepping methods to solve various
                  types of wave equations and on abstracts for
                  large-scale parallel optimization frameworks.},
  keywords = {SIAM, time domain, modelling, finite differences, HPC},
  note = {(SIAM Conference on Computational Science and Engineering, Atlanta)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2017/louboutin2017SIAMras/louboutin2017SIAMras_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2017/louboutin2017SIAMras/louboutin2017SIAMras_pres.mov}
}


@CONFERENCE{oghenekohwo2017EAGEitl,
  author = {Felix Oghenekohwo and Felix J. Herrmann},
  title = {Improved time-lapse data repeatability with randomized sampling and distributed compressive sensing},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2017},
  month = {06},
  abstract = {Recently, new ideas on randomized sampling for
                  time-lapse seismic acquisition have been proposed to
                  address some of the challenges of replicating
                  time-lapse surveys. These ideas, which stem from
                  distributed compressed sensing (DCS) led to the
                  birth of a joint recovery model (JRM) for processing
                  time-lapse data (noise-free) acquired from
                  non-replicated acquisition geometries. However, when
                  the earth does not change---i.e. no time-lapse—the
                  recovered vintages from two non-replicated surveys
                  should show high repeatability measured in terms of
                  normalized RMS, which is a standard metric for
                  quantifying time-lapse data repeatability. Under
                  this assumption of no time-lapse change, we
                  demonstrate improved repeatability (with JRM) of the
                  recovered data from non-replicated random samplings,
                  first with noisy data and secondly in situations
                  where there are calibration errors i.e. where the
                  acquisition parameters such as source/receiver
                  coordinates are not precise.},
  keywords = {EAGE, repeatability, time lapse, compressive sensing, calibration, noise},
  note = {(EAGE, Paris)},
  doi = {10.3997/2214-4609.201701389},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2017/oghenekohwo2017EAGEitl/oghenekohwo2017EAGEitl.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2017/oghenekohwo2017EAGEitl/oghenekohwo2017EAGEitl_poster.pdf}
}


@CONFERENCE{witte2017EAGEspl,
  author = {Philipp A. Witte and Mengmeng Yang and Felix J. Herrmann},
  title = {Sparsity-promoting least-squares migration with the linearized inverse scattering imaging condition},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2017},
  month = {06},
  abstract = {Reverse-time migration (RTM) with the conventional
                  cross-correlation imaging condition suffers from
                  low-frequency artifacts that result from
                  backscattered energy in the background velocity
                  models. This problem translates to least-squares
                  reverse-time migration (LS-RTM), where these
                  artifacts slow down the convergence, as many of the
                  initial iterations are spent on removing them. In
                  RTM, this problem has been successfully addressed by
                  the introduction of the so-called inverse scattering
                  imaging condition, which naturally removes these
                  artifacts. In this work, we derive the corresponding
                  linearized forward operator of the inverse
                  scattering imaging operator and incorporate this
                  forward/adjoint operator pair into a
                  sparsity-promoting (SPLS-RTM) workflow. We
                  demonstrate on a challenging salt model, that LS-RTM
                  with the inverse scattering imaging condition is far
                  less prone to low-frequency artifacts than the
                  conventional cross-correlation imaging condition,
                  improves the convergence and does not require any
                  type of additional image filters within the
                  inversion. Through source subsampling and sparsity
                  promotion, we reduce the computational cost in terms
                  of PDE solves to a number comparable to conventional
                  RTM, making our workflow applicable to large-scale
                  problems.},
  keywords = {EAGE, least-squares migration, imaging condition, linearized Bregman},
  note = {(EAGE, Paris)},
  doi = {10.3997/2214-4609.201701125},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2017/witte2017EAGEspl/witte2017EAGEspl.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2017/witte2017EAGEspl/witte2017EAGEspl_poster.pdf}
}


@CONFERENCE{witte2017SIAMlsw,
  author = {Philipp A. Witte and Mathias Louboutin and Felix J. Herrmann},
  title = {Large-scale workflows for wave-equation based inversion in {Julia}},
  booktitle = {SIAM Conference on Computational Science and Engineering},
  year = {2017},
  month = {02-03},
  abstract = {Full-waveform inversion is a PDE-constrained
                  optimisation problem involving massive amounts of
                  data (petabytes) and large numbers of unknowns
                  (O(10^9)). This well known compute-intensive and
                  data-intensive is extremely challenging for several
                  reasons. First, there is the complexity of having to
                  handle extremely large data volumes with metadata
                  related to experimental details in the field, and
                  the discretization of the unknown earth parameters
                  and approximate physics. Second, reduced or
                  adjoint-state methods call for computationally
                  intensive PDE solves for each source experiment (of
                  which there are thousands) for each iteration of a
                  gradient-based optimization scheme. The talks will
                  give an overview how carefully chosen layers of
                  abstraction can help manage both the complexity and
                  scale of inversion while still achieving the high
                  degree of computational performance required to make
                  full-waveform a practical tool. Specifically, the
                  presentations will focus on domain specific stencil
                  language for time-stepping methods to solve various
                  types of wave equations and on abstracts for
                  large-scale parallel optimization frameworks.},
  keywords = {SIAM, Julia, full-waveform inversion, least-squares migration, large scale},
  note = {(SIAM Conference on Computational Science and Engineering, Atlanta)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2017/witte2017SIAMlsw/witte2017SIAMlsw_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2017/witte2017SIAMlsw/witte2017SIAMlsw_pres.mov}
}


%-----2016-----%

@CONFERENCE{bougher2016SEGava,
  author = {Ben B. Bougher and Felix J. Herrmann},
  title = {AVA classification as an unsupervised machine-learning problem},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2016},
  month = {10},
  pages = {553-556},
  abstract = {Much of AVA analysis relies on characterizing background
                  trends and anomalies in pre-stack seismic
                  data. Analysts reduce a seismic section into a small
                  number of these trends and anomalies, suggesting
                  that a low-dimensional structure can be inferred
                  from the data. We describe AVA-attribute
                  characterization as an unsupervised-learning
                  problem, where AVA classes are learned directly from
                  the data without any prior assumptions on physics
                  and geological settings. The method is demonstrated
                  on the Marmousi II elastic model, where a gas
                  reservoir was successfully delineated from a
                  background trend in a depth migrated image.},
  keywords = {AVA, machine learning, SEG},
  note = {(SEG, Dallas)},
  doi = {10.1190/segam2016-13874419.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/bougher2016SEGava/bougher2016SEGava.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/bougher2016SEGava/bougher2016SEGava_pres.pdf}
}


@CONFERENCE{chai2016SEGlbm,
  author = {Xintao Chai and Mengmeng Yang and Philipp A. Witte and Rongrong Wang and Zhilong Fang and Felix J. Herrmann},
  title = {A linearized {Bregman} method for compressive waveform inversion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2016},
  month = {10},
  pages = {1449-1454},
  abstract = {We present an implementation of a recently developed and
                  relatively simple linearized Bregman method to solve
                  the large-scale $\ell_1$-norm sparsity-promoting
                  Gauss-Newton (GN) full-waveform inversion (FWI)
                  problem. Numerical experiments demonstrate that: the
                  much simpler linearized Bregman method does a
                  comparable and even superior job compared to a
                  state-of-the-art $\ell_1$-norm solver SPGL1, which
                  is previously used in the modified Gauss-Newton FWI;
                  the linearized Bregman method is also more efficient
                  (faster) than SPGL1; the FWI result with the
                  linearized Bregman method solving $\ell_1$-norm
                  sparsity-promoting problems to get the model updates
                  is better than that obtained by solving
                  $\ell_2$-norm least-squares problems to get the
                  model updates.},
  keywords = {Bregman method, waveform inversion, SEG},
  note = {(SEG, Dallas)},
  doi = {10.1190/segam2016-13848105.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/chai2016SEGlbm/chai2016SEGlbm.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/chai2016SEGlbm/chai2016SEGlbm_pres.pdf}
}


@CONFERENCE{dasilva2016SEGuse,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {A unified {2D/3D} software environment for large scale time-harmonic full waveform inversion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2016},
  month = {10},
  pages = {1169-1173},
  abstract = {Full-Waveform Inversion is a costly and complex
                  procedure for realistically sized 3D seismic
                  data. The performance-critical nature of this
                  problem often results in software environments that
                  are written entirely in low-level languages, making
                  them hard to understand, maintain, improve, and
                  extend. The unreadability of such codes can stymie
                  research developments, where the translation from
                  higher level mathematical ideas to high performance
                  codes can be lost, inhibiting the uptake of new
                  ideas in production-level codebases. We propose a
                  new software organization paradigm for Full-Waveform
                  Inversion and other PDE-constrained optimization
                  problems that is flexible, efficient, scalable, and
                  demonstrably correct. We decompose the various
                  structural components of FWI in to its constituent
                  components, from parallel computation to assembling
                  objective functions and gradients to lower level
                  matrix-vector multiplications. This decomposition
                  allows us to create a framework where individual
                  components can be easily swapped out to suit a
                  particular user's existing software environment. The
                  ease of applying high-level algorithms to the FWI
                  problem allows us to easily implement stochastic FWI
                  and demonstrate its effectiveness on a large scale
                  3D problem.},
  keywords = {software, full-waveform inversion, large scale, numerical analysis, SEG},
  note = {(SEG, Dallas)},
  doi = {10.1190/segam2016-13869051.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/dasilva2016SEGuse/dasilva2016SEGuse.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/dasilva2016SEGuse/dasilva2016SEGuse_pres.pdf}
}


@CONFERENCE{deaguiar2016SCdff,
  author = {Marcos de Aguiar and Gerard Gorman and Felix J. Herrmann and Navjot Kukreja and Michael Lange and Mathias Louboutin and Felippe Vieira Zacarias},
  title = {DeVito: fast finite difference computation},
  booktitle = {Super Computing (SC16)},
  year = {2016},
  month = {11},
  abstract = {Seismic imaging, used in energy exploration, is arguably
                  the most compute and data intensive application in
                  the private sector. The commonly used methods
                  involve solving the wave equations numerically using
                  finite difference formulations. Writing optimized
                  code for these applications involves multiple
                  man-years of effort that need to be repeated every
                  time a new development needs to be factored in – for
                  every target platform. DeVito is a new tool for
                  performing optimized Finite Difference (FD)
                  computation from high-level symbolic problem
                  definitions. The application developer needs to
                  provide a differential equation in symbolic
                  form. DeVito performs automated code generation and
                  Just-In-Time (JIT) compilation based on this
                  symbolic equation to create and execute highly
                  optimized Finite Difference kernels on multiple
                  computer platforms. DeVito has been designed to be
                  used as part of complex workflows involving data
                  flows across multiple applications over different
                  nodes of a cluster.},
  keywords = {finite differences, high performance computing, full-waveform inversion},
  note = {(Super Computing, Utah)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SC/2016/deaguiar2016SCdff/deaguiar2016SCdff_poster.pdf},
  url2 = {http://sc16.supercomputing.org/sc-archive/tech_poster/tech_poster_pages/post155.html}
}


@CONFERENCE{fang2016SEGuqw,
  author = {Zhilong Fang and Chia Ying Lee and Curt Da Silva and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Uncertainty quantification for {Wavefield} {Reconstruction} {Inversion} using a {PDE} free semidefinite {Hessian} and randomize-then-optimize method},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2016},
  month = {10},
  pages = {1390-1394},
  abstract = {The data of full-waveform inversion often contains
                  noise, which induces uncertainties in the inversion
                  results. Ideally, one would like to run a number of
                  independent inversions with different realizations
                  of the noise and assess model-side uncertainties
                  from the resulting models, however this is not
                  feasible because we collect the data only once. To
                  circumvent this restriction, various sampling
                  schemes have been devised to generate an ensemble of
                  models that fit the data to within the noise
                  level. Such sampling schemes typically involve
                  running multiple inversions or evaluating the
                  Hessian of the cost function, both of which are
                  computationally expensive. In this work, we propose
                  a new method to quantify uncertainties based on a
                  novel formulation of the full-waveform inversion
                  problem – wavefield reconstruction inversion. Based
                  on this formulation, we formulate a semidefinite
                  approximation of the corresponding Hessian
                  matrix. By precomputing certain quantities, we are
                  able to apply this Hessian to given input vectors
                  without additional solutions of the underlying
                  partial differential equations. To generate a
                  sample, we solve an auxiliary stochastic
                  optimization problem involving this Hessian. The
                  result is a computationally feasible method that,
                  with little overhead, can generate as many samples
                  as required at small additional cost. We test our
                  method on the synthetic BG Compass model and compare
                  the results to a direct-sampling approach. The
                  results show the feasibility of applying our method
                  to computing statistical quantities such as the mean
                  and standard deviation in the context of wavefield
                  reconstruction inversion.},
  keywords = {WRI, uncertainty quantification, SEG},
  note = {(SEG, Dallas)},
  doi = {10.1190/segam2016-13879108.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/fang2016SEGuqw/fang2016SEGuqw.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/fang2016SEGuqw/fang2016SEGuqw_pres.pdf}
}


@CONFERENCE{herrmann2016SEGWSctl,
  author = {Felix J. Herrmann and Rajiv Kumar and Felix Oghenekohwo and Shashin Sharan and Haneet Wason},
  title = {Compressive time-lapse marine acquisition},
  booktitle = {SEG Workshop on Low cost geophysics: How to be creative in a cost-challenged environment; Dallas},
  year = {2016},
  month = {10},
  keywords = {SEG, workshop, acquisition},
  note = {(SEG Workshop, Dallas)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/herrmann2016SEGWSctl/herrmann2016SEGWSctl_pres.pdf}
}


@CONFERENCE{herrmann2016SEGWScvp,
  author = {Felix J. Herrmann and Bas Peters},
  title = {Constraints versus penalties for edge-preserving full-waveform inversion},
  booktitle = {SEG Workshop on Where are we heading with FWI; Dallas},
  year = {2016},
  month = {10},
  abstract = {Full-waveform inversion is arguably one of the most
                  challenging inverse problems out there. For this
                  reason, it is remarkable that so much progress has
                  been made over the years. While we can look back
                  with confidence at an increasing number of success
                  stories, the application of full-waveform
                  technologies to deeper targets and complex geologies
                  remains challenging making it an expensive
                  proposition. So far, efforts to overcome some of
                  these challenges have mainly been directed towards
                  extended formulations such as Symes's and Biondi's
                  subsurface offset/rau-parameters, Mike Warner/Lluis
                  Guasch's Wiener filters in their adaptive
                  full-waveform inversion, and van Leeuwen/Herrmann's
                  Wavefield Reconstruction Inversion, where the
                  wave-equation appears as a least-squares term in the
                  objective. While this recent work --- including
                  other approaches such as different data-misfit
                  objective functions --- has removed some of FWI
                  cycle-skip issues it does, with the exception
                  perhaps of box constraints, not incorporate
                  rudimentary information on the geology on the
                  unknown model. Following recent work by Ernie Esser,
                  we present a general framework how to incorporate
                  this type of information in the form of
                  constraints. Contrary to Tikhonov-like
                  regularization or gradient filtering, imposing
                  constraints has several key advantages, namely (i)
                  they can be expressed in simple human understandable
                  terms such as lower and upper limits for the
                  velocity without the necessity to introduce
                  additional parameters and assumptions on the
                  probability distribution of the model; (ii) they can
                  consist of several constraints, e.g. box and
                  smoothness constraints, as long as their
                  intersection in non-empty; and (iii) they can be
                  imposed in a separate inner loop, which does not
                  affect gradients and Hessians of full-waveform
                  inversion itself. By means of examples, we will
                  demonstrate the advocacy of constrained formulations
                  as they apply to full-waveform inversion. This is
                  joint work with Bas Peters.},
  keywords = {SEG, workshop, full-waveform inversion},
  note = {(SEG Workshop, Dallas)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/herrmann2016SEGWScvp/herrmann2016SEGWScvp_pres.pdf}
}


@CONFERENCE{kukreja2016WOLFHPCdaf,
  author = {Navjot Kukreja and Mathias Louboutin and Felippe Vieira Zacarias and Fabio Luporini and Michael Lange and Gerard Gorman},
  title = {Devito: automated fast finite difference computation},
  booktitle = {WOLFHPC 2016 Workshop (Super Computing)},
  year = {2016},
  month = {11},
  abstract = {Domain specific languages have successfully been used in
                  a variety of fields to cleanly express scientific
                  problems as well as to simplify implementation and
                  performance optimization on different computer
                  architectures. Although a large number of stencil
                  languages are available, finite difference domain
                  specific languages have proved challenging to design
                  because most practical use cases require additional
                  features that fall outside the finite difference
                  abstraction. Inspired by the complexity of
                  real-world seismic imaging problems, we introduce
                  Devito, a domain specific language in which high
                  level equations are expressed using symbolic
                  expressions from the SymPy package. Complex
                  equations are automatically manipulated, optimized,
                  and translated into highly optimized C code that
                  aims to perform comparably or better than hand-tuned
                  code. All this is transparent to users, who only see
                  concise symbolic mathematical expressions.},
  keywords = {finite differences, high performance computing, modelling, acoustic, compiler, stencil},
  note = {(WOLFHPC Workshop (SC16), Utah)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SC/2016/WOLFHPC/kukreja2016WOLFHPCdaf/kukreja2016WOLFHPCdaf.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SC/2016/WOLFHPC/kukreja2016WOLFHPCdaf/kukreja2016WOLFHPCdaf_pres.pdf}
}


@CONFERENCE{kumar2016SEGtjm,
  author = {Rajiv Kumar and Shashin Sharan and Haneet Wason and Felix J. Herrmann},
  title = {Time-jittered marine acquisition---a rank-minimization approach for {5D} source separation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2016},
  month = {10},
  pages = {119-123},
  abstract = {Simultaneous source marine acquisition has been
                  recognized as an economic way of improving spatial
                  sampling and speedup acquisition time, where a
                  single- (or multiple-) source vessel fires at
                  jittered source locations and time
                  instances. Consequently, the acquired simultaneous
                  data volume is processed to separate the overlapping
                  shot records resulting in densely sampled data
                  volume. It has been shown in the past that the
                  simultaneous source acquisition design and source
                  separation process can be setup as a compressed
                  sensing problem, where conventional seismic data is
                  reconstructed from simultaneous data via a
                  sparsity-promoting optimization formulation. While
                  the recovery quality of separated data is reasonably
                  well, the recovery process can be computationally
                  expensive due to transform-domain redundancy. In
                  this paper, we present a computationally tractable
                  rank-minimization algorithm to separate simultaneous
                  data volumes. The proposed algorithm is suitable for
                  large-scale seismic data, since it avoids
                  singular-value decompositions and uses a low-rank
                  based factorized formulation instead. Results are
                  illustrated for simulations of simultaneous
                  time-jittered continuous recording for a 3D
                  ocean-bottom cable survey.},
  keywords = {5D, marine, time-jittered acquisition, source separation, SEG},
  note = {(SEG, Dallas)},
  doi = {10.1190/segam2016-13878249.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/kumar2016SEGtjm/kumar2016SEGtjm.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/kumar2016SEGtjm/kumar2016SEGtjm_pres.pdf}
}


@CONFERENCE{lange2016dtg,
  author = {Michael Lange and Navjot Kukreja and Mathias Louboutin and Fabio Luporini and Felippe Vieira Zacarias and Vincenzo Pandolfo and Paulius Velesko and Paulius Kazakas and Gerard Gorman},
  title = {Devito: {Towards} a generic finite difference {DSL} using symbolic python},
  booktitle = {6th Workshop on Python for High-Performance and Scientific Computing},
  year = {2016},
  month = {11},
  pages = {67-75},
  abstract = {Domain specific languages (DSL) have been used in a
                  variety of fields to express complex scientific
                  problems in a concise manner and provide automated
                  performance optimization for a range of
                  computational architectures. As such DSLs provide a
                  powerful mechanism to speed up scientific Python
                  computation that goes beyond traditional
                  vectorization and pre-compilation approaches, while
                  allowing domain scientists to build applications
                  within the comforts of the Python software
                  ecosystem. In this paper we present Devito, a new
                  finite difference DSL that provides optimized
                  stencil computation from high-level problem
                  specifications based on symbolic Python
                  expressions. We demonstrate Devito's symbolic API
                  and performance advantages over traditional Python
                  acceleration methods before highlighting its use in
                  the scientific context of seismic inversion
                  problems.},
  keywords = {HPC, python, finite differences, acoustic, modelling, inversion, software optimization},
  note = {(PyHPC, Utah)},
  doi = {10.1109/PyHPC.2016.9},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/PyHPC/2016/lange2016dtg/lange2016dtg.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/PyHPC/2016/lange2016dtg/lange2016dtg_pres.pdf}
}


@CONFERENCE{lopez2016CMSogl,
  author = {Oscar Lopez and Rajiv Kumar and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Off-the-grid low-rank matrix recovery: seismic data reconstruction},
  booktitle = {Canadian Mathematical Society Summer Meeting},
  year = {2016},
  month = {06},
  abstract = {This talk discusses a modified low-rank matrix recovery
                  work-flow that admits unstructured observations. By
                  incorporating a regularization operator which
                  accurately maps structured data to unstructured
                  data, into the nuclear-norm minimization problem,
                  this approach is able to compensate for data
                  irregularity. Furthermore, by construction this
                  formulation yields output that is supported on a
                  structured grid. Recovery error bounds are
                  established for the methodology with matrix sensing
                  and matrix completion numerical experiments
                  including applications to seismic trace
                  interpolation to demonstrate the potential of the
                  approach.},
  keywords = {CMS, matrix completion, matrix sensing, nonuniform discrete Fourier transform, nuclear-norm, seismic data interpolation},
  note = {(CMS, Edmonton, Alberta)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CMS/2016/lopez2016CMSogl/lopez2016CMSogl_pres.pdf}
}


@CONFERENCE{peters2016SEGprs,
  author = {Bas Peters and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Parallel reformulation of the sequential adjoint-state method},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2016},
  month = {10},
  pages = {1411-1415},
  abstract = {This abstract is about reducing the total computation
                  time to solve full-waveform inversion problems. A
                  common problem formulation is a nonlinear
                  least-squares problem where the gradient is computed
                  using the adjoint-state algorithm. This formulation
                  offers parallelism for the computation of gradients
                  for different sources and frequencies. The
                  adjoint-state algorithm itself is sequential however
                  and this is a limiting factor when a lot of compute
                  nodes are available and only a few wavefields need
                  to be computed. This situation occurs when
                  stochastic optimization strategies are used to
                  minimize the objective function. We present a
                  parallel reformulation of the sequential
                  adjoint-state algorithm, which allows the forward-
                  and adjoint wavefields to be computed in
                  parallel. Both algorithms are mathematically
                  equivalent but the parallel version is twice as fast
                  in run time. An important characteristic of the
                  proposed algorithm is that one wavefield needs to be
                  computed per source and one per receiver. These
                  fields can be used to apply the (inverse)
                  Gauss-Newton Hessian to a vector without recomputing
                  wavefields. A 2D example shows that good
                  full-waveform inversion results are obtained, even
                  when a small number of sources and receivers is
                  used.},
  keywords = {numerical linear algebra, PDE-constrained optimization, FWI, parallel computing, SEG},
  note = {(SEG, Dallas)},
  doi = {10.1190/segam2016-13966771.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/peters2016SEGprs/peters2016SEGprs.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/peters2016SEGprs/peters2016SEGprs_pres.pdf}
}


@CONFERENCE{sharan2016SEGspj,
  author = {Shashin Sharan and Rongrong Wang and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Sparsity-promoting joint microseismic source collocation and source-time function estimation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2016},
  month = {10},
  pages = {2574-2579},
  abstract = {In this work, we propose a new method to simultaneously
                  locate microseismic events (e.g., induced by
                  hydraulic fracturing) and estimate the source
                  signature of these events. We use the method of
                  linearized Bregman. This algorithm focuses unknown
                  sources at their true locations by promoting
                  sparsity along space and at the same time keeping
                  the energy along time in check. We are particularly
                  interested in situations where the microseismic data
                  is noisy, sources have different signatures and we
                  only have access to the smooth background-velocity
                  model. We perform numerical experiments to
                  demonstrate the usability of the proposed method. We
                  also compare our results with full-waveform
                  inversion based microseismic event collocation
                  methods. Our method gives flexibility to
                  simultaneously get a more accurate source image
                  along with an estimate of the source-time function,
                  which carries important information on the rupturing
                  process and source mechanism.},
  keywords = {microseismic, sparse, sources, wave equation, noise, SEG},
  note = {(SEG, Dallas)},
  doi = {10.1190/segam2016-13871022.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/sharan2016SEGspj/sharan2016SEGspj.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/sharan2016SEGspj/sharan2016SEGspj_pres.pdf}
}


@CONFERENCE{wang2016SEGfde,
  author = {Rongrong Wang and Felix J. Herrmann},
  title = {Frequency down-extrapolation with {TV} norm minimization},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2016},
  month = {10},
  pages = {1380-1384},
  abstract = {In this work, we present a total-variation (TV)-norm
                  minimization method to perform model-free
                  low-frequency data extrapolation for the purpose of
                  assisting full-waveform inversion. Low-frequency
                  extrapolation is the process of extending reliable
                  frequency bands of the raw data towards the lower
                  end of the spectrum. To this end, we propose to
                  solve a TV-norm based convex optimization problem
                  that has a global minimum and is equipped with a
                  fast solver. The approach takes into account both
                  the sparsity of the reflectivity series associated
                  with a single trace, as well as the inter-trace
                  correlations. A favorable byproduct of this approach
                  is that it allows one to work with coarsely sampled
                  trace along time (as coarse as 0.02s per sample),
                  hence substantially reduces the size of the proposed
                  optimization problem. Last, we show the
                  effectiveness of the method for frequency-domain FWI
                  on the Marmousi model.},
  keywords = {extrapolation, sparse, FWI, SEG},
  note = {(SEG, Dallas)},
  doi = {10.1190/segam2016-13879674.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/wang2016SEGfde/wang2016SEGfde.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/wang2016SEGfde/wang2016SEGfde_pres.pdf}
}


@CONFERENCE{witte2016SEGpve,
  author = {Philipp A. Witte and Christiaan C. Stolk and Felix J. Herrmann},
  title = {Phase velocity error minimizing scheme for the anisotropic pure {P-wave} equation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2016},
  month = {10},
  pages = {452-457},
  abstract = {Pure P-wave equations for acoustic modeling in
                  transverse isotropic media are derived by
                  approximating the exact pure Pwave dispersion
                  relation. In this work, we present an alternative
                  approach to the approximate dispersion relation of
                  Etgen and Brandsberg-Dahl, in which we approximate
                  the exact dispersion relation through a polynomial
                  expansion and determine its coefficients by solving
                  a linear least squares problem that minimizes the
                  phase velocity error over the entire range of phase
                  angles. The coefficients are also optimized over a
                  pre-defined range of Thomsen parameters, so that the
                  phase error is small for models with spatially
                  varying anisotropy. Phase velocity error analysis
                  shows that the optimized pure P-wave equation is up
                  to one order of magnitude more accurate than other
                  popular pure P-wave equations, even for highly
                  non-elliptic anisotropy. The optimized equation can
                  be easily turned into a time-domain forward modeling
                  scheme and comparisons of the modeled waveforms with
                  analytical travel times once more illustrate its
                  high accuracy. We also provide an efficient
                  implementation of our approach for 3D tilted TI
                  media that limits the count of fast Fourier
                  transforms per time step to a number that is
                  comparable to other pure P-wave equations.},
  keywords = {anisotropy, modelling, TTI, least squares, SEG},
  note = {(SEG, Dallas)},
  doi = {10.1190/segam2016-13844850.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/witte2016SEGpve/witte2016SEGpve.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/witte2016SEGpve/witte2016SEGpve_poster.pdf}
}


@CONFERENCE{yang2016SEGtds,
  author = {Mengmeng Yang and Philipp A. Witte and Zhilong Fang and Felix J. Herrmann},
  title = {Time-domain sparsity-promoting least-squares migration with source estimation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2016},
  month = {10},
  pages = {4225-4229},
  abstract = {Traditional reverse-time migration (RTM) gives images
                  with wrong amplitudes and low
                  resolution. Least-squares RTM (LS-RTM) on the other
                  hand, is capable of obtaining true-amplitude images
                  as solutions of $\ell_2$-norm norm minimization
                  problems by fitting the synthetic and observed
                  reflection data. The shortcoming of this approach is
                  that solutions of these $\ell_2$ problems are
                  typically smoothed, tend to be overfitted, and
                  computationally too expensive because it requires
                  compared to standard RTM too many iterations. By
                  working with randomized subsets of data only, the
                  computational costs of LS-RTM can be brought down to
                  an acceptable level while producing artifact-free
                  high-resolution images without overfitting the
                  data. While initial results of these "compressive
                  imaging" methods were encouraging various open
                  issues remain including guaranteed convergence,
                  algorithmic complexity of the solver, and lack of
                  on-the-fly source estimation for LS-RTMs with
                  wave-equation solvers based on time-stepping. By
                  including on-the-fly source-time function estimation
                  into the method of Linearized Bregman (LB), on which
                  we reported before, we tackle all these issues
                  resulting in a easy-to-implement algorithm that
                  offers flexibility in the trade-off between the
                  number of iterations and the number of wave-equation
                  solves per iteration for a fixed total number of
                  wave-equation solves. Application of our algorithm
                  on a 2D synthetic shows that we are able to obtain
                  high-resolution images, including accurate estimates
                  of the wavelet, for a single pass through the
                  data. The produced image, which is by virtue of the
                  inversion deconvolved with respect to the wavelet,
                  is roughly of the same quality as the image obtained
                  given the correct source function.},
  keywords = {time domain, least squares, RTM, source estimation, SEG},
  note = {(SEG, Dallas)},
  doi = {10.1190/segam2016-13850609.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/yang2016SEGtds/yang2016SEGtds.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/yang2016SEGtds/yang2016SEGtds_pres.pdf}
}



%-----2015-----%

@CONFERENCE{bougher2015CSEGpsu,
  author = {Ben B. Bougher and Felix J. Herrmann},
  title = {Prediction of stratigraphic units from spectral co-occurance coefficients of well logs},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2015},
  month = {05},
  abstract = {Well logging is the process of making physical
                  measurements down bore holes in order to
                  characterize geological and structural
                  properties. Logs are visually interpreted and
                  correlated to classify regions that are similar in
                  structure, a process that can be modelled with
                  machine learning. This project applies supervised
                  learning methods to labelled well logs from the
                  Trenton Black River data set in order to classify
                  major stratigraphic units. Spectral co-occurance
                  coefficients were used for feature extraction, and a
                  k-nearest-neighbours approach was used for
                  classification. This novel approach was applied to
                  real field data in a high-impact domain, yielding
                  promising results for future research.},
  keywords = {CSEG, scattering transform, well logs, machine learning},
  note = {(CSEG, Calgary)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2015/bougher2015CSEGpsu/bougher2015CSEGpsu.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2015/bougher2015CSEGpsu/bougher2015CSEGpsu_pres.pdf}
}


@CONFERENCE{dasilva2015EAGEogt,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Off the grid tensor completion for seismic data interpolation},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2015},
  month = {06},
  abstract = {The practical realities of acquiring seismic data in a
                  realistic survey are often at odds with the
                  stringent requirements of Shannon-Nyquist-based
                  sampling theory. The unpredictable movement of the
                  ocean`s currents can be detrimental in acquiring
                  exactly equally-spaced samples while sampling at
                  Nyquist-rates are expensive, given the huge
                  dimensionality and size of the data volume. Recent
                  work in matrix and tensor completion for seismic
                  data interpolation aim to alleviate such stringent
                  Nyquist-based sampling requirements but are
                  fundamentally posed on a regularly-spaced grid. In
                  this work, we extend our previous results in using
                  the so-called Hierarchical Tucker (HT) tensor format
                  for recovering seismic data to the irregularly
                  sampled case. We introduce an interpolation operator
                  that resamples our tensor from a regular grid (in
                  which we impose our low-rank constraints) to our
                  irregular sampling grid. Our framework is very
                  flexible and efficient, depending primarily on the
                  computational costs of this operator. We demonstrate
                  the superiority of this approach on a realistic BG
                  data set compared to using low-rank tensor methods
                  that merely use binning.},
  keywords = {EAGE, hierarchical tucker, structured tensor, tensor interpolation, off the grid, irregular sampling},
  note = {(EAGE, Madrid)},
  doi = {10.3997/2214-4609.201412978},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/dasilva2015EAGEogt/dasilva2015EAGEogt.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/dasilva2015EAGEogt/dasilva2015EAGEogt_pres.pdf}
}


@CONFERENCE{dasilva2015WSLRigt,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Irregular grid tensor completion},
  booktitle = {Workshop on Low-rank Optimization and Applications},
  year = {2015},
  month = {06},
  abstract = {Low rank tensor completion has recently garnered the
                  attention of researchers owing to the ubiquity of
                  tensors in the sciences and the theoretical and
                  numerical challenges compared to matrix
                  completion. Here we consider a tensor completion
                  scheme where the data arises from sampling a
                  continuous function. When the sampling grid is not
                  periodic, the resulting tensor may not be low rank
                  in the Hierarchical Tucker sense, which can
                  adversely affect reconstruction quality when there
                  are missing samples. In order to compensate for this
                  off-grid sampling, we introduce a resampling
                  operator (here, the non-uniform Fourier transform)
                  that accounts for this non-uniformity moreso than
                  merely treating the sampling grid as
                  periodic. Numerical experiments demonstrate that
                  this approach can improve reconstruction quality
                  when there is relatively little data.},
  keywords = {hierarchical tucker, structured tensor, tensor interpolation, off the grid, irregular sampling},
  note = {(Workshop on Low-rank Optimization and Applications, University of Bonn, Germany)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/WS_LR/2015/dasilva2015WSLRigt/dasilva2015WSLRigt_poster.pdf}
}


@CONFERENCE{esser2015EAGElcs,
  author = {Ernie Esser and Tim T.Y. Lin and Rongrong Wang and Felix J. Herrmann},
  title = {A lifted $\ell_1/\ell_2$ constraint for sparse blind deconvolution},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2015},
  month = {06},
  abstract = {We propose a modification to a sparsity constraint based
                  on the ratio of $\ell_1$ and $\ell_2$ norms for
                  solving blind seismic deconvolution problems in
                  which the data consist of linear convolutions of
                  different sparse reflectivities with the same source
                  wavelet. We also extend the approach to the
                  Estimation of Primaries by Sparse Inversion (EPSI)
                  model, which includes surface related multiples.
                  Minimizing the ratio of $\ell_1$ and $\ell_2$ norms
                  has been previously shown to promote sparsity in a
                  variety of applications including blind
                  deconvolution. Most existing implementations are
                  heuristic or require smoothing the $\ell_1/\ell_2$
                  penalty. Lifted versions of $\ell_1/\ell_2$
                  constraints have also been proposed but are
                  challenging to implement. Inspired by the lifting
                  approach, we propose to split the sparse signals
                  into positive and negative components and apply an
                  $\ell_1/\ell_2$ constraint to the difference,
                  thereby obtaining a constraint that is easy to
                  implement without smoothing the $\ell_1$ or $\ell_2$
                  norms. We show that a method of multipliers
                  implementation of the resulting model can recover
                  source wavelets that are not necessarily minimum
                  phase and approximately reconstruct the sparse
                  reflectivities. Numerical experiments demonstrate
                  robustness to the initialization as well as to noise
                  in the data.},
  keywords = {EAGE, blind deconvolution, EPSI},
  note = {(EAGE, Madrid)},
  doi = {10.3997/2214-4609.201413420},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/esser2015EAGElcs/esser2015EAGElcs.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/esser2015EAGElcs/esser2015EAGElcs_poster.pdf}
}


@CONFERENCE{esser2015SEGasd,
  author = {Ernie Esser and Llu\'{i}s Guasch and Tristan van Leeuwen and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Automatic salt delineation --- {Wavefield} {Reconstruction} {Inversion} with convex constraints},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2015},
  month = {10},
  pages = {1337-1343},
  abstract = {We extend full-waveform inversion by Wavefield
                  Reconstruction Inversion by including convex
                  constraints on the model. Contrary to the
                  conventional adjoint-state formulations, Wavefield
                  Reconstruction Inversion has the advantage that the
                  Gauss-Newton Hessian is well approximated by a
                  diagonal scaling, which allows us to add convex
                  constraints, such as the box- and the
                  edge-preserving total-variation constraint, on the
                  square slowness without incurring significant
                  increases in computational costs. As the examples
                  demonstrate, including these constraints yields far
                  superior results in complex geological areas that
                  contain high-velocity high-contrast bodies
                  (e.g. salt or basalt). Without these convex
                  constraints, adjoint-state and Wavefield
                  Reconstruction Inversion get trapped in local minima
                  for poor starting models.},
  keywords = {SEG, full-waveform inversion, convex constraints, total-variation norm, salt},
  note = {(SEG, New Orleans)},
  doi = {10.1190/segam2015-5877995.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2015/esser2015SEGasd/esser2015SEGasd.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2015/esser2015SEGasd/esser2015SEGasd_pres.pdf}
}


@CONFERENCE{esser2015SEGWSasf,
  author = {Ernie Esser and Llu\'{i}s Guasch and Felix J. Herrmann},
  title = {Automatic salt flooding with hinged {FWI}},
  booktitle = {SEG Workshop on Subsalt model buiding long to short wavelengths; New Orleans},
  year = {2015},
  month = {10},
  keywords = {SEG, workshop, waveform inversion},
  note = {(SEG, New Orleans)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2015/esser2015SEGWSasf/esser2015SEGWSasf_pres.pdf}
}


@CONFERENCE{esser2015CAMSAPrsa,
  author = {Ernie Esser and Rongrong Wang and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Resolving scaling ambiguities with the $\ell_1/\ell_2$ norm in a blind deconvolution problem with feedback},
  booktitle = {IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing},
  year = {2015},
  month = {12},
  pages = {365-368},
  abstract = {Compared to more mundane blind deconvolution problems,
                  blind deconvolution in seismic applications involves
                  a feedback mechanism related to the free
                  surface. The presence of this feedback mechanism
                  gives us an unique opportunity to remove ambiguities
                  that have plagued blind deconvolution for a long
                  time. While beneficial, this feedback by itself is
                  insufficient to remove the ambiguities even with
                  $\ell_1$ constraints. However, when paired with an
                  $\ell_1/\ell_2$ constraint the feedback allows us to
                  resolve the scaling ambiguity under relatively mild
                  assumptions. Inspired by lifting approaches, we
                  propose to split the sparse signal into positive and
                  negative components and apply an $\ell_1/\ell_2$
                  constraint to the difference, thereby obtaining a
                  constraint that is easy to implement. Numerical
                  experiments demonstrate robustness to the
                  initialization as well as to noise in the data.},
  keywords = {CAMSAP, blind deconvolution, $\ell_1/\ell_2$, lifting, method of multipliers},
  note = {(IEEE CAMSAP Workshop, Canc\'{u}n, Mexico)},
  doi = {10.1109/CAMSAP.2015.7383812},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CAMSAP/2015/esser2015CAMSAPrsa/esser2015CAMSAPrsa.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/CAMSAP/2015/esser2015CAMSAPrsa/esser2015CAMSAPrsa_ext.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CAMSAP/2015/esser2015CAMSAPrsa/esser2015CAMSAPrsa_poster.pdf}
}


@CONFERENCE{fang2015EAGEsew,
  author = {Zhilong Fang and Felix J. Herrmann},
  title = {Source estimation for {Wavefield} {Reconstruction} {Inversion}},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2015},
  month = {06},
  abstract = {Wavefield reconstruction inversion is a new approach to
                  waveform based inversion that helps overcome the
                  `cycle skipping' problem. However, like most
                  waveform based inversion methods, wavefield
                  reconstruction inversion also requires good source
                  wavelets. Without correct source wavelets,
                  wavefields cannot be reconstructed correctly and the
                  velocity model cannot be updated correctly
                  neither. In this work, we propose a source
                  estimation method for wavefield reconstruction
                  inversion based on the variable projection method.
                  In this method, we reconstruct wavefields and
                  estimate source wavelets simultaneously by solving
                  an extended least-squares problem, which contains
                  source wavelets. This approach does not increase the
                  computational cost compared to conventional
                  wavefield reconstruction inversion. Numerical
                  results illustrates with our source estimation
                  method we are able to recover source wavelets and
                  obtain inversion results that are comparable to
                  results obtained with true source wavelets.},
  keywords = {EAGE, source estimation, WRI},
  note = {(EAGE, Madrid)},
  doi = {10.3997/2214-4609.201412588},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/fang2015EAGEsew/fang2015EAGEsew.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/fang2015EAGEsew/fang2015EAGEsew_pres.pdf}
}


@CONFERENCE{fang2015EAGEuqw,
  author = {Zhilong Fang and Chia Ying Lee and Curt Da Silva and Felix J. Herrmann and Rachel Kuske},
  title = {Uncertainty quantification for {Wavefield} {Reconstruction} {Inversion}},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2015},
  month = {06},
  abstract = {In this work, we propose a method to quantify the
                  uncertainty of wavefield reconstruction inversion
                  under the framework of Bayesian inference. Unlike
                  the conventional method using the wave equation as
                  the forward mapping, we involve the wave equation
                  misfit in the posterior distribution and propose a
                  new posterior distribution. The negative
                  log-likelihood of the new distribution is less
                  oscillatory than that of the conventional posterior
                  distribution, and its Gauss-Newton Hessian is a
                  diagonal matrix that can be generated without any
                  additional computational cost. We use the diagonal
                  Gauss-Newton Hessian to derive an approximate
                  Gaussian distribution at the maximum likelihood
                  point to quantify the uncertainty. This method makes
                  the uncertainty quantification for WRI
                  computationally tractable and is able to provide
                  reasonable uncertainty analysis based on our
                  numerical results.},
  keywords = {EAGE, UQ, WRI, FWI},
  note = {(EAGE, Madrid)},
  doi = {10.3997/2214-4609.201413198},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/fang2015EAGEuqw/fang2015EAGEuqw.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/fang2015EAGEuqw/fang2015EAGEuqw_pres.pdf}
}


@CONFERENCE{fang2015IIPFWIsoa,
  author = {Zhilong Fang and Tim Burgess and Felix J. Herrmann},
  title = {Stochastic optimization and its application to seismic inversion},
  booktitle = {Inaugural Full-Waveform Inversion Workshop},
  year = {2015},
  month = {08-09},
  keywords = {FWI, stochastic, optimization, inversion},
  note = {(Natal, Brazil)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IIPFWI/fang2015IIPFWIsoa/fang2015IIPFWIsoa_pres.pdf}
}


@CONFERENCE{fang2015IIPFWIuqw,
  author = {Zhilong Fang and Chia Ying Lee and Curt Da Silva and Felix J. Herrmann and Rachel Kuske},
  title = {Uncertainty quantification for {Wavefield} {Reconstruction} {Inversion}},
  booktitle = {Inaugural Full-Waveform Inversion Workshop},
  year = {2015},
  month = {08-09},
  abstract = {In this work, we propose a method to quantify the
                  uncertainty of wavefield reconstruction inversion
                  under the framework of Bayesian inference. Unlike
                  the conventional method using the wave equation as
                  the forward mapping, we involve the wave equation
                  misfit in the posterior distribution and propose a
                  new posterior distribution. The negative
                  log-likelihood of the new distribution is less
                  oscillatory than that of the conventional posterior
                  distribution, and its Gauss-Newton Hessian is a
                  diagonal matrix that can be generated without any
                  additional computational cost. We use the diagonal
                  Gauss-Newton Hessian to derive an approximate
                  Gaussian distribution at the maximum likelihood
                  point to quantify the uncertainty. This method makes
                  the uncertainty quantification for WRI
                  computationally tractable and is able to provide
                  reasonable uncertainty analysis based on our
                  numerical results.},
  keywords = {FWI, WRI, uncertainity},
  note = {(Natal, Brazil)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IIPFWI/fang2015IIPFWIuqw/fang2015IIPFWIuqw_pres.pdf}
}


@CONFERENCE{fang2015IIPFWIwri,
  author = {Zhilong Fang and Felix J. Herrmann},
  title = {Wavefield {Reconstruction} {Inversion} with source estimation and its application to 2014 {Chevron} synthetic blind test dataset},
  booktitle = {Inaugural Full-Waveform Inversion Workshop},
  year = {2015},
  month = {08-09},
  abstract = {We present a robust wavefield reconstruction inversion
                  with source estimation. The source wavelet is
                  estimated with the reconstruction of wavefield
                  simultaneously by solving an extended data
                  augmentation problem. We apply this method to the
                  2014 Chevron synthetic blind test dataset and show
                  the robustness of our method.},
  keywords = {FWI, WRI, source estimation},
  note = {(Natal, Brazil)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IIPFWI/fang2015IIPFWIwri/fang2015IIPFWIwri_pres.pdf}
}


@CONFERENCE{herrmann2015ASEGrae,
  author = {Felix J. Herrmann},
  title = {Randomized algorithms in exploration seismology},
  booktitle = {ASEG Annual Conference Proceedings},
  year = {2015},
  month = {02},
  abstract = {As in several other industries, progress in exploration
                  seismology relies on the collection and processing
                  of massive data volumes that grow exponentially in
                  size as the survey area and desired resolution
                  increase. This exponential growth --- in combination
                  with the increased complexity of the next-generation
                  of iterative wave-equation based inversion
                  algorithms --- puts strains on our acquisition
                  systems and computational back ends impeding
                  progress in our field. During this talk, I will
                  review how recent randomized algorithms from
                  Compressive Sensing and Machine learning can be used
                  to overcome some of these challenges by
                  fundamentally rethinking how we sample and process
                  seismic data. The key idea here is to reduce
                  acquisition and computational costs by deliberately
                  working on small randomized subsets of the data at a
                  desired accuracy. I will illustrate these concepts
                  using a variety of compelling examples on realistic
                  synthetics and field data.},
  keywords = {ASEG, compressed sensing, seismology, acquisition, imaging, FWI},
  note = {(ASEG, Perth)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ASEG/2015/herrmann2015ASEGrae/herrmann2015ASEGrae.pdf}
}


@CONFERENCE{herrmann2015EAGEfom,
  author = {Felix J. Herrmann and Ning Tu and Ernie Esser},
  title = {Fast "online" migration with {Compressive} {Sensing}},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2015},
  month = {06},
  abstract = {We present a novel adaptation of a recently developed
                  relatively simple iterative algorithm to solve
                  large-scale sparsity-promoting optimization
                  problems. Our algorithm is particularly suitable to
                  large-scale geophysical inversion problems, such as
                  sparse least-squares reverse-time migration or
                  Kirchoff migration since it allows for a tradeoff
                  between parallel computations, memory allocation,
                  and turnaround times, by working on subsets of the
                  data with different sizes. Comparison of the
                  proposed method for sparse least-squares imaging
                  shows a performance that rivals and even exceeds the
                  performance of state-of-the art one-norm solvers
                  that are able to carry out least-squares migration
                  at the cost of a single migration with all data.},
  keywords = {EAGE, LSRTM},
  note = {(EAGE, Madrid)},
  doi = {10.3997/2214-4609.201412942},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/herrmann2015EAGEfom/herrmann2015EAGEfom.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/herrmann2015EAGEfom/herrmann2015EAGEfom_pres.pdf}
}


@CONFERENCE{herrmann2015IIPFWIras,
  author = {Felix J. Herrmann},
  title = {Randomized algorithms in seismic imaging},
  booktitle = {Inaugural Full-Waveform Inversion Workshop},
  year = {2015},
  month = {08-09},
  abstract = {As in several other industries, progress in exploration
                  seismology relies on the collection and processing
                  of massive data volumes that grow exponentially in
                  size as the survey area and desired resolution
                  increase. This exponential growth – in combination
                  with the increased complexity of the next-generation
                  of iterative wave-equation based inversion
                  algorithms – puts strains on our acquisition systems
                  and computational back ends impeding progress in our
                  field. During this talk, I will review how recent
                  randomized algorithms from Compressive Sensing and
                  Machine learning can be used to overcome some of
                  these challenges by fundamentally rethinking how we
                  sample and process seismic data. The key idea here
                  is to reduce acquisition and computational costs by
                  deliberately working on small randomized subsets of
                  the data at a desired accuracy. I will illustrate
                  these concepts using a variety of compelling
                  examples on realistic synthetics and field data.},
  keywords = {FWI, randomization, seismic, imaging},
  note = {(Natal, Brazil)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IIPFWI/herrmann2015IIPFWIras/herrmann2015IIPFWIras_pres.pdf}
}


@CONFERENCE{herrmann2015IIPFWIwri,
  author = {Felix J. Herrmann},
  title = {Wavefield-{Reconstruction} {Inversion} — {WRI}},
  booktitle = {Inaugural Full-Waveform Inversion Workshop},
  year = {2015},
  month = {08-09},
  abstract = {We discuss a recently proposed novel method for waveform
                  inversion: Wavefield Reconstruction Inversion
                  (WRI). As opposed to conventional FWI — which
                  attempts to minimize the error between observed and
                  predicted data obtained by solving a wave equation –
                  WRI reconstructs a wave-field from the data and
                  extracts a model-update from this wavefield by
                  minimizing the wave-equation residual. The method
                  does not require explicit computation of an adjoint
                  wavefield as all the necessary information is
                  contained in the reconstructed wavefield. We show
                  how the corresponding model updates can be
                  interpreted physically analogously to the
                  conventional imaging-condition-based approach.},
  keywords = {FWI, WRI, inversion},
  note = {(Natal, Brazil)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IIPFWI/herrmann2015IIPFWIwri/herrmann2015IIPFWIwri_pres.pdf}
}


@CONFERENCE{herrmann2015IIPFWIwcc,
  author = {Felix J. Herrmann and Ernie Esser and Llu\'{i}s Guasch},
  title = {Wavefield {Reconstruction} {Inversion} with convex constraints},
  booktitle = {Inaugural Full-Waveform Inversion Workshop},
  year = {2015},
  month = {08-09},
  abstract = {During this talk, we discuss how to exploit the special
                  structure of Wavefield Reconstruction Inversion
                  (WRI) to include convex bound and total-variation
                  constraints in a computationally feasible
                  manner. The resulting method shows promising results
                  on challenging models that include high-velocity
                  high-contrast inclusions such as salt or
                  basalt. This is joint work with Ernie Esser who
                  passed away and who is dearly missed.},
  keywords = {FWI, regularization, convex},
  note = {(Natal, Brazil)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IIPFWI/herrmann2015IIPFWIwcc/herrmann2015IIPFWIwcc_pres.pdf}
}


@CONFERENCE{herrmann2015SIAMpcf,
  author = {Felix J. Herrmann and Bas Peters},
  title = {Pros and cons of full- and reduced-space methods for {Wavefield} {Reconstruction} {Inversion}},
  booktitle = {SIAM Conference on Mathematical and Computational Issues in the Geosciences},
  year = {2015},
  month = {06-07},
  abstract = {By insisting on fitting observed data, Wavefield
                  Reconstruction Inversion (WRI) is no longer cycle
                  skipped and therefore less reliant on the accuracy
                  of starting models. While extending the search space
                  mitigates local minima, there are challenges scaling
                  to 3D seismic when using reduced-space methods that
                  require accurate solves. Conversely, full-space
                  methods allow for inaccurate solves but require
                  storage of all wavefields. We weigh pros and cons of
                  these two approaches in the seismic context.},
  keywords = {SIAM, WRI, FWI},
  note = {(SIAM Conference on Mathematical and Computational Issues in the Geosciences, Stanford University, California)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2015/herrmann2015SIAMpcf/herrmann2015SIAMpcf_pres.pdf}
}


@CONFERENCE{herrmann2015SIAMwri,
  author = {Felix J. Herrmann},
  title = {Wavefield {Reconstruction} {Inversion} – reaping the benefits from extending the search space},
  booktitle = {SIAM Conference on Mathematical and Computational Issues in the Geosciences},
  year = {2015},
  month = {06-07},
  keywords = {SIAM, WRI},
  note = {(SIAM Conference on Mathematical and Computational Issues in the Geosciences, Stanford University, California)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2015/herrmann2015SIAMwri/herrmann2015SIAMwri_pres.pdf}
}


@CONFERENCE{herrmann2015SEGWSasf,
  author = {Felix J. Herrmann},
  title = {Automatic salt flooding with constrained wavefield reconstruction inversion},
  booktitle = {SEG Summer Research Workshop on FWI Applications from Imaging to Reservoir Characterization; Houston},
  year = {2015},
  month = {07},
  keywords = {SEG, workshop, wavefield reconstruction inversion},
  note = {(SEG Summer Workshop, Houston)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2015/herrmann2015SEGWSasf/herrmann2015SEGWSasf_pres.pdf}
}


@CONFERENCE{herrmann2015SEGWScse,
  author = {Felix J. Herrmann},
  title = {Compressive {Sensing} in {Exploration} {Seismology} - where we came from, where we are now, and where we need to go},
  booktitle = {SEG Geophysical Compressed Sensing Workshop; Beijing, China},
  year = {2015},
  month = {12},
  keywords = {SEG, workshop, compressed sensing},
  note = {(SEG Workshop, Beijing, China)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2015/herrmann2015SEGWScse/herrmann2015SEGWScse_pres.pdf}
}


@CONFERENCE{kumar2015EAGEmcu,
  author = {Rajiv Kumar and Oscar Lopez and Ernie Esser and Felix J. Herrmann},
  title = {Matrix completion on unstructured grids : {2-D} seismic data regularization and interpolation},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2015},
  month = {06},
  abstract = {Seismic data interpolation via rank-minimization
                  techniques has been recently introduced in the
                  seismic community. All the existing
                  rank-minimization techniques assume the recording
                  locations to be on a regular grid, e.g., sampled
                  periodically, but seismic data are typically
                  irregularly sampled along spatial axes. Other than
                  the irregularity of the sampled grid, we often have
                  missing data. In this paper, we study the effect of
                  grid irregularity to conduct matrix completion on a
                  regular grid for unstructured data. We propose an
                  improvement of existing rank-minimization techniques
                  to do regularization. We also demonstrate that we
                  can perform seismic data regularization and
                  interpolation simultaneously. We illustrate the
                  advantages of the modification using a real seismic
                  line from the Gulf of Suez to obtain high quality
                  results for regularization and interpolation, a key
                  application in exploration geophysics.},
  keywords = {EAGE, regularization, interpolation, matrix completion, NFFT},
  note = {(EAGE, Madrid)},
  doi = {10.3997/2214-4609.201413448},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/kumar2015EAGEmcu/kumar2015EAGEmcu.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/kumar2015EAGEmcu/kumar2015EAGEmcu_poster.pdf}
}


@CONFERENCE{kumar2015CSEGlse,
  author = {Rajiv Kumar and Ning Tu and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Least-squares extended imaging with surface-related multiples},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2015},
  month = {05},
  abstract = {Common image gathers are used in building velocity
                  models, inverting anisotropy parameters, and
                  analyzing reservoir attributes. Often primary
                  reflections are used to form image gathers and
                  multiples are typically attenuated in processing to
                  remove strong coherent artifacts generated by
                  multiples that interfere with the imaged
                  reflectors. However, researchers have shown that, if
                  correctly used, multiples can actually provide extra
                  illumination of the subsurface in seismic imaging,
                  especially for delineating the near-surface
                  features. In this work, we borrow ideas from
                  literatures on imaging with surface-related
                  multiples, and apply these ideas to extended
                  imaging. This way we save the massive computation
                  cost in separating multiples from the data before
                  using them during the formation of image
                  gathers. Also, we mitigate the strong coherent
                  artifacts generated by multiples which can send the
                  migration velocity analysis type algorithms in wrong
                  direction. Synthetic examples on a three-layer model
                  show the efficacy of the proposed formulation.},
  keywords = {CSEG, image-gather, surface-related multiples},
  note = {(CSEG, Calgary)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2015/kumar2015CSEGlse/kumar2015CSEGlse.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2015/kumar2015CSEGlse/kumar2015CSEGlse_pres.pdf}
}


@CONFERENCE{kumar2015IIPFWIrmb,
  author = {Rajiv Kumar and Curt Da Silva and Oscar Lopez and Aleksandr Y. Aravkin and Hassan Mansour and Haneet Wason and Ernie Esser and Felix J. Herrmann},
  title = {Rank minimization based seismic data processing and inversion},
  booktitle = {Inaugural Full-Waveform Inversion Workshop},
  year = {2015},
  month = {08-09},
  abstract = {We present the low-rank extensions of full-waveform
                  inversion to mitigate the local-minima issues. The
                  purpose formulation is computationally tractable and
                  does not involve any extra computational cost
                  compared to the adjoint-state method.},
  keywords = {FWI, rank minimization, processing, inversion},
  note = {(Natal, Brazil)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IIPFWI/kumar2015IIPFWIrmb/kumar2015IIPFWIrmb_pres.pdf}
}


@CONFERENCE{lin2015IIPFWIsws,
  author = {Tim T.Y. Lin and Curt Da Silva and Felix J. Herrmann},
  title = {Software and workflows at {SLIM}/{SINBAD}},
  booktitle = {Inaugural Full-Waveform Inversion Workshop},
  year = {2015},
  month = {08-09},
  keywords = {FWI, SLIM, software},
  note = {(Natal, Brazil)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IIPFWI/lin2015IIPFWIsws/lin2015IIPFWIsws_pres.pdf}
}


@CONFERENCE{lin2015IIPFWIsdh,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {The student-driven {HPC} environment at {SLIM}},
  booktitle = {Inaugural Full-Waveform Inversion Workshop},
  year = {2015},
  month = {08-09},
  abstract = {A major role of academic environments is to provide
                  learning experiences for students to critically
                  analyze and develop methods, both at a high-level of
                  mathematical rigour, and at a low-enough level of
                  implementation in order to yield experimental
                  results on real datasets. Often these two goals are
                  in conflict with each other, in terms of both
                  learning time and attention. At SLIM, we strive to
                  strike a balance between the two by abstracting away
                  many of the low-level aspects of distributed HPC
                  under a framework that matches syntactically with
                  the mathematics of our field, while exposing enough
                  control parameters for tuning performance
                  characteristics. This talk will touch on the history
                  of our efforts at SLIM, and culminate in an overview
                  of our current method of interacting with the
                  in-house compute cluster.},
  keywords = {FWI, HPC},
  note = {(Natal, Brazil)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IIPFWI/lin2015IIPFWIsdh/lin2015IIPFWIsdh_pres.pdf}
}


@CONFERENCE{lopez2015EAGErma,
  author = {Oscar Lopez and Rajiv Kumar and Felix J. Herrmann},
  title = {Rank minimization via alternating optimization: seismic data interpolation},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2015},
  month = {06},
  abstract = {Low-rank matrix completion techniques have recently
                  become an effective tool for seismic trace
                  interpolation problems. In this talk, we consider an
                  alternating optimization scheme for nuclear norm
                  minimization and discuss the applications to large
                  scale wave field reconstruction. By adopting a
                  factorization approach to the rank minimization
                  problem we write our low-rank matrix in bi-linear
                  form, and modify this workflow by alternating our
                  optimization to handle a single matrix factor at a
                  time. This allows for a more tractable procedure
                  that can robustly handle large scale, highly
                  oscillatory and critically subsampled seismic data
                  sets. We demonstrate the potential of this approach
                  with several numerical experiments on a seismic line
                  from the Nelson 2D data set and a frequency slice
                  from the Gulf of Mexico data set.},
  keywords = {EAGE, matrix completion, low-rank, interpolation},
  note = {(EAGE, Madrid)},
  doi = {10.3997/2214-4609.201413453},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/lopez2015EAGErma/lopez2015EAGErma.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/lopez2015EAGErma/lopez2015EAGErma_poster.pdf}
}


@CONFERENCE{lopez2015SEGWSsfm,
  author = {Oscar Lopez and Haneet Wason and Curt Da Silva and Rajiv Kumar and Felix J. Herrmann},
  title = {{SVD}-free matrix completion in randomized marine acquisition},
  booktitle = {SEG Workshop on Rank-Reduction and Other Sparse Transform Methods with Application to Data Reconstruction, De-Noising, De-Blending and Imaging; New Orleans},
  year = {2015},
  month = {10},
  abstract = {During this presentation, we will illustrate how
                  insights from matrix-free matrix completion can be
                  used to solve problems in (on- and off-the-grid)
                  missing trace interpolation and time-jittered
                  (simultaneous) marine acquisition. In particular, we
                  will demonstrate the importance of data organization
                  to favor recovery by rank minimization and the use
                  of deterministic spectral properties to predict the
                  recovery. We also make make comparisons between
                  wavefield reconstructions based on transform-domain
                  sparsity promotion and matrix completion.},
  keywords = {SEG, workshop, matrix completion, rank reduction, interpolation, acquisition},
  note = {(SEG, New Orleans)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2015/lopez2015SEGWSsfm/lopez2015SEGWSsfm_pres.pdf}
}


@CONFERENCE{louboutin2015IIPFWIrwi,
  author = {Mathias Louboutin and Bas Peters and Brendan R. Smithyman and Felix J. Herrmann},
  title = {Regularizing waveform inversion by projections onto convex sets},
  booktitle = {Inaugural Full-Waveform Inversion Workshop},
  year = {2015},
  month = {08-09},
  abstract = {A framework is proposed for regularizing the waveform
                  inversion problem by projections onto intersections
                  of convex sets. Multiple pieces of prior information
                  about the geology are represented by multiple convex
                  sets, for example limits on the velocity or minimum
                  smoothness conditions on the model. The data-misfit
                  is then minimized, such that the estimated model is
                  always in the intersection of the convex
                  sets. Therefore, it is clear what properties the
                  estimated model will have at each iteration. This
                  approach does not require any quadratic penalties to
                  be used and thus avoids the known problems and
                  limitations of those types of penalties. It is shown
                  that by formulating waveform inversion as a
                  constrained problem, regularization ideas such as
                  Tikhonov regularization and gradient filtering can
                  be incorporated into one framework. The algorithm is
                  generally applicable, in the sense that it works
                  with any (differentiable) objective function and
                  does not require significant additional
                  computation. The method is demonstrated on the
                  inversion of the 2D marine isotropic elastic
                  synthetic seismic benchmark by Chevron using an
                  acoustic modeling code. To highlight the effect of
                  the projections, we apply no data
                  pre-processing. This is joint work with Bas Peters.},
  keywords = {FWI, regularization, convex},
  note = {(Natal, Brazil)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IIPFWI/louboutin2015IIPFWIrwi/louboutin2015IIPFWIrwi_pres.pdf}
}


@CONFERENCE{louboutin2015SEGtcs,
  author = {Mathias Louboutin and Felix J. Herrmann},
  title = {Time compressively sampled full-waveform inversion with stochastic optimization},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2015},
  month = {10},
  pages = {5153-5157},
  abstract = {Time-domain Full-Waveform Inversion (FWI) aims to image
                  the subsurface of the earth accurately from field
                  recorded data and can be solved via the reduced
                  adjoint-state method. However, this method requires
                  access to the forward and adjoint wavefields that
                  are meet when computing gradient updates. The
                  challenge here is that the adjoint wavefield is
                  computed in reverse order during time stepping and
                  therefore requires storage or other type of
                  mitigation because storing the full time history of
                  the forward wavefield is too expensive in realistic
                  3D settings. To overcome this challenge, we propose
                  an approximate adjoint-state method where the
                  wavefields are subsampled randomly, which
                  drastically the amount of storage needed. By using
                  techniques from stochastic optimization, we control
                  the errors induced by the subsampling. Examples of
                  the proposed technique on a synthetic but realistic
                  2D model show that the subsampling-related artifacts
                  can be reduced significantly by changing the
                  sampling for each source after each model
                  update. Combination of this gradient approximation
                  with a quasi-Newton method shows virtually artifact
                  free inversion results requiring only 5% of storage
                  compared to saving the history at Nyquist. In
                  addition, we avoid having to recompute the
                  wavefields as is required by checkpointing.},
  keywords = {SEG, Full-waveform inversion, Acoustic, Subsampling, Time-domain, Inversion, Stochastic optimization},
  note = {(SEG, New Orleans)},
  doi = {10.1190/segam2015-5924937.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2015/louboutin2015SEGtcs/louboutin2015SEGtcs.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2015/louboutin2015SEGtcs/louboutin2015SEGtcs_poster.pdf}
}


@CONFERENCE{oghenekohwo2015EAGEuci,
  author = {Felix Oghenekohwo and Rajiv Kumar and Ernie Esser and Felix J. Herrmann},
  title = {Using common information in compressive time-lapse full-waveform inversion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2015},
  month = {06},
  abstract = {The use of time-lapse seismic data to monitor changes in
                  the subsurface has become standard practice in
                  industry. In addition, full-waveform inversion has
                  also been extended to time-lapse seismic to obtain
                  useful time-lapse information. The computational
                  cost of this method are becoming more pronounced as
                  the volume of data increases. Therefore, it is
                  necessary to develop fast inversion algorithms that
                  can also give improved time-lapse results. Rather
                  than following existing joint inversion algorithms,
                  we are motivated by a joint recovery model which
                  exploits the common information among the baseline
                  and monitor data. We propose a joint inversion
                  framework, leveraging ideas from distributed
                  compressive sensing and the modified Gauss-Newton
                  method for full-waveform inversion, by using the
                  shared information in the time-lapse data. Our
                  results on a realistic synthetic example highlight
                  the benefits of our joint inversion approach over a
                  parallel inversion method that does not exploit the
                  shared information. Preliminary results also
                  indicate that our formulation can address time-lapse
                  data with inconsistent acquisition geometries.},
  keywords = {EAGE, time-lapse, FWI},
  note = {(EAGE, Madrid)},
  doi = {10.3997/2214-4609.201413086},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/oghenekohwo2015EAGEuci/oghenekohwo2015EAGEuci.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/oghenekohwo2015EAGEuci/oghenekohwo2015EAGEuci_poster.pdf}
}


@CONFERENCE{oghenekohwo2015IIPFWItlf,
  author = {Felix Oghenekohwo and Rajiv Kumar and Ernie Esser and Felix J. Herrmann},
  title = {Time-lapse {FWI} with distributed compressed sensing},
  booktitle = {Inaugural Full-Waveform Inversion Workshop},
  year = {2015},
  month = {08-09},
  abstract = {In this talk, I will discuss our most recent application
                  of full-waveform inversion (FWI) to time-lapse
                  seismic data inversion. Specifically, I will
                  illustrate how a joint recovery model (JRM) can be
                  used in a joint inversion framework, leveraging
                  ideas from distributed compressed sensing (DCS). The
                  key idea involves exploiting the common information
                  in the baseline and monitor gradients during the
                  inversion.},
  keywords = {FWI, time lapse, distributed compressed sensing},
  note = {(Natal, Brazil)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IIPFWI/oghenekohwo2015IIPFWItlf/oghenekohwo2015IIPFWItlf_pres.pdf}
}


@CONFERENCE{oghenekohwo2015PIMSntc,
  author = {Felix Oghenekohwo and Felix J. Herrmann},
  title = {A new take on compressive time-lapse seismic acquisition, imaging and inversion},
  booktitle = {PIMS Workshop on Advances in Seismic Imaging and Inversion},
  year = {2015},
  month = {05},
  abstract = {Compressive sensing (CS), a sampling paradigm that is
                  changing how data is acquired and processed in many
                  fields, has attracted tremendous attention in recent
                  years. In exploration seismology, particularly in 3D
                  seismic technology, there have been published work
                  on the application of CS to seismic data
                  acquisition, processing and inversion. However, very
                  little is known about the implications of CS in
                  time-lapse (4D) seismic. Leveraging ideas from
                  distributed CS where more than one signal is
                  acquired and processed, as in 4D seismic, we present
                  a joint recovery algorithm for time-lapse seismic
                  data acquisition, imaging and inversion. The
                  algorithm exploits the common information in the
                  data sets (baseline and monitor) as part of an
                  inversion procedure. Application of our algorithm to
                  realistic synthetic examples highlight its
                  advantages over other conventional approaches for
                  processing time-lapse seismic data.},
  keywords = {PIMS, workshop, time-lapse},
  note = {(PIMS Workshop on Advances in Seismic Imaging and Inversion, University of Alberta, Edmonton)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/PIMS/2015/oghenekohwo2015PIMSntc/oghenekohwo2015PIMSntc_pres.pdf}
}


@CONFERENCE{oghenekohwo2015CSEGctl,
  author = {Felix Oghenekohwo and Felix J. Herrmann},
  title = {Compressive time-lapse seismic data processing using shared information},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2015},
  month = {05},
  abstract = {Time-lapse images void of acquisition and processing
                  artifacts can provide more useful information about
                  subsurface changes compared to those with
                  acquisition footprints and other unwanted
                  anomalies. Although, several pre-processing
                  techniques are being developed and used to mitigate
                  these unwanted artifacts, these operations can be
                  very expensive, challenging and data
                  dependent. Migration, as a processing tool, using a
                  sparsity constraint has been shown to reduce
                  artifacts drastically but little is known about the
                  significance for compressed time-lapse seismic
                  data. Leveraging ideas from distributed compressed
                  sensing, and motivated by our earlier work on
                  recovery of densely sampled time-lapse data from
                  compressively sampled measurements, we present a
                  sparsity-constrained migration for time-lapse data
                  that uses a common component shared by the baseline
                  and monitor data. Our algorithm tested on a
                  synthetic example highlights the advantages of
                  exploiting the common information, compared to ad
                  hoc methods that involve parallel processing of the
                  time-lapse data before differencing.},
  keywords = {CSEG, time-lapse},
  note = {(CSEG, Calgary)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2015/oghenekohwo2015CSEGctl/oghenekohwo2015CSEGctl.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2015/oghenekohwo2015CSEGctl/oghenekohwo2015CSEGctl_pres.pdf}
}


@CONFERENCE{peters2015AIPwri,
  author = {Bas Peters and Felix J. Herrmann},
  title = {Wavefield-reconstruction inversion},
  booktitle = {Conference on Applied Inverse Problems},
  year = {2015},
  month = {05},
  abstract = {Wavefield Reconstruction Inversion is a method for
                  PDE-constrained optimization, which revolves around
                  the estimation of fields using the PDE as well as
                  the observed data in a least-squares sense. The
                  method is quadratic penalty based, which offers some
                  interesting possibilities for the construction of
                  algorithms, compared to the Lagrangian form. One of
                  the main benefits of the method is when the initial
                  guess is far from the global minimizer.
                  Reduced-space and full-space algorithms are
                  discussed, including illustrative examples. The
                  method was developed with seismic applications in
                  mind, but applies to other PDE-constrained
                  optimization problems as well.},
  keywords = {AIP, Waveform reconstruction inversion, penalty method, optimization, full-waveform inversion},
  note = {(AIP, Helsinki)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/AIP/2015/peters2015AIPwri/peters2015AIPwri.pdf}
}


@CONFERENCE{peters2015PRECONasl,
  author = {Bas Peters and Chen Greif and Felix J. Herrmann},
  title = {An algorithm for solving least-squares problems with a {Helmholtz} block and multiple right-hand-sides},
  booktitle = {International Conference On Preconditioning Techniques For Scientific And Industrial Applications},
  year = {2015},
  month = {06},
  keywords = {least-squares, matrix-free, CGMN, randomized linear algebra, Helmholtz},
  note = {(PRECON, The Netherlands)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/PRECON/2015/peters2015PRECONasl/peters2015PRECONasl_pres.pdf}
}


@CONFERENCE{peters2015SEGWSsew,
  author = {Bas Peters and Felix J. Herrmann},
  title = {Simultaneous estimation of wavefields and medium parameters - reduced-space versus full-space waveform inversion},
  booktitle = {SEG Workshop on The Limit of FWI in Subsurface Parameter Recovery; New Orleans},
  year = {2015},
  month = {10},
  keywords = {SEG, workshop, waveform inversion, full-space, PDE-constrained optimization, quadratic penalty method},
  note = {(SEG, New Orleans)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2015/peters2015SEGWSsew/peters2015SEGWSsew_pres.pdf}
}


@CONFERENCE{peters2015SIAMmfq,
  author = {Bas Peters and Felix J. Herrmann and Chen Greif},
  title = {Matrix-free quadratic-penalty methods for {PDE}-constrained optimization},
  booktitle = {SIAM Conference on Computational Science and Engineering},
  year = {2015},
  month = {03},
  abstract = {The large scale of seismic waveform inversion makes
                  matrix free implementations essential. We show how
                  to exploit the quadratic penalty structure to
                  construct matrix free reduced-space and full-space
                  algorithms, which have some advantages over the
                  commonly used Lagrangian based methods for
                  PDE-constrained optimization. This includes the
                  construction of effective and sparse Hessian
                  approximations and reduced sensitivity to the
                  initial guess. A computational bottleneck is the
                  need to solve a large least squares problem with a
                  PDE block. When direct solvers are not available, we
                  propose a fast matrix free iterative approach with
                  reasonable memory requirements. It takes advantage
                  of the structure of the least squares problem with a
                  combination of preconditioning, low rank
                  decomposition and deflation.},
  keywords = {quadratic-penalty, least-squares, low-rank, waveform inversion, PDE-constrained optimization},
  note = {(SIAM, Salt Lake City)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2015/peters2015SIAMmfq/peters2015SIAMmfq.pdf}
}


@CONFERENCE{smithyman2015EAGEcwi,
  author = {Brendan R. Smithyman and Bas Peters and Felix J. Herrmann},
  title = {Constrained waveform inversion of colocated {VSP} and surface seismic data},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2015},
  month = {06},
  abstract = {Constrained Full-Waveform Inversion (FWI) is applied to
                  produce a high-resolution velocity model from both
                  Vertical Seismic Profiling (VSP) and surface seismic
                  data. The case study comes from the Permian Basin in
                  Texas, USA. This dataset motivates and tests several
                  new developments in methodology that enable recovery
                  of model results that sit within multiple a priori
                  constraint sets. These constraints are imposed
                  through a Projected Quasi-Newton (PQN) approach,
                  wherein the projection set is the intersection of
                  physical property bounds and anisotropic wavenumber
                  filtering. This enables the method to recover
                  geologically-reasonable models while preserving the
                  fast model convergence offered by a quasi-Newton
                  optimization scheme like l-BFGS. In the Permian
                  Basin example, low-frequency data from both arrays
                  are inverted together and regularized by this
                  projection approach. Careful choice of the
                  constraint sets is possible without requiring
                  tradeoff parameters as in a quadratic penalty
                  approach to regularization. Multiple 2D FWI results
                  are combined to produce an interpolated 3D model
                  that is consistent with the models from migration
                  velocity analysis and VSP processing, while offering
                  improved resolution and illumination of features
                  from both datasets.},
  keywords = {EAGE, waveform inversion, VSP},
  note = {(EAGE, Madrid)},
  doi = {10.3997/2214-4609.201412906},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/smithyman2015EAGEcwi/smithyman2015EAGEcwi.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/smithyman2015EAGEcwi/smithyman2015EAGEcwi_pres.pdf}
}


@CONFERENCE{tu2015PIMSsls,
  author = {Ning Tu and Felix J. Herrmann},
  title = {Sparse least-squares seismic imaging with source estimation utilizing multiples},
  booktitle = {PIMS Workshop on Advances in Seismic Imaging and Inversion},
  year = {2015},
  month = {05},
  abstract = {We present an least-squares seismic imaging method that
                  (1) is efficient by subsampling the monochromatic
                  source experiments; (2) does not require the prior
                  knowledge of the source wavelet, by inverting for
                  the seismic image and the source wavelet
                  simultaneously using variable projection; and (3)
                  makes active use of surface-related multiples to
                  increase the illumination and to resolve the scaling
                  ambiguity in source estimation, by tightly
                  integrating the well-established
                  surface-related-multiple-prediction into the
                  wave-equation based modelling. As a result, we are
                  able to obtain ambiguity-resolved, high-fidelity
                  least-squares inverted seismic images with
                  computational costs that are comparable to
                  conventional reverse-time migrations with all the
                  data.  We demonstrate the efficacy of the proposed
                  method using synthetic examples.},
  keywords = {PIMS, workshop, least-squares migration, source estimation, multiples, inversion, seismic imaging},
  note = {(PIMS Workshop on Advances in Seismic Imaging and Inversion, University of Alberta, Edmonton)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/PIMS/2015/tu2015PIMSsls/tu2015PIMSsls_pres.pdf}
}


@CONFERENCE{vanleeuwen2015EAGEafs,
  author = {Tristan van Leeuwen and Rajiv Kumar and Felix J. Herrmann},
  title = {Affordable full subsurface image volume---an application to {WEMVA}},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2015},
  month = {06},
  abstract = {Common image gathers are used in building velocity
                  models, inverting for anisotropy parameters, and
                  analyzing reservoir attributes. In this paper, we
                  offer a new perspective on image gathers, where we
                  glean information from the image volume via
                  efficient matrix-vector products. The proposed
                  formulation make the computation of full subsurface
                  image volume feasible. We illustrate how this
                  matrix-vector product can be used to construct
                  objective functions for automatic MVA.},
  keywords = {EAGE, MVA, wave-equation, randomized trace estimation},
  note = {(EAGE Workshop on Wave Equation based Migration Velocity Analysis, Madrid)},
  doi = {10.3997/2214-4609.201413498},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/vanleeuwen2015EAGEafs/vanleeuwen2015EAGEafs.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/vanleeuwen2015EAGEafs/vanleeuwen2015EAGEafs_pres.pdf}
}


@CONFERENCE{wang2015SIAMwds,
  author = {Rongrong Wang and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Wavefield-denoising and source encoding},
  booktitle = {SIAM Conference on Mathematical and Computational Issues in the Geosciences},
  year = {2015},
  month = {06-07},
  keywords = {SIAM, WRI, source encoding},
  note = {(SIAM Conference on Mathematical and Computational Issues in the Geosciences, Stanford University, California)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2015/wang2015SIAMwds/wang2015SIAMwds_pres.pdf}
}


@CONFERENCE{wason2015EAGEcsm,
  author = {Haneet Wason and Felix Oghenekohwo and Felix J. Herrmann},
  title = {Compressed sensing in {4-D marine}---recovery of dense time-lapse data from subsampled data without repetition},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2015},
  month = {06},
  abstract = {We present an extension of our time-jittered marine
                  acquisition for time-lapse surveys by working on
                  more realistic field acquisition scenarios by
                  incorporating irregular spatial grids without
                  insisting on repeatability between the
                  surveys. Since we are always subsampled in both the
                  baseline and monitor surveys, we are interested in
                  recovering the densely sampled baseline and monitor,
                  and then the (complete) 4-D difference from
                  subsampled/incomplete baseline and monitor data.},
  keywords = {EAGE, simultaneous acquisition, time-lapse, off-the-grid, NFFT},
  note = {(EAGE, Madrid)},
  doi = {10.3997/2214-4609.201413088},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/wason2015EAGEcsm/wason2015EAGEcsm.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/wason2015EAGEcsm/wason2015EAGEcsm_poster.pdf}
}


@CONFERENCE{witte2015IIPFWIspl,
  author = {Philipp A. Witte and Ning Tu and Ernie Esser and Mengmeng Yang and Mathias Louboutin and Felix J. Herrmann},
  title = {Sparsity-promoting least-square migration with linearized {Bregman} and compressive sensing},
  booktitle = {Inaugural Full-Waveform Inversion Workshop},
  year = {2015},
  month = {08-09},
  abstract = {We present a novel adaptation of a recently developed
                  relatively simple iterative algorithm to solve
                  large-scale sparsity-promoting optimization
                  problems. Our algorithm is particularly suitable to
                  large-scale geophysical inversion problems, such as
                  sparse least-squares reverse-time migration or
                  Kirchoff migration since it allows for a tradeoff
                  between parallel computations, memory allocation,
                  and turnaround times, by working on subsets of the
                  data with different sizes. Comparison of the
                  proposed method for sparse least-squares imaging
                  shows a performance that rivals and even exceeds the
                  performance of state-of-the art one-norm solvers
                  that are able to carry out least-squares migration
                  at the cost of a single migration with all data.},
  keywords = {FWI, sparsity promotion, migration, Bregman, compressed sensing, time domain},
  note = {(Natal, Brazil)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IIPFWI/witte2015IIPFWIspl/witte2015IIPFWIspl_pres.pdf}
}


@CONFERENCE{witte2015IIPFWItdf,
  author = {Philipp A. Witte and Mathias Louboutin and Felix J. Herrmann},
  title = {Time-domain {FWI} in {TTI} media},
  booktitle = {Inaugural Full-Waveform Inversion Workshop},
  year = {2015},
  month = {08-09},
  abstract = {We develop an inversion workflow for tilted transverse
                  isotropic (TTI) media using a purely acoustic
                  formulation of the wave equation. The anisotropic
                  modeling kernel is used for the forward modeling
                  operator, as well as for the adjoint Jacobian to
                  back propagate the data residual, thus providing the
                  true gradient of the FWI objective function.},
  keywords = {FWI, TTI, time domain},
  note = {(Natal, Brazil)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IIPFWI/witte2015IIPFWItdf/witte2015IIPFWItdf_pres.pdf}
}


%-----2014-----%

@CONFERENCE{herrmann2014SEGWSfli,
  author = {Ning Tu and Felix J. Herrmann},
  title = {Fast linearized inversion with surface-related multiples with source estimation},
  booktitle = {SEG Workshop on Using Multiples as Signal for Imaging; Denver},
  year = {2014},
  month = {10},
  abstract = {In the well-known SRME relation, multiples are expressed
                  as the multi-dimensional convolution between the
                  subsurface Green’s function and the downgoing
                  receiver wavefield. Therefore these multiples can be
                  considered as the response of the subsurface to an
                  "areal" source term given by the same downgoing
                  receiver wavefield. This relation can be used to
                  treat these multiples as signals instead of
                  considering them as noise that must removed before
                  seismic imaging. However, when we use conventional
                  reverse-time migration to image these multiple
                  events, it results in acausal imaging artifacts
                  caused by cross-correlations between wrong pairs of
                  up- and downgoing wavefields. We find that these
                  artifacts can be removed by adopting a
                  sparsity-promoting least-squares inversion
                  approach. However, iterative inversions go at the
                  expense of excessive computational costs. By
                  combining the SRME relation and wave-equation based
                  linearized modelling, we are able to significantly
                  reduce the costs by avoiding the dense matrix-matrix
                  multiplications of SRME and the number of
                  wave-equation solves via source subsampling. As a
                  result, we arrive at a method with a cost comparable
                  to that of a single reverse-time migration with all
                  shots.},
  keywords = {SEG, workshop, multiples, inversion},
  note = {(SEG Workshop, Denver)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/herrmann2014SEGWSfli/herrmann2014SEGWSfli.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/herrmann2014SEGWSfli/herrmann2014SEGWSfli_pres.pdf}
}


@CONFERENCE{herrmann2014SEGWS,
  author = {Felix J. Herrmann and Ernie Esser and Tristan van Leeuwen and Bas Peters},
  title = {Wavefield {Reconstruction} {Inversion} ({WRI}) – a new take on wave-equation based inversion},
  booktitle = {SEG Workshop on Full Waveform Inversion - Elastic Approaches and Issues with Anisotropy, Nonshallow Inversion, Poor Starting Model; Denver},
  year = {2014},
  month = {10},
  keywords = {SEG, workshop, WRI, FWI},
  note = {(SEG Workshop, Denver)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/herrmann2014SEGWSwri/herrmann2014SEGWSwri_pres.pdf}
}


@CONFERENCE{herrmann2014SEGAGUWSiii,
  author = {Felix J. Herrmann},
  title = {Imaging/{Inversion} with irregular/random sampled spatial arrays},
  booktitle = {SEG-AGU Workshop on Advances in Active + Passive "Full Wavefield" Seismic Imaging: From Reservoirs to Plate Tectonics},
  year = {2014},
  month = {07},
  keywords = {SEG-AGU, workshop, imaging, inversion, randomization},
  note = {(SEG-AGU Workshop, 22-24 July, Vancouver)}
}


@CONFERENCE{miao2014SEGrhss,
  author = {Lina Miao and Polina Zheglova and Felix J. Herrmann},
  title = {Randomized {HSS} acceleration for full-wave-equation depth stepping migration},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2014},
  month = {10},
  pages = {3752-3756},
  abstract = {In this work we propose to use the spectral projector
                  (Kenney and Laub, 1995) and randomized HSS technique
                  (Chandrasekaran et al., 2006) to achieve a stable
                  and affordable two-way wave equation depth stepping
                  migration algorithm.},
  keywords = {SEG, acoustic, randomized SVD, spectral projector, full wave equation migration, depth extrapolation},
  note = {(SEG)},
  doi = {http://dx.doi.org/10.1190/segam2014-1417.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/miao2014SEGrhss/miao2014SEGrhss.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/miao2014SEGrhss/miao2014SEGrhss_pres.pdf}
}


@CONFERENCE{wason2014SEGsss,
  author = {Haneet Wason and Rajiv Kumar and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Source separation via {SVD}-free rank minimization in the hierarchical semi-separable representation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2014},
  month = {10},
  pages = {120-126},
  abstract = {Recent developments in matrix rank optimization have
                  allowed for new computational approaches in the
                  field of source separation or deblending. In this
                  paper, we propose a source separation algorithm for
                  blended marine acquisition, where two sources are
                  deployed at different depths (over/under
                  acquisition). The separation method incorporates the
                  Hierarchical Semi-Separable structure (HSS) inside
                  rank-regularized least-squares formulations. The
                  proposed approach is suitable for large scale
                  problems, since it avoids SVD computations and uses
                  a low-rank factorized formulation instead. We
                  illustrate the performance of the new HSS-based
                  deblending approach by simulating an over/under
                  blended acquisition, wherein uniformly random time
                  delays (of < 1 second) are applied to one of the
                  sources.},
  keywords = {SEG, source separation, deblending, marine, acquisition, rank, HSS},
  note = {(SEG)},
  doi = {http://dx.doi.org/10.1190/segam2014-1583.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/wason2014SEGsss/wason2014SEGsss.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/wason2014SEGsss/wason2014SEGsss_pres.pdf}
}


@CONFERENCE{wason2014SEGrrt,
  author = {Haneet Wason and Felix Oghenekohwo and Felix J. Herrmann},
  title = {Randomization and repeatability in time-lapse marine acquisition},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2014},
  month = {10},
  pages = {46-51},
  abstract = {We present an extension of our time-jittered
                  simultaneous marine acquisition to time-lapse
                  surveys where the requirement for repeatability in
                  acquisition can be waived provided we know the
                  acquisition geometry afterwards. Our method, which
                  does not require repetition, gives 4-D signals
                  comparable to conventional methods where
                  repeatability is key to their success.},
  keywords = {SEG, marine, acquisition, time-laspe, deblending},
  note = {(SEG)},
  doi = {http://dx.doi.org/10.1190/segam2014-1677.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/wason2014SEGrrt/wason2014SEGrrt.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/wason2014SEGrrt/wason2014SEGrrt_pres.pdf}
}


@CONFERENCE{peters2014SEGsrh,
  author = {Bas Peters and Felix J. Herrmann},
  title = {A sparse reduced Hessian approximation for multi-parameter wavefield reconstruction inversion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2014},
  month = {10},
  pages = {1206-1210},
  abstract = {Multi-Parameter full-waveform inversion is a challenging
                  problem, because the unknown parameters appear in
                  the same wave equation and the magnitude of the
                  parameters can vary many orders of magnitude. This
                  makes accurate estimation of multiple-parameters
                  very difficult. To mitigate the problems, sequential
                  strategies, regularization methods and scalings of
                  gradients and quasi-Newton Hessians have been
                  proposed. All of these require design, fine-tuning
                  and adaptation to different waveform inversion
                  problems. We propose to use a sparse approximation
                  to the Hessian derived from a penalty-formulation of
                  the objective function. Sparseness allows to have
                  the Hessian in memory and compute update directions
                  at very low cost. This results in decent
                  reconstruction of the multiple parameters at very
                  low additional memory and computational expense.},
  keywords = {SEG, full-waveform inversion, optimization, Hessian, penalty method},
  note = {(SEG)},
  doi = {http://dx.doi.org/10.1190/segam2014-1667.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/peters2014SEGsrh/peters2014SEGsrh.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/peters2014SEGsrh/peters2014SEGsrh_pres.pdf}
}


@CONFERENCE{oghenekohwo2014SEGrsw,
  author = {Felix Oghenekohwo and Rajiv Kumar and Felix J. Herrmann},
  title = {Randomized sampling without repetition in time-lapse surveys},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2014},
  month = {10},
  pages = {4848-4852},
  abstract = {Vouching for higher levels of repeatability in
                  acquisition and processing of time-lapse (4D)
                  seismic data has become the standard with oil and
                  gas contractor companies, with significant
                  investment in the design of acquisition systems and
                  processing algorithms that attempt to address some
                  of the current 4D challenges, in particular, imaging
                  weak 4D signals. Recent developments from the field
                  of compressive sensing have shown the benefits of
                  variants of randomized sampling in marine seismic
                  acquisition and its impact for the future of seismic
                  exploration. Following these developments, we show
                  that the requirement for accurate survey repetition
                  in time-lapse seismic data acquisition can be waived
                  provided we solve a sparsity-promoting convex
                  optimization program that makes use of the shared
                  component between the baseline and monitor data. By
                  setting up a framework for inversion of the stacked
                  sections of a time-lapse data, given the pre-stack
                  data volumes, we are able to extract 4D signals with
                  relatively highfidelity from significant
                  subsamplings. Our formulation is applied to
                  time-lapse data that has been acquired with
                  different source/receiver geometries, paving the way
                  for an efficient approach to dealing with time-lapse
                  data acquired with initially poor repeatability
                  levels, provided the survey geometry details are
                  known afterwards.},
  keywords = {SEG, acquisition, repetition, 4D, time-lapse, random},
  note = {(SEG)},
  doi = {http://dx.doi.org/10.1190/segam2014-1627.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/oghenekohwo2014SEGrsw/oghenekohwo2014SEGrsw.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/oghenekohwo2014SEGrsw/oghenekohwo2014SEGrsw_pres.pdf}
}


@CONFERENCE{ghadermarzy2014SEGsti,
  author = {Navid Ghadermarzy and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Seismic trace interpolation with approximate message passing},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2014},
  month = {10},
  pages = {3621-3626},
  abstract = {Approximate message passing (AMP) is a computationally
                  effective algorithm for recovering high dimensional
                  signals from a few compressed measurements. In this
                  paper we use AMP to solve the seismic trace
                  interpolation problem. We also show that we can
                  exploit the fast AMP algorithm to improve the
                  recovery results of seismic trace interpolation in
                  curvelet domain, both in terms of convergence speed
                  and recovery performance by using AMP in Fourier
                  domain as a preprocessor for the L1 recovery in
                  Curvelet domain.},
  keywords = {SEG, interpolation, AMP},
  note = {(SEG)},
  doi = {http://dx.doi.org/10.1190/segam2014-0577.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/ghadermarzy2014SEGsti/ghadermarzy2014SEGsti.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/ghadermarzy2014SEGsti/ghadermarzy2014SEGsti_pres.pdf}
}


@CONFERENCE{lin2014SEGmdg,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Mitigating data gaps in the estimation of primaries by sparse inversion without data reconstruction},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2014},
  month = {10},
  pages = {4157-4161},
  abstract = {We propose to solve the Estimation of Primaries by
                  Sparse Inversion problem from a sesimic record with
                  missing near-offsets and large holes without any
                  explicit data reconstruction, by instead simulating
                  the missing multiple contributions with terms
                  involving auto-convolutions of the primary
                  wavefield. Exclusion of the unknown data as an
                  inversion variable from the REPSI process is
                  desireable, since it eliminates a significant source
                  of local minima that arises from attempting to
                  invert for the unobserved traces using primary and
                  multiple models that may be far-away from the true
                  solution. In this talk we investigate the necessary
                  modifications to the Robust EPSI algorithm to
                  account for the resulting non-linear modeling
                  operator, and demonstrate that just a few
                  auto-convolution terms are enough to satisfactorily
                  mitigate the effects of data gaps during the
                  inversion process.},
  keywords = {SEG, EPSI, REPSI, multiples, inversion, algorithm},
  note = {(SEG)},
  doi = {http://dx.doi.org/10.1190/segam2014-1680.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/lin2014SEGmdg/lin2014SEGmdg.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/lin2014SEGmdg/lin2014SEGmdg_pres.pdf}
}


@CONFERENCE{herrmann2014BIRSlrb,
  author = {Felix J. Herrmann},
  title = {Low-rank based matrix/tensor completions for the "real" (seismic) world},
  year = {2014},
  month = {10},
  date = {5-10},
  booktitle = {Workshop on Sparse Representations, Numerical Linear Algebra, and Optimization; Banff},
  keywords = {workshop, low-rank, matrix completion, hierarchical tucker, source separation},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/BIRS/2014/herrmann2014BIRSlrb_pres.pdf},
  note = {(Workshop at the Banff International Research Station for Mathematical Innovation and Discovery)}
}


@CONFERENCE{esser2014SIAMISsdc,
  author = {Ernie Esser},
  title = {Solving {DC} programs that promote group 1-sparsity},
  year = {2014},
  month = {05},
  booktitle = {SIAM Conference on Imaging Science},
  abstract = {Many interesting applications require solving nonconvex
                  problems that would be convex if not for a group
                  1-sparsity constraint. Splitting methods that are
                  effective for convex problems can still work well in
                  this setting. We propose several nonconvex penalties
                  that can be used to promote group 1-sparsity in the
                  framework of difference of convex or primal dual
                  hybrid gradient (PDHG) methods. Applications to
                  nonlocal inpainting, linear unmixing and phase
                  unwrapping are demonstrated.},
  keywords = {group 1-sparsity, difference of convex, phase unwrapping, nonconvex PDHG, operator splitting},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2014/esser2014SIAMISsdc_pres.pdf},
  note = {(SIAM Conference on Imaging Science)}
}


@CONFERENCE{herrmann2014ROSErpe,
  author = {Felix J. Herrmann},
  title = {Relax the physics and expand the search space – {FWI} via {Wavefield} {Reconstruction} {Inversion}},
  year = {2014},
  month = {05},
  booktitle = {ROSE Consortium; Norway},
  keywords = {ROSE, Consortium, FWI, WRI},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ROSE/2014/herrmann2014ROSErpe.pdf},
  note = {(ROSE Consortium)}
}


@CONFERENCE{herrmann2014EAGEWSrrt,
  author = {Haneet Wason and Felix Oghenekohwo and Felix J. Herrmann},
  title = {Randomization and repeatability in time-lapse marine acquisition},
  year = {2014},
  month = {04},
  booktitle = {EAGE Workshop on Land and Ocean Bottom; Broadband Full Azimuth Seismic Surveys; Spain},
  keywords = {EAGE, workshop, 4D seismic, marine acquisition},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/herrmann2014EAGEWSrrt.pdf},
  note = {(EAGE Workshop, Spain)}
}


@CONFERENCE{lago2014CMCCRMN,
  author = {Rafael Lago and Art Petrenko and Zhilong Fang and Felix J. Herrmann},
  title = {{CRMN} method for solving time-harmonic wave equation},
  year = {2014},
  month = {04},
  booktitle = {Copper Mountain Conference},
  abstract = {We address the solution of PDEs associated with the wave
                  propagation phenomena in heterogeneous media using
                  CGMN method. It consists of a conjugate gradients
                  method using Kaczmarz double sweeps as
                  preconditioner. This preconditioner has the property
                  of ``symmetrizing" the problem allowing short
                  recursion and Lanczos type of Krylov methods to be
                  employed. In this talk we propose the use of CR, a
                  minimal residual Krylov method closely related to
                  CG. We study the proposed method which we call CRMN
                  and discuss crucial aspects as the behaviour of the
                  norm of the residual and the norm of the true
                  error. We also discuss the result of an inversion of
                  a small subsample of the 3D velocity model SEG/EAGE
                  Overthrust using frugal full-waveform inversion
                  method with CRMN and CGMN for solving the associated
                  PDE, showing a strong interest in studying the
                  behaviour of CRMN for larger realistic cases.},
  keywords = {CRMN, CGMN, forward modelling, Helmholtz equation},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CMC/2014/lago2014CMCCRMN.pdf},
  note = {(Copper Mountain)}
}


@CONFERENCE{lago2014EAGEfst,
  author = {Rafael Lago and Art Petrenko and Zhilong Fang and Felix J. Herrmann},
  title = {Fast solution of time-harmonic wave-equation for full-waveform inversion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2014},
  month = {06},
  abstract = {For many full-waveform inversion techniques, the most
                  computationally intensive step is the computation of
                  a numerical solution for the wave equation on every
                  iteration. In the frequency domain approach, this
                  requires the solution of very large, complex,
                  sparse, ill-conditioned linear systems. In this
                  extended abstract we bring out attention
                  specifically to CGMN method for solving PDEs, known
                  for being flexible (i.e. it is able to treat equally
                  acoustic data as well as visco-elastic or more
                  complex scenarios) efficient with respect both to
                  memory and computation time, and controllable
                  accuracy of the final approximation. We propose an
                  improvement for the known CGMN method by imposing a
                  minimal residual condition, which incurs in one
                  extra model vector storage. The resulting algorithm
                  called CRMN enjoys several interesting properties as
                  monotonically nonincreasing behaviour of the norm of
                  the residual and minimal residual, guaranteeing
                  optimal convergence for the relative residual
                  criterion.  We discuss numerical experiments both in
                  an isolated PDE solve and also within the inversion
                  procedure, showing that in a realistic scenario we
                  can expect a speedup around 25\% when using CRMN
                  rather than CGMN.},
  keywords = {CRMN, CGMN, FWI, time-harmonic wave equation, EAGE},
  doi = {10.3997/2214-4609.20140812},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/lago2014EAGEfst/lago2014EAGEfst.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/lago2014EAGEfst/lago2014EAGEfst_pres.pdf}
}


@CONFERENCE{peters2014EAGEweb,
  author = {Bas Peters and Felix J. Herrmann and Tristan van Leeuwen},
  title = {Wave-equation based inversion with the penalty method: adjoint-state versus wavefield-reconstruction inversion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2014},
  month = {06},
  abstract = {In this paper we make a comparison between wave-equation
                  based inversions based on the adjoint-state and
                  penalty methods. While the adjoint-state method
                  involves the minimization of a data-misfit and exact
                  solutions of the wave-equation for the current
                  velocity model, the penalty-method aims to first
                  find a wavefield that jointly fits the data and
                  honours the physics, in a least-squares sense. Given
                  this reconstructed wavefield, which is a proxy for
                  the true wavefield in the true model, we calculate
                  updates for the velocity model. Aside from being
                  less nonlinear–the acoustic wave equation is linear
                  in the wavefield and model parameters but not in
                  both–the inversion is carried out over a solution
                  space that includes both the model and the
                  wavefield. This larger search space allows the
                  algortihm to circumnavigate local minima, very much
                  in the same way as recently proposed model
                  extentions try to acomplish. We include examples for
                  low frequencies, where we compare full-waveform
                  inversion results for both methods, for good and bad
                  starting models, and for high frequencies where we
                  compare reverse-time migration with linearized
                  imaging based on wavefield-reconstruction
                  inversion. The examples confirm the expected
                  benefits of the proposed method.},
  keywords = {full-waveform inversion, optimization, imaging, penalty method, EAGE},
  doi = {10.3997/2214-4609.20140704},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/peters2014EAGEweb/peters2014EAGEweb.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/peters2014EAGEweb/peters2014EAGEweb_pres.pdf}
}


@CONFERENCE{leeuwen2014EAGEntf,
  author = {Tristan van Leeuwen and Felix J. Herrmann and Bas Peters},
  title = {A new take on {FWI}: wavefield reconstruction inversion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2014},
  month = {06},
  abstract = {We discuss a recently proposed novel method for waveform
                  inversion: Wavefield Reconstruction Inversion
                  (WRI). As opposed to conventional FWI -- which
                  attempts to minimize the error between observed and
                  predicted data obtained by solving a wave equation
                  -- WRI reconstructs a wave-field from the data and
                  extracts a model-update from this wavefield by
                  minimizing the wave-equation residual. The method
                  does not require explicit computation of an adjoint
                  wavefield as all the necessary information is
                  contained in the reconstructed wavefield. We show
                  how the corresponding model updates can be
                  interpreted physically analogously to the
                  conventional imaging-condition-based approach.},
  keywords = {wavefield reconstruction inversion, penalty method, optimization, full-waveform inversion, EAGE},
  doi = {10.3997/2214-4609.20140703},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/leeuwen2014EAGEntf/leeuwen2014EAGEntf.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/leeuwen2014EAGEntf/leeuwen2014EAGEntf_pres.pdf}
}


@CONFERENCE{oghenekohwo2014EAGEtls,
  author = {Felix Oghenekohwo and Ernie Esser and Felix J. Herrmann},
  title = {Time-lapse seismic without repetition: reaping the benefits from randomized sampling and joint recovery},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2014},
  month = {06},
  abstract = {In the current paradigm of 4-D seismic, guaranteeing
                  repeatability in acquisition and processing of the
                  baseline and monitor surveys ranks highest amongst
                  the technical challenges one faces in detecting
                  time-lapse signals. By using recent insights from
                  the field of compressive sensing, we show that the
                  condition of survey repeatability can be relaxed as
                  long as we carry out a sparsitypromoting program
                  that exploits shared information between the
                  baseline and monitor surveys. By inverting for the
                  baseline and monitor survey as the common
                  "background", we are able to compute high-fidelity
                  4-D differences from carefully selected synthetic
                  surveys that have different sets of source/receivers
                  missing. This synthetic example is proof of concept
                  of an exciting new approach to randomized 4-D
                  acquisition where time-lapse signal can be computed
                  as long as the survey details, such as
                  source/receiver locations are known afterwards.},
  keywords = {4-D seismic, time-lapse, joint recovery, EAGE},
  doi = {10.3997/2214-4609.20141478},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/oghenekohwo2014EAGEtls/oghenekohwo2014EAGEtls.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/oghenekohwo2014EAGEtls/oghenekohwo2014EAGEtls_pres.pdf}
}


@CONFERENCE{dasilva2014EAGEhtucknoisy,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Low-rank promoting transformations and tensor interpolation - applications to seismic data denoising},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2014},
  month = {06},
  abstract = {In this abstract, we extend our previous work in
                  Hierarchical Tucker (HT) tensor completion, which
                  uses an extremely efficient representation for
                  representing high-dimensional tensors exhibit- ing
                  low-rank structure, to handle subsampled tensors
                  with noisy entries. We consider a "low-noise" case,
                  so that the energies of the noise and the signal are
                  nearly indistinguishable, and a ’high-noise’ case,
                  in which the noise energy is now scaled to the
                  amplitude of the entire data volume. We examine the
                  effect of the noise in terms of the singular values
                  along different matricizations of the data, i.e.,
                  reshaping of the tensor along different modes. By
                  interpreting this effect in the context of tensor
                  completion, we demonstrate the inefficacy of
                  denoising by this method in the source-receiver do-
                  main. In light of this observation, we transform the
                  decimated, noisy data in to the midpoint-offset
                  domain, which promotes low-rank behaviour in the
                  signal and high-rank behaviour in the noise. This
                  distinction between signal and noise allows low-rank
                  interpolation to effectively denoise the signal with
                  only a marginal increase in computational cost. We
                  demonstrate the effectiveness of this approach on a
                  4D frequency slice.},
  keywords = {hierarchical tucker, structured tensor, tensor interpolation, Riemannian optimization, low-rank transform, seismic denoising, EAGE},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/dasilva2014EAGEhtucknoisy/dasilva2014EAGEhtucknoisy.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/dasilva2014EAGEhtucknoisy/dasilva2014EAGEhtucknoisy_pres.pdf}
}


@CONFERENCE{esser2014EAGEacp,
  author = {Ernie Esser and Felix J. Herrmann},
  title = {Application of a convex phase retrieval method to blind seismic deconvolution},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2014},
  month = {06},
  abstract = {A classical strategy for blind seismic deconvolution is
                  to first estimate the autocorrelation of the unknown
                  source wavelet from the data and then recover the
                  wavelet by assuming it has minimum phase. However,
                  computing the minimum phase wavelet directly from
                  the amplitude spectrum can be sensitive to even
                  extremely small errors, especially in the
                  coefficients close to zero. Since the minimum phase
                  requirement follows from an assumption that the
                  wavelet should be as impulsive as possible, we
                  propose to directly estimate an impulsive wavelet by
                  minimizing a weighted l2 penalty subject to a
                  constraint on its amplitude spectrum. This nonconvex
                  model has the form of a phase retrieval problem, in
                  this case recovering a signal given only estimates
                  of the magnitudes of its Fourier
                  coefficients. Following recent work on convex
                  relaxations of phase retrieval problems, we propose
                  a convex semidefinite program for computing an
                  impulsive minimum phase wavelet whose amplitude
                  spectrum is close to a given estimate, and we show
                  that this can be robustly solved by a Douglas
                  Rachford splitting method for convex optimization.},
  keywords = {source wavelet estimation, blind deconvolution, convex phase retrieval, EAGE},
  doi = {10.3997/2214-4609.20141590},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/esser2014EAGEacp/esser2014EAGEacp.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/esser2014EAGEacp/esser2014EAGEacp_poster.pdf}
}


@CONFERENCE{lin2014EAGEmas,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Multilevel acceleration strategy for the robust estimation of primaries by sparse inversion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2014},
  month = {06},
  abstract = {We propose a method to substantially reduce the
                  computational costs of the Robust Estimation of
                  Primaries by Sparse Inversion algorithm, based on a
                  multilevel inversion strategy that shifts early
                  iterations of the method to successively coarser
                  spatial sampling grids. This method requires no
                  change in the core implementation of the original
                  algorithm, and additionally only relies on trace
                  decimation, low-pass filtering, and rudimentary
                  interpolation techniques. We furthermore demonstrate
                  with a synthetic seismic line significant
                  computational speedups using this approach.},
  keywords = {multiples, EPSI, REPSI, multigrid, multilevel, multiscale, EAGE},
  doi = {10.3997/2214-4609.20140672},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/lin2014EAGEmas/lin2014EAGEmas.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/lin2014EAGEmas/lin2014EAGEmas_pres.pdf}
}


@CONFERENCE{zheglova2014EAGEams,
  author = {Polina Zheglova and Felix J. Herrmann},
  title = {Application of matrix square root and its inverse to downward wavefield extrapolation},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2014},
  month = {06},
  abstract = {In this paper we propose a method for computation of the
                  square root of the Helmholtz operator and its
                  inverse that arise in downward extrapolation methods
                  based on one-way wave equation. Our approach
                  involves factorization of the discretized Helmholtz
                  operator at each depth by extracting the matrix
                  square root after performing the spectral projector
                  in order to eliminate the evanescent modes. The
                  computation of the square root of the discrete
                  Helmholtz operator and its inverse is done using
                  polynomial recursions and can be combined with low
                  rank matrix approximations to reduce the
                  computational cost for large problems. The resulting
                  square root operator is able to model the
                  propagating modes kinematically correctly at the
                  angles of up to 90 degree. Preliminary results on
                  convergence of iterations are presented in this
                  abstract. Potential applications include seismic
                  modeling, imaging and inversion.},
  keywords = {square root, modelling, one-way wave equation, extrapolation, EAGE},
  doi = {10.3997/2214-4609.20141184},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/zheglova2014EAGEams/zheglova2014EAGEams.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/zheglova2014EAGEams/zheglova2014EAGEams_pres.pdf}
}


@CONFERENCE{fang2014EAGEfuq,
  author = {Zhilong Fang and Curt Da Silva and Felix J. Herrmann},
  title = {Fast uncertainty quantification for {2D} full-waveform inversion with randomized source subsampling},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2014},
  month = {06},
  abstract = {Uncertainties arise in every area of seismic
                  exploration, especially in full-waveform inversion,
                  which is highly non-linear. In the framework of
                  Bayesian inference, uncertainties can be analyzed by
                  sampling the posterior probability density
                  distribution with a Markov chain Monte-Carlo (McMC)
                  method. We reduce the cost of computing the
                  posterior distribution by working with randomized
                  subsets of sources. These approximations, together
                  with the Gaussian assumption and approximation of
                  the Hessian, leads to a computational tractable
                  uncertainty quantification. Application of this
                  approach to a synthetic leads to standard deviations
                  and confidence intervals that are qualitatively
                  consistent with our expectations.},
  keywords = {Uncertainty quantification, FWI, Markov chain Monte Carlo, randomized sources subsampling, EAGE},
  doi = {10.3997/2214-4609.20140715},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/fang2014EAGEfuq/fang2014EAGEfuq.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/fang2014EAGEfuq/fang2014EAGEfuq_pres.pdf}
}


@CONFERENCE{petrenko2014EAGEaih,
  author = {Art Petrenko and Tristan van Leeuwen and Diego Oriato and Simon Tilbury and Felix J. Herrmann},
  title = {Accelerating an iterative {Helmholtz} solver with {FPGAs}},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2014},
  month = {06},
  abstract = {We implement the Kaczmarz row-projection algorithm
                  (Kaczmarz (1937)) on a CPU host + FPGA accelerator
                  platform using techniques of dataflow
                  programming. This algorithm is then used as the
                  preconditioning step in CGMN, a modified version of
                  the conjugate gradients method (Björck and Elfving
                  (1979)) that we use to solve the time-harmonic
                  acoustic isotropic constant density wave
                  equation. Using one accelerator we achieve a
                  speed-up of over 2× compared with one Intel core.},
  keywords = {CGMN, FPGA, Helmholtz equation, Kaczmarz, reconfigurable computing, EAGE},
  doi = {10.3997/2214-4609.20141141},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/petrenko2014EAGEaih/petrenko2014EAGEaih.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/petrenko2014EAGEaih/petrenko2014EAGEaih_pres.pdf}
}


@CONFERENCE{kumar2014EAGErank,
  author = {Rajiv Kumar and Aleksandr Y. Aravkin and Ernie Esser and Hassan Mansour and Felix J. Herrmann},
  title = {{SVD}-free low-rank matrix factorization : wavefield reconstruction via jittered subsampling and reciprocity},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2014},
  month = {06},
  abstract = {Recently computationally efficient rank optimization
                  techniques have been studied extensively to develop
                  a new mathematical tool for the seismic data
                  interpolation. So far, matrix completion problems
                  have been discussed where sources are subsample
                  according to a discrete uniform distribution. In
                  this paper, we studied the effect of two different
                  subsampling techniques on seismic data interpolation
                  using rank-regularized formulations, namely jittered
                  subsampling over uniform random subsampling. The
                  other objective of this paper is to combine the fact
                  of source-receiver reciprocity with the
                  rank-minimization techniques to enhance the accuracy
                  of missing-trace interpolation. We illustrate the
                  advantages of jittered subsampling and reciprocity
                  using a seismic line from Gulf of Suez to obtain
                  high quality results for interpolation, a key
                  application in exploration geophysics.},
  keywords = {low-rank, interpolation, reciprocity, EAGE},
  doi = {10.3997/2214-4609.20141394},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/kumar2014EAGErank/kumar2014EAGErank.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/kumar2014EAGErank/kumar2014EAGErank_pres.pdf}
}


@CONFERENCE{kumar2014EAGEeia,
  author = {Rajiv Kumar and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Extended images in action: efficient {WEMVA} via randomized probing},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2014},
  month = {06},
  abstract = {Image gathers as a function of subsurface offset are an
                  important tool for velocity analysis in areas of
                  complex geology. In this paper, we offer a new
                  perspective on image gathers by organizing the
                  extended image as a function of all subsurface
                  offsets and all subsurface points into a matrix
                  whose (i,j)^{th} entry captures the interaction
                  between gridpoints i and j. For even small problems,
                  it is infeasible to form and store this
                  matrix. Instead, we propose an efficient algorithm
                  to glean information from the image volume via
                  efficient matrix-vector products. We illustrate how
                  this can be used to construct objective functions
                  for automatic MVA.},
  keywords = {extended imaging, MVA, probing, EAGE},
  doi = {10.3997/2214-4609.20141492},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/kumar2014EAGEeia/kumar2014EAGEeia.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/kumar2014EAGEeia/kumar2014EAGEeia_pres.pdf}
}


@CONFERENCE{petrenko2014OGHPCaih,
  author = {Art Petrenko and Felix J. Herrmann and Diego Oriato and Simon Tilbury and Tristan van Leeuwen},
  title = {Accelerating an iterative {Helmholtz} solver with {FPGAs}},
  year = {2014},
  month = {03},
  booktitle = {OGHPC},
  abstract = {We implement the Kaczmarz row-projection algorithm
                  [Kaczmarz, 1937] on a CPU host + FPGA accelerator
                  platform using techniques of dataflow programming.
                  This algorithm is then used as the preconditioning
                  step in CGMN, a modified version of the conjugate
                  gradients method [Bj?rck and Elfving, 1979] that we
                  use to solve the time-harmonic acoustic isotropic
                  constant density wave equation.},
  keywords = {OGHPC, Kaczmarz, CGMN, FPGA, reconfigurable computing, Helmholtz equation},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/OGHPC/2014/petrenko2014OGHPCaih/petrenko2014OGHPCaih.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/OGHPC/2014/petrenko2014OGHPCaih/petrenko2014OGHPCaih_poster.pdf}
}


@CONFERENCE{herrmann2014CSEGbsw,
  author = {Felix J. Herrmann},
  title = {Breaking structure - why randomized sampling matters},
  booktitle = {CSEG Technical Luncheon},
  year = {2014},
  month = {01},
  abstract = {During this talk, I will explain how ideas from
                  compressive sensing and big data can be used to
                  reduce costs of seismic data acquisition and
                  wave-equation based inversion. The key idea is to
                  explore structure within the data by deliberately
                  breaking this structure with randomized sampling,
                  e.g., by randomizing source/receiver positions or by
                  source encoding, followed by an optimization
                  procedure that restores the structure and therefore
                  recovers the fully sampled data. These techniques
                  not only underpin recent advances in missing trace
                  interpolation and simultaneous acquisition but they
                  are also responsible for significant improvements in
                  full-waveform inversion and reverse-time
                  migration. We will illustrate these concepts using a
                  variety of compelling examples on realistic
                  synthetics and field data.},
  keywords = {CSEG, randomized sampling},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2014/herrmann2014CSEGbsw_pres.pdf}
}


%-----2013-----%

@CONFERENCE{herrmann2013EAGEfrtm,
  author = {Felix J. Herrmann and Ning Tu},
  title = {Fast {RTM} with multiples and source estimation},
  booktitle = {EAGE/SEG Forum - Turning noise into geological information: The next big step?},
  year = {2013},
  month = {11},
  abstract = {During this talk, we present a computationally efficient
                  (cost of 1-2 RTM's with all data) iterative
                  sparsity-promoting inversion framework where
                  surface-related multiples are jointly imaged with
                  primaries and where the source signature is
                  estimated on the fly. Our imaging algorithm is
                  computationally efficient because it works during
                  each iteration with small independent randomized
                  subsets of data. The multiples are handled by
                  introducing an areal source term that includes the
                  upgoing wavefield. We update the source signature
                  for each iteration using a variable projection
                  method. The resulting algorithm removes imaging
                  artifacts from surface-related multiples, estimates
                  and removes the imprint of the source, recovers true
                  amplitudes, is fast, and robust to linearization
                  errors by virtue of the statistical independence of
                  the subsets of data we are working with at each
                  iteration.},
  keywords = {EAGE, SEG, RTM, multiples, source estimation},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2013/herrmann2013EAGEfrtm/herrmann2013EAGEfrtm_pres.pdf}
}


@CONFERENCE{herrmann2013SPIErse,
  author = {Felix J. Herrmann},
  title = {Randomized sampling in exploration seismology},
  booktitle = {SPIE Optics and Photonics: Wavelets and Sparsity XV},
  year = {2013},
  month = {08},
  keywords = {SPIE, randomized sampling, exploration seismology},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SPIE/2013/herrmann2013SPIErse/herrmann2013SPIErse_pres.pdf}
}


@CONFERENCE{herrmann2013SEGpmc,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {A penalty method for PDE-constrained optimization with applications to wave-equation based seismic inversion},
  booktitle = {SEG Workshop on Computational Mathematics for Geophysics; Houston},
  year = {2013},
  month = {09},
  keywords = {SEG, workshop, penalty method, optimization},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/herrmann2013SEGpmc/herrmann2013SEGpmc_pres.pdf}
}


@CONFERENCE{dasilva2013SEGhtuck,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Structured tensor missing-trace interpolation in the {Hierarchical} {Tucker} format},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2013},
  month = {09},
  volume = {32},
  pages = {3623-3627},
  publisher = {SEG},
  abstract = {Owing to the large scale and dimensionality of a 3D
                  seismic experiment, acquiring fully-sampled data
                  according to the Nyquist criterion is an exceedingly
                  arduous and cost-prohibitive task. In this paper, we
                  develop tools to interpolate 5D seismic volumes with
                  randomly missing sources or receivers using a
                  relatively novel tensor format known as the
                  Hierarchical Tucker (HT) format. By exploiting the
                  underlying smooth structure of HT tensors,
                  specifically its smooth manifold structure, we
                  develop solvers which are fast, immediately
                  parallelizable, and SVD-free, making these solvers
                  amenable to large-scale problems where SVD-based
                  projection methods are far too costly. We also build
                  on intuition of multidimensional sampling from the
                  perspective of matrix-completion and demonstrate the
                  ability of our algorithms to recover frequency
                  slices even amidst very high levels of source
                  subsampling on a synthetic large-scale 3D North Sea
                  dataset.},
  keywords = {SEG, hierarchical tucker, structured tensor, tensor interpolation, differential geometry, Riemannian optimization, Gauss Newton},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/dasilva2013SEGhtuck/dasilva2013SEGhtuck.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/dasilva2013SEGhtuck/dasilva2013SEGhtuck_pres.pdf},
  doi = {10.1190/segam2013-0709.1}
}


@CONFERENCE{kumar2013SEGHSS,
  author = {Rajiv Kumar and Hassan Mansour and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Reconstruction of seismic wavefields via low-rank matrix factorization in the hierarchical-separable matrix representation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2013},
  volume = {32},
  month = {09},
  pages = {3628-3633},
  abstract = {Recent developments in matrix rank optimization have
                  allowed for new computational approaches in the
                  field of seismic data interpolation. In this paper,
                  we propose an approach for seismic data
                  interpolation which incorporates the Hierarchical
                  Semi-Separable Structure (HSS) inside
                  rank-regularized least-squares formulations for the
                  missing-trace interpolation problem. The proposed
                  approach is suitable for large scale problems, since
                  it avoids SVD computations and uses a low-rank
                  factorized formulation instead. We illustrate the
                  advantages of the new HSS approach by interpolating
                  a seismic line from the Gulf of Suez and compare the
                  reconstruction with conventional rank minimization.},
  keywords = {SEG, interpolation, HSS},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/kumar2013SEGHSS/kumar2013SEGHSS.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/kumar2013SEGHSS/kumar2013SEGHSS_pres.pdf},
  doi = {10.1190/segam2013-1165.1}
}


@CONFERENCE{kumar2013SEGAVA,
  author = {Rajiv Kumar and Tristan van Leeuwen and Felix J. Herrmann},
  title = {{AVA} analysis and geological dip estimation via two-way wave-equation based extended images},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2013},
  volume = {32},
  month = {09},
  pages = {423-427},
  abstract = {In this paper, we present an efficient way to compute
                  extended images for all subsurface offsets without
                  explicitly calculating the source and receiver
                  wavefields for all the sources. Because the extended
                  images contain all possible subsurface offsets, we
                  compute the angle-domain image gathers by selecting
                  the subsurface offset that is aligned with the local
                  dip. We also propose a method to compute the local
                  dip information directly from common-image-point
                  gathers. To assess the quality of the angle-domain
                  common-image-points gathers we compute the
                  angle-dependent reflectivity coefficients and
                  compare them with theoretical reflectivity
                  coefficients yielded by the (linearized) Zoeppritz
                  equations for a few synthetic models.},
  keywords = {SEG, AVA, dip, wave-equation, extended images},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/kumar2013SEGAVA/kumar2013SEGAVA.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/kumar2013SEGAVA/kumar2013SEGAVA_pres.pdf},
  doi = {10.1190/segam2013-1348.1}
}


@CONFERENCE{kumar2013SEGMVA,
  author = {Rajiv Kumar and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Efficient {WEMVA} using extended images},
  year = {2013},
  month = {09},
  booktitle = {SEG Workshop on Advances in Model Building, Imaging, and FWI; Houston},
  abstract = {Image gathers as a function of subsurface offset are an
                  important tool for velocity analysis in areas of
                  complex geology. Here, we offer a new perspective on
                  image gathers by organizing the extended image as a
                  function of all subsurface offsets and all
                  subsurface points in to a matrix whose (i,j)th entry
                  captures the interaction between gridpoints i and
                  j. For even small problems, it is infeasible to form
                  and store this matrix. Instead, we propose an
                  efficient algorithm to glean information from the
                  image volume via efficient matrix-vector
                  products. We illustrate how this can be used to
                  construct objective functions for automated MVA.},
  keywords = {SEG, workshop, MVA, inversion, poster},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/kumar2013SEGMVA/kumar2013SEGMVA_poster.pdf}
}


@CONFERENCE{li2013SEGodmvdaiedwawe,
  author = {Xiang Li and Anais Tamalet and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Optimization driven model-space versus data-space approaches to invert elastic data with the acoustic wave equation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2013},
  month = {09},
  volume = {32},
  pages = {986-990},
  abstract = {Inverting data with elastic phases using an acoustic
                  wave equation can lead to erroneous results,
                  especially when the number of iterations is too
                  high, which may lead to over fitting the
                  data. Several approaches have been proposed to
                  address this issue. Most commonly, people apply
                  "data-independent" filtering operations that are
                  aimed to deemphasize the elastic phases in the data
                  in favor of the acoustic phases. Examples of this
                  approach are nested loops over offset range and
                  Laplace parameters. In this paper, we discuss two
                  complementary optimization-driven methods where the
                  minimization process decides adaptively which of the
                  data or model components are consistent with the
                  objective. Specifically, we compare the Student's t
                  misfit function as the data-space alternative and
                  curvelet-domain sparsity promotion as the
                  model-space alternative. Application of these two
                  methods to a realistic synthetic lead to comparable
                  results that we believe can be improved by combining
                  these two methods.},
  keywords = {SEG, full-waveform inversion, elastic, least-squares},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/li2013SEGodmvdaiedwawe/li2013SEGodmvdaiedwawe.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/li2013SEGodmvdaiedwawe/li2013SEGodmvdaiedwawe_pres.pdf},
  doi = {10.1190/segam2013-1375.1}
}


@CONFERENCE{tu2013SEGldi,
  author = {Ning Tu and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Limitations of the deconvolutional imaging condition for two-way propagators},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2013},
  volume = {32},
  month = {09},
  pages = {3916-3920},
  abstract = {The deconvolutional imaging condition has gained wide
                  attention in recent years, as it is often used to
                  image surface-related multiples. However, we noticed
                  on close inspection that this condition was derived
                  from one-way propagation principles. Now that
                  two-way wave-equation based simulations have become
                  more affordable, we revisit the deconvolutional
                  imaging condition and reveal its limitations for
                  two-way propagators. First, it can distort the image
                  due to receiver-side propagation effects. Second,
                  when used to image surface-related multiples, it is
                  not capable of removing all interfering phantom
                  reflectors.},
  keywords = {SEG, migration, inversion, multiples},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/tu2013SEGldi/tu2013SEGldi.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/tu2013SEGldi/tu2013SEGldi_pres.pdf},
  doi = {10.1190/segam2013-1440.1}
}


@CONFERENCE{tu2013SEGcle,
  author = {Ning Tu and Xiang Li and Felix J. Herrmann},
  title = {Controlling linearization errors in $\ell_1$ regularized inversion by rerandomization},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2013},
  volume = {32},
  month = {09},
  pages = {4640-4644},
  abstract = {Linearized inversion is a data fitting procedure that
                  tries to match the observed seismic data with data
                  predicted by linearized modelling. In practice, the
                  observed data is not necessarily in the column space
                  of the linearized modelling operator. This can be
                  caused by lack of an accurate background velocity
                  model or by coherent noises not explained by
                  linearized modelling. Through carefully designed
                  experiments, we ob- serve that a moderate data
                  mismatch does not pose an issue if we can use all
                  the data in the inversion. However, artifacts do
                  arise from the mismatch when randomized
                  dimensionality reduction techniques are adopted to
                  speed up the inversion. To stabilize the inversion
                  for dimensionality reduction with randomized source
                  aggregates, we propose to rerandomize by drawing
                  independent simultaneous sources occasionally during
                  the inversion. The effect of this rerandomization is
                  remarkable because it results in virtually
                  artifact-free images at a cost comparable to a
                  single reverse-time migration. Implications of our
                  method are profound because we are now able to
                  resolve fine-scale steep subsalt features in a
                  computationally feasible manner.},
  keywords = {SEG, sparsity, inversion, rerandomization, message passing},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/tu2013SEGcle/tu2013SEGcle.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/tu2013SEGcle/tu2013SEGcle_pres.pdf},
  doi = {10.1190/segam2013-1302.1}
}


@CONFERENCE{wason2013SEGtjo,
  author = {Haneet Wason and Felix J. Herrmann},
  title = {Time-jittered ocean bottom seismic acquisition},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2013},
  month = {09},
  volume = {32},
  pages = {1-6},
  abstract = {Leveraging ideas from the field of compressed sensing,
                  we show how simultaneous or blended acquisition can
                  be setup as a -- compressed sensing problem. This
                  helps us to design a pragmatic time-jittered
                  marine acquisition scheme where multiple source
                  vessels sail across an ocean-bottom array firing
                  airguns at -- jittered source locations and
                  instances in time, resulting in better spatial
                  sampling, and speedup acquisition. Furthermore, we
                  can significantly impact the reconstruction quality
                  of conventional seismic data (from jittered data)
                  and demonstrate successful recovery by sparsity
                  promotion. In contrast to random (under)sampling,
                  acquisition via jittered (under)sampling helps in
                  controlling the maximum gap size, which is a
                  practical requirement of wavefield reconstruction
                  with localized sparsifying transforms. Results are
                  illustrated with simulations of time-jittered
                  marine acquisition, which translates to jittered
                  source locations for a given speed of the source
                  vessel, for two source vessels.},
  keywords = {SEG, acquisition, marine, OBC, jittered sampling, blending, deblending, interpolation},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/wason2013SEGtjo/wason2013SEGtjo.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/wason2013SEGtjo/wason2013SEGtjo_pres.pdf},
  doi = {10.1190/segam2013-1391.1}
}


@CONFERENCE{lin2013SEGdss,
  author = {Tim T.Y. Lin and Haneet Wason and Felix J. Herrmann},
  title = {Dense shot-sampling via time-jittered marine sources},
  booktitle = {SEG Workshop on Simultaneous Sources; Houston},
  year = {2013},
  month = {09},
  keywords = {SEG, workshop, acquisition, simultaneous sources},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/lin2013SEGdss/lin2013SEGdss_pres.pdf}
}


@CONFERENCE{dasilva2013SAMPTAhtuck,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Hierarchical {Tucker} tensor optimization - applications to tensor completion},
  year = {2013},
  month = {07},
  abstract = {In this work, we develop an optimization framework for
                  problems whose solutions are well-approximated by
                  Hierarchical Tucker tensors, an efficient structured
                  tensor format based on recursive subspace
                  factorizations. Using the differential geometric
                  tools presented here, we construct standard
                  optimization algorithms such as Steepest Descent and
                  Conjugate Gradient, for interpolating tensors in HT
                  format. We also empirically examine the importance
                  of one's choice of data organization in the success
                  of tensor recovery by drawing upon insights from the
                  Matrix Completion literature. Using these
                  algorithms, we recover various seismic data sets
                  with randomly missing source pairs.},
  keywords = {SAMPTA, hierarchical tucker, structured tensor, tensor interpolation, differential geometry, riemannian optimization},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SAMPTA/2013/dasilva2013SAMPTAhtuck/dasilva2013SAMPTAhtuck.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SAMPTA/2013/dasilva2013SAMPTAhtuck/dasilva2013SAMPTAhtuck_pres.pdf}
}


@CONFERENCE{dasilva2013EAGEhtucktensor,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Hierarchical {Tucker} tensor optimization - applications to {4D} seismic data interpolation},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2013},
  month = {06},
  abstract = {In this work, we develop optimization algorithms on the
                  manifold of Hierarchical Tucker (HT) tensors, an
                  extremely efficient format for representing
                  high-dimensional tensors exhibiting particular
                  low-rank structure. With some minor alterations to
                  existing theoretical developments, we develop an
                  optimization framework based on the geometric
                  understanding of HT tensors as a smooth manifold, a
                  generalization of smooth curves/surfaces. Building
                  on the existing research of solving optimization
                  problems on smooth manifolds, we develop Steepest
                  Descent and Conjugate Gradient methods for HT
                  tensors. The resulting algorithms converge quickly,
                  are immediately parallelizable, and do not require
                  the computation of SVDs. We also extend ideas about
                  favourable sampling conditions for missing-data
                  recovery from the field of Matrix Completion to
                  Tensor Completion and demonstrate how the
                  organization of data can affect the success of
                  recovery. As a result, if one has data with randomly
                  missing source pairs, using these ideas, coupled
                  with an efficient solver, one can interpolate
                  large-scale seismic data volumes with missing
                  sources and/or receivers by exploiting the
                  multidimensional dependencies in the data. We are
                  able to recover data volumes amidst extremely high
                  subsampling ratios (in some cases, > 75\%) using this
                  approach.},
  keywords = {EAGE, structured tensor, 3D data interpolation, riemannian optimization},
  doi = {10.3997/2214-4609.20130390},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2013/dasilva2013EAGEhtucktensor/dasilva2013EAGEhtucktensor.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2013/dasilva2013EAGEhtucktensor/dasilva2013EAGEhtucktensor_pres.pdf}
}


@CONFERENCE{kumar2013EAGEsind,
  author = {Rajiv Kumar and Aleksandr Y. Aravkin and Hassan Mansour and Ben Recht and Felix J. Herrmann},
  title = {Seismic data interpolation and denoising using {SVD}-free low-rank matrix factorization},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2013},
  month = {06},
  abstract = {Recent developments in rank optimization have allowed
                  new approaches for seismic data interpolation and
                  denoising. In this paper, we propose an approach for
                  simultaneous seismic data interpolation and
                  denoising using robust rank-regularized
                  formulations. The proposed approach is suitable for
                  large scale problems, since it avoids SVD
                  computations by using factorized formulations. We
                  illustrate the advantages of the new approach using
                  a seismic line from Gulf of Suez and 5D synthetic
                  seismic data to obtain high quality results for
                  interpolation and denoising, a key application in
                  exploration geophysics.},
  keywords = {EAGE, interpolation, denoising},
  doi = {10.3997/2214-4609.20130388},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2013/kumar2013EAGEsind/kumar2013EAGEsind.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2013/kumar2013EAGEsind/kumar2013EAGEsind_pres.pdf}
}


@CONFERENCE{lin2013EAGEcsd,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Cosparse seismic data interpolation},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2013},
  month = {06},
  abstract = {Many modern seismic data interpolation and redatuming
                  algorithms rely on the promotion of transform-domain
                  sparsity for high-quality results. Amongst the large
                  diversity of methods and different ways of realizing
                  sparse reconstruction lies a central question that
                  often goes unaddressed: is it better for the
                  transform-domain sparsity to be achieved through
                  explicit construction of sparse representations
                  (e.g., by thresholding of small transform-domain
                  coefficients), or by demanding that the algorithm
                  return physical signals which produces sparse
                  coefficients when hit with the forward transform?
                  Recent results show that the two approaches give
                  rise to different solutions when the transform is
                  redundant, and that the latter approach imposes a
                  whole new class of constraints related to where the
                  forward transform produces zero coefficients. From
                  this framework, a new reconstruction algorithm is
                  proposed which may allow better reconstruction from
                  subsampled signaled than what the sparsity
                  assumption alone would predict. In this work we
                  apply the new framework and algorithm to the case of
                  seismic data interpolation under the curvelet
                  domain, and show that it admits better
                  reconstruction than some existing L1 sparsity-based
                  methods derived from compressive sensing for a range
                  of subsampling factors.},
  keywords = {EAGE, cosparsity, interpolation, curvelet, algorithm, optimization},
  doi = {10.3997/2214-4609.20130387},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2013/lin2013EAGEcsd/lin2013EAGEcsd.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2013/lin2013EAGEcsd/lin2013EAGEcsd_pres.pdf}
}


@CONFERENCE{tu2013EAGElsm,
  author = {Ning Tu and Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Fast least-squares migration with multiples and source estimation},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2013},
  month = {06},
  abstract = {The advent of modern computing has made it possible to
                  do seismic imaging using least-squares reverse-time
                  migration. We obtain superior images by solving an
                  optimization problem that recovers the
                  true-amplitude images. However, its success hinges
                  on overcoming several issues, including overwhelming
                  problem size, unknown source wavelet, and
                  interfering coherent events like multiples. In this
                  abstract, we reduce the problem size by using ideas
                  from compressive sensing, and estimate source
                  wavelet by generalized variable projection. We also
                  demonstrate how to invert for subsurface information
                  encoded in surface-related multiples by
                  incorporating the free-surface operator as an areal
                  source in reverse-time migration. Our synthetic
                  examples show that multiples help to improve the
                  resolution of the image, as well as remove the
                  amplitude ambiguity in wavelet estimation.},
  keywords = {EAGE, imaging, sparse, source estimation, multiples},
  doi = {10.3997/2214-4609.20130727},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2013/tu2013EAGElsm/tu2013EAGElsm.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2013/tu2013EAGElsm/tu2013EAGElsm_pres.pdf}
}


@CONFERENCE{vanleeuwen2013EAGErobustFWI,
  author = {Tristan van Leeuwen and Aleksandr Y. Aravkin and Henri Calandra and Felix J. Herrmann},
  title = {In which domain should we measure the misfit for robust full waveform inversion?},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2013},
  month = {06},
  abstract = {Full-waveform inversion relies on minimizing the
                  difference between observed and modeled data, as
                  measured by some penalty function. A popular choice,
                  of course, is the least-squares penalty. However,
                  when outliers are present in the data, the use of
                  robust penalties such as the Huber or Student's t
                  may significantly improve the results since they put
                  relatively less weight on large residuals. In order
                  for robust penalties to be effective, the outliers
                  must be somehow localized and distinguishable from
                  the good data. We propose to first transform the
                  residual into a domain where the outliers are
                  localized before measuring the misfit with a robust
                  penalty. This is exactly how one would normally
                  devise filters to remove the noise before applying
                  conventional FWI. We propose to merge the two steps
                  and let the inversion process implicitly filter out
                  the noise. Results on a synthetic dataset show the
                  effectiveness of the approach.},
  keywords = {EAGE, full waveform inversion},
  doi = {10.3997/2214-4609.20130839},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2013/vanleeuwen2013EAGErobustFWI/vanleeuwen2013EAGErobustFWI.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2013/vanleeuwen2013EAGErobustFWI/vanleeuwen2013EAGErobustFWI_pres.pdf}
}


@CONFERENCE{wason2013EAGEobs,
  author = {Haneet Wason and Felix J. Herrmann},
  title = {Ocean bottom seismic acquisition via jittered sampling},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2013},
  month = {06},
  abstract = {We present a pragmatic marine acquisition scheme where
                  multiple source vessels sail across an ocean-bottom
                  array firing at airgunsjittered source locations and
                  instances in time. Following the principles of
                  compressive sensing, we can significantly impact the
                  reconstruction quality of conventional seismic data
                  (from jittered data) and demonstrate successful
                  recovery by sparsity promotion. In contrast to
                  random (under)sampling, acquisition via jittered
                  (under)sampling helps in controlling the maximum gap
                  size, which is a practical requirement of wavefield
                  reconstruction with localized sparsifying
                  transforms. Results are illustrated with simulations
                  of time-jittered marine acquisition, which
                  translates to jittered source locations for a given
                  speed of the source vessel, for two source vessels.},
  keywords = {EAGE, acquisition, blended, marine, deblending, interpolation},
  doi = {10.3997/2214-4609.20130379},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2013/wason2013EAGEobs/wason2013EAGEobs.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2013/wason2013EAGEobs/wason2013EAGEobs_pres.pdf}
}


@CONFERENCE{aravkin2013ICASSPssi,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and Ning Tu},
  title = {Sparse seismic imaging using variable projection},
  booktitle = {ICASSP},
  year = {2013},
  month = {05},
  abstract = {We consider an important class of signal processing
                  problems where the signal of interest is known to be
                  sparse, and can be recovered from data given
                  auxiliary information about how this data was
                  generated. For example, a sparse green's function
                  may be recovered from seismic experimental data
                  using sparsity optimization when the source
                  signature is known. Unfortunately, in practice this
                  information is often missing, and must be recovered
                  from data along with the signal using deconvolution
                  techniques. In this paper, we present a novel
                  methodology to simulta- neously solve for the sparse
                  signal and auxiliary parameters using a recently
                  proposed variable projection technique. Our main
                  contribution is to combine variable projection with
                  spar- sity promoting optimization, obtaining an
                  efficient algorithm for large-scale sparse
                  deconvolution problems. We demon- strate the
                  algorithm on a seismic imaging example.},
  keywords = {imaging, sparsity, optimization, variable projection},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ICASSP/2013/aravkin2013ICASSPssi/aravkin2013ICASSPssi.pdf}
}


@CONFERENCE{petrenko2013HPCSsaoc,
  author = {Art Petrenko and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Software acceleration of {CARP}, an iterative linear solver and preconditioner},
  organization = {HPCS},
  year = {2013},
  month = {06},
  abstract = {We present the results of software optimization of a
                  row-wise preconditioner (Component Averaged Row
                  Projections) for the method of conjugate gradients,
                  which is used to solve the diagonally banded
                  Helmholtz system representing frequency domain,
                  isotropic acoustic seismic wave simulation. We
                  demonstrate that in our application, a
                  preconditioner bound to one processor core and
                  accessing memory contiguously reduces execution time
                  by 7\% for matrices having on the order of 108
                  non-zeros. For reference we note that our C
                  implementation is over 80 times faster than the
                  corresponding code written for a high-level
                  numerical analysis language.},
  keywords = {HPCS, Helmholtz equation, Kaczmarz, software, wave propagation, frequency-domain},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/HPCS/2013/petrenko2013HPCSsaoc/petrenko2013HPCSsaoc.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/HPCS/2013/petrenko2013HPCSsaoc/petrenko2013HPCSsaoc_poster.pdf}
}


@CONFERENCE{herrmann2013KAUSTrse,
  author = {Felix J. Herrmann},
  title = {Randomized sampling in exploration seismology},
  booktitle = {KAUST},
  organization = {KAUST},
  year = {2013},
  month = {05},
  keywords = {KAUST, randomized sampling, exploration seismology},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/KAUST/2013/herrmann2013KAUSTrse/herrmann2013KAUSTrse_pres.pdf}
}


@CONFERENCE{herrmann2013SEGOMANrdw,
  author = {Felix J. Herrmann},
  title = {Recent developments in wave-equation based inversion technology},
  booktitle = {SEG Workshop on FWI; Oman},
  year = {2013},
  month = {04},
  keywords = {SEG, workshop, randomized sampling, exploration seismology, 3D, FWI},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/herrmann2013SEGOMANrdw/herrmann2013SEGOMANrdw_pres.pdf}
}


@CONFERENCE{herrmann2013SIAMdrfwi,
  author = {Felix J. Herrmann},
  title = {Dimensionality reduction in {FWI}},
  booktitle = {SIAM},
  year = {2013},
  month = {02},
  keywords = {SIAM, FWI, dimensionality reduction},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2013/herrmann2013SIAMdrfwi/herrmann2013SIAMdrfwi_pres.pdf}
}


@CONFERENCE{miao2013CSEGaospsa,
  author = {Lina Miao and Felix J. Herrmann},
  title = {Acceleration on sparse promoting seismic applications},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2013},
  month = {05},
  abstract = {Sparse promoting oriented problems are never new in
                  seismic applications. Back in 1970s, geophysicists
                  had well exploited the robustness of sparse
                  solutions. Moreover, with the emerging usage of
                  compressed sensing in recent years, sparse recovery
                  have been favored in dealing with 'curse of
                  dimensionality' in various seismic field
                  acquisition, data processing, and imaging
                  applications. Although sparsity has provided a
                  promising approach, solving for it presents a big
                  challenge. How to work efficiently with the
                  extremely large-scale seismic problem, and how to
                  improve the convergence rate reducing computation
                  time are most frequently asked questions in this
                  content. In this abstract, the author proposed a new
                  algorithm -- PQN$\ell_1$, trying to address those
                  questions. One example on seismic data processing is
                  included.},
  keywords = {CSEG, sparsity-promotion, SPG$\ell_1$, projected Quasi Newton},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2013/miao2013CSEGaospsa/miao2013CSEGaospsa.pdf}
}


@CONFERENCE{oghenekohwo2013CSEGnratld,
  author = {Felix Oghenekohwo and Felix J. Herrmann},
  title = {Assessing the need for repeatability in acquisition of time-lapse data},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2013},
  month = {05},
  abstract = {There are several factors that affect the repeatability
                  of 4D(time-lapse) seismic data. One of the most
                  significant factors is the repeatability of the
                  acquisition, particularly the locations of the
                  sources and receivers. It is important to repeat the
                  source-receiver locations, used during the baseline
                  survey, in the monitor or repeat survey. Also, it is
                  essential that the stacked data volumes used for
                  time-lapse analysis are created using the same
                  offset ranges for each survey. This condition is
                  crucial in order to be able to produce an image of
                  the same location over a period of time and enhances
                  proper reservoir characterization. The cost of
                  repeating the seismic acquisition is very expensive,
                  as often times, the receiver array has to be left at
                  the same location over the period for which the data
                  will be acquired. In other words, it is important to
                  repeat the acquisition geometry as much as
                  possible. In this talk, we investigate the results
                  of changing the acquisition geometry, by a random
                  placement of the receivers for both the baseline
                  surveys and newer (monitor) surveys. Results show
                  that we are still able to observe any time-lapse
                  effects from the proposed acquisition geometry. Our
                  experiments have been performed on a synthetic
                  model.},
  keywords = {acquisition, CSEG, time-lapse, migration},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2013/oghenekohwo2013CSEGnratld/oghenekohwo2013CSEGnratld.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2013/oghenekohwo2013CSEGnratld/oghenekohwo2013CSEGnratld_pres.pdf}
}


@CONFERENCE{Mansour11TRwmmw,
  author = {Hassan Mansour and Ozgur Yilmaz},
  title = {Weighted -$\ell_1$ minimization with multiple weighting sets},
  booktitle = {Proc. SPIE},
  year = {2011},
  month = {09},
  volume = {8138},
  pages = {813809-813809-13},
  abstract = {In this paper, we study the support recovery conditions
                  of weighted -$\ell_1$ minimization for signal
                  reconstruction from compressed sensing measurements
                  when multiple support estimate sets with different
                  accuracy are available. We identify a class of
                  signals for which the recovered vector from
                  -$\ell_1$ minimization provides an accurate support
                  estimate. We then derive stability and robustness
                  guarantees for the weighted -$\ell_1$ minimization
                  problem with more than one support estimate. We show
                  that applying a smaller weight to support estimate
                  that enjoy higher accuracy improves the recovery
                  conditions compared with the case of a single
                  support estimate and the case with standard, i.e.,
                  non-weighted,-$\ell_1$ minimization. Our theoretical
                  results are supported by numerical simulations on
                  synthetic signals and real audio signals.},
  keywords = {compressive sensing, optimization},
  notes = {TR-2011-07},
  doi = {10.1117/12.894165},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SPIE/2011/Mansour11TRwmmw/Mansour11TRwmmw.pdf}
}


@CONFERENCE{aravkin2011EAGEnspf,
  author = {Aleksandr Y. Aravkin and James V. Burke and Felix J. Herrmann and Tristan van Leeuwen},
  title = {A nonlinear sparsity promoting formulation and algorithm for full waveform inversion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2011},
  month = {05},
  abstract = {Full Waveform Inversion (FWI) is a computational
                  procedure to extract medium parameters from seismic
                  data. FWI is typically formulated as a nonlinear
                  least squares optimization problem, and various
                  regularization techniques are used to guide the
                  optimization because the problem is illposed. In
                  this paper, we propose a novel sparse regularization
                  which exploits the ability of curvelets to
                  efficiently represent geophysical images. We then
                  formulate a corresponding sparsity promoting
                  constrained optimization problem, which we call
                  Nonlinear Basis Pursuit Denoise (NBPDN) and present
                  an algorithm to solve this problem to recover medium
                  parameters. The utility of the NBPDN formulation and
                  efficacy of the algorithm are demonstrated on a
                  stylized cross-well experiment, where a sparse
                  velocity perturbation is recovered with higher
                  quality than the standard FWI formulation (solved
                  with LBFGS). The NBPDN formulation and algorithm can
                  recover the sparse perturbation even when the data
                  volume is compressed to 5 \% of the original
                  size using random superposition.},
  keywords = {EAGE, full-waveform inversion, optimization},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2011/aravkin11EAGEnspf/aravkin11EAGEnspf_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2011/aravkin11EAGEnspf/aravkin11EAGEnspf.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=50199}
}


@CONFERENCE{aravkin2012ICASSProbustb,
  author = {Aleksandr Y. Aravkin and Michael P. Friedlander and Tristan van Leeuwen},
  title = {Robust inversion via semistochastic dimensionality reduction},
  booktitle = {ICASSP},
  year = {2012},
  pages = {5245-5248},
  organization = {ICASSP},
  abstract = {We consider a class of inverse problems where it is
                  possible to aggregate the results of multiple
                  experiments. This class includes problems where the
                  forward model is the solution operator to linear
                  ODEs or PDEs. The tremendous size of such problems
                  motivates the use dimensionality reduction (DR)
                  techniques based on randomly mixing
                  experiments. These techniques break down, however,
                  when robust data-fitting formulations are used,
                  which are essential in cases of missing data,
                  unusually large errors, and systematic features in
                  the data unexplained by the forward model. We survey
                  robust methods within a statistical framework, and
                  propose a sampling optimization approach that allows
                  DR. The efficacy of the methods are demonstrated for
                  a large-scale seismic inverse problem using the
                  robust Student's t-distribution, where a useful
                  synthetic velocity model is recovered in the extreme
                  scenario of 60\% corrupted data. The sampling
                  approach achieves this recovery using 20\% of the
                  effort required by a direct robust approach.},
  keywords = {ICASSP},
  doi = {10.1109/ICASSP.2012.6289103},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ICASSP/2012/AravkinFriedlanderLeeuwen/AravkinFriedlanderLeeuwen.pdf }
}


@CONFERENCE{aravkin2011SIAMfwi,
  author = {Aleksandr Y. Aravkin and Felix J. Herrmann and Tristan van Leeuwen and James V. Burke and Xiang Li},
  title = {Full waveform inversion with compressive updates},
  booktitle = {SIAM},
  year = {2011},
  organization = {SIAM CS\&E 2011},
  abstract = {Full-waveform inversion relies on large multi-experiment
                  data volumes. While improvements in acquisition and
                  inversion have been extremely successful, the
                  current push for higher quality models reveals
                  fundamental shortcomings handling increasing problem
                  sizes numerically. To address this fundamental
                  issue, we propose a randomized
                  dimensionality-reduction strategy motivated by
                  recent developments in stochastic optimization and
                  compressive sensing. In this formulation
                  conventional Gauss-Newton iterations are replaced by
                  dimensionality-reduced sparse recovery problems with
                  source encodings.},
  keywords = {SLIM, full-waveform inversion},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2011/aravkin2011SIAMfwi/aravkin2011SIAMfwi.pdf}
}


@CONFERENCE{herrmann2011SEGffw,
  author = {Aleksandr Y. Aravkin and Felix J. Herrmann and Tristan van Leeuwen and Xiang Li},
  title = {Fast full-waveform inversion with compressive sensing},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2011},
  keywords = {SEG, SLIM, full-waveform inversion},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2011/HerrmannSEG2011fws/HerrmannSEG2011fws.pdf}
}


@CONFERENCE{aravkin2011EAGEspfwi,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and James V. Burke and Felix J. Herrmann},
  title = {Sparsity promoting formulations and algorithms for {FWI}},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2011},
  keywords = {EAGE, full-waveform inversion},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2011/Aravkin2011EAGEspfwi/Aravkin2011EAGEspfwi.pdf}
}


@CONFERENCE{aravkin2011ICIAMspf,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and James V. Burke and Felix J. Herrmann},
  title = {Sparsity promoting formulations and algorithms for {FWI}},
  booktitle = {ICIAM},
  year = {2011},
  organization = {ICIAM 2011},
  abstract = {Full Waveform Inversion (FWI) is a computational
                  procedure to extract medium parameters from seismic
                  data. FWI is typically formulated as a nonlinear
                  least squares optimization problem, and various
                  regularization techniques are used to guide the
                  optimization because the problem is ill-posed. We
                  propose a novel sparse regularization which exploits
                  the ability of curvelets to efficiently represent
                  geophysical images.  We then formulate a
                  corresponding sparsity promoting constrained
                  optimization problem, which we solve using an open
                  source algorithm. The techniques are applicable to
                  any inverse problem where sparsity modeling is
                  appropriate. We demonstrate the efficacy of the
                  formulation on a toy example (stylized cross-well
                  experiment) and on a realistic Seismic example
                  (partial Marmoussi model). We also discuss the
                  tradeoff between model fit and sparsity promotion,
                  with a view to extend existing techniques for linear
                  inverse problems to the case where the forward model
                  is nonlinear.},
  date-added = {2011-07-15},
  keywords = {SLIM,Presentation,Full-waveform inversion,Optimization},
  month = {07},
  note = {Presented at AMP Medical and Seismic Imaging, 2011, Vancouver BC.},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ICIAM/2011/aravkin2011ICIAMspf/aravkin2011ICIAMspf_pres.pdf}
}


@CONFERENCE{aravkin2011ICIAMrfwiu,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Robust {FWI} using {Student's} t-distribution},
  booktitle = {ICIAM},
  year = {2011},
  organization = {ICIAM 2011},
  abstract = {Iterative inversion algorithms require repeated
                  simulation of 3D time-dependent acoustic, elastic,
                  or electromagnetic wave fields, extending hundreds
                  of wavelengths and hundreds of periods. Also,
                  seismic data is rich in information at every
                  representable scale. Thus simulation-driven
                  optimization approaches to inversion impose great
                  demands on simulator efficiency and accuracy. While
                  computer hardware advances have been of critical
                  importance in bringing inversion closer to practical
                  application, algorithmic advances in simulator
                  methodology have been equally important. Speakers in
                  this two-part session will address a variety of
                  numerical issues arising in the wave simulation, and
                  in its application to inversion. },
  date-added = {2011-07-20},
  keywords = {SLIM, ICIAM, full-waveform inversion, optimization},
  month = {07},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ICIAM/2011/aravkin2011ICIAMrfwiu/aravkin2011ICIAMrfwiu.pdf}
}


@CONFERENCE{aravkin2011SEGrobust,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Robust full-waveform inversion using the {Student's} t-distribution},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2011},
  month = {09},
  volume = {30},
  pages = {2669-2673},
  organization = {SEG},
  abstract = {Full-waveform inversion (FWI) is a computational
                  procedure to extract medium parameters from seismic
                  data. Robust methods for FWI are needed to overcome
                  sensitivity to noise and in cases where modeling is
                  particularly poor or far from the real data
                  generating process. We survey previous robust
                  methods from a statistical perspective, and use this
                  perspective to derive a new robust method by
                  assuming the random errors in our model arise from
                  the Student's t-distribution. We show that in
                  contrast to previous robust methods, the new method
                  progres- sively down-weighs large outliers,
                  effectively ignoring them once they are large
                  enough. This suggests that the new method is more
                  robust and suitable for situations with very poor
                  data quality or modeling. Experiments show that the
                  new method recovers as well or better than previous
                  robust methods, and can recover models with quality
                  comparable to standard methods on noise-free data
                  when some of the data is completely corrupted, and
                  even when a marine acquisition mask is entirely
                  ignored in the modeling. The ability to ignore a
                  marine acquisition mask via robust FWI methods
                  offers an opportunity for stochastic optimization
                  methods in marine acquisition.},
  keywords = {SEG, full-waveform inversion, optimization},
  doi = {10.1190/1.3627747},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2011/aravkin11SEGrobust/aravkin11SEGrobust.pdf}
}


@CONFERENCE{aravkin2012ICASSPfastseis,
  author = {Aleksandr Y. Aravkin and Xiang Li and Felix J. Herrmann },
  title = {Fast seismic imaging for marine data},
  booktitle = {ICASSP},
  year = {2012},
  organization = {ICASSP},
  abstract = {Seismic imaging can be formulated as a linear inverse
                  problem where a medium perturbation is obtained via
                  minimization of a least-squares misfit
                  functional. The demand for higher resolution images
                  in more geophysically complex areas drives the need
                  to develop techniques that handle problems of
                  tremendous size with limited computational
                  resources. While seismic imaging is amenable to
                  dimensionality reduction techniques that collapse
                  the data volume into a smaller set of "super-shots",
                  these techniques break down for complex acquisition
                  geometries such as marine acquisition, where sources
                  and receivers move during acquisition. To meet these
                  challenges, we propose a novel method that combines
                  sparsity-promoting (SP) solvers with random subset
                  selection of sequential shots, yielding a SP
                  algorithm that only ever sees a small portion of the
                  full data, enabling its application to very
                  large-scale problems. Application of this technique
                  yields excellent results for a complicated
                  synthetic, which underscores the robustness of
                  sparsity promotion and its suitability for seismic
                  imaging.},
  keywords = {ICASSP},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ICASSP/2012/AravkinLiHerrmann/AravkinLiHerrmann.pdf}
}


@CONFERENCE{aravkin2012SEGST,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and Kenneth Bube and Felix J. Herrmann},
  title = {On non-uniqueness of the {Student's} t-formulation for linear inverse problems},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2012},
  month = {11},
  volume = {31},
  pages = {1-5},
  organization = {SEG},
  abstract = {We review the statistical interpretation of inverse
                  problem formulations, and the motivations for
                  selecting non-convex penalties for robust behaviour
                  with respect to measurement outliers or artifacts in
                  the data. An important downside of using non-convex
                  formulations such as the Student's t is the
                  potential for non-uniqueness, and we present a
                  simple example where the Student's t penalty can be
                  made to have many local minima by appropriately
                  selecting the degrees of freedom parameter. On the
                  other hand, the non-convexity of the Student's t is
                  precisely what gives it the ability to ignore
                  artifacts in the data. We explain this idea, and
                  present a stylized imaging experiment, where the
                  Student's t is able to recover a velocity
                  perturbation from data contaminated by a very
                  peculiar artifact --- data from a different velocity
                  perturbation. The performance of Student's t
                  inversion is investigated empirically for different
                  values of the degrees of freedom parameter, and
                  different initial conditions.},
  keywords = {Student's t, robust, non-convex, uniqueness, SEG},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2012/aravkin2012SEGST/aravkin2012SEGST.pdf},
  doi = {10.1190/segam2012-1558.1}
}


@CONFERENCE{aravkin2012EAGErobust,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and Henri Calandra and Felix J. Herrmann},
  title = {Source estimation for frequency-domain {FWI} with robust penalties},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2012},
  month = {06},
  abstract = {Source estimation is an essential component of full
                  waveform inversion. In the standard frequency domain
                  formulation, there is closed form solution for the
                  the optimal source weights, which can thus be
                  cheaply estimated on the fly. A growing body of work
                  underscores the importance of robust modeling for
                  data with large outliers or artifacts that are not
                  captured by the forward model. Effectively, the
                  least-squares penalty on the residual is replaced by
                  a robust penalty, such as Huber, Hybrid `1-`2 or
                  Student’s t. As we will demonstrate, it is essential
                  to use the same robust penalty for source
                  estimation. In this abstract, we present a general
                  approach to robust waveform inversion with robust
                  source estimation. In this general formulation,
                  there is no closed form solution for the optimal
                  source weights so we need to solve a scalar
                  optimization problem to obtain these weights.  We
                  can efficiently solve this optimization problem with
                  a Newton-like method in a few iterations. The
                  computational cost involved is of the same order as
                  the usual least-squares source estimation procedure.
                  We show numerical examples illustrating robust
                  source estimation and robust waveform inversion on
                  synthetic data with outliers.},
  keywords = {EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/aravkin2012EAGErobust/aravkin2012EAGErobust_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/aravkin2012EAGErobust/aravkin2012EAGErobust.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=59196}
}


@CONFERENCE{vandenberg07VONipo,
  author = {Ewout van den Berg and Michael P. Friedlander},
  title = {In pursuit of a root},
  year = {2007},
  month = {07},
  organization = {Von Neumann Symposium},
  quality = {1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/vonNeuman/2007/vandenberg07VONipo/vandenberg07VONipo.pdf}
}


@CONFERENCE{beyreuther2005SEGcot,
  author = {Moritz Beyreuther and Jamin Cristall and Felix J. Herrmann},
  title = {Computation of time-lapse differences with {3-D} directional frames},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2005},
  volume = {24},
  pages = {2488-2491},
  organization = {SEG},
  abstract = {We present an alternative method of extracting
                  production related differences from time-lapse
                  seismic data sets. Our method is not based on the
                  actual subtraction of the two data sets, risking the
                  enhancement of noise and introduction of artifacts
                  due to local phase rotation and slightly misaligned
                  events. Rather, it mutes events of the monitor
                  survey with respect to the baseline survey based on
                  the magnitudes of coefficients in a sparse and local
                  atomic decomposition. Our technique is demonstrated
                  to be an effective tool for enhancing the time-lapse
                  signal from surveys which have been cross-equalized.
                  {\copyright}2005 Society of Exploration
                  Geophysicists},
  keywords = {SLIM, SEG},
  doi = {10.1190/1.2148227},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2005/Beyreuther05SEGcot/Beyreuther05SEGcot.pdf}
}


@CONFERENCE{beyreuther2004EAGEcdo,
  author = {Moritz Beyreuther and Felix J. Herrmann and Jamin Cristall},
  title = {Curvelet denoising of {4-D} seismic},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2004},
  month = {06},
  abstract = {With burgeoning world demand and a limited rate of
                  discovery of new reserves, there is increasing
                  impetus upon the industry to optimize recovery from
                  already existing fields. 4D, or time-lapse, seismic
                  imaging is an emerging technology that holds great
                  promise to better monitor and optimise reservoir
                  production. The basic idea behind 4D seismic is that
                  when multiple 3D surveys are acquired at separate
                  calendar times over a producing field, the reservoir
                  geology will not change from survey to survey but
                  the state of the reservoir fluids will change. Thus,
                  taking the difference between two 3D surveys should
                  remove the static geologic contribution to the data
                  and isolate the time- varying fluid flow
                  component. However, a major challenge in 4D seismic
                  is that acquisition and processing differences
                  between 3D surveys often overshadow the changes
                  caused by fluid flow. This problem is compounded
                  when 4D effects are sought to be derived from
                  vintage 3D data sets that were not originally
                  acquired with 4D in mind. The goal of this study is
                  to remove the acquisition and imaging artefacts from
                  a 4D seismic difference cube using Curveket
                  processing techniques.},
  keywords = {SLIM, EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2004/Beyreuther04EAGEcdo/Beyreuther04EAGEcdo_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2004/Beyreuther04EAGEcdo/beyreuther2004EAGEcdo_paper.pdf},
  url2 = {https://circle.ubc.ca/bitstream/handle/2429/453/EAGE4D2004.pdf?sequence=1},
  url3 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=2323}
}


@CONFERENCE{herrmann2012SEGfwi,
  author = {Andrew J. Calvert and Ian Hanlon and Mostafa Javanmehri and Rajiv Kumar and Tristan van Leeuwen and Xiang Li and Brendan R. Smithyman and Eric Takam Takougang and Haneet Wason and Felix J. Herrmann},
  title = {{FWI} from the {West} {Coasts}: lessons learned from "Gulf of Mexico Imaging
	Challenges: What Can Full Waveform Inversion Achieve?"},
  booktitle = {SEG Workshop on FWI; Las Vegas},
  year = {2012},
  organization = {SEG},
  keywords = {workshop, FWI, SEG},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2012/herrmann2012SEGfwi/herrmann2012SEGfwi_pres.pdf}
}


@CONFERENCE{cristall2004CSEGcpa,
  author = {Jamin Cristall and Moritz Beyreuther and Felix J. Herrmann},
  title = {Curvelet processing and imaging: {4-D} adaptive subtraction},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2004},
  organization = {CSEG},
  abstract = {With burgeoning world demand and a limited rate of
                  discovery of new reserves, there is increasing
                  impetus upon the industry to optimize recovery from
                  already existing fields. 4D, or time-lapse, seismic
                  imaging holds great promise to better monitor and
                  optimise reservoir production. The basic idea behind
                  4D seismic is that when multiple 3D surveys are
                  acquired at separate calendar times over a producing
                  field, the reservoir geology will not change from
                  survey to survey but the state of the reservoir
                  fluids will change. Thus, taking the difference
                  between two 3D surveys should remove the static
                  geologic contribution to the data and isolate the
                  time-varying fluid flow component. However, a major
                  challenge in 4D seismic is that acquisition and
                  processing differences between 3D surveys often
                  overshadow the changes caused by fluid flow. This
                  problem is compounded when 4D effects are sought to
                  be derived from legacy 3D data sets that were not
                  originally acquired with 4D in mind. The goal of
                  this study is to remove the acquisition and imaging
                  artefacts from a 4D seismic difference cube using
                  Curvelet processing techniques.},
  keywords = {SLIM},
  month = {05},
  url = {http://www.cseg.ca/assets/files/resources/abstracts/2004/059S0201-Cristall_J_Curvelet_4D.pdf}
}


@CONFERENCE{dasilva2012EAGEprobingprecond,
  author = {Curt {Da Silva} and Felix J. Herrmann},
  title = {Matrix probing and simultaneous sources: a new approach for preconditioning the {Hessian}},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2012},
  month = {06},
  abstract = {Recent advances based on the mathematical understanding
                  of the Hessian as, under certain conditions, a
                  pseudo-differential operator have resulted in a new
                  preconditioner by L. Demanet et al. Basing their
                  approach on a suitable basis expansion for the
                  Hessian, by suitably 'probing' the Hessian,
                  i.e. applying the Hessian to a small number of
                  randomized model perturbations, one can obtain an
                  approximation to the inverse Hessian in an efficient
                  manner. Building upon this approach, we consider
                  this preconditioner in the context of least-squares
                  migration and Full Waveform Inversion and
                  specifically dimensionality reduction techniques in
                  these domains. By utilizing previous work in
                  simultaneous sources, we are able to develop an
                  efficient least-squares migration scheme which
                  recovers higher quality images and hence higher
                  quality search directions in the context of a
                  Gauss-Newton method for Full Waveform Inversion
                  while simultaneously avoiding inordinate amounts of
                  additional work.},
  keywords = {EAGE, matrix probing, pseudo-differential operator},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/dasilva2012EAGEprobingprecond/dasilva2012EAGEprobingprecond_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/dasilva2012EAGEprobingprecond/dasilva2012EAGEprobingprecond.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=59193}
}


@CONFERENCE{erlangga2009EAGEmwi,
  author = {Yogi A. Erlangga and Felix J. Herrmann},
  title = {Migration with implicit solvers for the time-harmonic {Helmholtz} equation},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2009},
  month = {06},
  abstract = {From the measured seismic data, the location and the
                  amplitude of reflectors can be determined via a
                  migration algorithm. Classically, following
                  Claerbout{\textquoteright}s imaging principle [2], a
                  reflector is located at the position where the
                  source{\textquoteright}s forward-propagated
                  wavefield correlates with the backward-propagated
                  wavefield of the receiver data. Lailly and Tarantola
                  later showed that this imaging principle is an
                  instance of inverse problems, with the associated
                  migration operator formulated via a least-squares
                  functional; see [6, 12, 13]. Furthermore, they
                  showed that the migrated image is associated with
                  the gradient of this functional with respect to the
                  image. If the solution of the least-squares
                  functional is done iteratively, the
                  correlation-based image coincides up to a constant
                  with the first iteration of a gradient method. In
                  practice, this migration is done either in the time
                  domain or in the frequency domain. In the
                  frequency-domain migration, the main bottleneck thus
                  far, which renders its full implementation to large
                  scale problems, is the lack of efficient solvers for
                  computing wavefields. Robust direct methods easily
                  run into excessive memory requirements as the size
                  of the problem increases. On the other hand,
                  iterative methods, which are less demanding in terms
                  of memory, suffered from lack of convergence. During
                  the past years, however, progress has been made in
                  the development of an efficient iterative method [4,
                  3] for the frequency-domain wavefield
                  computations. In this paper, we will show the
                  significance of this method (called MKMG) in the
                  context of the frequency-domain migration, where
                  multi-shot-frequency wavefields (of order of 10,000
                  related wavefields) need to be computed.},
  keywords = {EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2009/Erlangga09EAGEmwi/Erlangga09EAGEmwi_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2009/Erlangga09EAGEmwi/Erlangga09EAGEmwi.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=23955}
}


@CONFERENCE{erlangga2009SEGswi,
  author = {Yogi A. Erlangga and Felix J. Herrmann},
  title = {Seismic waveform inversion with {Gauss-Newton-Krylov} method},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  month = {10},
  volume = {28},
  pages = {2357-2361},
  organization = {SEG},
  abstract = {This abstract discusses an implicit implementation of
                  the Gauss-Newton method, used for the
                  frequency-domain full-waveform inversion, where the
                  inverse of the Hessian for the update is never
                  formed explicitly. Instead, the inverse of the
                  Hessian is computed approximately by a conjugate
                  gradient (CG) method, which only requires the action
                  of the Hessian on the CG search direction. This
                  procedure avoids an excessive computer storage,
                  usually needed for storing the Hessian, at the
                  expense of extra computational work in CG. An
                  effective preconditioner for the Hessian is
                  important to improve the convergence of CG, and
                  hence to reduce the overall computational work.},
  keywords = {SEG},
  doi = {10.1190/1.3255332},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/erlangga09SEGswi/erlangga09SEGswi_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/erlangga09SEGswi/erlangga09SEGswi.pdf}
}


@CONFERENCE{erlangga2008SEGaim,
  author = {Yogi A. Erlangga and Felix J. Herrmann},
  title = {An iterative multilevel method for computing wavefields in frequency-domain seismic inversion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  pages = {1957-1960},
  organization = {SEG},
  abstract = {We describe an iterative multilevel method for solving
                  linear systems representing forward modeling and
                  back propagation of wavefields in frequency-domain
                  seismic inversions. The workhorse of the method is
                  the so-called multilevel Krylov method, applied to a
                  multigrid-preconditioned linear system, and is
                  called multigrid-multilevel Krylov (MKMG) method.
                  Numerical experiments are presented for 2D Marmousi
                  synthetic model for a range of frequencies. The
                  convergence of the method is fast, and depends only
                  mildly on frequency. The method can be considered as
                  the first viable alternative to LU factorization,
                  which is practically prohibitive for 3D seismic
                  inversions.},
  keywords = {SLIM, SEG},
  doi = {10.1190/1.3059279},
  month = {11},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/erlangga08SEGaim/erlangga08SEGaim_pres.pdf},
  url = { https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/erlangga08SEGaim/erlangga08SEGaim.pdf }
}


@CONFERENCE{eso2008SEGira,
  author = {R. A. Eso and S. Napier and Felix J. Herrmann and D. W. Oldenburg},
  title = {Iterative reconstruction algorithm for non-linear operators},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  pages = {579-583},
  organization = {SEG},
  abstract = {Iterative soft thresholding of a models wavelet
                  coefficients can be used to obtain models that are
                  sparse with respect to a known basis function. We
                  generate sparse models for non-linear forward
                  operators by applying the soft thresholding operator
                  to the model obtained through a Gauss-Newton
                  iteration and apply the technique in a synthetic
                  2.5D DC resistivity crosswell tomographic example.},
  keywords = {SLIM, SEG},
  doi = {10.1190/1.3063719},
  month = {11},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/eso08SEGira/eso08SEGira.pdf }
}


@CONFERENCE{fomel2007ICASSPrepro,
  author = {Sergey Fomel and Gilles Hennenfent},
  title = {Reproducible computational experiments using scons},
  booktitle = {ICASSP},
  year = {2007},
  organization = {ICASSP},
  abstract = {SCons (from Software Construction) is a well-known open-
                  source program designed primarily for building
                  software. In this paper, we describe our method of
                  extending SCons for managing data processing flows
                  and reproducible computational experiments. We
                  demonstrate our usage of SCons with a simple
                  example.},
  keywords = {ICASSP},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ICASSP/2007/fomel07ICASSPrepro/fomel07ICASSPrepro.pdf }
}


@CONFERENCE{friedlander2009NUalssr,
  author = {Michael P. Friedlander},
  title = {Algorithms for large-scale sparse reconstruction},
  booktitle = {IEMS},
  year = {2009},
  address = {Northwestern University},
  organization = {IEMS Colloquim Speaker},
  keywords = {minimization, SLIM}
}


@CONFERENCE{friedlander2009VIETcsgpa,
  author = {Michael P. Friedlander},
  title = {Computing sparse and group-sparse approximations},
  booktitle = {VIET},
  year = {2009},
  address = {Hanoi, Vietnam},
  organization = {2009 High Performance Scientific Computing Conference},
  keywords = {minimization, SLIM}
}


@CONFERENCE{friedlander2008SIAMasa,
  author = {Michael P. Friedlander},
  title = {Active-set approaches to basis pursuit denoising},
  booktitle = {SIAM Optimization},
  year = {2008},
  organization = {SIAM Optimization},
  file = {http://www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf:PDF},
  keywords = {SLIM},
  month = {05},
  url = {http://www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf}
}


@CONFERENCE{friedlander2008SINBADafl,
  author = {Michael P. Friedlander},
  title = {Algorithms for large-scale sparse reconstruction},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {Many signal processing applications seek to approximate
                  a signal as a linear combination of only a few
                  elementary atoms drawn from a large collection. This
                  is known as sparse reconstruction, and the theory of
                  compressed sensing allows us to pose it as a
                  structured convex optimization problem. I will
                  discuss the role of duality in revealing some
                  unexpected and useful properties of these problems,
                  and will show how they can lead to practical,
                  large-scale algorithms. I will also describe some
                  applications of these algorithms.},
  keywords = {SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/friedlander2008SINBADafl/friedlander2008SINBADafl.pdf}
}


@CONFERENCE{friedlander2008WCOMasm,
  author = {Michael P. Friedlander and M. A. Saunders},
  title = {Active-set methods for basis pursuit},
  booktitle = {WCOM},
  year = {2008},
  organization = {West Coast Opitmization Meeting (WCOM)},
  abstract = {Many imaging and compressed sensing applications seek
                  sparse solutions to large under-determined
                  least-squares problems. The basis pursuit (BP)
                  approach minimizes the 1-norm of the solution, and
                  the BP denoising (BPDN) approach balances it against
                  the least-squares fit. The duals of these problems
                  are conventional linear and quadratic programs.  We
                  introduce a modified parameterization of the BPDN
                  problem and explore the effectiveness of active-set
                  methods for solving its dual. Our basic algorithm
                  for the BP dual unifies several existing algorithms
                  and is applicable to large-scale examples.},
  file = {http://www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf:PDF},
  month = {07},
  url = {http://www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf}
}


@CONFERENCE{frijlink2010EAGEcos,
  author = {M. O. Frijlink and Reza Shahidi and Felix J. Herrmann and R. G. van Borselen},
  title = {Comparison of standard adaptive subtraction and primary-multiple separation in the curvelet domain},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2010},
  month = {06},
  abstract = {In recent years, data-driven multiple prediction methods
                  and wavefield extrapolation methods have proven to
                  be powerful methods to attenuate multiples from data
                  acquired in complex 3-D geologic environments.
                  These methods make use of a two-stage approach,
                  where first the multiples (surface-related and / or
                  internal) multiples are predicted before they are
                  subtracted from the original input data in an
                  adaptively. The quality of these predicted multiples
                  often raises high expectations for the adaptive
                  subtraction techniques, but for various reasons
                  these expectations are not always met in
                  practice. Standard adaptive subtraction methods use
                  the well-known minimum energy criterion, stating
                  that the total energy after optimal multiple
                  attenuation should be minimal. When primaries and
                  multiples interfere, the minimum energy criterion is
                  no longer appropriate. Also, when multiples of
                  different orders interfere, adaptive energy
                  minimization will lead to a compromise between
                  different amplitudes corrections for the different
                  orders of multiples. This paper investigates the
                  performance of two multiple subtraction schemes for
                  a real data set that exhibits both interference
                  problems. Results from an adaptive subtraction in
                  the real curvelet domain, separating primaries and
                  multiples, are compared to those obtained using a
                  more conventional adaptive subtraction method in the
                  spatial domain.},
  keywords = {EAGE},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2010/frijlink10EAGEcos/frijlink10EAGEcos.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=39875}
}


@CONFERENCE{hennenfent2008SINBADnii2,
  author = {Gilles Hennenfent},
  title = {New insights into one-norm solvers from the {Pareto} curve},
  booktitle = {SINBAD},
  year = {2008},
  abstract = {Several geophysical ill-posed inverse problems are
                  successfully solved by promoting sparsity using
                  one-norm regularization. The practicality of this
                  approach depends on the effectiveness of the
                  one-norm solver used and on its robustness under
                  limited number of iterations. We propose an approach
                  to understand the behavior and evaluate the
                  performance of one-norm solvers. The technique
                  consists of tracking on a graph the data misfit
                  versus the one norm of successive iterates. By
                  comparing the solution paths to the Pareto curve, we
                  are able to assess the performance of the solvers
                  and the quality of the solutions. Such an assessment
                  is particularly relevant given the renewed interest
                  in one-norm regularization.},
  keywords = {SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/hennenfent2008SINBADnii2/hennenfent2008SINBADnii2.pdf}
}


@CONFERENCE{hennenfent2008SINBADsdw2,
  author = {Gilles Hennenfent},
  title = {Simply denoise: wavefield reconstruction via jittered undersampling},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We present a new discrete undersampling scheme designed
                  to favor wavefield reconstruction by
                  sparsity-promoting inversion with transform elements
                  that are localized in the Fourier domain. Our work
                  is motivated by empirical observations in the
                  seismic community, corroborated by recent results
                  from compressive sampling, which indicate favorable
                  (wavefield) reconstructions from random as opposed
                  to regular undersampling. As predicted by theory,
                  random undersampling renders coherent aliases into
                  harmless incoherent random noise, effectively
                  turning the interpolation problem into a much
                  simpler denoising problem. A practical requirement
                  of wavefield reconstruction with localized
                  sparsifying transforms is the control on the maximum
                  gap size. Unfortunately, random undersampling does
                  not provide such a control and the main purpose of
                  this paper is to introduce a sampling scheme, coined
                  jittered undersampling, that shares the benefits of
                  random sampling, while offering control on the
                  maximum gap size. Our contribution of jittered
                  sub-Nyquist sampling proofs to be key in the
                  formulation of a versatile wavefield
                  sparsity-promoting recovery scheme that follows the
                  principles of compressive sampling. After studying
                  the behavior of the jittered-undersampling scheme in
                  the Fourier domain, its performance is studied for
                  curvelet recovery by sparsity-promoting inversion
                  (CRSI). Our findings on synthetic and real seismic
                  data indicate an improvement of several decibels
                  over recovery from regularly-undersampled data for
                  the same amount of data collected.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/hennenfent2008SINBADsdw2/hennenfent2008SINBADsdw2.pdf}
}


@CONFERENCE{hennenfent07gradsem,
  author = {Gilles Hennenfent},
  title = {Reproducible research in computational (geo)sciences},
  booktitle = {Graduate seminar series},
  year = {2007},
  month = {01},
  organization = {Graduate Seminar Series},
  owner = {Shruti},
  quality = {1},
  timestamp = {2013.01.16},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/Misc/hennenfent07gradsem.pdf}
}


@CONFERENCE{hennenfent2007SINBADjdn,
  author = {Gilles Hennenfent},
  title = {Just denoise: nonlinear recovery from randomly sampled data},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {In this talk, we turn the interpolation problem of
                  coarsely-sampled data into a denoising problem. From
                  this point of view, we illustrate the benefit of
                  random sampling at sub-Nyquist rate over regular
                  sampling at the same rate. We show that, using
                  nonlinear sparsity-promoting optimization, coarse
                  random sampling may actually lead to significantly
                  better wavefield reconstruction than equivalent
                  regularly sampled data.},
  keywords = {Presentation, SINBAD, SLIM}
}


@CONFERENCE{hennenfent06SINBADscons,
  author = {Gilles Hennenfent},
  title = {Basic Processing flows with SCons},
  booktitle = {SINBAD},
  year = {2006},
  organization = {SINBAD},
  quality = {1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/hennenfent06SINBADscons/hennenfent06SINBADscons.pdf}
}


@CONFERENCE{hennenfent06SINBADssr,
  author = {Gilles Hennenfent},
  title = {A primer on stable signal recovery},
  booktitle = {SINBAD},
  year = {2006},
  organization = {SINBAD},
  quality = {1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/hennenfent06SINBADssr/hennenfent06SINBADssr.pdf}
}


@CONFERENCE{hennenfent2006SINBADapo,
  author = {Gilles Hennenfent},
  title = {A primer on sparsity transforms: curvelets and wave atoms},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {During this presentation an introduction will be given
                  on the method of stable recovery from noisy and
                  incomplete data. Strong recovery conditions that
                  guarantee the recovery for arbitrary acquisition
                  geometries will be reviewed and numerical recovery
                  examples will be presented.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/hennenfent2006SINBADapo/hennenfent2006SINBADapo.pdf}
}


@CONFERENCE{hennenfent2006SINBADros,
  author = {Gilles Hennenfent},
  title = {Recovery of seismic data: practical considerations},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {We propose a method for seismic data interpolation based
                  on 1) the reformulation of the problem as a stable
                  signal recovery problem and 2) the fact that seismic
                  data is sparsely represented by curvelets. This
                  method does not require information on the seismic
                  velocities. Most importantly, this formulation
                  potentially leads to an explicit recovery
                  condition. We also propose a large-scale problem
                  solver for the l1-regularization minimization
                  involved in the recovery and successfully illustrate
                  the performance of our algorithm on 2D synthetic and
                  real examples.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/hennenfent2006SINBADros/hennenfent2006SINBADros.pdf}
}


@CONFERENCE{hennenfent2006SINBADtnf,
  author = {Gilles Hennenfent},
  title = {The {Nonuniform} {Fast} {Discrete} {Curvelet} {Transform} ({NFDCT})},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {The authors present an extension of the fast discrete
                  curvelet transform (FDCT) to nonuniformly sampled
                  data. This extension not only restores curvelet
                  compression rates for nonuniformly sampled data but
                  also removes noise and maps the data to a regular
                  grid.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/hennenfent2006SINBADtnf/hennenfent2006SINBADtnf.pdf}
}


@CONFERENCE{hennenfent2008SEGonri,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {One-norm regularized inversion: learning from the {Pareto} curve},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  organization = {SEG},
  abstract = {Geophysical inverse problems typically involve a trade
                  off between data misfit and some prior. Pareto
                  curves trace the optimal trade off between these two
                  competing aims. These curves are commonly used in
                  problems with two-norm priors where they are plotted
                  on a log-log scale and are known as L-curves. For
                  other priors, such as the sparsity-promoting one
                  norm, Pareto curves remain relatively
                  unexplored. First, we show how these curves provide
                  an objective criterion to gauge how robust one-norm
                  solvers are when they are limited by a maximum
                  number of matrix-vector products that they can
                  perform. Second, we use Pareto curves and their
                  properties to define and compute one-norm
                  compressibilities. We argue this notion is key to
                  understand one-norm regularized inversion. Third, we
                  illustrate the correlation between the one-norm
                  compressibility and the perfor- mance of Fourier and
                  curvelet reconstructions with sparsity promoting
                  inversion.},
  keywords = {SEG},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/hennenfent08SEGonri/hennenfent08SEGonri.pdf}
}


@CONFERENCE{hennenfent2007EAGEcrw,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Curvelet reconstruction with sparsity-promoting inversion: successes and challenges},
  booktitle = {EAGE Workshop on Curvelets, contourlets, seislets, … in seismic data processing - where are we and where are we going?},
  year = {2007},
  month = {06},
  abstract = {In this overview of the recent Curvelet Reconstruction
                  with Sparsity-promoting Inversion (CRSI) method, we
                  present our latest 2-D and 3-D interpolation results
                  on both synthetic and real datasets. We compare
                  these results to interpolated data using other
                  existing methods. Finally, we discuss the challenges
                  related to sparsity-promoting solvers for the
                  large-scale problems the industry faces.},
  keywords = {Presentation, SLIM, EAGE, workshop},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEcrw/hennenfent07EAGEcrw_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEcrw/hennenfent07EAGEcrw.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=7553}
}


@CONFERENCE{hennenfent2007EAGEisf,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Irregular sampling: from aliasing to noise},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2007},
  month = {06},
  abstract = {Seismic data is often irregularly and/or sparsely
                  sampled along spatial coordinates. We show that
                  these acquisition geometries are not necessarily a
                  source of adversity in order to accurately
                  reconstruct adequately-sampled data. We use two
                  examples to illustrate that it may actually be
                  better than equivalent regularly subsampled
                  data. This comment was already made in earlier works
                  by other authors. We explain this behavior by two
                  key observations. Firstly, a noise-free
                  underdetermined problem can be seen as a noisy
                  well-determined problem. Secondly, regularly
                  subsampling creates strong coherent acquisition
                  noise (aliasing) difficult to remove unlike the
                  noise created by irregularly subsampling that is
                  typically weaker and Gaussian-like.},
  keywords = {Presentation, SLIM, EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEisf/hennenfent07EAGEisf_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEisf/hennenfent07EAGEisf.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEisf/hennenfent07EAGEisf_WS.pdf},
  url3 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=6487}
}


@CONFERENCE{hennenfent2007SEGrsn,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Random sampling: new insights into the reconstruction of coarsely sampled wavefields},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  pages = {2575-2579},
  organization = {SEG},
  abstract = {In this paper, we turn the interpolation problem of
                  coarsely-sampled data into a denoising problem. From
                  this point of view, we illustrate the benefit of
                  random sampling at sub-Nyquist rate over regular
                  sampling at the same rate. We show that, using
                  nonlinear sparsity-promoting optimization, coarse
                  random sampling may actually lead to significantly
                  better wavefield reconstruction than equivalent
                  regularly sampled data. {\copyright}2007 Society of
                  Exploration Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.2793002},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/hennenfent07SEGrsn/hennenfent07SEGrsn_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/hennenfent07SEGrsn/hennenfent07SEGrsn.pdf }
}


@CONFERENCE{hennenfent2007SINBADrii,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Recent insights in $\ell_1$ solvers},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {During this talk, an overview is given on our work on
                  norm-one solvers as part of the DNOISE
                  project. Gilles will explain the ins and outs of our
                  iterative thresholding solver based on log cooling
                  while Felix will present the work of Michael
                  Friedlander "A Newton root-finding algorithms for
                  large-scale basis pursuit denoise". Both approaches
                  involve the solution of the basis pursuit problem
                  that seeks a minimum one-norm solution of an
                  underdetermined least-squares problem. Basis pursuit
                  denoise (BPDN) fits the least-squares problem only
                  approximately, and a single parameter determines a
                  curve that traces the trade-off between the
                  least-squares fit and the one-norm of the
                  solution. In the work of Friedlander, it is shown
                  show that the function that describes this curve is
                  convex and continuously differentiable over all
                  points of interest. They describe an efficient
                  procedure for evaluating this function and its
                  derivatives. As a result, they can compute arbitrary
                  points on this curve. Their method is suitable for
                  large-scale problems. Only matrix-vector operations
                  are required. This is joint work with Ewout van der
                  Berg and Michael P. Friedlander},
  keywords = {Presentation, SINBAD, SLIM}
}


@CONFERENCE{hennenfent2006SEGaos,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Application of stable signal recovery to seismic data interpolation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2006},
  pages = {2797-2801},
  organization = {SEG},
  abstract = {We propose a method for seismic data interpolation based
                  on 1) the reformulation of the problem as a stable
                  signal recovery problem and 2) the fact that seismic
                  data is sparsely represented by curvelets. This
                  method does not require information on the seismic
                  velocities. Most importantly, this formulation
                  potentially leads to an explicit recovery
                  condition. We also propose a large-scale problem
                  solver for the 1-regularization minimization
                  involved in the recovery and successfully illustrate
                  the performance of our algorithm on 2D synthetic and
                  real examples. {\copyright}2006 Society of
                  Exploration Geophysicists},
  keywords = {Presentation, SLIM, curvelets, interpolation, seismic data, regularization
	minimization, iterative thresholding, amplitude, SEG, continuity, fast transform},
  doi = {10.1190/1.2370105},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2006/hennenfent06SEGaos/hennenfent06SEGaos_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2006/hennenfent06SEGaos/hennenfent06SEGaos.pdf}
}


@CONFERENCE{hennenfent2005SEGscd,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Sparseness-constrained data continuation with frames: applications to missing traces and aliased signals in {2/3-D}},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2005},
  pages = {2162-2165},
  organization = {SEG},
  abstract = {We present a robust iterative sparseness-constrained
                  interpolation algorithm using 2-/3-D curvelet frames
                  and Fourier-like transforms that exploits continuity
                  along reflectors in seismic data. By choosing
                  generic transforms, we circumvent the necessity to
                  make parametric assumptions (e.g. through
                  linear/parabolic Radon or demigration) regarding the
                  shape of events in seismic data. Simulation and real
                  data examples for data with moderately sized gaps
                  demonstrate that our algorithm provides interpolated
                  traces that accurately reproduce the wavelet shape
                  as well as the AVO behavior. Our method also shows
                  good results for de-aliasing judged by the behavior
                  of the ($f-k$)-spectrum before and after
                  regularization. {\copyright}2005 Society of
                  Exploration Geophysicists},
  keywords = {Presentation, SEG, SLIM},
  doi = {10.1190/1.2148142},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2005/Hennenfent05SEGscd/Hennenfent05SEGscd_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2005/Hennenfent05SEGscd/Hennenfent05SEGscd.pdf }
}


@CONFERENCE{hennenfent2004SEGtta,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Three-term amplitude-versus-offset ({AVO}) inversion revisited by curvelet and wavelet transforms},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2004},
  pages = {211-214},
  organization = {SEG},
  abstract = {We present a new method to stabilize the three-term AVO
                  inversion using Curvelet and Wavelet
                  transforms. Curvelets are basis functions that
                  effectively represent otherwise smooth objects
                  having discontinuities along smooth curves. The
                  applied formalism explores them to make the most of
                  the continuity along reflectors in seismic
                  images. Combined with Wavelets, Curvelets are used
                  to denoise the data by penalizing high frequencies
                  and small contributions in the AVO-cube. This
                  approach is based on the idea that rapid amplitude
                  changes along the ray-parameter axis are most likely
                  due to noise. The AVO-inverse problem is linearized,
                  formulated and solved for all (x, z) at once. Using
                  densities and velocities of the Marmousi model to
                  define the fluctuations in the elastic properties,
                  the performance of the proposed method is studied
                  and compared with the smoothing along the
                  ray-parameter direction only. We show that our
                  method better approximates the true data after the
                  denoising step, especially when noise level
                  increases. {\copyright}2004 Society of Exploration
                  Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.1851201},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2004/Hennenfent04SEGtta/Hennenfent04SEGtta_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2004/Hennenfent04SEGtta/Hennenfent04SEGtta.pdf }
}


@CONFERENCE{hennenfent2005CSEGscs,
  author = {Gilles Hennenfent and Felix J. Herrmann and R. Neelamani},
  title = {Sparseness-constrained seismic deconvolution with curvelets},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2005},
  organization = {CSEG},
  abstract = {Continuity along reflectors in seismic images is used
                  via Curvelet representation to stabilize the
                  convolution operator inversion. The Curvelet
                  transform is a new multiscale transform that
                  provides sparse representations for images that
                  comprise smooth objects separated by piece-wise
                  smooth discontinuities (e.g. seismic images). Our
                  iterative Curvelet-regularized deconvolution
                  algorithm combines conjugate gradient-based
                  inversion with noise regularization performed using
                  non-linear Curvelet coefficient thresholding. The
                  thresholding operation enhances the sparsity of
                  Curvelet representations. We show on a synthetic
                  example that our algorithm provides improved
                  resolution and continuity along reflectors as well
                  as reduced ringing effect compared to the iterative
                  Wiener-based deconvolution approach.},
  keywords = {Presentation, SLIM},
  month = {05},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2005/Hennenfent05CSEGscs/Hennenfent05CSEGscs_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2005/Hennenfent05CSEGscs/Hennenfent05CSEGscs.pdf}
}


@CONFERENCE{hennenfent2005EAGEsdr,
  author = {Gilles Hennenfent and R. Neelamani and Felix J. Herrmann},
  title = {Seismic deconvolution revisited with curvelet frames},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2005},
  month = {06},
  abstract = {We propose an efficient iterative curvelet-regularized
                  deconvolution algorithm that exploits continuity
                  along reflectors in seismic images.  Curvelets are a
                  new multiscale transform that provides sparse
                  representations for images (such as seismic images)
                  that comprise smooth objects separated by piece-wise
                  smooth discontinuities. Our technique combines
                  conjugate gradient-based convolution operator
                  inversion with noise regularization that is
                  performed using non-linear curvelet coefficient
                  shrinkage (thresholding). The shrinkage operation
                  leverages the sparsity of curvelets
                  representations. Simulations demonstrate that our
                  algorithm provides improved resolution compared to
                  the traditional Wiener-based deconvolution
                  approach.},
  keywords = {Presentation, SLIM, EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2005/Hennenfent05EAGEsdr/Hennenfent05EAGEsdr_poster.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2005/Hennenfent05EAGEsdr/Hennenfent05EAGEsdr.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=1383}
}


@CONFERENCE{herrmann2003SPIEmsa,
  author = {Felix J. Herrmann},
  title = {Multifractional splines: application to seismic imaging},
  booktitle = {Proceedings of SPIE Technical Conference on Wavelets: Applications
	in Signal and Image Processing X},
  year = {2003},
  editor = {Michael A. Unser and Akram Aldroubi and Andrew F. Laine},
  volume = {5207},
  pages = {240-258},
  organization = {SPIE},
  abstract = {Seismic imaging commits itself to locating singularities
                  in the elastic properties of the
                  Earth{\textquoteright}s subsurface. Using the
                  high-frequency ray-Born approximation for scattering
                  from non-intersecting smooth interfaces, seismic
                  data can be represented by a generalized Radon
                  transform mapping the singularities in the medium to
                  seismic data. Even though seismic data are bandwidth
                  limited, signatures of the singularities in the
                  medium carry through this transform and its inverse
                  and this mapping property presents us with the
                  possibility to develop new imaging techniques that
                  preserve and characterize the singularities from
                  incomplete, bandwidth-limited and noisy data. In
                  this paper we propose a non-adaptive
                  Curvelet/Contourlet technique to image and preserve
                  the singularities and a data-adaptive Matching
                  Pursuit method to characterize these imaged
                  singularities by Multi-fractional Splines. This
                  first technique borrows from the ideas within the
                  Wavelet-Vaguelette/Quasi-SVD approach. We use the
                  almost diagonalization of the scattering operator to
                  approximately compensate for (i) the coloring of the
                  noise and hence facilitate estimation; (ii) the
                  normal operator itself. Results of applying these
                  techniques to seismic imaging are encouraging
                  although many open fundamental questions remain.},
  keywords = {Presentation, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SPIE/2003/herrmann2003SPIEmsa/herrmann2003SPIEmsa.pdf}
}


@CONFERENCE{Herrmann13NIPSrse,
  author = {Felix J. Herrmann},
  title = {Randomized sampling in exploration seismology},
  booktitle = {NIPS},
  year = {2013},
  timestamp = {2013.01.09},
  url = {http://techtalks.tv/talks/randomized-sampling-in-exploration-seismology/57871/},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/NIPS/2013/Herrmann13NIPSrse/Herrmann13NIPSrse.pdf}
}


@CONFERENCE{herrmann2012EAGEpmr,
  author = {Felix J. Herrmann},
  title = {Pass on the message: recent insights in large-scale sparse recovery},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2012},
  month = {06},
  abstract = {Data collection, data processing, and imaging in
                  exploration seismology increasingly hinge on
                  large-scale sparsity promoting solvers to remove
                  artifacts caused by efforts to reduce costs. We show
                  how the inclusion of a "message term" in the
                  calculation of the residuals improves the
                  convergence of these iterative solvers by breaking
                  correlations that develop between the model iterate
                  and the linear system that needs to be inverted. We
                  compare this message-passing scheme to
                  state-of-the-art solvers for problems in
                  missing-trace interpolation and in
                  dimensionality-reduced imaging with phase encoding.},
  keywords = {EAGE, message passing, sparse inversion},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/herrmann2012EAGEpmr/herrmann2012EAGEpmr_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/herrmann2012EAGEpmr/herrmann2012EAGEpmr.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=58935}
}


@CONFERENCE{herrmann2012SEGals,
  author = {Felix J. Herrmann},
  title = {Accelerated large-scale inversion with message passing},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2012},
  month = {11},
  volume = {31},
  pages = {1-6},
  organization = {SEG},
  abstract = {To meet current-day challenges, exploration seismology
                  increasingly relies on more and more sophisticated
                  algorithms that require multiple paths through all
                  data. This requirement leads to problems because the
                  size of seismic data volumes is increasing
                  exponentially, exposing bottlenecks in IO and
                  computational capability. To overcome these
                  bottlenecks, we follow recent trends in machine
                  learning and compressive sensing by proposing a
                  sparsity-promoting inversion technique that works on
                  small randomized subsets of data only. We boost the
                  performance of this algorithm significantly by
                  modifying a state-of-the-art l1-norm solver to
                  benefit from message passing, which breaks the build
                  up of correlations between model iterates and the
                  randomized linear forward model. We demonstrate the
                  performance of this algorithm on a toy
                  sparse-recovery problem and on a realistic
                  reverse-time-migration example with random source
                  encoding. The improvements in speed, memory use, and
                  output quality are truly remarkable.},
  keywords = {imaging, optimization, compressive sensing, SEG},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2012/herrmann2012SEGals/herrmann2012SEGals_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2012/herrmann2012SEGals/herrmann2012SEGals.pdf},
  doi = {10.1190/segam2012-0847.1}
}


@CONFERENCE{herrmann2012SSPamp,
  author = {Felix J. Herrmann},
  title = {Approximate message passing meets exploration seismology},
  booktitle = {2012 IEEE Statistical Signal Processing Workshop (SSP) (SSP'12)},
  year = {2012},
  address = {Ann Arbor, Michigan, USA},
  organization = {IEEE},
  abstract = {Data collection, data processing, and imaging in
                  exploration seismology increasingly hinge on
                  large-scale sparsity promoting solvers to remove
                  artifacts caused by efforts to reduce costs. We show
                  how the inclusion of a 'message term' in the
                  calculation of the residuals improves the
                  convergence of these iterative solvers by breaking
                  correlations that develop between the model iterate
                  and the linear system that needs to be inverted. We
                  compare this message-passing scheme to
                  state-of-the-art solvers for problems in
                  missing-trace interpolation and in
                  dimensionality-reduced imaging with phase en-
                  coding.},
  keywords = {exploration seismology, compressive sensing, transform-domain sparsity
	promotion, seismic imaging},
  month = {03},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SSP/2012/herrmann2012SSPamp/herrmann2012SSPamp_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SSP/2012/herrmann2012SSPamp/herrmann2012SSPamp.pdf}
}


@CONFERENCE{herrmann2012UW,
  author = {Felix J. Herrmann},
  title = {Compressive sensing and sparse recovery in exploration seismology},
  booktitle = {Talk at University of Wisconsin},
  year = {2012},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/Wisconsin/2012/herrmann2012UW/herrmann2012UW.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/Wisconsin/2012/herrmann2012UW/herrmann2012UW.pdf}
}


@CONFERENCE{herrmann11SLIMsummer2,
  author = {Felix J. Herrmann},
  title = {Lecture 2. {Gene} {Golub} {SIAM} {Summer} {School} {July} 4 - 15, 2011},
  booktitle = {SLIM},
  year = {2011},
  keywords = {Presentation},
  month = {08},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2011/herrmann11SLIMsummer2/herrmann11SLIMsummer2.pdf},
  timestamp = {2011.08.05}
}


@CONFERENCE{herrmann2011SLIMsummer1,
  author = {Felix J. Herrmann},
  title = {{Gene} {Golub} {SIAM} {Summer} {School} {July} 4 - 15, 2011},
  booktitle = {SLIM},
  year = {2011},
  keywords = {Presentation},
  month = {08},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2011/herrmann11SLIMsummer1/herrmann11SLIMsummer1.pdf},
  timestamp = {2011.08.05}
}


@CONFERENCE{herrmann2010EAGErss,
  author = {Felix J. Herrmann},
  title = {Randomized sampling strategies},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2010},
  month = {06},
  abstract = {Seismic exploration relies on the collection of massive
                  data volumes that are subsequently mined for
                  information during seismic processing. While this
                  approach has been extremely successful in the past,
                  the current trend towards higher quality images in
                  increasingly complicated regions continues to reveal
                  fundamental shortcomings in our workflows for
                  high-dimensional data volumes. Two causes can be
                  identified. First, there is the so-called
                  {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which puts disproportionate strain on
                  current acquisition and processing systems as the
                  size and desired resolution of our survey areas
                  continues to increase. Secondly, there is the recent
                  {\textquoteleft}{\textquoteleft}departure from
                  Moore{\textquoteright}s
                  law{\textquoteright}{\textquoteright} that forces us
                  to lower our expectations to compute ourselves out
                  of this curse of dimensionality. In this paper, we
                  offer a way out of this situation by a deliberate
                  randomized subsampling combined with
                  structure-exploiting transform-domain sparsity
                  promotion. Our approach is successful because it
                  reduces the size of seismic data volumes without
                  loss of information. As such we end up with a new
                  technology where the costs of acquisition and
                  processing are no longer dictated by the size of the
                  acquisition but by the transform-domain sparsity of
                  the end-product.},
  keywords = {Presentation, EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2010/herrmann10EAGErss/herrmann10EAGErss_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2010/herrmann10EAGErss/herrmann10EAGErss.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=39131}
}


@CONFERENCE{herrmann2010IRISsns,
  author = {Felix J. Herrmann},
  title = {Sub-{Nyquist} sampling and sparsity: getting more information from fewer samples},
  booktitle = {IRIS},
  year = {2010},
  abstract = {Many seismic exploration techniques rely on the
                  collection of massive data volumes. While this
                  approach has been extremely successful in the past,
                  current efforts toward higher resolution images in
                  increasingly complicated regions of the Earth
                  continue to reveal fundamental shortcomings in our
                  workflows. Chiefly amongst these is the so-called
                  {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which disproportionately strains current
                  acquisition and processing systems as the size and
                  desired resolution of our survey areas continues to
                  increase. In this presentation, we offer an
                  alternative sampling method leveraging recent
                  insights from compressive sensing towards seismic
                  acquisition and processing of severely under-sampled
                  data. The main outcome of this approach is a new
                  technology where acquisition and processing related
                  costs are no longer determined by overly stringent
                  sampling criteria, such as Nyquist. At the heart of
                  our approach lies randomized incoherent sampling
                  that breaks subsampling related interferences by
                  turning them into harmless noise, which we
                  subsequently remove by promoting transform-domain
                  sparsity. Now, costs no longer grow significantly
                  with resolution and dimensionality of the survey
                  area, but instead depend on transform-domain
                  sparsity only. Our contribution is twofold. First,
                  we demonstrate by means of carefully designed
                  numerical experiments that compressive sensing can
                  successfully be adapted to seismic
                  exploration. Second, we show that accurate recovery
                  can be accomplished for compressively sampled data
                  volumes sizes that exceed the size of conventional
                  transform-domain data volumes by only a small
                  factor. Because compressive sensing combines
                  transformation and encoding by a single linear
                  encoding step, this technology is directly
                  applicable to acquisition and to dimensionality
                  reduction during processing. In either case,
                  sampling, storage, and processing costs scale with
                  transform-domain sparsity. Many seismic exploration
                  techniques rely on the collection of massive data
                  volumes. While this approach has been extremely
                  successful in the past, current efforts toward
                  higher resolution images in increasingly complicated
                  regions of the Earth continue to reveal fundamental
                  shortcomings in our workflows. Chiefly amongst these
                  is the so-called
                  {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which disproportionately strains current
                  acquisition and processing systems as the size and
                  desired resolution of our survey areas continues to
                  increase. In this presentation, we offer an
                  alternative sampling method leveraging recent
                  insights from compressive sensing towards seismic
                  acquisition and processing of severely under-sampled
                  data. The main outcome of this approach is a new
                  technology where acquisition and processing related
                  costs are no longer determined by overly stringent
                  sampling criteria, such as Nyquist. At the heart of
                  our approach lies randomized incoherent sampling
                  that breaks subsampling related interferences by
                  turning them into harmless noise, which we
                  subsequently remove by promoting transform-domain
                  sparsity. Now, costs no longer grow significantly
                  with resolution and dimensionality of the survey
                  area, but instead depend on transform-domain
                  sparsity only. Our contribution is twofold. First,
                  we demonstrate by means of carefully designed
                  numerical experiments that compressive sensing can
                  successfully be adapted to seismic
                  exploration. Second, we show that accurate recovery
                  can be accomplished for compressively sampled data
                  volumes sizes that exceed the size of conventional
                  transform-domain data volumes by only a small
                  factor. Because compressive sensing combines
                  transformation and encoding by a single linear
                  encoding step, this technology is directly
                  applicable to acquisition and to dimensionality
                  reduction during processing. In either case,
                  sampling, storage, and processing costs scale with
                  transform-domain sparsity.},
  keywords = {Presentation},
  note = {Presented at the IRIS Workshop},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IRIS/2010/herrmann2010IRISsns/herrmann2010IRISsns.pdf}
}


@CONFERENCE{herrmann2010MATHIAScssr,
  author = {Felix J. Herrmann},
  title = {Compressive sensing and sparse recovery in exploration seismology},
  booktitle = {MATHIAS},
  year = {2010},
  abstract = {During this presentation, I will talk about how recent
                  results from compressive sensing and sparse recovery
                  can be used to solve problems in exploration
                  seismology where incomplete sampling is ubiquitous.
                  I will also talk about how these ideas apply to
                  dimensionality reduction of full-waveform inversion
                  by randomly phase encoded sources.},
  keywords = {Presentation},
  note = {Presented at MATHIAS 2010 organized by Total SA. Paris},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/MATHIAS/2010/herrmann2010MATHIAScssr/herrmann2010MATHIAScssr.pdf}
}


@CONFERENCE{herrmann2009PIMScssr1,
  author = {Felix J. Herrmann},
  title = {Compressed sensing and sparse recovery in exploration seismology},
  booktitle = {PIMS},
  year = {2009},
  abstract = {In this course, I will present how recent results from
                  compressed sensing and sparse recovery apply to
                  exploration seismology. During the first lecture, I
                  will present the basic principles of compressive
                  sensing; the importance of random jitter sampling
                  and sparsifying transforms; and large-scale one-norm
                  solvers. I will discuss the application of these
                  techniques to missing trace interpolation. The
                  second lecture will be devoted to coherent signal
                  separation based on curveletdomain matched filtering
                  and Bayesian separation with sparsity
                  promotion. Applications of these techniques to the
                  primary-multiple wavefield-separation problem on
                  real data will be discussed as well. The third
                  lecture will be devoted towards sparse recovery in
                  seismic modeling and imaging and includes the
                  problem of preconditioning the imaging operators,
                  and the recovery from simultaneous source-acquired
                  data},
  keywords = {Presentation},
  note = {Lecture I presented at the PIMS Summer School on Seismic Imaging, Seattle},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/PIMS/2009/herrmann2009PIMScssr1/herrmann2009PIMScssr1.pdf}
}


@CONFERENCE{herrmann2009PIMScssr2,
  author = {Felix J. Herrmann},
  title = {Compressed sensing and sparse recovery in exploration seismology},
  booktitle = {PIMS},
  year = {2009},
  abstract = {In this course, I will present how recent results from
                  compressed sensing and sparse recovery apply to
                  exploration seismology. During the first lecture, I
                  will present the basic principles of compressive
                  sensing; the importance of random jitter sampling
                  and sparsifying transforms; and large-scale one-norm
                  solvers. I will discuss the application of these
                  techniques to missing trace interpolation. The
                  second lecture will be devoted to coherent signal
                  separation based on curveletdomain matched filtering
                  and Bayesian separation with sparsity
                  promotion. Applications of these techniques to the
                  primary-multiple wavefield-separation problem on
                  real data will be discussed as well. The third
                  lecture will be devoted towards sparse recovery in
                  seismic modeling and imaging and includes the
                  problem of preconditioning the imaging operators,
                  and the recovery from simultaneous source-acquired
                  data.},
  keywords = {Presentation},
  note = {Lecture II presented at the PIMS Summer School on Seismic Imaging, Seattle},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/PIMS/2009/herrmann2009PIMScssr2/herrmann2009PIMScssr2.pdf}
}


@CONFERENCE{herrmann2009PIMScssr3,
  author = {Felix J. Herrmann},
  title = {Compressed sensing and sparse recovery in exploration seismology},
  booktitle = {PIMS},
  year = {2009},
  abstract = {In this course, I will present how recent results from
                  compressed sensing and sparse recovery apply to
                  exploration seismology. During the first lecture, I
                  will present the basic principles of compressive
                  sensing; the importance of random jitter sampling
                  and sparsifying transforms; and large-scale one-norm
                  solvers. I will discuss the application of these
                  techniques to missing trace interpolation. The
                  second lecture will be devoted to coherent signal
                  separation based on curveletdomain matched filtering
                  and Bayesian separation with sparsity
                  promotion. Applications of these techniques to the
                  primary-multiple wavefield-separation problem on
                  real data will be discussed as well. The third
                  lecture will be devoted towards sparse recovery in
                  seismic modeling and imaging and includes the
                  problem of preconditioning the imaging operators,
                  and the recovery from simultaneous source-acquired
                  data.},
  keywords = {Presentation},
  note = {Lecture III presented at the PIMS Summer School on Seismic Imaging, Seattle},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/PIMS/2009/herrmann2009PIMScssr3/herrmann2009PIMScssr3.pdf}
}


@CONFERENCE{herrmann2009SEGcib,
  author = {Felix J. Herrmann},
  title = {Compressive imaging by wavefield inversion with group sparsity},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  month = {10},
  volume = {28},
  pages = {2337-2341},
  organization = {SEG},
  abstract = {Migration relies on multi-dimensional correlations
                  between source- and residual wavefields. These
                  multi-dimensional correlations are computationally
                  expensive because they involve operations with
                  explicit and full matrices that contain both
                  wavefields. By leveraging recent insights from
                  compressive sampling, we present an alternative
                  method where linear correlation-based imaging is
                  replaced by imaging via multidimensional
                  deconvolutions of compressibly sampled wavefields.
                  Even though this approach goes at the expense of
                  having to solve a sparsity-promotion recovery
                  program for the image, our wavefield inversion
                  approach has the advantage of reducing the system
                  size in accordance to transform-domain sparsity of
                  the image. Because seismic images also exhibit a
                  focusing of the energy towards zero offset, the
                  compressive-wavefield inversion itself is carried
                  out using a recent extension of one-norm solver
                  technology towards matrix-valued problems. These
                  so-called hybrid $(1,\,2)$-norm solvers allow us to
                  penalize pre-stack energy away from zero offset
                  while exploiting joint sparsity amongst near-offset
                  images. Contrary to earlier work to reduce modeling
                  and imaging costs through random phase-encoded
                  sources, our method compressively samples wavefields
                  in model space. This approach has several advantages
                  amongst which improved system-size reduction, and
                  more flexibility during subsequent inversions for
                  subsurface properties.},
  keywords = {Presentation, SEG},
  doi = {10.1190/1.3255328},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/herrmann09SEGcib/herrmann09SEGcib_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/herrmann09SEGcib/herrmann09SEGcib.pdf}
}


@CONFERENCE{herrmann2009SEGrpl,
  author = {Felix J. Herrmann},
  title = {Reflector-preserved lithological upscaling},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  month = {10},
  volume = {28},
  pages = {3466-3470},
  organization = {SEG},
  abstract = {By combining Percolation models with lithological
                  smoothing, we arrive at method for upscaling rock
                  elastic constants that preserves reflections. In
                  this approach, the Percolation model predicts sharp
                  onsets in the elastic moduli of sand-shale mixtures
                  when the shales reach a critical volume fraction. At
                  that point, the shale inclusions form a connected
                  cluster, and the macroscopic rock properties change
                  with the power-law growth of the cluster. This
                  switch-like nonlinearity preserves singularities,
                  and hence reflections, even if no sharp transition
                  exists in the lithology or if they are smoothed out
                  using standard upscaling procedures.},
  keywords = {Presentation, SEG},
  doi = {10.1190/1.3255582},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/herrmann09SEGrpl/herrmann09SEGrpl_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/herrmann09SEGrpl/herrmann09SEGrpl.pdf}
}


@CONFERENCE{herrmann2009SEGsns,
  author = {Felix J. Herrmann},
  title = {Sub-{Nyquist} sampling and sparsity: how to get more information from fewer samples},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  month = {10},
  volume = {28},
  pages = {3410-3415},
  organization = {SEG},
  abstract = {Seismic exploration relies on the collection of massive
                  data volumes that are subsequently mined for
                  information during seismic processing. While this
                  approach has been extremely successful in the past,
                  the current trend of incessantly pushing for higher
                  quality images in increasingly complicated regions
                  of the Earth continues to reveal fundamental
                  shortcomings in our workflows to handle massive
                  high-dimensional data volumes. Two causes can be
                  identified as the main culprits responsible for this
                  barrier. First, there is the so-called
                  {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which puts disproportionate strain on
                  current acquisition and processing systems as the
                  size and desired resolution of our survey areas
                  continues to increase. Secondly, there is the recent
                  {\textquoteleft}{\textquoteleft}departure from
                  Moore{\textquoteright}s
                  law{\textquoteright}{\textquoteright} that forces us
                  to lower our expectations to compute ourselves out
                  of this curse of dimensionality. In this paper, we
                  offer a way out of this situation by a deliberate
                  \emph{randomized} subsampling combined with
                  structure-exploiting transform-domain sparsity
                  promotion. Our approach is successful because it
                  reduces the size of seismic data volumes without
                  loss of information. Because of this size reduction
                  both impediments are removed and we end up with a
                  new technology where the costs of acquisition and
                  processing are no longer dictated by the \emph{size
                  of the acquisition} but by the transform-domain
                  \emph{sparsity} of the end-product after
                  processing.},
  keywords = {Presentation, SEG},
  doi = {10.1190/1.3255570},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/herrmann09SEGsns/herrmann09SEGsns_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/herrmann09SEGsns/herrmann09SEGsns.pdf}
}


@CONFERENCE{herrmann2008IONcsa,
  author = {Felix J. Herrmann},
  title = {Compressive sampling: a new paradigm for seismic data acquistion and processing?},
  booktitle = {ION},
  year = {2008},
  abstract = {Seismic data processing and imaging are firmly rooted in
                  the well-established paradigm of regular Nyquist
                  sampling. Faced with a typical uncooperative
                  environment, practitioners of seismic data
                  acquisition make all efforts to comply to this
                  theory by creating regularly-sampled seismic-data
                  volumes that are suitable for Fourier-based
                  processing flows. The current advent of new
                  alternative transform domains{\textendash}- such as
                  the sparsifying curvelet domain, where seismic data
                  is decomposed into localized, multiscale and
                  multidirectional plane waves{\textendash}- opens the
                  possibility to change this paradigm by no longer
                  combating sampling irregularity but by embracing
                  it. During this talk, we show that as long as
                  seismic data volumes permit a compressible
                  representation{\textendash}-i.e., data can be
                  represented as a superposition of relatively few
                  number of elementary waveforms{\textendash}- Nyquist
                  sampling is unnecessary pessimistic. So far, nothing
                  new, we all know from the work on Fourier- or other
                  transform-based seismic-data regularization
                  methodologies that wavefields can be recovered
                  accurately from sub-Nyquist samplings through some
                  sort of optimization procedure. What is new,
                  however, are recent insights from the field of
                  "compressive sampling", which dictate the conditions
                  that guarantee or, at least, in practice provide
                  conditions that favor sparsity-promoting recovery
                  from sub-Nyquist sampling. Random sub-sampling, or
                  to be more precise, jitter sub-sampling creates
                  favorable conditions for curvelet-based recovery. We
                  explain this phenomenon by arguing that this type of
                  sampling leads to noisy data, hence our slogan
                  "Simply denoise: wavefield reconstruction via
                  jittered undersampling", where we bank on separating
                  incoherent sub-sampling noise with curvelet-domain
                  sparsity promotion. During our presentation, we
                  introduce you to what curvelets are, why random
                  jitter sampling is important and why this opens a
                  pathway towards a new paradigm of curvelet-domain
                  seismic data processing. Our claims will be
                  supported by examples on synthetic and field
                  data. This is joint work with Gilles Hennenfent,
                  PhD. student at SLIM.},
  keywords = {ION, Presentation, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/ION/herrmann2008IONcsa/herrmann08ion_pres.pdf}
}


@CONFERENCE{herrmann2008SEGcdm3,
  author = {Felix J. Herrmann},
  title = {Curvelet-domain matched filtering},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  pages = {3643-3649},
  organization = {SEG},
  abstract = {Matching seismic wavefields lies at the heart of seismic
                  processing whether one is adaptively subtracting
                  multiples predictions or groundroll. In both cases,
                  the predictions are matched to the actual
                  to-be-separated wavefield components in the observed
                  data. The success of these wavefield matching
                  procedures depends on our ability to (i) control
                  possible overfitting, which may lead to accidental
                  removal of primary energy, (ii) handle data with
                  nonunique dips, and (iii) apply wavefield separation
                  after matching stably. In this paper, we show that
                  the curvelet transform allows us to address these
                  issues by imposing smoothness in phase space, by
                  using their capability to handle conflicting dips,
                  and by leveraging their ability to represent seismic
                  data sparsely.},
  keywords = {SEG, SLIM},
  doi = {10.1190/1.3064089},
  month = {11},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/Herrmann08SEGcdm3/Herrmann08SEGcdm3.pdf }
}


@CONFERENCE{herrmann2008SEGgbu,
  author = {Felix J. Herrmann},
  title = {Seismic noise: the good, the bad, \& the ugly},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  keywords = {Presentation, SEG, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/herrmann08SEGgbu/herrmann08SEGgbu.pdf}
}


@CONFERENCE{herrmann2008SINBADacd2,
  author = {Felix J. Herrmann},
  title = {Adaptive curvelet-domain primary-multiple separation},
  booktitle = {SINBAD},
  year = {2008},
  organization = {SINBAD},
  note = {SINBAD 2008},
  abstract = {In many exploration areas, successful separation of
                  primaries and multiples greatly determines the
                  quality of seismic imaging. Despite major advances
                  made by Surface-Related Multiple Elimination (SRME),
                  amplitude errors in the predicted multiples remain a
                  problem. When these errors vary for each type of
                  multiple differently (as a function of offset, time
                  and dip), these amplitude errors pose a serious
                  challenge for conventional least-squares matching
                  and for the recently introduced separation by
                  curvelet-domain thresholding. We propose a
                  data-adaptive method that corrects amplitude errors,
                  which vary smoothly as a function of location, scale
                  (frequency band) and angle. In that case, the
                  amplitudes can be corrected by an element-wise
                  curvelet-domain scaling of the predicted
                  multiples. We show that this scaling leads to a
                  successful estimation of the primaries, despite
                  amplitude, sign, timing and phase errors in the
                  predicted multiples. Our results on synthetic and
                  real data show distinct improvements over
                  conventional least-squares matching, in terms of
                  better suppression of multiple energy and
                  high-frequency clutter and better recovery of the
                  estimated primaries.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/herrmann2008SINBADacd2/herrmann2008SINBADacd2.pdf}
}


@CONFERENCE{herrmann2008SINBADfwr,
  author = {Felix J. Herrmann},
  title = {(De)-{Focused} wavefield reconstructions},
  booktitle = {SINBAD 2008},
  year = {2008},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/herrmann2008SINBADfwr/herrmann2008SINBADfwr.pdf}
}


@CONFERENCE{herrmann2008SINBADpsm,
  author = {Felix J. Herrmann},
  title = {Phase-space matched filtering and migration preconditioning},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {During this talk, I will report on new phase-space
                  regularization functionals defined in terms of
                  splines. This spline representation reduces the
                  dimensionality of estimating our phase-space matched
                  filter. We will discuss how this filter can be used
                  in migration preconditioning. This is joint work
                  with Christiaan Stolk.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/herrmann2008SINBADpsm/herrmann2008SINBADpsm.pdf}
}


@CONFERENCE{herrmann2008SINBADs2c,
  author = {Felix J. Herrmann},
  title = {{SINBAD} 2008 Consortium meeting},
  booktitle = {SINBAD 2008},
  year = {2008},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/herrmann2008SINBADs2c/herrmann2008SINBADs2c.pdf}
}


@CONFERENCE{herrmann2007AIPsit,
  author = {Felix J. Herrmann},
  title = {Seismic inversion through operator overloading},
  booktitle = {AIP},
  year = {2007},
  abstract = {Inverse problems in (exploration) seismology are known
                  for their large to very large scale. For instance,
                  certain sparsity-promoting inversion techniques
                  involve vectors that easily exceed 230 unknowns
                  while seismic imaging involves the construction and
                  application of matrix-free discretized operators
                  where single matrix-vector evaluations may require
                  hours, days or even weeks on large compute
                  clusters. For these reasons, software development in
                  this field has remained the domain of highly
                  technical codes programmed in low-level languages
                  with little eye for easy development, code reuse and
                  integration with (nonlinear) programs that solve
                  inverse problems. Following ideas from the
                  Symes{\textquoteright} Rice Vector Library and
                  Bartlett{\textquoteright}s C++ object-oriented
                  interface, Thyra, and Reduction/Transformation
                  operators (both part of the Trilinos software
                  package), we developed a software-development
                  environment based on overloading. This environment
                  provides a pathway from in-core prototype
                  development to out-of-core and MPI
                  {\textquoteright}production{\textquoteright} code
                  with a high level of code reuse. This code reuse is
                  accomplished by integrating the out-of-core and MPI
                  functionality into the dynamic object-oriented
                  programming language Python. This integration is
                  implemented through operator overloading and allows
                  for the development of a coordinate-free solver
                  framework that (i) promotes code reuse; (ii)
                  analyses the statements in an abstract syntax tree
                  and (iii) generates executable statements. In the
                  current implementation, we developed an interface to
                  generate executable statements for the out-of-core
                  unix-pipe based (seismic) processing package
                  RSF-Madagascar (rsf.sf.net). The modular design
                  allows for interfaces to other seismic processing
                  packages and to in-core Python packages such as
                  numpy. So far, the implementation overloads linear
                  operators and elementwise reduction/transformation
                  operators. We are planning extensions towards
                  nonlinear operators and integration with existing
                  (parallel) solver frameworks such as Trilinos.},
  keywords = {AIP, Presentation, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/AIP/2007/herrmann07AIPsit/herrmann07AIPsit.pdf}
}


@CONFERENCE{herrmann2007AIPssd,
  author = {Felix J. Herrmann},
  title = {Stable seismic data recovery},
  booktitle = {AIP},
  year = {2007},
  abstract = {In this talk, directional frames, known as curvelets,
                  are used to recover seismic data and images from
                  noisy and incomplete data. Sparsity and invariance
                  properties of curvelets are exploited to formulate
                  the recovery by a {\textquoteleft}1-norm promoting
                  program. It is shown that our data recovery approach
                  is closely linked to the recent theory of
                  {\textquoteleft}{\textquoteleft}compressive
                  sensing{\textquoteright}{\textquoteright} and can be
                  seen as a first step towards a nonlinear sampling
                  theory for wavefields. The second problem that will
                  be discussed concerns the recovery of the amplitudes
                  of seismic images in clutter. There, the invariance
                  of curvelets is used to approximately invert the
                  Gramm operator of seismic imaging. In the
                  high-frequency limit, this Gramm matrix corresponds
                  to a pseudo-differential operator, which is near
                  diagonal in the curvelet domain.In this talk,
                  directional frames, known as curvelets, are used to
                  recover seismic data and images from noisy and
                  incomplete data. Sparsity and invariance properties
                  of curvelets are exploited to formulate the recovery
                  by a l1-norm promoting program. It is shown that our
                  data recovery approach is closely linked to the
                  recent theory of
                  {\textquoteleft}{\textquoteleft}compressive
                  sensing{\textquoteright}{\textquoteright} and can be
                  seen as a first step towards a nonlinear sampling
                  theory for wavefields. The second problem that will
                  be discussed concerns the recovery of the amplitudes
                  of seismic images in clutter. There, the invariance
                  of curvelets is used to approximately invert the
                  Gramm operator of seismic imaging.  In the
                  high-frequency limit, this Gramm matrix corresponds
                  to a pseudo-differential operator, which is near
                  diagonal in the curvelet domain.},
  keywords = {AIP, Presentation, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/AIP/2007/herrmann07AIPssd/herrmann07AIPssd.pdf}
}


@CONFERENCE{herrmann2007AMScsi,
  author = {Felix J. Herrmann},
  title = {Compressive seismic imaging},
  booktitle = {AMS Von Neumann},
  year = {2007},
  abstract = {Seismic imaging involves the solution of an
                  inverse-scattering problem during which the energy
                  of (extremely) large data volumes is collapsed onto
                  the Earth's reflectors. We show how the ideas from
                  "compressive sampling" can alleviate this task by
                  exploiting the curvelet transform's "wavefront-set
                  detection" capability and "invariance" property
                  under wave propagation. First, a wavelet-vaguellete
                  technique is reviewed, where seismic amplitudes are
                  recovered from complete data by diagonalizing the
                  Gramm matrix of the linearized scattering
                  problem. Next, we show how the recovery of seismic
                  wavefields from incomplete data can be cast into a
                  compressive sampling problem, followed by a proposal
                  to compress wavefield extrapolation operators via
                  compressive sampling in the modal domain. During the
                  latter approach, we explicitly exploit the mutual
                  incoherence between the eigenfunctions of the
                  Helmholtz operator and the curvelet frame elements
                  that compress the extrapolated wavefield. This is
                  joint work with Gilles Hennenfent, Peyman Moghaddam,
                  Tim Lin, Chris Stolk and Deli Wang.},
  keywords = {AMS Von Neumann, Presentation, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/vonNeuman/2007/herrmann07AMScsi/herrmann07AMScsi_pres.pdf}
}


@CONFERENCE{herrmann2007COIPpti,
  author = {Felix J. Herrmann},
  title = {Phase transitions in explorations seismology: statistical mechanics meets information theory},
  booktitle = {COIP},
  year = {2007},
  abstract = {In this paper, two different applications of phase
                  transitions to exploration seismology will be
                  discussed. The first application concerns a phase
                  diagram ruling the recovery conditions for seismic
                  data volumes from incomplete and noisy data while
                  the second phase transition describes the behavior
                  of bi-compositional mixtures as a function of the
                  volume fraction. In both cases, the phase
                  transitions are the result of randomness in large
                  system of equations in combination with
                  nonlinearity. The seismic recovery problem from
                  incomplete data involves the inversion of a
                  rectangular matrix. Recent results from the field of
                  "compressive sensing" provide the conditions for a
                  successful recovery of functions that are sparse in
                  some basis (wavelet) or frame (curvelet)
                  representation, by means of a sparsity
                  ($\ell_1$-norm) promoting nonlinear program. The
                  conditions for a successful recovery depend on a
                  certain randomness of the matrix and on two
                  parameters that express the matrix{\textquoteright}
                  aspect ratio and the ratio of the number of nonzero
                  entries in the coefficient vector for the sparse
                  signal representation over the number of
                  measurements. It appears that the ensemble average
                  for the success rate for the recovery of the sparse
                  transformed data vector by a nonlinear sparsity
                  promoting program, can be described by a phase
                  transition, demarcating the regions for the two
                  ratios for which recovery of the sparse entries is
                  likely to be successful or likely to
                  fail. Consistent with other phase transition
                  phenomena, the larger the system the sharper the
                  transition. The randomness in this example is
                  related to the construction of the matrix, which for
                  the recovery of spike trains corresponds to the
                  randomly restricted Fourier matrix. It is shown,
                  that these ideas can be extended to the curvelet
                  recovery by sparsity-promoting inversion (CRSI). The
                  second application of phase transitions in
                  exploration seismology concerns the upscaling
                  problem. To counter the intrinsic smoothing of
                  singularities by conventional equivalent medium
                  upscaling theory, a percolation-based nonlinear
                  switch model is proposed. In this model, the
                  transport properties of bi-compositional mixture
                  models for rocks undergo a sudden change in the
                  macroscopic transport properties as soon as the
                  volume fraction of the stronger material reaches a
                  critical point. At this critical point, the stronger
                  material forms a connected cluster, which leads to
                  the creation of a cusp-like singularity in the
                  elastic moduli, which in turn give rise to specular
                  reflections. In this model, the reflectivity is no
                  longer explicitly due to singularities in the rocks
                  composition. Instead, singularities are created
                  whenever the volume fraction exceeds the critical
                  point. We will show that this concept can be used
                  for a singularity-preserved lithological upscaling.},
  keywords = {Presentation, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/COIP/2007/herrmann07COIPpti/herrmann07COIPpti_pres.pdf}
}


@CONFERENCE{herrmann2007CYBERsmc,
  author = {Felix J. Herrmann},
  title = {Seismology meets compressive sampling},
  booktitle = {Cyber},
  year = {2007},
  abstract = {Presented at Cyber-Enabled Discovery and Innovation:
                  Knowledge Extraction as a success story lecture. See
                  for more detail
                  https://www.ipam.ucla.edu/programs/cdi2007/},
  keywords = {Cyber, Presentation, SLIM},
  note = {Presented at the joint NSF-IPAM meeting. Los Angeles. October, 2007},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/Cyber/2007/herrmann07CYBERsmc/herrmann07CYBERsmc.pdf}
}


@CONFERENCE{herrmann2007EAGErdi,
  author = {Felix J. Herrmann},
  title = {Recent developments in curvelet-based seismic processing},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2007},
  month = {06},
  abstract = {Combinations of parsimonious signal representations with
                  nonlinear sparsity promoting programs hold the key
                  to the next-generation of seismic data processing
                  algorithms, since they allow for a formulation that
                  is stable w.r.t. noise \& incomplete data do not
                  require prior information on the velocity or
                  locations and dips of the events},
  keywords = {Presentation, SLIM, EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/herrmann07EAGErdi/herrmann07EAGErdi_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/herrmann07EAGErdi/herrmann07EAGErdi.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=7548}
}


@CONFERENCE{herrmann2007EAGEsrm,
  author = {Felix J. Herrmann},
  title = {Surface related multiple prediction from incomplete data},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2007},
  month = {06},
  abstract = {Incomplete data, unknown source-receiver signatures and
                  free-surface reflectivity represent challenges for a
                  successful prediction and subsequent removal of
                  multiples. In this paper, a new method will be
                  represented that tackles these challenges by
                  combining what we know about wavefield
                  (de-)focussing, by weighted
                  convolutions/correlations, and recently developed
                  curvelet-based recovery by sparsity-promoting
                  inversion (CRSI). With this combination, we are able
                  to leverage recent insights from wave physics
                  towards a nonlinear formulation for the
                  multiple-prediction problem that works for
                  incomplete data and without detailed knowledge on
                  the surface effects.},
  keywords = {Presentation, SLIM, EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/herrmann07EAGEsrm/herrmann07EAGEsrm_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/herrmann07EAGEsrm/herrmann07EAGEsrm.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=6496}
}


@CONFERENCE{herrmann2007PIMScsm,
  author = {Felix J. Herrmann},
  title = {Compressive sampling meets seismic imaging},
  booktitle = {PIMS},
  year = {2007},
  keywords = {PIMS, Presentation, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/PIMS/2007/herrmann07PIMScsm/herrmann07PIMScsm_pres.pdf}
}


@CONFERENCE{herrmann2007SEGmpf,
  author = {Felix J. Herrmann},
  title = {Multiple prediction from incomplete data with the focused curvelet transform},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  pages = {2505-2600},
  abstract = {Incomplete data represents a major challenge for a
                  successful prediction and subsequent removal of
                  multiples. In this paper, a new method will be
                  represented that tackles this challenge in a
                  two-step approach. During the first step, the
                  recenly developed curvelet-based recovery by
                  sparsity-promoting inversion (CRSI) is applied to
                  the data, followed by a prediction of the
                  primaries. During the second high-resolution step,
                  the estimated primaries are used to improve the
                  frequency content of the recovered data by combining
                  the focal transform, defined in terms of the
                  estimated primaries, with the curvelet
                  transform. This focused curvelet transform leads to
                  an improved recovery, which can subsequently be used
                  as input for a second stage of multiple prediction
                  and primary-multiple separation.},
  keywords = {SEG, Presentation, SLIM},
  doi = {10.1190/1.2792987},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/herrmann07SEGmpf/herrmann07SEGmpf_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/herrmann07SEGmpf/herrmann07SEGmpf.pdf}
}


@CONFERENCE{herrmann2007SINBADcwe,
  author = {Felix J. Herrmann},
  title = {Compressed wavefield extrapolation},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {An explicit algorithm for the extrapolation of one-way
                  wavefields is proposed which combines recent
                  developments in information theory and theoretical
                  signal processing with the physics of wave
                  propagation. Because of excessive memory
                  requirements, explicit formulations for wave
                  propagation have proven to be a challenge in 3-D. By
                  using ideas from
                  {\textquoteleft}{\textquoteleft}compressed
                  sensing{\textquoteright}{\textquoteright}, we are
                  able to formulate the (inverse) wavefield
                  extrapolation problem on small subsets of the data
                  volume, thereby reducing the size of the
                  operators. According to compressed sensing theory,
                  signals can successfully be recovered from an
                  incomplete set of measurements when the measurement
                  basis is incoherent with the representation in which
                  the wavefield is sparse. In this new approach, the
                  eigenfunctions of the Helmholtz operator are
                  recognized as a basis that is incoherent with
                  curvelets that are known to compress seismic
                  wavefields. By casting the wavefield extrapolation
                  problem in this framework, wavefields can
                  successfully be extrapolated in the modal domain via
                  a computationally cheaper operation. A proof of
                  principle for the
                  {\textquoteleft}{\textquoteleft}compressed
                  sensing{\textquoteright}{\textquoteright} method is
                  given for wavefield extrapolation in 2-D. The
                  results show that our method is stable and produces
                  identical results compared to the direct application
                  of the full extrapolation operator. This is joint
                  work with Tim Lin.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2007/herrmann2007SINBADcwe/herrmann2007SINBADcwe.pdf}
}


@CONFERENCE{herrmann2007SINBADfrw,
  author = {Felix J. Herrmann},
  title = {Focused recovery with the curvelet transform},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {Incomplete data represents a major challenge for a
                  successful prediction and subsequent removal of
                  multiples. In this paper, a new method will be
                  represented that tackles this challenge in a
                  two-step approach. During the first step, the
                  recently developed curvelet-based recovery by
                  sparsity-promoting inversion (CRSI) is applied to
                  the data, followed by a prediction of the
                  primaries. During the second high-resolution step,
                  the estimated primaries are used to improve the
                  frequency content of the recovered data by combining
                  the focal transform, defined in terms of the
                  estimated primaries, with the curvelet
                  transform. This focused curvelet transform leads to
                  an improved recovery, which can subsequently be used
                  as input for a second stage of multiple prediction
                  and primary-multiple separation. This is joint work
                  with Deli Wang and Gilles Hennenfent.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/sites/data/Papers/Herrmann2007SINBADfoc.pdf}
}


@CONFERENCE{herrmann2007SINBADrdi2,
  author = {Felix J. Herrmann},
  title = {Recent developments in primary-multiple separation},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {In this talk, we present a novel primary-multiple
                  separation scheme which makes use of the sparsity of
                  both primaries and multiples in a transform domain,
                  such as the curvelet transform, to provide estimates
                  of each. The proposed algorithm utilizes seismic
                  data as well as the output of a preliminary step
                  that provides (possibly) erroneous predictions of
                  the multiples. The algorithm separates the signal
                  components, i.e., the primaries and multiples, by
                  solving an optimization problem that assumes noisy
                  input data and can be derived from a Bayesian
                  perspective. More precisely, the optimization
                  problem can be arrived at via an assumption of a
                  weighted Laplacian distribution for the primary and
                  multiple coefficients in the transform domain and of
                  white Gaussian noise contaminating both the seismic
                  data and the preliminary prediction of the
                  multiples, which both serve as input to the
                  algorithm. Time permitted, we will also briefly
                  discuss a propasal for adaptive curvelet-domain
                  matched filtering. This is joint work with Deli
                  Wang, Rayan Saaba, {\o}zgur Yilmaz and Eric
                  Verschuur.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2007/herrmann07SINBADrdi2/herrmann07SINBADrdi2_pres.pdf}
}


@CONFERENCE{herrmann2007SINBADsia2,
  author = {Felix J. Herrmann},
  title = {Seismic image amplitude recovery},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {In this talk, we recover the amplitude of a seismic
                  image by approximating the normal
                  (demigration-migration) operator. In this
                  approximation, we make use of the property that
                  curvelets remain invariant under the action of the
                  normal operator. We propose a seismic amplitude
                  recovery method that employs an eigenvalue like
                  decomposition for the normal operator using
                  curvelets as eigen-vectors. Subsequently, we propose
                  an approximate nonlinear singularity-preserving
                  solution to the least-squares seismic imaging
                  problem with sparseness in the curvelet domain and
                  spatial continuity constraints. Our method is tested
                  with a reverse-time
                  {\textquoteright}wave-equation{\textquoteright}
                  migration code simulating the acoustic wave equation
                  on the SEG-AA salt model. This is joint work with
                  Peyman Moghaddam and Chris Stolk (University of
                  Twente)},
  keywords = {Presentation, SINBAD, SLIM}
}


@CONFERENCE{herrmann2007SLIMfsd,
  author = {Felix J. Herrmann},
  title = {From seismic data to the composition of rocks: an interdisciplinary and multiscale approach to exploration seismology},
  booktitle = {Berkhout{\textquoteright}s valedictory address: the conceptual approach of understanding},
  year = {2007},
  abstract = {In this essay, a nonlinear and multidisciplinary
                  approach is presented that takes seismic data to the
                  composition of rocks. The presented work has deep
                  roots in the
                  {\textquoteleft}gedachtengoed{\textquoteright}
                  (philosophy) of Delphi spearheaded by Guus
                  Berkhout. Central themes are multiscale,
                  object-orientation and a multidisciplinary
                  approach.},
  keywords = {SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/Misc/herrmann07SLIMfsd/herrmann07SLIMfsd.pdf }
}


@CONFERENCE{herrmann2006SINBADapo1,
  author = {Felix J. Herrmann},
  title = {A primer on sparsity transforms: curvelets and wave atoms},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {During this presentation an overview will be given on
                  the different sparsity transforms that are used at
                  SLIM. Emphasis will be on two directional and
                  multiscale wavelet transforms, namely the curvelet
                  and the recently introduced wave-atom
                  transforms. The main properties of these transforms
                  will be listed and their performance on seismic data
                  will be discussed.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/hennenfent2006SINBADapo/hennenfent2006SINBADapo.pdf}
}


@CONFERENCE{herrmann2006SINBADapow,
  author = {Felix J. Herrmann},
  title = {A primer on weak conditions for stable recovery},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {During this presentation an introduction will be given
                  on the method of stable recovery from noisy and
                  incomplete data. Weak recovery conditions that
                  guarantee the recovery for typical acquisition
                  geometries will be reviewed and numerical recovery
                  examples will be presented. The advantage of these
                  weak conditions is that they are less pessimistic
                  and {\textquoteleft}verifiable{\textquoteright} or
                  very large-scale acquisition geometries.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/herrmann2006SINBADapow/herrmann2006SINBADapow.pdf}
}


@CONFERENCE{herrmann2006SINBADmpf,
  author = {Felix J. Herrmann},
  title = {Multiple prediction from incomplete data},
  booktitle = {SINBAD 2006},
  year = {2006},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/herrmann2006SINBADmpf/herrmann2006SINBADmpf.pdf}
}


@CONFERENCE{herrmann2006SINBADom,
  author = {Felix J. Herrmann},
  title = {Opening meeting},
  booktitle = {SINBAD 2006},
  year = {2006},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/herrmann2006SINBADom/herrmann2006SINBADom.pdf}
}


@CONFERENCE{herrmann2006SINBADsac,
  author = {Felix J. Herrmann},
  title = {Sparsity- and continuity-promoting seismic image recovery with curvelets},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {A nonlinear singularity-preserving solution to seismic
                  image recovery with sparseness and continuity
                  constraints is proposed. The method explicitly
                  explores the curvelet transform as a directional
                  frame expansion that, by virtue of its sparsity on
                  seismic images and its invariance under the Hessian
                  of the linearized imaging problem, allows for a
                  stable recovery of the migration amplitudes from
                  noisy data. The method corresponds to a
                  preconditioning that corrects the amplitudes during
                  a post-processing step. The solution is formulated
                  as a nonlinear optimization problem where sparsity
                  in the curvelet domain as well as continuity along
                  the imaged reflectors are jointly promoted. To
                  enhance sparsity, the l1-norm on the curvelet
                  coefficients is minimized while continuity is
                  promoted by minimizing an anisotropic diffusion norm
                  on the image. The performance of the recovery scheme
                  is evaluated with
                  {\textquoteright}wave-equation{\textquoteright}
                  migration code on a synthetic dataset. This is joint
                  work with Peyman Moghaddam.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/herrmann2006SINBADsac/herrmann2006SINBADsac.pdf}
}


@CONFERENCE{herrmann2006SINBADsra,
  author = {Felix J. Herrmann},
  title = {Stable recovery and separation of seismic data},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {During this presentation an overview will be given on
                  how seismic data regularization and separation
                  problems can be cast into the framework of stable
                  signal recovery. It is shown that the successful
                  solution of these two problems depends on the
                  existence of signal expansions that are
                  compressible. Preliminary examples will be shown.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/herrmann2006SINBADsra/herrmann2006SINBADsra.pdf}
}


@CONFERENCE{herrmann2004CSEGcia,
  author = {Felix J. Herrmann},
  title = {Curvelet imaging and processing: an overview},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2004},
  organization = {CSEG},
  abstract = {In this paper an overview is given on the application of
                  directional basis functions, known under the name
                  Curvelets/Contourlets, to various aspects of seismic
                  processing and imaging. Key conceps in the approach
                  are the use of (i) directional basis functions that
                  localize in both domains (e.g. space and angle);
                  (ii) non-linear estimation, which corresponds to
                  localized muting on the coefficients, possibly
                  supplemented by constrained optimization (iii)
                  invariance of the basis functions under the imaging
                  operators. We will discuss applications that include
                  multiple and ground roll removal;
                  sparseness-constrained least-squares migration and
                  the computation of 4-D difference cubes.},
  keywords = {Presentation, SLIM},
  month = {05},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2004/Herrmann04CSEGcia/Herrmann04CSEGcia.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2004/Herrmann04CSEGcia/Herrmann04CSEGcia_paper.pdf}
}


@CONFERENCE{herrmann2003SEGoiw,
  author = {Felix J. Herrmann},
  title = {"Optimal" imaging with curvelets},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2003},
  volume = {22},
  pages = {997-1000},
  abstract = {In this paper we present a non-linear edge-preserving
                  solution to linear inverse scattering problems based
                  on optimal basis-function decompositions. Optimality
                  of the basis functions allow us to (i) reduce the
                  dimensionality of the inverse problem; (ii) devise
                  non-linear thresholding operators that approximate
                  minimax (minimize the maximal mean square error
                  given the worst possible prior) and that
                  significantly improve the signal-to-noise ratio on
                  the image. We present a reformulation of the
                  standard generalized least-squares formulation of
                  the seismic inversion problem into a formulation
                  based on thresholding, where the singular values,
                  vectors and linear estimators are replaced by
                  quasi-singular values, basis-functions and
                  thresholding. To limit the computational burden we
                  use a Monte-Carlo sampling method to compute the
                  quasi-singular values. With the proposed method, we
                  aim to significantly improve the signal-to-noise
                  ratio (SNR) on the model space and hence the
                  resolution of the seismic image. While classical
                  Tikhonov-regularized methods only gain the
                  square-root of the SNR on the data for the SNR on
                  the model our method scales almost linearly.  This
                  significant improvement of the SNR allows us to
                  discern events at high frequencies which would
                  normally be in the noise.},
  keywords = {Presentation, SEG, SLIM},
  doi = {10.1190/1.1818117},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2003/Herrmann03SEGoiw/Herrmann03SEGoiw_pres.pdf}
}


@CONFERENCE{herrmann2001EAGEsas,
  author = {Felix J. Herrmann},
  title = {Scaling and seismic reflectivity: implications of scaling on {AVO}},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2001},
  abstract = {AVO analysis of seismic data is based on the assumption
                  that transitions in the earth consist of jump
                  discontinuities only. Generalization of these
                  transitions to more realistic transitions shows a
                  drastic change in observed AVO behavior, especially
                  for the large angles currently attained by
                  increasing cable lengths. We propose a simple
                  ities. After renormalization, the inverted
                  fluctuations regain their relative magnitudes which,
                  due to the scaling, may have been significantly
                  distorted.},
  keywords = {SLIM},
  month = {06},
}


@CONFERENCE{herrmann2011ICIAMconvexcompfwi,
  author = {Felix J. Herrmann and Aleksandr Y. Aravkin and Tristan van Leeuwen and Xiang Li},
  title = {{FWI} with sparse recovery: a convex-composite approach},
  booktitle = {ICIAM},
  year = {2011},
  organization = {ICIAM 2011},
  abstract = {Iterative inversion algorithms require repeated
                  simulation of 3D time-dependent acoustic, elastic,
                  or electromagnetic wave fields, extending hundreds
                  of wavelengths and hundreds of periods. Also,
                  seismic data is rich in information at every
                  representable scale. Thus simulation-driven
                  optimization approaches to inversion impose great
                  demands on simulator efficiency and accuracy. While
                  computer hardware advances have been of critical
                  importance in bringing inversion closer to practical
                  application, algorithmic advances in simulator
                  methodology have been equally important. Speakers in
                  this two-part session will address a variety of
                  numerical issues arising in the wave simulation, and
                  in its application to inversion.},
  date-added = {2011-07-20},
  keywords = {ICIAM, full-waveform inversion, optimization},
  month = {07},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ICIAM/2011/herrmann2011ICIAMconvexcompfwi/herrmann2011ICIAMconvexcompfwi.pdf}
}


@CONFERENCE{herrmann2011SLRAfwi,
  author = {Felix J. Herrmann and Aleksandr Y. Aravkin and Xiang Li and Tristan van Leeuwen},
  title = {Full waveform inversion with compressive updates},
  booktitle = {SLRA},
  year = {2011},
  organization = {Sparse and Low Rank Approximation 2011},
  abstract = {Full-waveform inversion relies on large multi-experiment
                  data volumes. While improvements in acquisition and
                  inversion have been extremely successful, the
                  current push for higher quality models reveals
                  fundamental shortcomings handling increasing problem
                  sizes numerically. To address this fundamental
                  issue, we propose a randomized
                  dimensionality-reduction strategy motivated by
                  recent developments in stochastic optimization and
                  compressive sensing. In this formulation
                  conventional Gauss-Newton iterations are replaced by
                  dimensionality-reduced sparse recovery problems with
                  source encodings.},
  keywords = {full-waveform inversion},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SLRA/2011/herrmann2011SLRAfwi/herrmann2011SLRAfwi.pdf}
}


@CONFERENCE{herrmann2006SINBADpms,
  author = {Felix J. Herrmann and Urs Boeniger and D. J. Verschuur},
  title = {Primary-multiple separation by curvelet frames},
  booktitle = {SINBAD 2006},
  year = {2006},
  volume = {170},
  pages = {781-799},
  organization = {Geophysical Journal International},
  abstract = {Predictive multiple suppression methods consist of two
                  main steps: a prediction step, during which
                  multiples are predicted from seismic data, and a
                  primary-multiple separation step, during which the
                  predicted multiples are
                  {\textquoteright}matched{\textquoteright} with the
                  true multiples in the data and subsequently
                  removed. The last step is crucial in practice: an
                  incorrect separation will cause residual multiple
                  energy in the result or may lead to a distortion of
                  the primaries, or both. To reduce these adverse
                  effects, a new transformed-domain method is proposed
                  where primaries and multiples are separated rather
                  than matched. This separation is carried out on the
                  basis of differences in the multiscale and
                  multidirectional characteristics of these two signal
                  components. Our method uses the curvelet transform,
                  which maps multidimensional data volumes into almost
                  orthogonal localized multidimensional prototype
                  waveforms that vary in directional and
                  spatio-temporal content. Primaries-only and
                  multiples-only signal components are recovered from
                  the total data volume by a nonlinear optimization
                  scheme that is stable under noisy input data. During
                  the optimization, the two signal components are
                  separated by enhancing sparseness (through weighted
                  l1-norms) in the transformed domain subject to
                  fitting the observed data as the sum of the
                  separated components to within a user-defined
                  tolerance level. Whenever the prediction for the two
                  signal components in the transformed domain
                  correlate, the recovery is suppressed while for
                  regions where the correlation is small the method
                  seeks the sparsest set of coefficients that
                  represent each signal component. Our algorithm does
                  not seek a matched filter and as such it differs
                  fundamentally from traditional adaptive subtraction
                  methods. The method derives its stability from the
                  sparseness obtained by a non-parametric multiscale
                  and multidirectional overcomplete signal
                  representation. This sparsity serves as prior
                  information and allows for a Bayesian interpretation
                  of our method during which the log-likelihood
                  function is minimized while the two signal
                  components are assumed to be given by a
                  superposition of prototype waveforms, drawn
                  independently from a probability function that is
                  weighted by the predicted primaries and
                  multiples. In this paper, the predictions are based
                  on the data-driven surface-related multiple
                  elimination (SRME) method. Synthetic and field data
                  examples show a clean separation leading to a
                  considerable improvement in multiple suppression
                  compared to the conventional method of adaptive
                  matched filtering. This improved separation
                  translates into an improved stack.},
  keywords = {Presentation, SINBAD, SLIM},
  doi = {10.1111/j.1365-246X.2007.03360.x},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/herrmann2006SINBADpms/herrmann2006SINBADpms.pdf}
}


@CONFERENCE{herrmann2009EAGEcsa,
  author = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title = {Compressive sensing applied to full-waveform inversion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2009},
  month = {06},
  abstract = {With the recent resurgence of full-waveform inversion,
                  the computational cost of solving forward modeling
                  problems has become{\textendash}-aside from issues
                  with non-uniqueness{\textendash}-one of the major
                  impediments withstanding successful application of
                  this technology to industry-size data volumes. To
                  overcome this impediment, we argue that further
                  improvements in this area will depend on a problem
                  formulation with a computational complexity that is
                  no longer strictly determined by the size of the
                  discretization but by transform-domain sparsity of
                  its solution. In this new paradigm, we bring
                  computational costs in par with our ability to
                  compress seismic data and images. This premise is
                  related to two recent developments. First, there is
                  the new field of compressive sensing (CS in short
                  throughout the paper, Cand{\textquoteleft}es et al.,
                  2006; Donoho, 2006){\textendash}-where the argument
                  is made, and rigorously proven, that compressible
                  signals can be recovered from severely sub-Nyquist
                  sampling by solving a sparsity promoting
                  program. Second, there is in the seismic community
                  the recent resurgence of simultaneous-source
                  acquisition (Beasley, 2008; Krohn and Neelamani,
                  2008; Herrmann et al., 2009; Berkhout, 2008;
                  Neelamani et al., 2008), and continuing efforts to
                  reduce the cost of seismic modeling, imaging, and
                  inversion through phase encoding of simultaneous
                  sources (Morton and Ober, 1998; Romero et al., 2000;
                  Krohn and Neelamani, 2008; Herrmann et al., 2009),
                  removal of subsets of angular frequencies (Sirgue
                  and Pratt, 2004; Mulder and Plessix, 2004; Lin et
                  al., 2008) or plane waves (Vigh and Starr, 2008). By
                  using CS principles, we remove sub-sampling
                  interferences asocciated with these approaches
                  through a combination of exploiting transform-domain
                  sparsity, properties of certain sub-sampling
                  schemes, and the existence of sparsity promoting
                  solvers.},
  keywords = {EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2009/Herrmann09EAGEcsa/Herrmann09EAGEcsa_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2009/Herrmann09EAGEcsa/Herrmann09EAGEcsa.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=23961}
}


@CONFERENCE{herrmann2009IAPcsisa,
  author = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title = {Compressive seismic imaging with simultaneous acquisition},
  booktitle = {IAP},
  year = {2009},
  abstract = {The shear size of seismic data volumes forms one of the
                  major impediments for the inversion of seismic
                  data. Turning forward modeling and inversion into a
                  compressive sensing (CS) problem - where simulated
                  data are recovered from a relatively small number of
                  independent sources - can effectively mitigate this
                  high-cost impediment. Our key contribution lies in
                  the design of a sub-sampling operator that commutes
                  with the time-harmonic Helmholtz system. As in
                  compressive sensing, this leads to a reduction of
                  simulation cost. This reduction is commensurate with
                  the transform-domain sparsity of the solution.,
                  implying that computational costs are no longer
                  determined by the size of the discretization but by
                  transform-domain sparsity of the solution of the CS
                  problem that recovers the data. The combination of
                  this sub-sampling strategy with our recent work on
                  preconditioned implicit solvers for the
                  time-harmonic Helmholtz equation provides a viable
                  alternative to full-waveform inversion schemes based
                  on explicit time-domain finite-difference methods.},
  keywords = {Presentation},
  note = {Presented at the IAP meeting, Vienna},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/AIP/2009/herrmann2009IAPcsisa/Herrmann09AIP1.pdf}
}


@CONFERENCE{herrmann2009SAMPTAcws,
  author = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title = {Compressive-wavefield simulations},
  booktitle = {SAMPTA},
  year = {2009},
  organization = {SAMPTA},
  abstract = {Full-waveform inversion{\textquoteright}s high demand on
                  computational resources forms, along with the
                  non-uniqueness problem, the major impediment
                  withstanding its widespread use on industrial-size
                  datasets. Turning modeling and inversion into a
                  compressive sensing problem{\textendash}-where
                  simulated data are recovered from a relatively small
                  number of independent simultaneous
                  sources{\textendash}-can effectively mitigate this
                  high-cost impediment. The key is in showing that we
                  can design a sub-sampling operator that commutes
                  with the time-harmonic Helmholtz system. As in
                  compressive sensing, this leads to a reduction in
                  simulation cost. Moreover, this reduction is
                  commensurate with the transform-domain sparsity of
                  the solution, implying that computational costs are
                  no longer determined by the size of the
                  discretization but by transform-domain sparsity of
                  the solution of the CS problem which forms our data.
                  The combination of this sub-sampling strategy with
                  our recent work on implicit solvers for the
                  Helmholtz equation provides a viable alternative to
                  full-waveform inversion schemes based on explicit
                  finite-difference methods.},
  keywords = {SAMPTA},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SAMPTA/2009/Herrmann09SAMPTAcws/Herrmann09SAMPTAcws.pdf}
}


@CONFERENCE{herrmann2008SIAMcsm,
  author = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title = {Compressive sampling meets seismic imaging},
  booktitle = {SIAM},
  year = {2008},
  abstract = {Compressive sensing has led to fundamental new insights
                  in the recovery of compressible signals from
                  sub-Nyquist samplings. It is shown how jittered
                  subsampling can be used to create favorable recovery
                  conditions. Applications include mitigation of
                  incomplete acquisitions and wavefield
                  computations. While the former is a direct
                  adaptation of compressive sampling, the latter
                  application represents a new way of compressing
                  wavefield extrapolation operators. Operators are not
                  diagonalized but are compressively sampled reducing
                  the computational costs.},
  keywords = {Presentation, SIAM, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2008/herrmann2008SIAMcsm/herrmann2008SIAMcsm.pdf}
}


@CONFERENCE{herrmann2008SINBADitc,
  author = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin and Cody R. Brown},
  title = {Introduction to compressive (wavefield) computation},
  booktitle = {SINBAD 2008},
  year = {2008},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/herrmann2008SINBADitc/herrmann2008SINBADitc.pdf}
}


@CONFERENCE{herrmann2005CSEGnld,
  author = {Felix J. Herrmann and Gilles Hennenfent},
  title = {Non-linear data continuation with redundant frames},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2005},
  organization = {CSEG},
  abstract = {We propose an efficient iterative data interpolation
                  method using continuity along reflectors in seismic
                  images via curvelet and discrete cosine
                  transforms. The curvelet transform is a new
                  multiscale transform that provides sparse
                  representations for images that comprise smooth
                  objects separated by piece-wise smooth
                  discontinuities (e.g. seismic images). The advantage
                  of using curvelets is that these frames are sparse
                  for high-frequency caustic-free solutions of the
                  wave-equation. Since we are dealing with less than
                  ideal data (e.g. bandwidth-limited), we compliment
                  the curvelet frames with the discrete cosine
                  transform.  The latter is motivated by the
                  successful data continuation with the discrete
                  Fourier transform. By choosing generic basis
                  functions we circumvent the necessity to make
                  parametric assumptions (e.g., through
                  linear/parabolic Radon or demigration) regarding the
                  shape of events in seismic data. Synthetic and real
                  data examples demonstrate that our algorithm
                  provides interpolated traces that accurately
                  reproduce the wavelet shape as well as the AVO
                  behavior along events in shot gathers.},
  keywords = {Presentation, SLIM},
  month = {05},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2005/Herrmann05CSEGnld/Herrmann05CSEGnld_pres.pdf},
  url = {http://www.cseg.ca/assets/files/resources/abstracts/2005/101S0201-Herrmann_F_Non_Linear_Data_Continuation.pdf}
}


@CONFERENCE{herrmann2005EAGErcd,
  author = {Felix J. Herrmann and Gilles Hennenfent},
  title = {Robust curvelet-domain data continuation with sparseness constraints},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2005},
  month = {06},
  abstract = {A robust data interpolation method using curvelets
                  frames is presented. The advantage of this method is
                  that curvelets arguably provide an optimal sparse
                  representation for solutions of wave equations with
                  smooth coefficients. As such curvelets frames
                  circum- vent {\textendash} besides the assumption of
                  caustic-free data {\textendash} the necessity to
                  make parametric assumptions (e.g. through
                  linear/parabolic Radon or demigration) regarding the
                  shape of events in seismic data. A brief sketch of
                  the theory is provided as well as a number of
                  examples on synthetic and real data.},
  keywords = {Presentation, SLIM, EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2005/Herrmann05EAGErcd/Herrmann05EAGErcd.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2005/Herrmann05EAGErcd/Herrmann05EAGErcd.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=1112}
}


@CONFERENCE{herrmann2007EAGEsia,
  author = {Felix J. Herrmann and Gilles Hennenfent and Peyman P. Moghaddam},
  title = {Seismic imaging and processing with curvelets},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2007},
  month = {06},
  abstract = {In this paper, we present a nonlinear curvelet-based
                  sparsity-promoting formulation for three problems in
                  seismic processing and imaging namely, seismic data
                  regularization from data with large percentages of
                  traces missing; seismic amplitude recovery for
                  sub-salt images obtained by reverse-time migration
                  and primary-multiple separation, given an inaccurate
                  multiple prediction. We argue why these nonlinear
                  formulations are beneficial.},
  keywords = {Presentation, SLIM, EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/herrmann07EAGEsia/herrmann07EAGEsia_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/herrmann07EAGEsia/herrmann07EAGEsia.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=7075}
}


@CONFERENCE{Herrmann2011BG,
  author = {Felix J. Herrmann and Tristan van Leeuwen},
  title = {{SINBAD's} research program},
  year = {2011},
  month = {11},
  owner = {Shruti},
  quality = {1},
  timestamp = {2013.01.16},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/Misc/Herrmann2011BG.pdf}
}


@CONFERENCE{herrmann2011EAGEefmsp,
  author = {Felix J. Herrmann and Xiang Li},
  title = {Efficient least-squares migration with sparsity promotion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2011},
  month = {05},
  abstract = {Seismic imaging relies on the collection of
                  multi-experimental data volumes in combination with
                  a sophisticated back-end to create high-fidelity
                  inversion results. While significant improvements
                  have been made in linearized inversion, the current
                  trend of incessantly pushing for higher quality
                  models in increasingly complicated regions reveals
                  fundamental shortcomings in handling increasing
                  problem sizes numerically. The so-called ``curse of
                  dimensionality" is the main culprit because it leads
                  to an exponential growth in the number of sources
                  and the corresponding number of wavefield
                  simulations required by ``wave-equation"
                  migration. We address this issue by reducing the
                  number of sources by a randomized dimensionality
                  reduction technique that combines recent
                  developments in stochastic optimization and
                  compressive sensing. As a result, we replace the
                  cur- rent formulations of imaging that rely on all
                  data by a sequence of smaller imaging problems that
                  use the output of the previous inversion as input
                  for the next. Empirically, we find speedups of at
                  least one order-of-magnitude when each reduced
                  experiment is considered theoretically as a separate
                  compressive-sensing experiment.},
  keywords = {Presentation, EAGE, imaging},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2011/herrmann11EAGEefmsp/herrmann11EAGEefmsp_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2011/herrmann11EAGEefmsp/herrmann11EAGEefmsp.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=50333}
}


@CONFERENCE{herrmann2010EAGErds,
  author = {Felix J. Herrmann and Xiang Li},
  title = {Randomized dimensionality reduction for full-waveform inversion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2010},
  month = {06},
  abstract = {Full-waveform inversion relies on the collection of
                  large multi-experiment data volumes in combination
                  with a sophisticated back-end to create
                  high-fidelity inversion results. While improvements
                  in acquisition and inversion have been extremely
                  successful, the current trend of incessantly pushing
                  for higher quality models in increasingly
                  complicated regions of the Earth continues to reveal
                  fundamental shortcomings in our ability to handle
                  the ever increasing problem size numerically. Two
                  causes can be identified as the main culprits
                  responsible for this barrier. First, there is the
                  so-called {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which puts disproportionate strain on
                  current acquisition and processing systems as the
                  size and desired resolution of our survey areas
                  continues to increase. Secondly, there is the recent
                  {\textquoteleft}{\textquoteleft}departure from
                  Moore{\textquoteright}s
                  law{\textquoteright}{\textquoteright} that forces us
                  to lower our expectations to compute ourselves out
                  of this. In this paper, we address this situation by
                  randomized dimensionality reduction, which we adapt
                  from the field of compressive sensing. In this
                  approach, we combine deliberate randomized
                  subsampling with structure-exploiting
                  transform-domain sparsity promotion. Our approach is
                  successful because it reduces the size of seismic
                  data volumes without loss of information. With this
                  reduction, we compute Newton-like updates at the
                  cost of roughly one gradient update for the
                  fully-sampled wavefield.},
  keywords = {Presentation, EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2010/herrmann10EAGErds/herrmann10EAGErds_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2010/herrmann10EAGErds/herrmann10EAGErds.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=39352}
}


@CONFERENCE{herrmann2011SPIEmsp,
  author = {Felix J. Herrmann and Xiang Li and Aleksandr Y. Aravkin and Tristan van Leeuwen},
  title = {A modified, sparsity promoting, {Gauss-Newton} algorithm for seismic waveform inversion},
  booktitle = {Proc. SPIE},
  year = {2011},
  number = {81380V},
  abstract = {Images obtained from seismic data are used by the oil
                  and gas industry for geophysical
                  exploration. Cutting-edge methods for transforming
                  the data into interpretable images are moving away
                  from linear approximations and high-frequency
                  asymptotics towards Full Waveform Inversion (FWI), a
                  nonlinear data-fitting procedure based on full data
                  modeling using the wave-equation. The size of the
                  problem, the nonlinearity of the forward model, and
                  ill-posedness of the formulation all contribute to a
                  pressing need for fast algorithms and novel
                  regularization techniques to speed up and improve
                  inversion results. In this paper, we design a
                  modified Gauss-Newton algorithm to solve the PDE-
                  constrained optimization problem using ideas from
                  stochastic optimization and compressive
                  sensing. More specifically, we replace the
                  Gauss-Newton subproblems by randomly subsampled,
                  -$\ell_1$ regularized subproblems. This allows us us
                  significantly reduce the computational cost of
                  calculating the updates and exploit the
                  compressibility of wavefields in Curvelets. We
                  explain the relationships and connections between
                  the new method and stochastic optimization and
                  compressive sensing (CS), and demonstrate the
                  efficacy of the new method on a large-scale
                  synthetic seismic example.},
  issn = {1},
  keywords = {SLIM, compressive sensing, optimization, full-waveform inversion},
  notes = {TR-2011-05},
  doi = {10.1117/12.893861},
  month = {08},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SPIE/2011/herrmann2011SPIEmsp/herrmann2011SPIEmsp.pdf}
}


@CONFERENCE{herrmann2007EAGEjda,
  author = {Felix J. Herrmann and Peyman P. Moghaddam},
  title = {Just diagonalize: a curvelet-based approach to seismic amplitude recovery},
  booktitle = {EAGE Workshop on Curvelets, contourlets, seislets, … in seismic data processing - where are we and where are we going?},
  year = {2007},
  month = {06},
  abstract = {In his presentation we present a nonlinear
                  curvelet-based sparsity-promoting formulation for
                  the recovery of seismic amplitudes. We show that the
                  curvelet's wavefront detection capability and
                  invariance under wave propagation lead to a
                  formulation of this recovery problem that is stable
                  under noise and missing data. {\copyright}2007
                  Society of Exploration Geophysicists},
  keywords = {Presentation, SLIM, EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2007/herrmann2007EAGEjda/herrmann2007EAGEjda_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2007/herrmann2007EAGEjda/herrmann2007EAGEjda_paper.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=7555}
}


@CONFERENCE{herrmann2005CSEGnlr,
  author = {Felix J. Herrmann and Peyman P. Moghaddam},
  title = {Non-linear regularization in seismic imaging},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2005},
  organization = {CSEG},
  abstract = {Two complementary solution strategies to the
                  least-squares imaging problem with sparseness \&
                  continuity continuity constraints are proposed. The
                  applied formalism explores the sparseness of
                  curvelets coefficients of the reflectivity and their
                  invariance under the demigration-migration
                  operator. We achieve the solution by jointly
                  minimizing a weighted l1-norm on the curvelet
                  coefficients and an anisotropic difussion or total
                  variation norm on the imaged reflectivity model. The
                  l1-norm exploits the sparsenss of the reflectivity
                  in the curvelet domain whereas the anisotropic norm
                  enhances the continuity along the reflections while
                  removing artifacts residing in between
                  reflectors. While the two optimization methods
                  (convex versus non-convex) share the same type of
                  regularization, they differ in flexibility how to
                  handle additional constraints on the coefficients of
                  the imaged reflectivity and in computational
                  expense. A brief sketch of the theory is provided
                  along with a number of synthetic examples.},
  keywords = {Presentation, SLIM},
  month = {05},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2005/Herrmann05CSEGnlr/Herrmann05CSEGnlr_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2005/Herrmann05CSEGnlr/Herrmann05CSEGnlr.pdf}
}


@CONFERENCE{herrmann2004CSEGcia2,
  author = {Felix J. Herrmann and Peyman P. Moghaddam},
  title = {Curvelet imaging and processing: sparseness-constrained least-squares migration},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2004},
  organization = {CSEG},
  abstract = {A non-linear edge-preserving solution to the
                  least-squares migration problem with sparseness
                  constraints is introduced. The applied formalism
                  explores Curvelets as basis functions that, by
                  virtue of their sparseness and locality, not only
                  allow for a reduction of the dimensionality of the
                  imaging problem but which also naturally lead to a
                  non-linear solution with significantly improved
                  signal-to-noise ratio. Additional conditions on the
                  image are imposed by solving a constrained
                  optimization problem on the estimated Curvelet
                  coefficients initialized by thresholding. This
                  optimization is designed to also restore the
                  amplitudes by (approximately) inverting the normal
                  operator, which is, like-wise to the (de)-migration
                  operators, almost diagonalized by the Curvelet
                  transform.},
  keywords = {Presentation, SLIM},
  month = {05},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2004/Herrmann04CSEGcia2/Herrmann04CSEGcia2.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2004/Herrmann04CSEGcia2/Herrmann04CSEGcia2_paper.pdf}
}


@CONFERENCE{herrmann2004EAGEcdl,
  author = {Felix J. Herrmann and Peyman P. Moghaddam},
  title = {Curvelet-domain least-squares migration with sparseness constraints},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2004},
  month = {06},
  abstract = {A non-linear edge-preserving solution to the
                  least-squares migration problem with sparseness
                  constraints is introduced. The applied formalism
                  explores Curvelets as basis functions that, by
                  virtue of their sparseness and locality, not only
                  allow for a reduction of the dimensionality of the
                  imaging problem but which also naturally lead to a
                  non-linear solution with significantly improved
                  signal-to-noise ratio. Additional conditions on the
                  image are imposed by solving a constrained
                  optimization problem on the estimated Curvelet
                  coefficients initialized by thresholding.  This
                  optimization is designed to also restore the
                  amplitudes by (approximately) inverting the normal
                  operator, which is like-wise the (de)-migration
                  operators, almost diagonalized by the Curvelet
                  transform.},
  keywords = {Presentation, SLIM, EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2004/Herrmann04EAGEcdl/Herrmann04EAGEcdl_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2004/Herrmann04EAGEcdl/Herrmann04EAGEcdl.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=2073}
}


@CONFERENCE{herrmann2004EAGEcdp,
  author = {Felix J. Herrmann and Peyman P. Moghaddam},
  title = {Curvelet-domain preconditioned 'wave-equation' depth-migration with sparseness and illumination constraints},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2004},
  abstract = {A non-linear edge-preserving solution to the
                  least-squares migration problem with sparseness
                  constraints is introduced. The applied formalism
                  explores Curvelets as basis functions that, by
                  virtue of their sparseness and locality, not only
                  allow for a reduction of the dimensionality of the
                  imaging problem but which also naturally lead to a
                  non-linear solution with significantly improved
                  signal- to-noise ratio. Additional conditions on the
                  image are imposed by solving a constrained
                  optimization problem on the estimated Curvelet
                  coefficients initialized by thresholding.  This
                  optimization is designed to also restore the
                  amplitudes by (approximately) inverting the normal
                  operator, which is like-wise the (de)-migration
                  operators, almost diagonalized by the Curvelet
                  transform.},
  keywords = {SLIM},
  month = {06},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2004/herrmann2004EAGEcdp/herrmann2004EAGEcdp.pdf}
}


@CONFERENCE{herrmann2004SEGcbn,
  author = {Felix J. Herrmann and Peyman P. Moghaddam},
  title = {Curvelet-based non-linear adaptive subtraction with sparseness constraints},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2004},
  volume = {23},
  pages = {1977-1980},
  organization = {SEG},
  abstract = {In this paper an overview is given on the application of
                  directional basis functions, known under the name
                  Curvelets/Contourlets, to various aspects of seismic
                  processing and imaging, which involve adaptive
                  subtraction. Key concepts in the approach are the
                  use of directional basis functions that localize in
                  both domains (e.g. space and angle); non-linear
                  estimation, which corresponds to localized muting on
                  the coefficients, possibly supplemented by
                  constrained optimization. We will discuss
                  applications that include multiple, ground-roll
                  removal and migration denoising. {\copyright}2004
                  Society of Exploration Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.1851181},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2004/Herrmann04SEGcbn/Herrmann04SEGcbn_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2004/Herrmann04SEGcbn/Herrmann04SEGcbn.pdf }
}


@CONFERENCE{herrmann2005EAGEosf,
  author = {Felix J. Herrmann and Peyman P. Moghaddam and R. Kirlin},
  title = {Optimization strategies for sparseness- and continuity-enhanced imaging: theory},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2005},
  month = {06},
  abstract = {Two complementary solution strategies to the
                  least-squares migration problem with sparseness- and
                  continuity constraints are proposed. The applied
                  formalism explores the sparseness of curvelets on
                  the reflectivity and their invariance under the
                  demigration-migration operator. Sparseness is
                  enhanced by (approximately) minimizing a (weighted)
                  l1-norm on the curvelet coefficients. Continuity
                  along imaged reflectors is brought out by minimizing
                  the anisotropic diffusion or total variation norm
                  which penalizes variations along and in between
                  reflectors. A brief sketch of the theory is provided
                  as well as a number of synthetic examples. Technical
                  details on the implementation of the optimization
                  strategies are deferred to an accompanying paper:
                  implementation.},
  keywords = {SLIM, EAGE},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2005/Herrmann05EAGEosf/Herrmann05EAGEosf.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=1343}
}


@CONFERENCE{herrmann2008SEGcdm,
  author = {Felix J. Herrmann and Peyman P. Moghaddam and Deli Wang},
  title = {Curvelet-domain matched filtering},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  number = {3643-3649},
  organization = {SEG},
  abstract = {Matching seismic wavefields and images lies at the heart
                  of many pre-/post-processing steps part of seismic
                  imaging{\textendash}- whether one is matching
                  predicted wavefield components, such as multiples,
                  to the actual to-be-separated wavefield components
                  present in the data or whether one is aiming to
                  restore migration amplitudes by scaling, using an
                  image-to-remigrated-image matching procedure to
                  calculate the scaling coefficients. The success of
                  these wavefield matching procedures depends on our
                  ability to (i) control possible overfitting, which
                  may lead to accidental removal of energy or to
                  inaccurate image-amplitude corrections, (ii) handle
                  data or images with nonunique dips, and (iii) apply
                  subsequent wavefield separations or migraton
                  amplitude corrections stably. In this paper, we show
                  that the curvelet transform allows us to address all
                  these issues by imposing smoothness in phase space,
                  by using their capability to handle conflicting
                  dips, and by leveraging their ability to represent
                  seismic data and images sparsely. This latter
                  property renders curvelet-domain sparsity promotion
                  an effective prior.},
  keywords = {SLIM,Presentation, SEG},
  month = {08},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/herrmann08SEGcdm/herrmann08SEGcdm_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/herrmann08SEGcdm/herrmann08SEGcdm.pdf }
}


@CONFERENCE{herrmann09EAGEbnrs,
  author = {Felix J. Herrmann and Gang Tang and Reza Shahidi and Gilles Hennenfent
	and Tim T.Y. Lin},
  title = {Beating Nyquist by randomized sampling},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2009},
  note = {Presented at the EAGE (workshop), Amsterdam},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2009/herrmann09EAGEbnrs/herrmann09EAGEbnrs.pdf}
}


@CONFERENCE{herrmann2005EAGErcd1,
  author = {Felix J. Herrmann and D. J. Verschuur},
  title = {Robust curvelet-domain primary-multiple separation with sparseness constraints},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2005},
  month = {06},
  abstract = {A non-linear primary-multiple separation method using
                  curvelets frames is presented. The advantage of this
                  method is that curvelets arguably provide an optimal
                  sparse representation for both primaries and
                  multiples. As such curvelets frames are ideal
                  candidates to separate primaries from multiples
                  given inaccurate predictions for these two data
                  components. The method derives its robustness
                  regarding the presence of noise; errors in the
                  prediction and missing data from the curvelet
                  frame{\textquoteright}s ability (i) to represent
                  both signal components with a limited number of
                  multi-scale and directional basis functions; (ii) to
                  separate the components on the basis of differences
                  in location, orientation and scales and (iii) to
                  minimize correlations between the coefficients of
                  the two components. A brief sketch of the theory is
                  provided as well as a number of examples on
                  synthetic and real data.},
  keywords = {SLIM, EAGE},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2005/Herrmann05EAGErcd1/Herrmann05EAGErcd1.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=1384}
}


@CONFERENCE{herrmann2004CSEGcia1,
  author = {Felix J. Herrmann and D. J. Verschuur},
  title = {Curvelet imaging and processing: adaptive multiple elimination},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2004},
  organization = {CSEG},
  abstract = {Predictive multiple suppression methods consist of two
                  main steps: a prediction step, in which multiples
                  are predicted from the seismic data, and a
                  subtraction step, in which the predicted multiples
                  are matched with the true multiples in the data. The
                  last step appears crucial in practice: an incorrect
                  adaptive subtraction method will cause multiples to
                  be sub-optimally subtracted or primaries being
                  distorted, or both. Therefore, we propose a new
                  domain for separation of primaries and multiples via
                  the Curvelet transform. This transform maps the data
                  into almost orthogonal localized events with a
                  directional and spatial-temporal component. The
                  multiples are suppressed by thresholding the input
                  data at those Curvelet components where the
                  predicted multiples have large amplitudes. In this
                  way the more traditional filtering of predicted
                  multiples to fit the input data is avoided. An
                  initial field data example shows a considerable
                  improvement in multiple suppression.},
  keywords = {Presentation, SLIM},
  month = {05},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2004/Herrmann04CSEGcia1/Herrmann04CSEGcia1.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2004/Herrmann04CSEGcia1/Herrmann04CSEGcia1_paper.pdf}
}


@CONFERENCE{herrmann2004EAGEsop,
  author = {Felix J. Herrmann and D. J. Verschuur},
  title = {Separation of primaries and multiples by non-linear estimation in the curvelet domain},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2004},
  abstract = {Predictive multiple suppression methods consist of two
                  main steps: a prediction step, in which multiples
                  are predicted from the seismic data, and a
                  subtraction step, in which the predicted multiples
                  are matched with the true multiples in the data. The
                  last step appears crucial in practice: an incorrect
                  adaptive subtraction method will cause multiples to
                  be sub-optimally subtracted or primaries being
                  distorted, or both. Therefore, we propose a new
                  domain for separation of primaries and multiples via
                  the Curvelet transform. This transform maps the data
                  into almost orthogonal localized events with a
                  directional and spatial-temporal component. The
                  multiples are suppressed by thresholding the input
                  data at those Curvelet components where the
                  predicted multiples have large amplitudes. In this
                  way the more traditional filtering of predicted
                  multiples to fit the input data is avoided. An
                  initial field data example shows a considerable
                  improvement in multiple suppression.},
  keywords = {SLIM},
  month = {06},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2004/herrmann2004EAGEsop/herrmann2004EAGEsop.pdf}
}


@CONFERENCE{herrmann2004SEGcdm,
  author = {Felix J. Herrmann and D. J. Verschuur},
  title = {Curvelet-domain multiple elimination with sparseness constraints},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2004},
  volume = {23},
  pages = {1333-1336},
  organization = {SEG},
  abstract = {Predictive multiple suppression methods consist of two
                  main steps: a prediction step, in which multiples
                  are predicted from the seismic data, and a
                  subtraction step, in which the predicted multiples
                  are matched with the true multiples in the data. The
                  last step appears crucial in practice: an incorrect
                  adaptive subtraction method will cause multiples to
                  be sub-optimally subtracted or primaries being
                  distorted, or both. Therefore, we propose a new
                  domain for separation of primaries and multiples via
                  the Curvelet transform. This transform maps the data
                  into almost orthogonal localized events with a
                  directional and spatial-temporal component. The
                  multiples are suppressed by thresholding the input
                  data at those Curvelet components where the
                  predicted multiples have large amplitudes. In this
                  way the more traditional filtering of predicted
                  multiples to fit the input data is avoided. An
                  initial field data example shows a considerable
                  improvement in multiple suppression.
                  {\copyright}2004 Society of Exploration
                  Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.1851110},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2004/Herrmann04SEGcdm/Herrmann04SEGcdm_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2004/Herrmann04SEGcdm/Herrmann04SEGcdm.pdf}
}


@CONFERENCE{herrmann2008SEGswi,
  author = {Felix J. Herrmann and Deli Wang},
  title = {Seismic wavefield inversion with curvelet-domain sparsity promotion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  pages = {2497-2501},
  organization = {SEG},
  abstract = {Inverting seismic wavefields lies at the heart of
                  seismic data processing and imaging{\textendash}-
                  whether one is applying
                  {\textquoteleft}{\textquoteleft}a poor
                  man{\textquoteright}s
                  inverse{\textquoteright}{\textquoteright} by
                  correlating wavefields during imaging or whether one
                  inverts wavefields as part of a focal transform
                  interferrometric deconvolution or as part of
                  computing the {\textquoteright}data
                  verse{\textquoteright}. The success of these
                  wavefield inversions depends on the stability of the
                  inverse with respect to data imperfections such as
                  finite aperture, bandwidth limitation, and missing
                  data. In this paper, we show how curvelet domain
                  sparsity promotion can be used as a suitable prior
                  to invert seismic wavefields. Examples include,
                  seismic data regularization with the focused
                  curvelet-based recovery by sparsity-promoting
                  inversion (fCRSI), which involves the inversion of
                  the primary-wavefield operator, the prediction of
                  multiples by inverting the adjoint of the primary
                  operator, and finally the inversion of the data
                  itself {\textendash}- the so-called
                  {\textquoteright}data inverse{\textquoteright}. In
                  all cases, curvelet-domain sparsity leads to a
                  stable inversion.},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.3063862},
  month = {11},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/herrmann08SEGswi/herrmann08SEGswi_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/herrmann08SEGswi/herrmann08SEGswi.pdf }
}


@CONFERENCE{herrmann2007SEGsdp,
  author = {Felix J. Herrmann and Deli Wang and Gilles Hennenfent and Peyman P. Moghaddam},
  title = {Seismic data processing with curvelets: a multiscale and nonlinear approach},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  pages = {2220-2224},
  organization = {SEG},
  abstract = {In this abstract, we present a nonlinear curvelet-based
                  sparsity-promoting formulation of a seismic
                  processing flow, consisting of the following steps:
                  seismic data regularization and the restoration of
                  migration amplitudes. We show that the
                  curvelet{\textquoteright}s wavefront detection
                  capability and invariance under the
                  migration-demigration operator lead to a formulation
                  that is stable under noise and missing
                  data. {\copyright}2007 Society of Exploration
                  Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.2792927},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/herrmann07SEGsdp/herrmann07SEGsdp_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/herrmann07SEGsdp/herrmann07SEGsdp.pdf }
}


@CONFERENCE{herrmann2012EAGEcsm,
  author = {Felix J. Herrmann and Haneet Wason},
  title = {Compressive sensing in marine acquisition and beyond},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2012},
  month = {06},
  abstract = {Simultaneous-source marine acquisition is an example of
                  compressive sensing where acquisition with a single
                  vessel is replaced by simultaneous acquisition by
                  multiple vessels with sources that fire at randomly
                  dithered times. By identifying simultaneous
                  acquisition as compressive sensing, we are able to
                  design acquisitions that favour recovery by sparsity
                  promotion. Compared to conventional processing that
                  yields estimates for sequential data, sparse
                  recovery leads to significantly improved results for
                  simultaneous data volumes that are collected in
                  shorter times. These improvements are the result of
                  proper design of the acquisition, selection of the
                  appropriate transform domain, and solution of the
                  recovery problem by sparsity promotion. During this
                  talk, we will show how these design principles can
                  be applied to marine acquisition and to other
                  problems in exploration seismology that can benefit
                  from compressive sensing.},
  keywords = {EAGE, workshop, acquisition, marine},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/herrmann2012EAGEcsm/herrmann2012EAGEcsm_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/herrmann2012EAGEcsm/herrmann2012EAGEcsm.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=59854}
}


@CONFERENCE{herrmann2007SEGsnt,
  author = {Felix J. Herrmann and D. Wilkinson},
  title = {Seismic noise: the good, the bad and the ugly},
  booktitle = {SEG Summer Research Workshop: Seismic Noise: Origins Preventions, Mitigation, Utilization},
  year = {2007},
  note = {Presented at SEG Summer Research Workshop: Seismic Noise: Origins,
	Prevention, Mitigation, Utilization},
  abstract = {In this paper, we present a nonlinear curvelet-based
                  sparsity-promoting formulation for three problems
                  related to seismic noise, namely the
                  {\textquoteright}good{\textquoteright},
                  corresponding to noise generated by random sampling;
                  the {\textquoteright}bad{\textquoteright},
                  corresponding to coherent noise for which
                  (inaccurate) predictions exist and the
                  {\textquoteright}ugly{\textquoteright} for which no
                  predictions exist. We will show that the compressive
                  capabilities of curvelets on seismic data and images
                  can be used to tackle these three categories of
                  noise-related problems.},
  keywords = {SLIM, SEG},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/herrmann07SEGsnt/herrmann07SEGsnt.pdf }
}


@CONFERENCE{johnson2008SINBADsdi,
  author = {James Johnson and Gilles Hennenfent},
  title = {Seismic data interpolation with symmetry},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {Due to the physics of reciprocity seismic data sets are
                  symmetric in the source and receiver
                  coordinates. Often seismic data sets are incomplete
                  and the missing data must be
                  interpolated. Typically, missing traces do not occur
                  symmetrically. The purpose of this project is to
                  extend the current formulation for solving the
                  seismic interpolation problems in such a way that
                  they enforce reciprocity. The method decomposes the
                  seismic data volume into symmetric and antisymmetric
                  parts. This decomposition leads to an augmented
                  system of equations for the L1-solver that promotes
                  sparsity in the curvelet domain. Interpolation is
                  carried out on the entire system during which the
                  asymmetric component of the volume is forced to
                  zero, while the symmetric part of the data volume is
                  matched to the measured data.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/johnson2008SINBADsdi/johnson2008SINBADsdi.pdf}
}


@CONFERENCE{johnson2010EAGEeop,
  author = {James Johnson and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Estimation of primaries via sparse inversion with reciprocity},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2010},
  abstract = {Accurate removal of surface related multiples is a key
                  step in seismic data processing. The industry
                  standard for removing multiples is SRME, which
                  involves convolving the data with itself to predict
                  the multiples, followed by an adaptive subtraction
                  procedure to recover the primaries (Verschuur and
                  Berkhout, 1997). Other methods involve
                  multidimensional division of the up-going and
                  down-going wavefields (Amundsen, 2001). However,
                  this approach may suffer from stability
                  problems. With the introduction of the
                  {\textquoteleft}{\textquoteleft}estimation of
                  primaries by sparse
                  inversion{\textquoteright}{\textquoteright}(EPSI),
                  van Groenestijn and Verschuur (2009) recentely
                  reformulated SRME to jointly estimate the
                  surface-free impulse response and the source
                  signature directly from the data. The advantage of
                  EPSI is that it recovers the primary response
                  directly, and does not require a second processing
                  step for the subtraction of estimated multiples from
                  the original data. However, because it estimates
                  both the primary impulse response and source
                  signature from the data EPSI must be regularized.
                  Motivated by recent successful application of the
                  curvelet transform in seismic data processing
                  (Herrmann et al., 2007), we formulate EPSI as a
                  bi-convex optimization problem that seeks sparsity
                  on the surface-free Green{\textquoteright}s function
                  and Fourier-domain smoothness on the source
                  wavelet. Our main contribution compared to previous
                  work (Lin and Herrmann, 2009), and the contribution
                  of that author to the proceedings of this
                  meeting(Lin and Herrmann, 2010), is that we employ
                  the physical principle of as source-receiver
                  reciprocity to improve the inversion.},
  keywords = {EAGE},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2010/johnson10EAGEeop/johnson10EAGEeop.pdf}
}


@CONFERENCE{jumah2011SEGdrepsi,
  author = {Bander Jumah and Felix J. Herrmann},
  title = {Dimensionality-reduced estimation of primaries by sparse inversion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2011},
  month = {09},
  volume = {30},
  pages = {3520-3525},
  organization = {SEG},
  abstract = {Data-driven methods---such as the estimation of
                  primaries by sparse inversion---suffer from the
                  "curse of dimensionality", which leads to
                  disproportional growth in computational and storage
                  demands when moving to realistic 3-D field data. To
                  remove this fundamental impediment, we propose a
                  dimensionality reduction technique where the "data
                  matrix" is approximated adaptively by a randomized
                  low-rank approximation. Compared to conventional
                  methods, our approach has the advantage that the
                  cost of the low-rank approximation is reduced
                  significantly, which may lead to considerable
                  reductions in storage and computational costs of the
                  sparse inversion. Application of the proposed
                  formalism to synthetic data shows that significant
                  improvements are achievable at low computational
                  overhead required to compute the low-rank
                  approximations.},
  keywords = {Presentation, SEG, processing},
  doi = {10.1190/1.3627931},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2011/Jumah11SEGdrepsi/Jumah11SEGdrepsi_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2011/Jumah11SEGdrepsi/Jumah11SEGdrepsi.pdf}
}


@CONFERENCE{kumar2008SINBADcd,
  author = {Vishal Kumar},
  title = {Curvelet denoising},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {The separation of signal and noise is an important issue
                  in seismic data processing. By noise we refer to the
                  incoherent noise which is present in the data. In
                  our case, we showed curvelets concentrate seismic
                  signal energy in few significant coefficients unlike
                  noise energy that is spread all over the
                  coefficients. The sparsity of seismic data in the
                  curvelet domain makes curvelets an ideal choice for
                  separating the noise from the seismic data. In our
                  approach the denoising problem is framed as
                  curvelet-regularized inversion problem. After
                  initial processing, we applied the algorithm to the
                  poststack data and compared our results with
                  conventional wavelet denoising.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/kumar2008SINBADcd/kumar2008SINBADcd.pdf}
}


@CONFERENCE{kumar2008SINBADcrd,
  author = {Vishal Kumar},
  title = {Curvelet-regularized deconvolution},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {The removal of source signature from seismic data is an
                  important step in seismic data processing. The
                  Curvelet transform provides sparse representations
                  for images that comprise smooth objects separated by
                  piece-wise smooth discontinuities (e.g. seismic
                  reflectivity). In this approach the sparseness of
                  reflectivity in Curvelet domain is used as a prior
                  to stabilize the inversion process. Our
                  Curvelet-regularized deconvolution algorithm uses
                  recently developed SPGL1 solver which does adaptive
                  sampling of the trade-off curve. We applied the
                  algorithm on a synthetic example and compared our
                  results with that of Spiky deconvolution approach.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/kumar08SINBADcrd/kumar08SINBADcrd_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/kumar2008SINBADcrd/kumar2008SINBADcrd.pdf}
}


@CONFERENCE{kumar2009SEGins,
  author = {Vishal Kumar and Felix J. Herrmann},
  title = {Incoherent noise suppression with curvelet-domain sparsity},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  month = {10},
  volume = {28},
  pages = {3356-3360},
  organization = {SEG},
  abstract = {The separation of signal and noise is a key issue in
                  seismic data processing. By noise we refer to the
                  incoherent noise that is present in the data. We use
                  the recently introduced multiscale and
                  multidirectional curvelet transform for suppression
                  of random noise. The curvelet transform decomposes
                  data into directional plane waves that are local in
                  nature. The coherent features of the data occupy the
                  large coefficients in the curvelet domain, whereas
                  the incoherent noise lives in the small
                  coefficients. In other words, signal and noise have
                  minimal overlap in the curvelet domain. This gives
                  us a chance to use curvelets to suppress noise
                  present in data.},
  keywords = {Presentation, SEG},
  doi = {10.1190/1.3255557},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/kumar09SEGins/kumar09SEGins_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/kumar09SEGins/kumar09SEGins.pdf}
}


@CONFERENCE{kumar2008CSEGcrs,
  author = {Vishal Kumar and Felix J. Herrmann},
  title = {Curvelet-regularized seismic deconvolution},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2008},
  organization = {CSEG},
  abstract = {There is an inherent continuity along reflectors of a
                  seismic image. We use the recently introduced
                  multiscale and multidirectional curvelet transform
                  to exploit this continuity along reflectors for
                  cases in which the assumption of spiky reflectivity
                  may not hold. We show that such type of seismic
                  reflectivity can be represented in the
                  curvelet-domain by a vector whose entries decay
                  rapidly. This curvelet-domain compression of
                  reflectivity opens new perspectives towards solving
                  classical problems in seismic processing including
                  the deconvolution problem. In this paper, we present
                  a formulation that seeks curvelet-domain sparsity
                  for non-spiky reflectivity and we compare our
                  results with those of spiky deconvolution.},
  keywords = {Presentation, SLIM},
  month = {05},
  presentation = {http://slim.gatech.edu/Publications/Public/Conferences/CSEG/2008/kumar2008CSEGcrs/kumar2008CSEGcrs_pres.pdf},
  url = {http://slim.gatech.edu/Publications/Public/Conferences/CSEG/2008/kumar2008CSEGcrs/kumar2008CSEGcrs.pdf}
}


@CONFERENCE{kumar2008SEGdwc,
  author = {Vishal Kumar and Felix J. Herrmann},
  title = {Deconvolution with curvelet-domain sparsity},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  pages = {1996-2000},
  organization = {SEG},
  abstract = {There is an inherent continuity along reflectors of a
                  seismic image. We use the recently introduced
                  multiscale and multidirectional curvelet transform
                  to exploit this continuity along reflectors for
                  cases in which the assumption of spiky reflectivity
                  may not hold. We show that such type of seismic
                  reflectivity can be represented in the
                  curvelet-domain by a vector whose entries decay
                  rapidly. This curvelet-domain compression of
                  reflectivity opens new perspectives towards solving
                  classical problems in seismic processing including
                  the deconvolution problem. In this paper, we present
                  a formulation that seeks curvelet-domain sparsity
                  for non-spiky reflectivity and we compare our
                  results with those of spiky deconvolution.},
  keywords = {SLIM, Presentation, SEG},
  doi = {10.1190/1.3059287},
  month = {11},
  presentation = { https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/kumar08SEGdwc/kumar08SEGdwc_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/kumar08SEGdwc/kumar08SEGdwc.pdf }
}


@CONFERENCE{lebed2008SINBADaoc,
  author = {Evgeniy Lebed},
  title = {Curvelet / {Surfacelet} comparison},
  booktitle = {SINBAD},
  year = {2008},
  abstract = {Curvelets and Surfacelets are two transforms that aim to
                  achieve a multiscale and a multidirectional
                  decomposition of arbitrary N-dimensional ($N>=2$)
                  signals. While both transforms are Fourier-based,
                  their construction is intrinsically different. In
                  this talk we will give and overview of the
                  construction of the two transforms, and explore
                  their properties such as frequency domain / spatial
                  domain coherence, sparsity, redundancy and
                  computational complexity.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/lebed2008SINBADaoc/lebed2008SINBADaoc.pdf}
}


@CONFERENCE{lebed2008SINBADaoc1,
  author = {Evgeniy Lebed},
  title = {Applications of curvelets/surfacelets to seismic data processing},
  booktitle = {SINBAD},
  year = {2008},
  abstract = {In this talk we explore several applications of the
                  curvelet and surfacelet transforms to seismic data
                  processing. The first application is stable signal
                  recovery in the physical domain - seismic data
                  acquisition is often limited by physical and
                  economic constraints, and the goal is to interpolate
                  the data from a given subset of seismic traces. The
                  second application is signal recovery in a transform
                  domain - we assume that our data comes in a form of
                  a random subset of temporal frequencies and the goal
                  is to recover the missing frequencies from this
                  data. Since seismic signals are generally not
                  bandwidth limited, this in fact becomes an
                  anti-aliasing problem. In both these problems the
                  recovery is resolved via a robust $\ell$_1 solver
                  that exploits the sparsity of the signals in
                  curvelet/surfacelet domains. In the last application
                  we explore the problem of primary-multiple
                  separation by simple thresholding.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/lebed2008SINBADaoc1/lebed2008SINBADaoc1.pdf}
}


@CONFERENCE{lebed2008SEGhggt,
  author = {Evgeniy Lebed and Felix J. Herrmann},
  title = {A hitchhiker{\textquoteright}s guide to the galaxy of transform-domain sparsification},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  organization = {SEG},
  abstract = {The ability to efficiently and sparsely represent
                  seismic data is becoming an increasingly important
                  problem in geophysics. Over the last decade many
                  transforms such as wavelets, curvelets, contourlets,
                  surfacelets, shearlets, and many other types of
                  {\textquoteright}x-lets{\textquoteright} have been
                  developed to try to resolve this issue. In this
                  abstract we compare the properties of four of these
                  commonly used transforms, namely the shift-invariant
                  wavelets, complex wavelets, curvelets and
                  surfacelets. We also briefly explore the performance
                  of these transforms for the problem of recovering
                  seismic wavefields from incomplete measurements.},
  keywords = {SEG},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/lebed08SEGhggt/lebed08SEGhggt.pdf}
}


@CONFERENCE{vanleeuwen2011ICIAMcbmcwe,
  author = {Tristan van Leeuwen},
  title = {A correlation-based misfit criterion for wave-equation traveltime tomography},
  booktitle = {ICIAM},
  year = {2011},
  organization = {ICIAM 2011},
  abstract = {The inference of subsurface medium parameters from
                  seismic data can be posed as a PDE-constrained
                  data-fitting procedure. This approach is successful
                  in reconstructing medium perturbations that are in
                  the order of the wavelength. In practice, the data
                  lack low frequency content and this means that one
                  needs a good initial guess of the slowly varying
                  component of the medium. For a wrong starting model
                  an iterative reconstruction procedure is likely to
                  end up in a local minimum. We propose to use a
                  different measure of the misfit that makes the
                  optimization problem well-posed in terms of the
                  slowly varying velocity structures. This procedure
                  can be seen as a generalization of ray-based
                  traveltime tomography. We discuss the theoretical
                  underpinnings of the method and give some numerical
                  examples.},
  date-added = {2011-07-19},
  keywords = {Presentation,ICIAM,Imaging},
  month = {07},
  note = {Presented at ICIAM 2011, Vancouver BC},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ICIAM/2011/vanleeuwen2011ICIAMcbmcwe/vanleeuwen2011ICIAMcbmcwe.pdf}
}


@CONFERENCE{vanleeuwen2011SEGext,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {Probing the extended image volume},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2011},
  month = {09},
  volume = {30},
  pages = {4045-4050},
  organization = {SEG},
  abstract = {The prestack image volume can be defined as a cross-
                  correlation of the source and receivers wavefields
                  for non-zero space and time lags. If the background
                  velocity is kinemati- cally acceptable, this image
                  volume will have its main contributions at zero lag,
                  even for complex models. Thus, it is an ideal tool
                  for wave-equation migration velocity analysis in the
                  presence of strong lateral heterogeneity. In
                  particular, it allows us to pose migration velocity
                  analysis as a PDE-constrained optimization problem,
                  where the goal is to minimize the energy in the
                  image volume at non-zero lag subject to fitting the
                  data approximately. However, it is computationally
                  infeasi- ble to explicitly form the whole image
                  volume. In this paper, we discuss several ways to
                  reduce the computational costs involved in computing
                  the image volume and evaluating the focusing
                  criterion. We reduce the costs for calculating the
                  data by randomized source synthesis. We also present
                  an efficient way to subsample the image
                  volume. Finally, we propose an alternative
                  optimization criterion and suggest a multiscale
                  inversion strategy for wave-equation MVA.},
  keywords = {SEG, imaging},
  doi = {10.1190/1.3628051},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2011/vanleeuwen11SEGext/vanleeuwen11SEGext.pdf}
}


@CONFERENCE{vanleeuwen2011WAVESpeiv,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {Probing the extended image volume for seismic velocity inversion},
  booktitle = {WAVES},
  year = {2011},
  organization = {Waves 2011},
  abstract = {In seismic velocity inversion one aims to reconstruct a
                  kinematically correct subsurface velocity model that
                  can be used as input for further processing and
                  inversion of the data. An important tool in velocity
                  inversion is the prestack image volume. This image
                  volume can be defined as a cross-correlation of the
                  source and receivers wavefields for non-zero space
                  and time lags. If the background velocity is
                  kinematically acceptable, this image volume will
                  have its main contributions at zero lag, even for
                  complex models. Thus, it is an ideal tool for
                  wave-equation migration velocity analysis in the
                  presence of strong lateral heterogeneity. In
                  particular, it allows us to pose migration velocity
                  analysis as a PDE-constrained optimization problem,
                  where the goal is to minimize the energy in the
                  image volume at non-zero lag subject to fitting the
                  data approximately. However, it is computationally
                  infeasible to explicitly form the whole image
                  volume. In this paper, we discuss several ways to
                  reduce the computational costs involved in computing
                  the image volume and evaluating the focusing
                  criterion. We reduce the costs for calculating the
                  data by randomized source synthesis. We also present
                  an efficient way to subsample the image
                  volume. Finally, we propose an alternative
                  optimization criterion and suggest a multiscale
                  inversion strategy for wave-equation MVA.},
  date-added = {2011-07-29},
  keywords = {Presentation},
  month = {07},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ICIAM/2011/vanleeuwen2011WAVESpeiv/vanleeuwen2011WAVESpeiv.pdf}
}


@CONFERENCE{vanleeuwen2011EAGEhsdomwi,
  author = {Tristan van Leeuwen and Felix J. Herrmann and Mark Schmidt and Michael P. Friedlander},
  title = {A hybrid stochastic-deterministic optimization method for waveform inversion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2011},
  month = {05},
  abstract = {Present-day high quality 3D acquisition can give us
                  lower frequencies and longer offsets with which to
                  invert. However, the computational costs involved in
                  handling this data explosion are
                  tremendous. Therefore, recent developments in
                  full-waveform inversion have been geared towards
                  reducing the computational costs involved. A key
                  aspect of several approaches that have been proposed
                  is a dramatic reduction in the number of sources
                  used in each iteration. A reduction in the number of
                  sources directly translates to less PDE-solves and
                  hence a lower computational cost. Recent attention
                  has been drawn towards reducing the sources by
                  randomly combining the sources in to a few
                  supershots, but other strategies are also
                  possible. In all cases, the full data misfit, which
                  involves all the sequential sources, is replaced by
                  a reduced misfit that is much cheaper to evaluate
                  because it involves only a small number of sources
                  (batchsize). The batchsize controls the accuracy
                  with which the reduced misfit approximates the full
                  misfit. The optimization of such an inaccurate, or
                  noisy, misfit is the topic of stochastic
                  optimization. In this paper, we propose an
                  optimization strategy that borrows ideas from the
                  field of stochastic optimization. The main idea is
                  that in the early stage of the optimization, far
                  from the true model, we do not need a very accurate
                  misfit. The strategy consists of gradually
                  increasing the batchsize as the iterations
                  proceed. We test the proposed strategy on a
                  synthetic dataset. We achieve a very reasonable
                  inversion result at the cost of roughly 13
                  evaluations of the full misfit. We observe a
                  speed-up of roughly a factor 20.},
  keywords = {Presentation, EAGE, full-waveform inversion, optimization},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2011/vanleeuwen11EAGEhsdomwi/vanleeuwen11EAGEhsdomwi_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2011/vanleeuwen11EAGEhsdomwi/vanleeuwen11EAGEhsdomwi.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=50341}
}


@CONFERENCE{vanleeuwen2011SIAMGEOmawt,
  author = {Tristan van Leeuwen and Wim A. Mulder},
  title = {Multiscale aspects of waveform tomography},
  booktitle = {SIAMGEO},
  year = {2011},
  organization = {SIAM GeoSciences 2011},
  abstract = {We consider the inference of medium velocity from
                  transmitted acoustic waves. Typically, the
                  measurements are done in a narrow frequency band. As
                  a result the sensitivity of the data with respect to
                  velocity perturbations varies dramatically with the
                  scale of the perturbation.
                  {\textquoteleft}Smooth{\textquoteright}
                  perturbations will cause a phase shift, whereas
                  perturbations that vary on the wavelength-scale
                  cause amplitude variations. We investigate how to
                  incorporate this scale dependent behavior in the
                  formulation of the inverse problem.},
  keywords = {Presentation},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2011/vanleeuwen2011SIAMGEOmawt/vanleeuwen2011SIAMGEOmawt.pdf}
}


@CONFERENCE{vanleeuwen2011AMPhsdmwi,
  author = {Tristan van Leeuwen and Mark Schmidt and Michael P. Friedlander and Felix J. Herrmann},
  title = {A hybrid stocahstic-deterministic method for waveform inversion},
  booktitle = {AMP},
  year = {2011},
  organization = {WAVES 2011},
  abstract = {A lot of seismic and medical imaging problems can be
                  written as a least-squares data- fitting problem. In
                  particular, we consider the case of multi-experiment
                  data, where the data consists of a large number of
                  "independent" measurements. Solving the inverse
                  problem then involves repeatedly forward modeling
                  the data for each of these experiments. In case the
                  number of experiments is large and the modeling
                  kernel expensive to apply, such an approach may be
                  prohibitively expensive. We review techniques from
                  stochastic optimization which aim at dramatically
                  reducing the number of experiments that need to be
                  modeled at each iteration. This reduction is
                  typically achieved by randomly subsampling the
                  data. Special care needs to be taken in the
                  optimization to deal with the stochasticity that is
                  introduced in this way.},
  date-added = {2011-07-15},
  keywords = {Presentation},
  month = {07},
  note = {Presented at AMP Medical and Seismic Imaging, 2011, Vancouver BC},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ICIAM/2011/vanleeuwen2011AMPhsdmwi/vanleeuwen2011AMPhsdmwi.pdf}
}


@CONFERENCE{li2011EAGEfwirr,
  author = {Xiang Li and Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Full-waveform inversion with randomized {L1} recovery for the model updates},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2011},
  month = {05},
  abstract = {Full-waveform inversion (FWI) is a data fitting
                  procedure that relies on the collection of seismic
                  data volumes and sophisticated computing to create
                  high-resolution results. With the advent of FWI, the
                  improvements in acquisition and inversion have been
                  substantial, but these improvements come at a high
                  cost because FWI involves extremely large
                  multi-experiment data volumes. The main obstacle is
                  the "curse of dimensionality" exemplified by
                  NyquistÕs sampling criterion, which puts a
                  disproportionate strain on current acquisition and
                  processing systems as the size and desired
                  resolution increases. In this paper, we address the
                  "curse of dimensionality" by randomized
                  dimensionality reduction of the FWI problem adapted
                  from the field of CS. We invert for model updates by
                  replacing the Gauss-Newton linearized subproblem for
                  subsampled FWI with a sparsity promoting
                  formulation, and solve this formulation using the
                  SPGl1 algorithm. We speed up the algorithm and avoid
                  overfitting the data by solving for the linearized
                  updates only approximately. Our approach is
                  successful because it reduces the size of seismic
                  data volumes without loss of information. With this
                  reduction, we can compute a Newton-like update with
                  the reduced data volume at the cost of roughly one
                  gradient update for the fully sampled wavefield.},
  keywords = {Presentation, EAGE, full-waveform inversion},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2011/li11EAGEfwirr/li11EAGEfwirr_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2011/li11EAGEfwirr/li11EAGEfwirr.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=50345}
}


@CONFERENCE{li2011CSEGefimag,
  author = {Xiang Li and Felix J. Herrmann},
  title = {Efficient full-waveform inversion with marine acquisition geometry},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2012},
  abstract = {Full-waveform inversion (FWI) is a nonlinear data
                  fitting procedure based on seismic data to derive a
                  accurate velocity model. With the increasing demand
                  for high resolution images in complex geological
                  settings, the importance of improvements in
                  acquisition and inversion become more and more
                  critical. However, these improvements will be
                  obtained at high computational cost, as a typical
                  marine survey contains thousands of shot and
                  receiver positions, and FWI needs several passes
                  through massive seismic data. Computational cost of
                  FWI will grow exponentially as the size of seismic
                  data and desired resolution increase. In this paper
                  we present a modified Gauss-Newton (GN) method that
                  borrows ideas from compressive sensing, where we
                  compute the GN updates from a few randomly selected
                  sequential shots. Each subproblem is solved by using
                  a sparsity promoting algorithm. With this approach,
                  we dramatically reduce the size and hence the
                  computational costs of the problem, whilst we
                  control information loss by redrawing a different
                  set of sequential shots for each subproblem.},
  keywords = {CSEG},
  month = {02},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2012/li2011CSEGefimag/li2011CSEGefimag.pdf}
}


@CONFERENCE{li2012SEGspmamp,
  author = {Xiang Li and Felix J. Herrmann},
  title = {Sparsity-promoting migration accelerated by message passing},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2012},
  month = {11},
  volume = {31},
  pages = {1-6},
  organization = {SEG},
  abstract = {Seismic imaging via linearized inversion requires
                  multiple iterations to minimize the least-squares
                  misfit as a function of the medium
                  perturbation. Unfortunately, the cost for these
                  iterations are prohibitive because each iteration
                  requires many wave-equation simulations, which
                  without direct solvers require an expensive separate
                  solve for each source. To overcome this problem, we
                  use dimensionality-reduction to decrease the size of
                  seismic imaging problem by turning the large number
                  of sequential shots into a much small number of
                  simultaneous shots. In our approach, we take
                  advantage of sparsifying transforms to remove source
                  crosstalk resulting from randomly weighting and
                  stacking sequential shots into a few super shots. We
                  also take advantage of the fact that the convergence
                  of large-scale sparsity-promoting solvers can be
                  improved significantly by borrowing ideas from
                  message passing, which are designed to break
                  correlation built up between the linear system and
                  the model iterate. In this way, we arrive at a
                  formulation where we run the sparsity-promoting
                  solver for a relatively large number of very
                  iterations. Aside from leading to a significant
                  speed up, our approach had the advantage of greatly
                  reducing the memory imprint and IO requirements. We
                  demonstrate this feature by solving a
                  sparsity-promoting imaging problem with operators of
                  reverse-time migration, which is computationally
                  infeasible without the dimensionality reduction.},
  keywords = {SEG, imaging, inversion},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2012/li2012SEGspmamp/li2012SEGspmamp_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2012/li2012SEGspmamp/li2012SEGspmamp.pdf},
  doi = {10.1190/segam2012-1500.1}
}


@CONFERENCE{li2010SEGfwi,
  author = {Xiang Li and Felix J. Herrmann},
  title = {Full-waveform inversion from compressively recovered model updates},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2010},
  month = {10},
  volume = {29},
  pages = {1029-1033},
  organization = {SEG},
  abstract = {Full-waveform inversion relies on the collection of
                  large multi-experiment data volumes in combination
                  with a sophisticated back-end to create
                  high-fidelity inversion results. While improvements
                  in acquisition and inversion have been extremely
                  successful, the current trend of incessantly pushing
                  for higher quality models in increasingly
                  complicated regions of the Earth reveals fundamental
                  shortcomings in our ability to handle increasing
                  problem size numerically. Two main culprits can be
                  identified. First, there is the so-called
                  {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which puts disproportionate strain on
                  current acquisition and processing systems as the
                  size and desired resolution increases. Secondly,
                  there is the recent
                  {\textquoteleft}{\textquoteleft}departure from
                  Moore{\textquoteright}s
                  law{\textquoteright}{\textquoteright} that forces us
                  to lower our expectations to compute ourselves out
                  of this. In this paper, we address this situation by
                  randomized dimensionality reduction, which we adapt
                  from the field of compressive sensing. In this
                  approach, we combine deliberate randomized
                  subsampling with structure-exploiting
                  transform-domain sparsity promotion. Our approach is
                  successful because it reduces the size of seismic
                  data volumes without loss of information. With this
                  reduction, we compute Newton-like updates at the
                  cost of roughly one gradient update for the
                  fully-sampled wavefield.},
  keywords = {Presentation, SEG, full-waveform inversion},
  doi = {10.1190/1.3513022},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2010/li10SEGfwi/li10SEGfwi_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2010/li10SEGfwi/li10SEGfwi.pdf}
}


@CONFERENCE{li2011SBGFmgnsu,
  author = {Xiang Li and Felix J. Herrmann and Tristan van Leeuwen and Aleksandr Y. Aravkin},
  title = {Modified {Gauss-Newton} with sparse updates},
  booktitle = {SBGF},
  year = {2011},
  organization = {SBGF},
  abstract = {Full-waveform inversion (FWI) is a data fitting
                  procedure that relies on the collection of seismic
                  data volumes and sophisticated computing to create
                  high-resolution models.With the advent of FWI, the
                  improvements in acquisition and inversion have been
                  substantial, but these improvements come at a high
                  cost because FWI involves extremely large
                  multi-experiment data volumes. The main obstacle is
                  the {\textquoteleft}curse of
                  dimensionality{\textquoteright} exemplified by
                  Nyquist{\textquoteright}s sampling criterion, which
                  puts a disproportionate strain on current
                  acquisition and processing systems as the size and
                  desired resolution increases. In this paper, we
                  address the {\textquoteleft}curse of
                  dimensionality{\textquoteright} by using randomized
                  dimensionality reduction of the FWI problem, coupled
                  with a modified Gauss-Newton (GN) method designed to
                  promote curvelet-domain sparsity of model
                  updates. We solve for these updates using the
                  spectral projected gradient method, implemented in
                  the SPG￿1 software package. Our approach is
                  successful because it reduces the size of seismic
                  data volumes without loss of information. With this
                  reduction, we can compute Gauss-Newton updates with
                  the reduced data volume at the cost of roughly one
                  gradient update for the fully sampled wavefield},
  keywords = {SBGF, full-waveform inversion},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SBGF/2011/li11SBGFmgnsu/li11SBGFmgnsu.pdf}
}


@CONFERENCE{lin2006SINBADci,
  author = {Tim T.Y. Lin},
  title = {Compressed imaging},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {In 1998 Grimbergen et. al. introduced a new method for
                  computing wavefield propagation which improved on
                  the previously employed local explicit operator
                  method in that it exhibited no dip limitation,
                  accurately handled laterally varying background
                  ground velocity models, and is unconditionally
                  stable. These desirable properties are mainly
                  attributed to bringing the propagation problem into
                  an eigenvector basis that diagonalizes the
                  propagation operators. This modal-transform method,
                  however, requires at each depth-level the solution
                  of a large-scale sparse eigenvalue problem to
                  compute the square-root of the Helmholtz
                  operator. By using recent results from compressed
                  sensing, we hope to reduce these computational costs
                  that typically involve the synthesizes of the
                  imaging operators and the cost of matrix-vector
                  products.  To reduce these costs, we compress the
                  extrapolation operators by using only a fraction of
                  the positive eigenvalues and temporal frequencies.
                  This reduction not only leads to smaller matrices
                  but also to reduced synthesis costs. These
                  reductions go at the expense of solving a recovery
                  problem from incomplete data. During the
                  presentation, we show that wavefields can accurately
                  be extrapolated with a compressed operators and
                  competitive costs.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/lin2006SINBADci/lin2006SINBADci.pdf}
}


@CONFERENCE{lin2009SEGcsf,
  author = {Tim T.Y. Lin and Yogi A. Erlangga and Felix J. Herrmann},
  title = {Compressive simultaneous full-waveform simulation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  month = {10},
  volume = {28},
  pages = {2577-2581},
  organization = {SEG},
  abstract = {The fact that the computational complexity of wavefield
                  simulation is proportional to the size of the
                  discretized model and acquisition geometry, and not
                  to the complexity of the simulated wavefield, is a
                  major impediment within seismic imaging. By turning
                  simulation into a compressive sensing
                  problem{\textendash}where simulated data is
                  recovered from a relatively small number of
                  independent simultaneous sources{\textendash}we
                  remove this impediment by showing that compressively
                  sampling a simulation is equivalent to compressively
                  sampling the sources, followed by solving a reduced
                  system. As in compressive sensing, this allows for a
                  reduction in sampling rate and hence in simulation
                  costs. We demonstrate this principle for the
                  time-harmonic Helmholtz solver. The solution is
                  computed by inverting the reduced system, followed
                  by a recovery of the full wavefield with a sparsity
                  promoting program. Depending on the
                  wavefield{\textquoteright}s sparsity, this approach
                  can lead to a significant cost reduction, in
                  particular when combined with the implicit
                  preconditioned Helmholtz solver, which is known to
                  converge even for decreasing mesh sizes and
                  increasing angular frequencies. These properties
                  make our scheme a viable alternative to explicit
                  time-domain finite-difference.},
  keywords = {Presentation, SEG},
  doi = {10.1190/1.3255381},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/Lin09SEGcsf/Lin09SEGcsf_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/Lin09SEGcsf/Lin09SEGcsf.pdf}
}


@CONFERENCE{lin2011EAGEepsic,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Estimating primaries by sparse inversion in a curvelet-like representation domain},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2011},
  month = {05},
  abstract = {We present an uplift in the fidelity and wavefront
                  continuity of results obtained from the Estimation
                  of Primaries by Sparse Inversion (EPSI) program by
                  reconstructing the primary events in a hybrid
                  wavelet-curvelet representation domain. EPSI is a
                  multiple removal technique that belongs to the class
                  of wavefield inversion methods, as an alternative to
                  the traditional adaptive-subtraction process. The
                  main assumption is that the correct primary events
                  should be as sparsely-populated in time as
                  possible. A convex reformulation of the original
                  EPSI algorithm allows its convergence property to be
                  preserved even when the solution wavefield is not
                  formed in the physical domain. Since wavefronts and
                  edge-type singularities are sparsely represented in
                  the curvelet domain, sparse solutions formed in this
                  domain will exhibit vastly improved continuity when
                  compared to those formed in the physical domain,
                  especially for the low-energy events at later
                  arrival times. Further- more, a wavelet-type
                  representation domain will preserve sparsity in the
                  reflected events even if they originate from
                  non-zero-order discontinuities in the subsurface,
                  providing an additional level of robustness. This
                  method does not require any changes in the
                  underlying computational algorithm and does not
                  explicitly impose continuity constraints on each
                  update.},
  keywords = {Presentation, EAGE, processing},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2011/lin11EAGEepsic/lin11EAGEepsic_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2011/lin11EAGEepsic/lin11EAGEepsic.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=50427}
}


@CONFERENCE{lin2011SEGrssde,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Robust source signature deconvolution and the estimation of primaries by sparse inversion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2011},
  month = {09},
  volume = {30},
  pages = {4354-4359},
  organization = {Dept. of Earth and Ocean Sciences, University of British Columbia},
  abstract = {The past few years had seen some concentrated interest
                  on a particular wavefield-inversion approach to the
                  popular SRME multiple removal technique called
                  Estimation of Primaries by Sparse Inversion (EPSI).
                  EPSI promises greatly improved tolerance to noise,
                  missing data, edge effect, and other physi- cal
                  phenomenon generally not described by the SRME
                  relation (van Groenestijn and Verschuur,
                  2009a,b). It is based on the premise that it is
                  possible to stably invert for both the primary
                  impulse response and the source signature despite
                  beforehand having no (or very limited) explicit
                  knowledge of latter. The key to successful
                  applications of EPSI, as shown in very recent works
                  (Savels et al., 2010), is a robust way to
                  reconstruct very sparse primary impulse response
                  events as part of the inversion process. Based on
                  the various successful demonstrations in literature,
                  there is a very strong sense that EPSI will also
                  play an important role in future developments of
                  source sig- nature deconvolution and the general
                  recovering of wavefield spectrum.},
  keywords = {Presentation, deconvolution, SEG, sparse inversion, processing},
  doi = {10.1190/1.3628116},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2011/lin11SEGrssde/lin11SEGrssde_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2011/lin11SEGrssde/lin11SEGrssde.pdf}
}


@CONFERENCE{lin2010EAGEseo,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Stabilized estimation of primaries via sparse inversion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2010},
  month = {06},
  abstract = {Estimation of Primaries by Sparse Inversion (EPSI) is a
                  recent method for surface-related multiple removal
                  using a direct estimation method closely related to
                  Amundsen inversion, where under a sparsity
                  assumption the primary impulse response is
                  determined directly from a data-driven wavefield
                  inversion process. One of the major difficulties in
                  its practical adoption is that one must have precise
                  knowledge of a time-window that contains
                  multiple-free primaries during each
                  update. Moreover, due to the nuances involved in
                  regularizing the model impulse response in the
                  inverse problem, the EPSI approach has an additional
                  number of inversion parameters where it may be
                  difficult to choose a reasonable value. We show that
                  the specific sparsity constraint on the EPSI updates
                  lead to an inherently intractable problem, and that
                  the time-window and other inversion variables arise
                  in the context of additional regularizations that
                  attempts to drive towards a meaningful solution. We
                  furthermore suggest a way to remove almost all of
                  these parameters via convexification, which
                  stabilizes the inversion while preserving the
                  crucial sparsity assumption in the primary impulse
                  response model.},
  keywords = {Presentation, EAGE, processing},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2010/lin10EAGEseo/lin10EAGEseo_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2010/lin10EAGEseo/lin10EAGEseo.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=39122}
}


@CONFERENCE{lin2009EAGEdsa,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Designing simultaneous acquisitions with compressive sensing},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2009},
  month = {06},
  abstract = {The goal of this paper is in designing a functional
                  simultaneous acquisition scheme by applying the
                  principles of compressive sensing. By framing the
                  acquisition in a compressive sensing setting we
                  immediately gain insight into not only how to choose
                  the source signature and shot patterns, but also in
                  how well we can hope to demultiplex the data when
                  given a set amount of reduction in the number of
                  sweeps. The principles of compressive sensing
                  dictates that the quality of the demultiplexed data
                  is closely related to the transform-domain sparsity
                  of the solution. This means that, given an estimate
                  in the complexity of the expectant data wavefield,
                  it is possible to controllably reduce the number of
                  shots that needs to be recorded in the field. We
                  show a proof of concept by introducing an
                  acquisition compatible with compressive sensing
                  based on randomly phase-encoded vibroseis sweeps.},
  keywords = {Presentation, EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2009/lin09EAGEdsa/lin2009EAGEdsa_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2009/lin09EAGEdsa/lin09EAGEdsa.pdf},
  yrl2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=23951}
}


@CONFERENCE{lin2009SEGucs,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Unified compressive sensing framework for simultaneous acquisition with primary estimation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  month = {10},
  volume = {28},
  pages = {3113-3117},
  abstract = {The central promise of simultaneous acquisition is a
                  vastly improved crew efficiency during acquisition
                  at the cost of additional post-processing to obtain
                  conventional source-separated data volumes. Using
                  recent theories from the field of compressive
                  sensing, we present a way to systematically model
                  the effects of simultaneous acquisition. Our
                  formulation form a new framework in the study of
                  acquisition design and naturally leads to an
                  inversion-based approach for the separation of shot
                  records. Furthermore, we show how other
                  inversion-based methods, such as a recently proposed
                  method from van Groenestijn and Verschuur (2009) for
                  primary estimation, can be processed together with
                  the demultiplexing problem to achieve a better
                  result compared to a separate treatment of these
                  problems.},
  keywords = {Presentation, SEG},
  doi = {10.1190/1.3255502},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/lin09SEGucs/lin09SEGucs_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/lin09SEGucs/lin09SEGucs.pdf}
}


@CONFERENCE{lin2008SINBADcwe,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Compressed wavefield extrapolation},
  booktitle = {SINBAD},
  year = {2008},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/sites/data/Papers/lin08cwe.pdf}
}


@CONFERENCE{lin2007SEGcwe1,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Compressed wavefield extrapolation with curvelets},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  pages = {1997-2001},
  abstract = {An explicit algorithm for the extrapolation of one-way
                  wavefields is proposed which combines recent
                  developments in information theory and theoretical
                  signal processing with the physics of wave
                  propagation. Because of excessive memory
                  requirements, explicit formulations for wave
                  propagation have proven to be a challenge in 3-D. By
                  using ideas from
                  {\textquoteleft}{\textquoteleft}compressed
                  sensing{\textquoteright}{\textquoteright}, we are
                  able to formulate the (inverse) wavefield
                  extrapolation problem on small subsets of the data
                  volume, thereby reducing the size of the
                  operators. According to compressed sensing theory,
                  signals can successfully be recovered from an
                  imcomplete set of measurements when the measurement
                  basis is incoherent with the representation in which
                  the wavefield is sparse. In this new approach, the
                  eigenfunctions of the Helmholtz operator are
                  recognized as a basis that is incoherent with
                  curvelets that are known to compress seismic
                  wavefields. By casting the wavefield extrapolation
                  problem in this framework, wavefields can
                  successfully be extrapolated in the modal domain via
                  a computationally cheaper operation. A proof of
                  principle for the
                  {\textquoteleft}{\textquoteleft}compressed
                  sensing{\textquoteright}{\textquoteright} method is
                  given for wavefield extrapolation in 2-D. The
                  results show that our method is stable and produces
                  identical results compared to the direct application
                  of the full extrapolation operator. {\copyright}2007
                  Society of Exploration Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.2792882},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/lin07SEGcwe1/lin07SEGcwe1_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/lin07SEGcwe1/lin07SEGcwe1.pdf }
}


@CONFERENCE{lin2009DELPHIrwi,
  author = {Tim T.Y. Lin and Felix J. Herrmann and Yogi A. Erlangga},
  title = {Randomized wavefield inversion},
  booktitle = {DELPHI},
  year = {2009},
  keywords = {Presentation},
  note = {Presented at the DELPHI meeting. The Hague},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/Delphi/2009/lin2009DELPHIrwi/lin2009DELPHIrwi.pdf}
}


@CONFERENCE{lin2008SEGiso,
  author = {Tim T.Y. Lin and Evgeniy Lebed and Yogi A. Erlangga and Felix J. Herrmann},
  title = {Interpolating solutions of the {Helmholtz} equation with compressed sensing},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  pages = {2122-2126},
  organization = {SEG},
  abstract = {We present an algorithm which allows us to model
                  wavefields with frequency-domain methods using a
                  much smaller number of frequencies than that
                  typically required by the classical sampling theory
                  in order to obtain an alias-free result. The
                  foundation of the algorithm is the recent results on
                  the compressed sensing, which state that data can be
                  successfully recovered from an incomplete
                  measurement if the data is sufficiently
                  sparse. Results from numerical experiment show that
                  only 30\% of the total frequency spectrum is need to
                  capture the full wavefield information when working
                  in the hard 2D synthetic Marmousi model.},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.3059307},
  month = {01},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/lin08SEGiso/lin08SEGiso.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/lin08SEGiso/lin08SEGiso.pdf }
}


@CONFERENCE{lin2010SEGspm,
  author = {Tim T.Y. Lin and Ning Tu and Felix J. Herrmann},
  title = {Sparsity-promoting migration from surface-related multiples},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2010},
  month = {10},
  volume = {29},
  pages = {3333-3337},
  organization = {SEG},
  abstract = {Seismic imaging typically begins with the removal of
                  multiple energy in the data, out of fear that it may
                  introduce erroneous structure. However, seismic
                  multiples have effectively seen more of the
                  earth{\textquoteright}s structure, and if treated
                  correctly can potential supply more information to a
                  seismic image compared to primaries. Past approaches
                  to accomplish this leave ample room for improvement;
                  they either require extensive modification to
                  standard migration techniques, rely too much on
                  prior information, require extensive pre-processing,
                  or resort to full-waveform inversion. We take some
                  valuable lessons from these efforts and present a
                  new approach balanced in terms of ease of
                  implementation, robustness, efficiency and
                  well-posedness, involving a sparsity-promoting
                  inversion procedure using standard Born migration
                  and a data-driven multiple modeling approach based
                  on the focal transform.},
  keywords = {Presentation, SEG, processing},
  doi = {10.1190/1.3513540},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2010/lin10SEGspm/lin10SEGspm_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2010/lin10SEGspm/lin10SEGspm.pdf}
}


@CONFERENCE{mansour2012SSPwspgl1,
  author = {Hassan Mansour},
  title = {Beyond $\ell_1$ norm minimization for sparse signal recovery},
  booktitle = {2012 IEEE Statistical Signal Processing Workshop (SSP) (SSP'12)},
  year = {2012},
  address = {Ann Arbor, Michigan, USA},
  organization = {IEEE},
  abstract = {Sparse signal recovery has been dominated by the basis
                  pursuit denoise (BPDN) problem formulation for over
                  a decade. In this paper, we propose an algorithm
                  that outperforms BPDN in finding sparse solutions to
                  underdetermined linear systems of equations at no
                  additional computational cost. Our algorithm, called
                  WSPGL1, is a modification of the spectral projected
                  gradient for $\ell_1$ minimization (SPGL1) algorithm
                  in which the sequence of LASSO subproblems are
                  replaced by a sequence of weighted LASSO subproblems
                  with constant weights applied to a support
                  estimate. The support estimate is derived from the
                  data and is updated at every iteration. The
                  algorithm also modifies the Pareto curve at every
                  iteration to reflect the new weighted $\ell_1$
                  minimization problem that is being solved. We
                  demonstrate through extensive simulations that the
                  sparse recovery performance of our algorithm is
                  superior to that of $\ell_1$ minimization and
                  approaches the recovery performance of iterative
                  re-weighted $\ell_1$ (IRWL1) minimization of
                  Cand{\`e}s, Wakin, and Boyd. Moreover, our algorithm
                  has the computational cost of a single BPDN
                  problem.},
  keywords = {sparse recovery, compressed sensing, iterative algorithms, weighted $\ell_1$ minimization, partial support recovery},
  month = {03},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SSP/2012/mansour2012SSPwspgl1/mansour2012SSPwspgl1_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SSP/2012/mansour2012SSPwspgl1/mansour2012SSPwspgl1.pdf}
}


@CONFERENCE{mansour2011SBGFcspsma,
  author = {Hassan Mansour and Haneet Wason and Tim T.Y. Lin and Felix J. Herrmann},
  title = {A compressive sensing perspective on simultaneous marine acquisition},
  booktitle = {SBGF},
  year = {2011},
  organization = {SBGF},
  abstract = {The high cost of acquiring seismic data in marine
                  environments compels the adoption of simultaneous-
                  source acquisition - an emerging technology that is
                  stimulating both geophysical research and commercial
                  efforts. In this paper, we discuss the properties of
                  randomized simultaneous acquisition matrices and
                  demonstrate that sparsity-promoting recovery
                  improves the quality of the reconstructed seismic
                  data volumes. Simultaneous marine acquisition calls
                  for the development of a new set of design
                  principles and post-processing tools. Leveraging
                  established findings from the field of compressed
                  sensing, the recovery from simultaneous sources
                  depends on a sparsifying transform that compresses
                  seismic data, is fast, and reasonably incoherent
                  with the compressive sampling matrix. To achieve
                  this incoherence, we use random time dithering where
                  sequential acquisition with a single airgun is
                  replaced by continuous acquisition with multiple
                  airguns firing at random times and at random
                  locations. We demonstrate our results with
                  simulations of simultaneous Marine acquisition using
                  periodic and randomized time dithering.},
  keywords = {Presentation, SBGF, acquisition, compressive sensing},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SBGF/2011/Mansour11SBGFcspsma/Mansour11SBGFcspsma.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SBGF/2011/Mansour11SBGFcspsma/Mansour11SBGFcspsma.pdf}
}


@CONFERENCE{mansour2012ICASSadapt,
  author = {Hassan Mansour and Ozgur Yilmaz},
  title = {Adaptive compressed sensing for video acquisition},
  booktitle = {ICASSP},
  year = {2012},
  optorganization = {ICASSP},
  abstract = {In this paper, we propose an adaptive compressed sensing
                  scheme that utilizes a support estimate to focus the
                  measurements on the large valued coefficients of a
                  compressible signal. We embed a "sparse-filtering"
                  stage into the measurement matrix by weighting down
                  the contribution of signal coefficients that are
                  outside the support estimate. We present an
                  application which can benefit from the proposed
                  sampling scheme, namely, video compressive
                  acquisition. We demonstrate that our proposed
                  adaptive CS scheme results in a significant
                  improvement in reconstruction quality compared with
                  standard CS as well as adaptive recovery using
                  weighted $\ell_1$ minimization.},
  keywords = {ICASSP},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ICASSP/2012/MansourYilmazICASSPaCS/MansourYilmazICASSPaCS.pdf}
}


@CONFERENCE{mansour2012ICASSsupport,
  author = {Hassan Mansour and Ozgur Yilmaz},
  title = {Support driven reweighted $\ell_1$ minimization},
  booktitle = {ICASSP},
  year = {2012},
  optorganization = {ICASSP},
  abstract = {In this paper, we propose a support driven reweighted
                  $\ell_1$ minimization algorithm (SDRL1) that solves
                  a sequence of weighted $\ell_1$ problems and relies
                  on the support estimate accuracy. Our SDRL1
                  algorithm is related to the IRL1 algorithm proposed
                  by Candes, Wakin, and Boyd. We demonstrate that it
                  is sufficient to find support estimates with good
                  accuracy and apply constant weights instead of using
                  the inverse coefficient magnitudes to achieve gains
                  similar to those of IRL1. We then prove that given a
                  support estimate with sufficient accuracy, if the
                  signal decays according to a specific rate, the
                  solution to the weighted $\ell_1$ minimization
                  problem results in a support estimate with higher
                  accuracy than the initial estimate. We also show
                  that under certain conditions, it is possible to
                  achieve higher estimate accuracy when the
                  intersection of support estimates is considered. We
                  demonstrate the performance of SDRL1 through
                  numerical simulations and compare it with that of
                  IRL1 and standard $\ell_1$ minimization.},
  keywords = {ICASSP},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ICASSP/2012/MansourYilmazICASSPwL1/MansourYilmazICASSPwL1.pdf}
}


@CONFERENCE{maysami2006SINBADrro,
  author = {Mohammad Maysami},
  title = {Recent results on seismic deconvolution},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {One of the important steps in seismic imaging is to
                  provide suitable information about boundaries. Sharp
                  variation of physical properties at a layer boundary
                  cause reflection the wavefield. In previous work
                  done by C. M. Dupuis, seismic signal
                  characterization is divided into two steps:
                  detection and estimation. In the detection phase,
                  the goal is to find all singularities in a seismic
                  section regardless of their order and then to
                  categorize the data to different events by windowing
                  each singularity. In the estimation step, we
                  determine the order of singularity more precisely by
                  using a rough estimate based on the detection
                  phase. Traditionally, a redundant dictionary method
                  is employed for the detection part. However, we
                  attempt to instead use a new L1-solver developed by
                  D.L. Donoho: the Stagewise Orthogonal Matching
                  Pursuit (StOMP). It approximates the solution to
                  inverse problems while promoting the sparsity in the
                  solution vector. This algorithm will allow us to
                  experimentally confirm the recent analysis by
                  S. Mallat on spiky deconvolution limits, which
                  imposes a required minimum distance between
                  spikes. This required minimum distance between
                  different spikes is dependent on the number of
                  spikes as well as the width of the chosen source
                  wavelet used in convolution with the train. These
                  results allow for the design of more robust and
                  accurate detection schemes for seismic signal
                  characterization.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/maysami06SINBADrro/maysami06SINBADrro_pres.pdf}
}


@CONFERENCE{maysami2008SEGlcf,
  author = {Mohammad Maysami and Felix J. Herrmann},
  title = {Lithological constraints from seismic waveforms: application to opal-{A} to opal-{CT} transition},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  pages = {2011-2015},
  organization = {SEG},
  abstract = {In this paper, we present a new method for seismic
                  waveform characterization whose aim is threefold,
                  namely (i) extraction of detailed information on the
                  sharpness of transitions in the subsurface from
                  seismic waveforms, (ii) reflector modeling, based on
                  binary-mixture and percolation theory, and (iii)
                  establishment of well-seismic ties, through
                  parameterizations of our waveform and critical
                  reflector model. We test this methodology on the
                  opal-A (Amorphous) to opal-CT
                  (Cristobalite/Tridymite) transition imaged in a
                  migrated section of North Sea field data West of the
                  Shetlands.},
  keywords = {SEG, SLIM},
  doi = {10.1190/1.3059400},
  month = {11},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/maysami08SEGlcf/maysami08SEGlcf.pdf}
}


@CONFERENCE{maysami2007EAGEsrc,
  author = {Mohammad Maysami and Felix J. Herrmann},
  title = {Seismic reflector characterization by a multiscale detection-estimation method},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2007},
  month = {06},
  abstract = {Seismic transitions of the subsurface are typically
                  considered as zero-order singularities (step
                  functions). According to this model, the
                  conventional deconvolution problem aims at
                  recovering the seismic reflectivity as a sparse
                  spike train. However, recent multiscale analysis on
                  sedimentary records revealed the existence of
                  accumulations of varying order singularities in the
                  subsurface, which give rise to fractional-order
                  discontinuities. This observation not only calls for
                  a richer class of seismic reflection waveforms, but
                  it also requires a different methodology to detect
                  and characterize these reflection events. For
                  instance, the assumptions underlying conventional
                  deconvolution no longer hold. Because of the
                  bandwidth limitation of seismic data, multiscale
                  analysis methods based on the decay rate of wavelet
                  coefficients may yield ambiguous results. We avoid
                  this problem by formulating the estimation of the
                  singularity orders by a parametric nonlinear
                  inversion method.},
  keywords = {Presentation, SLIM, EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/maysami07EAGEsrc/maysami07EAGEsrc_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/maysami07EAGEsrc/maysami07EAGEsrc.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=7081}
}


@CONFERENCE{modzelewski2008SINBADdas,
  author = {Henryk Modzelewski},
  title = {Design and specifications for {SLIM{\textquoteright}s} software framework},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {The SLIM group is actively developing software for
                  seismic imaging. This talk will give a general
                  overview of the software development during SINBAD
                  project with focus on the final release in February
                  2008. The covered topics will include: 1) adopting
                  Python for object-oriented programming, 2) including
                  parallelism into the algorithms used in seismic
                  imaging/modeling, 3) in-house algorithms for seismic
                  imaging, and 4) contributions to Madagascar
                  (RSF). The talk will serve as an introduction to the
                  other presentations in the session "SINBAD Software
                  releases".},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/modzelewski2008SINBADdas/modzelewski2008SINBADdas.pdf}
}


@CONFERENCE{modzelewski2006SINBADdas,
  author = {Henryk Modzelewski},
  title = {Design and specifications for {SLIM{\textquoteright}s} software framework},
  booktitle = {SINBAD 2006},
  year = {2006},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/modzelewski2006SINBADdas/modzelewski2006SINBADdas.pdf}
}


@CONFERENCE{moghaddam2008SINBADrtm,
  author = {Peyman P. Moghaddam},
  title = {Reverse-time migration amplitude recovery with curvelets},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We recover the amplitude of a seismic image by
                  approximating the normal (demigration-migration)
                  operator. In this approximation, we make use of the
                  property that curvelets remain invariant under the
                  action of the normal operator. We propose a seismic
                  amplitude recovery method that employs an eigenvalue
                  like decomposition for the normal operator using
                  curvelets as eigenvectors. Subsequently, we propose
                  an approximate nonlinear singularity-preserving
                  solution to the least-squares seismic imaging
                  problem with sparseness in the curvelet domain and
                  spatial continuity constraints. Our method is tested
                  with a reverse-time
                  {\textquoteright}wave-equation{\textquoteright}
                  migration code simulating the acoustic wave
                  equation.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/moghaddam2008SINBADrtm/moghaddam2008SINBADrtm.pdf}
}


@CONFERENCE{moghaddam2006SINBADioa,
  author = {Peyman P. Moghaddam},
  title = {Imaging operator approximation using curvelets},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {In this presentation, the normal (demigation-migration)
                  operator is studied in terms of a
                  pseudo-differential operator. The invariance of
                  curvelets under this operator and their sparsity on
                  the seismic images is used to precondition the
                  migration operator. A brief overview will be given
                  on some of the theory from micro-local analysis
                  which proofs that curvelets remain approximately
                  invariant under the operator. The proper setting for
                  which a diagonal approximation in the curvelet
                  domain is accurate is discussed together with
                  different methods that estimate this diagonal from
                  of-the-shelf migration operators. This is joint work
                  with Chris Stolk.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/moghaddam2006SINBADioa/moghaddam2006SINBADioa.pdf}
}


@CONFERENCE{moghaddam2006SINBADsac,
  author = {Peyman P. Moghaddam},
  title = {Sparsity- and continuity-promoting norms for seismic images},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {During this presentation, the importance of sparsity and
                  continuity enhancing energy norms is emphasized for
                  seismic imaging and inversion. The continuity
                  promoting energy norm is justified by the apparent
                  smoothness of reflectors in the direction along and
                  the oscillatory behavior across the interfaces. This
                  energy norm is called anisotropic diffusion and will
                  be defined mathematically. Denoising examples will
                  be given during which seismic images are recovered
                  from the noise by a joint norm-one and continuity
                  promoting minimization.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/moghaddam2006SINBADsac/moghaddam2006SINBADsac.pdf}
}


@CONFERENCE{moghaddam2008SEGcbm,
  author = {Peyman P. Moghaddam and Cody R. Brown and Felix J. Herrmann},
  title = {Curvelet-based migration preconditioning},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  pages = {2211-2215},
  organization = {SEG},
  abstract = {In this paper, we introduce a preconditioner for seismic
                  imaging{\textendash}-i.e., the inversion of the
                  linearized Born scattering operator. This
                  preconditioner approximately corrects for the
                  {\textquoteleft}{\textquoteleft}square
                  root{\textquoteright}{\textquoteright} of the
                  normal{\textendash}-i.e., the demigration-migration
                  operator. This approach consists of three parts,
                  namely (i) a left preconditoner, defined by a
                  fractional time integration designed to make the
                  migration operator zero order, and two right
                  preconditioners that apply (ii) a scaling in the
                  physical domain accounting for a spherical
                  spreading, and (iii) a curvelet-domain scaling that
                  corrects for spatial and reflector-dip dependent
                  amplitude errors. We show that a combination of
                  these preconditioners lead to a significant
                  improvement of the convergence for iterative
                  least-squares solutions to the seismic imaging
                  problem based on reverse-time migration operators.},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.3059325},
  month = {11},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/moghaddam08SEGcbm/moghaddam08SEGcbm_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/moghaddam08SEGcbm/moghaddam08SEGcbm.pdf }
}


@CONFERENCE{moghaddam2010SEGrfw,
  author = {Peyman P. Moghaddam and Felix J. Herrmann},
  title = {Randomized full-waveform inversion: a dimenstionality-reduction approach},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2010},
  month = {10},
  volume = {29},
  pages = {977-982},
  organization = {SEG},
  abstract = {Full-waveform inversion relies on the collection of
                  large multi-experiment data volumes in combination
                  with a sophisticated back-end to create
                  high-fidelity inversion results. While improvements
                  in acquisition and inversion have been extremely
                  successful, the current trend of incessantly pushing
                  for higher quality models in increasingly
                  complicated regions of the Earth reveals fundamental
                  shortcomings in our ability to handle increasing
                  problem sizes numerically. Two main culprits can be
                  identified. First, there is the so-called
                  {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which puts disproportionate strain on
                  current acquisition and processing systems as the
                  size and desired resolution increases. Secondly,
                  there is the recent
                  {\textquoteleft}{\textquoteleft}departure from
                  Moore{\textquoteright}s
                  law{\textquoteright}{\textquoteright} that forces us
                  to develop algorithms that are amenable to
                  parallelization. In this paper, we discuss different
                  strategies that address these issues via randomized
                  dimensionality reduction.},
  keywords = {Presentation, SEG, full-waveform inversion, optimization},
  doi = {10.1190/1.3513940},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2010/moghaddam10SEGrfw/moghaddam10SEGrfw_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2010/moghaddam10SEGrfw/moghaddam10SEGrfw.pdf}
}


@CONFERENCE{moghaddam2004SEGmpw,
  author = {Peyman P. Moghaddam and Felix J. Herrmann},
  title = {Migration preconditioning with curvelets},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2004},
  volume = {23},
  pages = {2204-2207},
  organization = {SEG},
  abstract = {In this paper, the property of Curvelet transforms for
                  preconditioning the migration and normal operators
                  is investigated. These operators belong to the class
                  of Fourier integral operators and
                  pseudo-differential operator, respectively. The
                  effect of this pre-conditioner is shown in term of
                  improvement of sparsity, convergence rate, number of
                  iteration for the Krylov-subspace solver and
                  clustering of singular(eigen) values. The migration
                  operator, which we employed in this work is the
                  common-offset Kirchoff-Born
                  migration. {\copyright}2004 Society of Exploration
                  Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.1845213},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2004/Moghaddam04SEGmpw/Moghaddam04SEGmpw_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2004/Moghaddam04SEGmpw/Moghaddam04SEGmpw_paper.pdf}
}


@CONFERENCE{moghaddam2007CSEGmar,
  author = {Peyman P. Moghaddam and Felix J. Herrmann and Christiaan C. Stolk},
  title = {Migration amplitude recovery using curvelets},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2007},
  organization = {CSEG},
  abstract = {In this paper, we recover the amplitude of a seismic
                  image by approximating the normal operator and
                  subsequently inverting it. Normal operator
                  (migration followed by modeling) is an example of
                  pseudo-differential.  curvelets are proven to be
                  invariant under the action of pseudo-differential
                  operators under certain conditions. Subsequently,
                  curvelets are forming as eigen-vectors for such an
                  operator. We propose a seismic amplitude recovery
                  method that employs an eigen-value decomposition for
                  normal operator using curvelets as eigen-vectors and
                  to be estimated eigenvalues. A post-stack
                  reverse-time, wave-equation migration is used for
                  evaluation of the proposed method.},
  file = {http://cseg.ca/assets/files/resources/abstracts/2007/168S0131.pdf},
  keywords = {SLIM},
  month = {05},
  url = {http://cseg.ca/assets/files/resources/abstracts/2007/168S0131.pdf}
}


@CONFERENCE{moghaddam2007CSEGsac,
  author = {Peyman P. Moghaddam and Felix J. Herrmann and Christiaan C. Stolk},
  title = {Sparsity and continuity enhancing seismic imaging},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2007},
  organization = {CSEG},
  abstract = {A non-linear singularity-preserving solution to the
                  least-squares seismic imaging problem with
                  sparseness and continuity constraints is
                  proposed. The applied formalism explores curvelets
                  as a directional frame that, by their sparsity on
                  the image, and their invariance under the imaging
                  operators, allows for a stable recovery of the
                  amplitudes. Our method is based on the estimation of
                  the normal operator in the form of an
                  {\textquoteright}eigenvalue{\textquoteright}
                  decompsoition with curvelets as the
                  eigenvectors{\textquoteright}. Subsequently, we
                  propose an inversion method that derives from
                  estimation of the normal operator and is formulated
                  as a convex optimization problem. Sparsity in the
                  curvelet domain as well as continuity along the
                  reflectors in the image domain are promoted as part
                  of this optimization. Our method is tested with a
                  reverse-time
                  {\textquoteright}wave-equation{\textquoteright}
                  migration code simulating the acoustic wave
                  equation.},
  file = {http://cseg.ca/assets/files/resources/abstracts/2007/091S0130.pdf},
  keywords = {SLIM},
  month = {05},
  url = {http://cseg.ca/assets/files/resources/abstracts/2007/091S0130.pdf}
}


@CONFERENCE{moghaddam2007EAGEsar,
  author = {Peyman P. Moghaddam and Felix J. Herrmann and Christiaan C. Stolk},
  title = {Seismic amplitude recovery with curvelets},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2007},
  month = {06},
  abstract = {A non-linear singularity-preserving solution to the
                  least-squares seismic imaging problem with
                  sparseness and continuity constraints is
                  proposed. The applied formalism explores curvelets
                  as a directional frame that, by their sparsity on
                  the image, and their invariance under the imaging
                  operators, allows for a stable recovery of the
                  amplitudes. Our method is based on the estimation of
                  the normal operator in the form of an
                  {\textquoteright}eigenvalue{\textquoteright}
                  decompsoition with curvelets as the
                  {\textquoteright}eigenvectors{\textquoteright}.
                  Subsequently, we propose an inversion method that
                  derives from estimation of the normal operator and
                  is formulated as a convex optimization
                  problem. Sparsity in the curvelet domain as well as
                  continuity along the reflectors in the image domain
                  are promoted as part of this optimization. Our
                  method is tested with a reverse-time
                  {\textquoteright}wave-equation{\textquoteright}
                  migration code simulating the acoustic wave
                  equation.},
  keywords = {SLIM, EAGE},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/moghaddam07EAGEsar/moghaddam07EAGEsar.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=6935}
}


@CONFERENCE{moghaddam2007SEGrsi,
  author = {Peyman P. Moghaddam and Felix J. Herrmann and Christiaan C. Stolk},
  title = {Robust seismic-images amplitude recovery using curvelets},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  pages = {2225-2229},
  organization = {SEG},
  abstract = {In this paper, we recover the amplitude of a seismic
                  image by approximating the normal
                  (demigration-migration) operator. In this
                  approximation, we make use of the property that
                  curvelets remain invariant under the action of the
                  normal operator. We propose a seismic amplitude
                  recovery method that employs an eigenvalue like
                  decomposition for the normal operator using
                  curvelets as eigen-vectors. Subsequently, we propose
                  an approximate non-linear singularity-preserving
                  solution to the least-squares seismic imaging
                  problem with sparseness in the curvelet domain and
                  spatial continuity constraints. Our method is tested
                  with a reverse-time
                  {\textquoteleft}wave-equation{\textquoteright}
                  migration code simulating the acoustic wave equation
                  on the SEG-AA salt model. {\copyright}2007 Society
                  of Exploration Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.2792928},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/moghaddam07SEGrsi/moghaddam07SEGrsi_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/moghaddam07SEGrsi/moghaddam07SEGrsi.pdf }
}


@CONFERENCE{min2012CSEGrgfe,
  author = {Ju-Won Oh and Dong-Joo Min and Felix J. Herrmann},
  title = {Re-establishment of gradient in frequency-domain elastic waveform inversion},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2012},
  abstract = {To obtain solutions close to global minimum in waveform
                  inversion, the gradients computed at each frequency
                  need to be weighted to appropriately describe the
                  residuals between modeled and field data. While the
                  low-frequency components of the gradients should be
                  weighted to recover the long-wavelength structures,
                  the high-frequency components of the gradients need
                  to be weighted when the short-wavelength structures
                  are restored. However, the conventional elastic
                  waveform inversion algorithms cannot properly weight
                  the gradients computed at each frequency. When
                  gradients are scaled using the pseudo-Hessian matrix
                  inside the frequency loop, gradients obtained at
                  high frequencies are over-emphasized. When the
                  gradients are scaled outside the frequency loop,
                  gradients are weighted by the source spectra. In
                  this study, we propose applying weighting factors to
                  the gradients obtained at each frequency so that
                  gradients can properly reflect the differences
                  between the true and assumed models satisfying the
                  general inverse theory. The weighting factors are
                  composed by the backpropagated residuals. Numerical
                  examples for the simple rectangular-shaped model and
                  the modified version of the Marmousi-2 model show
                  that the weighting method enhances gradient images
                  and inversion results compared to the conventional
                  inversion algorithms.},
  keywords = {elastic, waveform inversion, frequency-domain, weighting factors},
  month = {02},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2012/min2012CSEGrgfe/min2012CSEGrgfe.pdf}
}


@CONFERENCE{min2012EAGEefwi,
  author = {Ju-Won Oh and Dong-Joo Min and Felix J. Herrmann},
  title = {Frequency-domain elastic waveform inversion using weighting factors related to source-deconvolved residuals},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2012},
  month = {06},
  abstract = {One of the limitations in seismic waveform inversion is
                  that inversion results are very sensitive to initial
                  guesses, which may be because the gradients computed
                  at each frequency are not properly weighted
                  depending on given models.Analyzingthe conventional
                  waveform inversion algorithms using the
                  pseudo-Hessian matrix as a pre-conditioner shows
                  that the gradientsdo not properly describe the
                  feature of given models or high- and low-end
                  frequencies do not contribute the model parameter
                  updates due to banded spectra of source wavelet. For
                  a better waveform inversion algorithm, we propose
                  applying weighting factors to gradients computed at
                  each frequency. The weighting factors are designed
                  using the source-deconvolved back-propagated
                  wavefields. Numerical results for the SEG/EAGE salt
                  model show that the weighting method improves
                  gradient images and its inversion results are
                  compatible with true velocities even with poorly
                  estimated initial guesses.},
  keywords = {EAGE, elastic, waveform inversion, frequency-domain, weighting factors},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/min2012EAGEefwi/min2012EAGEefwi.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=59623}
}


@CONFERENCE{ross2008SINBADsit,
  author = {Sean Ross-Ross},
  title = {Seismic inversion through operator overloading},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {Geophysical processing is dominated by many different
                  out of core memory software environments
                  (OOCE). Such environments include Madagascar and SU
                  and are designed to handle data that can not be
                  operated on in memory. Each base operation is
                  created as a main program that reads data from disk
                  and writes the result to disk. The main programs can
                  also be chained together on stdin/out pipes using a
                  shell only writing data to disk at the end. To be
                  efficient, the algorithm using an OOCE must chain
                  together the longest pipe to avoid disk I/O, as a
                  result it is very difficult to use iterative
                  techniques. The algorithms are written in shell
                  scripts can be difficult to read and understand.
                  SLIMpy is a software library that contains
                  definitions of coordinate free vectors and linear
                  operators. It allows the user to design and run
                  algorithms with any out of core package, in a Matlab
                  style interface while maintaining optimal efficiency
                  and speed. SLIMpy looks at each main program of each
                  OOCE as a Matrix vector operation or vector
                  reduction/transformation operation. It uses operator
                  overloading to generate an abstract syntax tree
                  (AST) which can be optimized in many ways before
                  executing its commands. The AST also provides a
                  pathway for embarrassingly parallel applications by
                  splitting the tree over different nodes and
                  processors. SLIMpy provides an interface to these
                  OOCE that allows for optimal construction of
                  commands and allows for iterative techniques. It
                  smoothes the transition from other languages such as
                  Matlab and allows the algorithm designer to write
                  readable and reusable code. SLIMpy also adds to OOCE
                  by allowing for easy parallelization.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/ross2008SINBADsit/ross2008SINBADsit.pdf}
}


@CONFERENCE{saab2008SINBADcps,
  author = {Rayan Saab},
  title = {Curvelet-based primary-multiple separation from a {Bayesian} perspective},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We present a novel primary-multiple separation scheme
                  which makes use of the sparsity of both primaries
                  and multiples in a transform domain, such as the
                  curvelet transform, to provide estimates of each.
                  The proposed algorithm utilizes seismic data as well
                  as the output of a preliminary step that provides
                  (possibly) erroneous predictions of the
                  multiples. The algorithm separates the signal
                  components, i.e., the primaries and multiples, by
                  solving an optimization problem that assumes noisy
                  input data and can be derived from a Bayesian
                  perspective. More precisely, the optimization
                  problem can be arrived at via an assumption of a
                  weighted Laplacian distribution for the primary and
                  multiple coefficients in the transform domain and of
                  white Gaussian noise contaminating both the seismic
                  data and the preliminary prediction of the
                  multiples, which both serve as input to the
                  algorithm.},
  date-modified = {2008-08-22 12:45:53 -0700},
  keywords = {SLIM, SINBAD, Presentation},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/saab2008SINBADcps/saab2008SINBADcp.pdf}
}


@CONFERENCE{saab2008ICASSPssa,
  author = {Rayan Saab and Rick Chartrand and Ozgur Yilmaz},
  title = {Stable sparse approximations via nonconvex optimization},
  booktitle = {ICASSP},
  year = {2008},
  organization = {ICASSP},
  abstract = {We present theoretical results pertaining to the ability
                  of lp minimization to recover sparse and
                  compressible signals from incomplete and noisy
                  measurements. In particular, we extend the results
                  of Cande`s, Romberg and Tao [1] to the p < 1
                  case. Our results indicate that depending on the
                  restricted isometry constants (see, e.g.,[2] and
                  [3]) and the noise level, lp minimization with
                  certain values of p < 1 provides better theoretical
                  guarantees in terms of stability and robustness than
                  l1 minimization does. This is especially true when
                  the restricted isometry constants are relatively
                  large.},
  keywords = {ICASSP},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ICASSP/2008/saab08ICASSPssa/saab08ICASSPssa.pdf }
}


@CONFERENCE{saab2007SEGcbp,
  author = {Rayan Saab and Deli Wang and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Curvelet-based primary-multiple separation from a {Bayesian} perspective},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  pages = {2510-2514},
  organization = {SEG},
  abstract = {In this abstract, we present a novel primary-multiple
                  separation scheme which makes use of the sparsity of
                  both primaries and multiples in a transform domain,
                  such as the curvelet transform, to provide estimates
                  of each. The proposed algorithm utilizes seismic
                  data as well as the output of a preliminary step
                  that provides (possibly) erroneous predictions of
                  the multiples. The algorithm separates the signal
                  components, i.e., the primaries and multiples, by
                  solving an optimization problem that assumes noisy
                  input data and can be derived from a Bayesian
                  perspective. More precisely, the optimization
                  problem can be arrived at via an assumption of a
                  weighted Laplacian distribution for the primary and
                  multiple coefficients in the transform domain and of
                  white Gaussian noise contaminating both the seismic
                  data and the preliminary prediction of the
                  multiples, which both serve as input to the
                  algorithm. {\copyright}2007 Society of Exploration
                  Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.2792988},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/saab07SEGcbp/saab07SEGcbp_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/saab07SEGcbp/saab07SEGcbp.pdf }
}


@CONFERENCE{saab2009SAMPTAnccs,
  author = {Rayan Saab and Ozgur Yilmaz},
  title = {A short note on non-convex compressed sensing},
  booktitle = {SAMPTA technical program},
  year = {2009},
  organization = {SAMPTA},
  abstract = {In this note, we summarize the results we recently
                  proved in\cite{SY08} on the theoretical performance
                  guarantees of the decoders $\Delta_p$.  These
                  decoders rely on $\ell^p$ minimization with $p {\i}n
                  (0,1)$ to recover estimates of sparse and
                  compressible signals from incomplete and inaccurate
                  measurements. Our guarantees generalize the results
                  of \cite{CRT05} and \cite{Wojtaszczyk08} about
                  decoding by $\ell_p$ minimization with $p = 1$, to
                  the setting where $p {\i}n (0,1)$ and are obtained
                  under weaker sufficient conditions. We also present
                  novel extensions of our results in \cite{SY08} that
                  follow from the recent work of DeVore et al. in
                  \cite{DPW08}. Finally, we show some insightful
                  numerical experiments displaying the trade-off in
                  the choice of $p \in (0,1]$ depending on certain
                  properties of the input signal.},
  keywords = {Presentation},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SAMPTA/2009/saab09SAMPTAnccs/saab09SAMPTAnccs.pdf}
}


@CONFERENCE{sastry2007SINBADnor,
  author = {Challa S. Sastry},
  title = {Norm-one recovery from irregular sampled data},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {Seismic traces are sampled irregularly and
                  insufficiently due to practical and economical
                  limitations. The use of such data in seismic imaging
                  results in image artifacts and poor spatial
                  resolution. Therefore, before being used, the
                  measurements are to be interpolated onto a regular
                  grid. One of the methods achieving this objective is
                  based on the Fourier reconstruction, which deals
                  with the under-determined system of equations. The
                  recent pursuit techniques (namely, basis pursuit,
                  matching pursuit etc) admit certain promising
                  features such as faster and simpler implementation
                  even in large scale settings.  The presentation
                  discusses the application of the pursuit algorithms
                  to the Fourier-based interpolation problem for the
                  signals that have sparse Fourier spectra. In
                  particular, the objective of the presentation
                  includes: 1). studying the performance of the
                  algorithm if, and how far, the measurement
                  coordinates can be shifted from uniform distribution
                  on the continuous interval. 2). studying what could
                  be the allowable misplacement in the measurement
                  coordinates that does not alter the quality of the
                  reconstruction process},
  keywords = {SLIM, SINBAD, Presentation}
}


@CONFERENCE{challa2007EAGEsrf,
  author = {Challa S. Sastry and Gilles Hennenfent and Felix J. Herrmann},
  title = {Signal reconstruction from incomplete and misplaced measurements},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2007},
  month = {06},
  abstract = {Constrained by practical and economical considerations,
                  one often uses seismic data with missing traces. The
                  use of such data results in image artifacts and poor
                  spatial resolution. Sometimes due to practical
                  limitations, measurements may be available on a
                  perturbed grid, instead of on the designated
                  grid. Due to algorithmic requirements, when such
                  measurements are viewed as those on the designated
                  grid, the recovery procedures may result in
                  additional artifacts. This paper interpolates
                  incomplete data onto regular grid via the Fourier
                  domain, using a recently developed greedy
                  algorithm. The basic objective is to study
                  experimentally as to what could be the size of the
                  perturbation in measurement coordinates that allows
                  for the measurements on the perturbed grid to be
                  considered as on the designated grid for faithful
                  recovery. Our experimental work shows that for
                  compressible signals, a uniformly distributed
                  perturbation can be offset with slightly more number
                  of measurements.},
  keywords = {SLIM, EAGE},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/challa07EAGEsrf/challa07EAGEsrf.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=6917}
}


@CONFERENCE{sastry2007SINBADrfu,
  author = {Challa S. Sastry and Gilles Hennenfent and Felix J. Herrmann},
  title = {Recovery from unstructured data},
  booktitle = {SINBAD 2006},
  year = {2006},
  keywords = {SLIM, SINBAD, Presentation},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/sastry2007SINBADrfu/sastry2007SINBADrfu.pdf}
}


@CONFERENCE{shahidi2009SEGcmf,
  author = {Reza Shahidi and Felix J. Herrmann},
  title = {Curvelet-domain matched filtering with frequency-domain regularization},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  month = {10},
  volume = {28},
  pages = {3645-3649},
  organization = {SEG},
  abstract = {In Herrmann et al. (2008), it is shown that zero-order
                  pseudodifferential operators, which model the
                  migration-demigration operator and the operator
                  mapping the predicted multiples to the true
                  multiples, can be represented by a diagonal
                  weighting in the curvelet domain. In that paper, a
                  smoothness constraint was introduced in the phase
                  space of the operator in order to regularize the
                  solution to make it unique.  In this paper, we use
                  recent results in Demanet and Ying (2008) on the
                  discrete symbol calculus to impose a further
                  smoothness constraint, this time in the frequency
                  domain. It is found that with this additional
                  constraint, faster convergence is realized. Results
                  on a synthetic pseudodifferential operator as well
                  as on an example of primary-multiple separation in
                  seismic data are included, comparing the model with
                  and without the new smoothness constraint, from
                  which it is found that results of improved quality
                  are also obtained.},
  keywords = {Presentation,SEG},
  doi = {10.1190/1.3255624},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/shahidi09SEGcmf/shahidi09segcmf.pdf}
}


@CONFERENCE{tang2009SEGhdb,
  author = {Gang Tang and Reza Shahidi and Felix J. Herrmann and Jianwei Ma},
  title = {Higher dimensional blue-noise sampling schemes for curvelet-based seismic data recovery},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  month = {10},
  volume = {28},
  pages = {191-195},
  organization = {SEG},
  abstract = {In combination with compressive sensing, a successful
                  reconstruction scheme called Curvelet-based Recovery
                  by Sparsity-promoting Inversion (CRSI) has been
                  developed, and has proven to be useful for seismic
                  data processing. One of the most important issues
                  for CRSI is the sampling scheme, which can greatly
                  affect the quality of reconstruction. Unlike usual
                  regular undersampling, stochastic sampling can
                  convert aliases to easy-to-eliminate noise. Some
                  stochastic sampling methods have been developed for
                  CRSI, e.g. jittered sampling, however most have only
                  been applied to 1D sampling along a line. Seismic
                  datasets are usually higher dimensional and very
                  large, thus it is desirable and often necessary to
                  develop higher dimensional sampling methods to deal
                  with these data. For dimensions higher than one, few
                  results have been reported, except uniform random
                  sampling, which does not perform well. In the
                  present paper, we explore 2D sampling methodologies
                  for curvelet-based reconstruction, possessing
                  sampling spectra with blue noise characteristics,
                  such as Poisson Disk sampling, Farthest Point
                  Sampling, and the 2D extension of jittered
                  sampling. These sampling methods are shown to lead
                  to better recovery and results are compared to the
                  other more traditional sampling protocols.},
  keywords = {Presentation, SEG},
  doi = {10.1190/1.3255230},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/tang09SEGhdb/tang09SEGhdb.pdf}
}


@CONFERENCE{thomson2006SINBADlss,
  author = {Darren Thomson},
  title = {Large-scale seismic data recovery by the parallel windowed curvelet transform},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {We propose using overlapping, tapered windows to process
                  seismic data in parallel. This method consists of
                  numerically tight linear operators and adjoints that
                  are suitable for use in iterative algorithms. This
                  method is also highly scalable and makes
                  parallelprocessing of large seismic data sets
                  feasible. We use this scheme to define the Parallel
                  Windowed Fast Discrete Curvelet Transform (PWFDCT),
                  which we have applied to a seismic data
                  interpolation algorithm. Some preliminary results
                  will be shown. Henryk Modzeleweski: Design and
                  specifications for SLIMPy's software framework The
                  SLIM group is actively developing software for
                  seismic imaging. This talk will give a general
                  overview of the software development philosophy
                  adopted by SLIM. The covered topics will include: 1)
                  adopting Python for object-oriented programming, 2)
                  including parallelism into the algorithms used in
                  seismic imaging/modeling, 3) in-house algorithms for
                  seismic imaging, and 4) contributions to Madagascar
                  (RSF). The talk will serve as an introduction to the
                  other presentations in the session "SINBAD Software
                  releases".},
  keywords = {SLIM, SINBAD, Presentation},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/thomson2006SINBADlss/thomson2006SINBADlss.pdf}
}


@CONFERENCE{thomson2006SINBADppe,
  author = {Darren Thomson},
  title = {{(P)SLIMPy}: parallel extension},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {The parallel extensions to the SLIMpy environment enable
                  pipe-based processing of large data sets in an
                  MPI-based parallel environment. Parallel processing
                  can be done by straightforward slicing of data, or
                  by using an overlapping domain decomposition that
                  requires communication between different
                  processors. The principal aim of the parallel
                  extensions is to leave abstract numerical algorithms
                  (ANA's) and applications programmed for use in
                  SLIMpy untouched when moving to parallel processing.
                  The object-oriented functionality of Python makes
                  this possible.},
  keywords = {SLIM, SINBAD, Presentation},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/thomson2006SINBADppe/thomson2006SINBADppe.pdf}
}


@CONFERENCE{thomson2006SEGpwfd,
  author = {Darren Thomson and Gilles Hennenfent and Henryk Modzelewski and Felix J. Herrmann},
  title = {A parallel windowed fast discrete curvelet transform applied to seismic processing},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2006},
  volume = {25},
  pages = {2767-2771},
  organization = {SEG},
  abstract = {We propose using overlapping, tapered windows to process
                  seismic data in parallel. This method consists of
                  numerically tight linear oper ators and adjoints
                  that are suitable for use in iterative algorithms.
                  This method is also highly scalable and makes
                  parallel processing of large seismic data sets
                  feasible. We use this scheme to define the Parallel
                  Windowed Fast Discrete Curvelet Transform (PWFDCT),
                  which we apply to a seismic data interpolation
                  algorithm. The successful performance of our
                  parallel processing scheme and algorithm on a
                  two-dimensional synthetic data is shown.},
  keywords = {SEG},
  doi = {10.1190/1.2370099},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2006/thomson06SEGpwfd/thomson06SEGpwfd.pdf }
}


@CONFERENCE{tu2012EAGElsm,
  author = {Ning Tu and Felix J. Herrmann},
  title = {Least-squares migration of full wavefield with source encoding},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2012},
  month = {06},
  abstract = {Multiples can provide valuable information that is
                  missing in primaries, and there is a growing
                  interest in using them for seismic imaging. In our
                  earlier work, we proposed to combine primary
                  estimation and migration to image from the total
                  up-going wavefield. The method proves to be
                  effective but computationally expensive. In this
                  abstract, we propose to reduce the computational
                  cost by removing the multi-dimensional convolution
                  required by primary estimation, and reducing the
                  number of PDE solves in migration by introducing
                  simultaneous sources with source renewal. We gain
                  great performance boost without compromising the
                  quality of the image.},
  keywords = {EAGE, depth migration, surface-related multiples},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/tu2012EAGElsm/tu2012EAGElsm_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/tu2012EAGElsm/tu2012EAGElsm.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=59688}
}


@CONFERENCE{tu2012SEGima,
  author = {Ning Tu and Felix J. Herrmann},
  title = {Imaging with multiples accelerated by message passing},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2012},
  month = {11},
  volume = {31},
  pages = {1-6},
  organization = {SEG},
  abstract = {With the growing realization that multiples can provide
                  valuable information, there is a paradigm shift from
                  removing them to using them. For instance, primary
                  estimation by sparse inversion has demonstrated its
                  superiority over surface-related multiple removal in
                  many aspects. Inspired by this shift, we propose a
                  method to image directly from the total up-going
                  wavefield, including surface-related multiples, by
                  sparse inversion. To address the high computational
                  cost associated with this method, we propose to
                  speed up the inversion by having the wave-equation
                  solver carry out the multi-dimensional convolutions
                  implicitly and cheaply by randomized subsampling. We
                  improve the overall performance of this algorithm by
                  selecting new independent copies of the randomized
                  modeling operator, which leads to a cancellation of
                  correlations that hamper the speed of convergence of
                  the solver. We show the merits of our approach on a
                  number of examples.},
  keywords = {SEG, imaging, multiples},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2012/tu2012SEGima/tu2012SEGima_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2012/tu2012SEGima/tu2012SEGima.pdf},
  doi = {10.1190/segam2012-1552.1}
}


@CONFERENCE{tu2011EAGEspmsrm,
  author = {Ning Tu and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Sparsity-promoting migration with surface-related multiples},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2011},
  month = {05},
  abstract = {Multiples, especially the surface-related multiples,
                  form a significant part of the total up-going wave-
                  field. If not properly dealt with, they can lead to
                  false reflectors in the final image. So
                  conventionally practitioners remove them prior to
                  migration. Recently research has revealed that
                  multiples can actually provide extra illumination so
                  different methods are proposed to address the issue
                  that how to use multiples in seismic imaging, but
                  with various kinds of limitations. In this abstract,
                  we combine primary estimation and sparsity-promoting
                  migration into one convex-optimization process to
                  include information from multiples. Synthetic
                  examples show that multiples do make active
                  contributions to seismic migration. Also by this
                  combination, we can benefit from better recoveries
                  of the Greens function by using sparsity-promoting
                  algorithms since reflectivity is sparser than the
                  Greens function.},
  keywords = {Presentation, EAGE, imaging, processing},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2011/tu11EAGEspmsrm/tu11EAGEspmsrm_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2011/tu11EAGEspmsrm/tu11EAGEspmsrm.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=50369}
}


@CONFERENCE{tu2011SEGmult,
  author = {Ning Tu and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Migration with surface-related multiples from incomplete seismic data},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2011},
  month = {09},
  volume = {30},
  pages = {3222-3227},
  organization = {SEG},
  abstract = {Seismic acquisition is confined by limited aperture that
                  leads to finite illumination, which, together with
                  other factors, hinders imaging of subsurface objects
                  in complex geological settings such as salt
                  structures. Conventional processing, including
                  surface-related multiple elimination, further
                  reduces the amount of information we can get from
                  seismic data. With the growing consensus that
                  multiples carry valuable information that is missing
                  from primaries, we are motivated to exploit the
                  extra illumination provided by multiples to image
                  the sub- surface. In earlier research, we proposed
                  such a method by combining primary estimation and
                  sparsity-promoting migration to invert for model
                  perturbations directly from the total up-going
                  wavefield. In this abstract, we focus on a
                  particular case. By exploiting the extra
                  illumination from surface-related multiples, we
                  mitigate the effects caused by migrating from
                  incomplete data with missing sources and missing
                  near-offsets.},
  keywords = {Presentation, SEG, imaging, processing},
  doi = {10.1190/1.3627865},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2011/tu11SEGmult/tu11SEGmult_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2011/tu11SEGmult/tu11SEGmult.pdf}
}


@CONFERENCE{vandenberg2008IAMesr,
  author = {Ewout {van den Berg}},
  title = {Exact sparse reconstruction and neighbourly polytopes},
  booktitle = {IAM},
  year = {2008},
  date-added = {2008-08-26 15:44:44 -0700},
  date-modified = {2008-08-26 15:45:58 -0700},
  keywords = {SLIM, IAM, Presentation, private},
  presentation = {https://slim.gatech.edu/Publications/Private/Conferences/IAM/2008/vandenberg2008IAMesr/vandenberg2008IAMesr.pdf}
}


@CONFERENCE{vandenberg2008SINBADsat,
  author = {Ewout {van den Berg}},
  title = {Sparco: A testing framework for sparse reconstruction},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {Sparco is a framework for testing and benchmarking
                  algorithms for sparse reconstruction. It includes a
                  large collection of sparse reconstruction problems
                  drawn from the imaging, compressed sensing, and
                  geophysics literature. Sparco is also a framework
                  for implementing new test problems and can be used
                  as a tool for reproducible research. We describe the
                  software environment, and demonstrate its usefulness
                  for testing and comparing solvers for sparse
                  reconstruction.},
  date-modified = {2008-08-22 12:54:25 -0700},
  keywords = {SLIM, SINBAD, Presentation},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/vandenberg2008SINBADsat/vandenberg2008SINBADsat.pdf}
}


@CONFERENCE{friedlander2009SCAIMspot,
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  title = {Spot: A linear-operator toolbox for Matlab},
  booktitle = {SCAIM},
  year = {2009},
  address = {University of British Columbia},
  organization = {SCAIM Seminar},
  keywords = {minimization, Presentation, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2009/VandenBerg-Mon-1130.pdf}
}


@CONFERENCE{vandenberg2007SINBADipo1,
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  title = {In pursuit of a root},
  booktitle = {2007 Von Neumann Symposium},
  year = {2007},
  keywords = {minimization, Presentation, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2007/vandenberg2007SINBADipo1/vandenberg2007SINBADipo1.pdf}
}


@CONFERENCE{vandenberg2009SLIMocf,
  author = {Ewout {van den Berg} and Mark Schmidt and Michael P. Friedlander and K. Murphy},
  title = {Optimizing costly functions with simple constraints: a limited-memory projected {Quasi-Newton} algorithm},
  booktitle = {SLIM},
  year = {2009},
  volume = {12},
  series = {Twelfth International Conference on Artificial Intelligence and Statistics},
  abstract = {An optimization algorithm for minimizing a smooth
                  function over a convex set is described. Each
                  iteration of the method computes a descent direction
                  by minimizing, over the original constraints, a
                  diagonal-plus low-rank quadratic approximation to
                  the function. The quadratic approximation is
                  constructed using a limited-memory quasi-Newton
                  update. The method is suitable for large-scale
                  problems where evaluation of the function is
                  substan- tially more expensive than projection onto
                  the constraint set. Numerical experiments on
                  one-norm regularized test problems indicate that the
                  proposed method is competitve with state-of-the-art
                  methods such as bound-constrained L-BFGS and
                  orthant-wise descent. We further show that the
                  method generalizes to a wide class of problems, and
                  substantially improves on state-of-the-art methods
                  for problems such as learning the structure of
                  Gaussian graphi- cal models (involving
                  positive-definite matrix constraints) and Markov
                  random fields (involving second-order cone
                  constraints).},
  date-added = {2009-01-29 17:16:34 -0800},
  date-modified = {2009-01-29 17:16:34 -0800},
  keywords = {SLIM},
  month = {04},
  url = {http://www.cs.ubc.ca/~mpf/papers/SchmidtBergFriedMurph09.pdf}
}


@CONFERENCE{vanderneut2012EAGEdecomp,
  author = {Joost {van der Neut} and Felix J. Herrmann},
  title = {Up / down wavefield decomposition by sparse inversion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2012},
  month = {06},
  abstract = {Expressions have been derived for the decomposition of
                  multi-component seismic recordings into up- and
                  down-going constituents. However, these expressions
                  contain singularities at critical angles and can be
                  sensitive for noise. By interpreting wavefield
                  decomposition as an inverse problem and imposing
                  constraints on the sparseness of the solution, we
                  arrive at a robust formalism that can be applied to
                  noisy data. The method is demonstrated on synthetic
                  data with multi-component receivers in a horizontal
                  borehole, but can also be applied for different
                  configurations, including OBC and dual-sensor
                  streamers.},
  keywords = {EAGE, wavefield decomposition, sparse inversion},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/vanderneut2012EAGEdecomp/vanderneut2012EAGEdecomp_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/vanderneut2012EAGEdecomp/vanderneut2012EAGEdecomp.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=58907}
}


@CONFERENCE{vanderneut2012SEGirs,
  author = {Joost {van der Neut} and Felix J. Herrmann and Kees Wapenaar},
  title = {Interferometric redatuming with simultaneous and missing sources using sparsity promotion in the curvelet domain},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2012},
  month = {11},
  volume = {31},
  pages = {1-7},
  organization = {SEG},
  abstract = {Interferometric redatuming is a velocity-independent
                  method to turn downhole receivers into virtual
                  sources. Accurate redatuming involves solving an
                  inverse problem, which can be highly ill-posed,
                  especially in the presence of noise, incomplete data
                  and limited aperture. We address these issues by
                  combining interferometric redatuming with
                  transform-domain sparsity promotion, leading to a
                  formulation that deals with data imperfections. We
                  show that sparsity promotion improves the retrieval
                  of virtual shot records under a salt flank. To
                  reduce acquisition costs, it can be beneficial to
                  reduce the number of sources or shoot them
                  simultaneously. It is shown that sparse inversion
                  can still provide a stable solution in such cases.},
  keywords = {processing, imaging, optimization, interferometry, SEG},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2012/vanderneut2012SEGirs/vanderneut2012SEGirs.pdf},
  doi = {10.1190/segam2012-0566.1}
}


@CONFERENCE{vanleeuwen2012EAGEcarpcg,
  author = {Tristan van Leeuwen and Dan Gordon and Rachel Gordon and Felix J. Herrmann},
  title = {Preconditioning the {Helmholtz} equation via row-projections},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2012},
  month = {06},
  abstract = {3D frequency-domain full waveform inversion relies on
                  being able to efficiently solve the 3D Helmholtz
                  equation. Iterative methods require sophisticated
                  preconditioners because the Helmholtz matrix is
                  typically indefinite. We review a preconditioning
                  technique that is based on row-projections. Notable
                  advantages of this preconditioner over existing ones
                  are that it has low algorithmic complexity, is
                  easily parallelizable and extendable to
                  time-harmonic vector equations.},
  keywords = {EAGE, Helmholtz equation, precondition},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/vanleeuwen2012EAGEcarpcg/vanleeuwen2012EAGEcarpcg_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/vanleeuwen2012EAGEcarpcg/vanleeuwen2012EAGEcarpcg.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=58891}
}


@CONFERENCE{vanleeuwen2012EAGEext,
  author = {Tristan {van Leeuwen} and Felix J. Herrmann},
  title = {Wave-equation extended images: computation and velocity continuation},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2012},
  month = {06},
  abstract = {An extended image is a multi-dimensional correlation of
                  source and receiver wavefields. For a kinematically
                  correct velocity, most of the energy will be
                  concentrated at zero offset. Because of the
                  computational cost involved in correlating the
                  wavefields for all offsets, such exteded images are
                  computed for a subsurface offset that is aligned
                  with the local dip. In this paper, we present an
                  efficient way to compute extended images for all
                  subsurface offsets without explicitly calculating
                  the receiver wavefields, thus making it
                  computationally feasible to compute such extended
                  images. We show how more conventional image gathers,
                  where the offset is aligned with the dip, can be
                  extracted from this extended image. We also present
                  a velocity continuation procedure that allows us to
                  compute the extended image for a given velocity
                  without recomputing all the source wavefields.},
  keywords = {EAGE, extended image, velocity continuation},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/vanleeuwen2012EAGEext/vanleeuwen2012EAGEext_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/vanleeuwen2012EAGEext/vanleeuwen2012EAGEext.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=59616}
}


@CONFERENCE{verschuur2007SEGmmp,
  author = {D. J. Verschuur and Deli Wang and Felix J. Herrmann},
  title = {Multiterm multiple prediction using separated reflections and diffractions combined with curvelet-based subtraction},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  pages = {2535-2539},
  organization = {SEG},
  abstract = {The surface-related multiple elimination (SRME) method
                  has proven to be successful on a large number of
                  data cases. Most of the applications are still 2D,
                  as the full 3D implementation is still expensive and
                  under development. However, the earth is a 3D
                  medium, such that 3D effects are difficult to
                  avoid. Most of the 3D effects come from diffractive
                  structures, whereas the specular reflections
                  normally have less of a 3D behavior. By separating
                  the seismic data in a specular reflecting and a
                  diffractive part, multiple prediction can be carried
                  out with these different subsets of the input data,
                  resulting in several categories of predicted
                  multiples. Because each category of predicted
                  multiples can be subtracted from the input data with
                  different adaptation filters, a more flexible SRME
                  procedure is obtained. Based on some initial results
                  from a Gulf of Mexico dataset, the potential of this
                  approach is investigated. {\copyright}2007 Society
                  of Exploration Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.2792993},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/verschuur07SEGmmp/verschuur07SEGmmp_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/verschuur07SEGmmp/verschuur07SEGmmp.pdf }
}


@CONFERENCE{wang2008SINBADrri,
  author = {Deli Wang and Rayan Saab and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Recent results in curvelet-based primary-multiple separation},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We present a nonlinear curvelet-based sparsity-promoting
                  formulation for the primary-multiple separation
                  problem. We show that these coherent signal
                  components can be separated robustly by explicitly
                  exploiting the locality of curvelets in phase space
                  (space-spatial frequency plane) and their ability to
                  compress data volumes that contain wavefronts. This
                  work is an extension of earlier results and the
                  presented algorithms are shown to be stable under
                  noise and moderately erroneous multiple
                  predictions.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/wang2008SINBADrri/wang2008SINBADrri.pdf}
}


@CONFERENCE{wang2007SEGrri,
  author = {Deli Wang and Rayan Saab and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Recent results in curvelet-based primary-multiple separation: application to real data},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  pages = {2500-2504},
  organization = {SEG},
  abstract = {In this abstract, we present a nonlinear curvelet-based
                  sparsity-promoting formulation for the
                  primary-multiple separation problem. We show that
                  these coherent signal components can be separated
                  robustly by explicitly exploting the locality of
                  curvelets in phase space (space-spatial frequency
                  plane) and their ability to compress data volumes
                  that contain wavefronts. This work is an extension
                  of earlier results and the presented algorithms are
                  shown to be stable under noise and moderately
                  erroneous multiple predictions. {\copyright}2007
                  Society of Exploration Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.2792986},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/wang07SEGrri/wang07SEGrri_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/wang07SEGrri/wang07SEGrri.pdf }
}


@CONFERENCE{wason2012CSEGode,
  author = {Haneet Wason and Felix J. Herrmann},
  title = {Only dither: efficient simultaneous marine acquisition},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2012},
  abstract = {Simultaneous-source acquisition is an emerging
                  technology that is stimulating both geophysical
                  research and commercial efforts. Simultaneous marine
                  acquisition calls for the development of a new set
                  of design principles and post-processing tools. The
                  focus here is on simultaneous-source marine
                  acquisition design and sparsity-promoting
                  sequential-source data recovery. We propose a
                  pragmatic simultaneous-source, randomized marine
                  acquisition scheme where multiple vessels sail
                  across an ocean-bottom array firing airguns at ---
                  sequential locations and randomly time-dithered
                  instances. By leveraging established findings from
                  the field of compressive sensing, where the choice
                  of the sparsifying transform needs to be incoherent
                  with the compressive sampling matrix, we can
                  significantly impact the reconstruction quality, and
                  demonstrate that the compressive sampling matrix
                  resulting from the proposed sampling scheme is
                  sufficiently incoherent with the curvelet transform
                  to yield successful recovery by sparsity
                  promotion. Results are illustrated with simulations
                  of “purely” random marine acquisition, which
                  requires an airgun to be located at each source
                  location, and random time-dithering marine
                  acquisition with one and two source vessels. Size of
                  the collected data volumes in all cases is the
                  same. Compared to the recovery from the former
                  acquisition scheme (SNR = 10.5dB), we get good
                  results by dithering with only one source vessel
                  (SNR = 8.06dB) in the latter scheme, which improve
                  at the cost of having an additional source vessel
                  (SNR = 9.85dB).},
  keywords = {CSEG, acquisition, marine, simultaneous},
  month = {02},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2012/wason2012CSEGode/wason2012CSEGode.pdf}
}


@CONFERENCE{wason2012EAGEode,
  author = {Haneet Wason and Felix J. Herrmann},
  title = {Only dither: efficient simultaneous marine acquisition},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2012},
  month = {06},
  abstract = {Simultaneous-source acquisition is an emerging
                  technology that is stimulating both geophysical
                  research and commercial efforts. The focus here is
                  on simultaneous-source marine acquisition design and
                  sparsity-promoting sequential-source data
                  recovery. We propose a pragmatic
                  simultaneous-source, randomized marine acquisition
                  scheme where multiple vessels sail across an
                  ocean-bottom array firing airguns at --- sequential
                  locations and randomly time-dithered
                  instances. Within the context of compressive
                  sensing, where the choice of the sparsifying
                  transform needs to be incoherent with the
                  compressive sampling matrix, we can significantly
                  impact the reconstruction quality, and demonstrate
                  that the compressive sampling matrix resulting from
                  the proposed sampling scheme is sufficiently
                  incoherent with the curvelet transform to yield
                  successful recovery by sparsity promotion. Results
                  are illustrated with simulations of ``purely" random
                  marine acquisition, which requires an airgun to be
                  located at each source location, and random
                  time-dithering marine acquisition with one and two
                  source vessels. Size of the collected data volumes
                  in all cases is the same. Compared to the recovery
                  from the former acquisition scheme (SNR = 10.5dB),
                  we get good results by dithering with only one
                  source vessel (SNR = 8.06dB) in the latter scheme,
                  which improve at the cost of having an additional
                  source vessel (SNR = 9.44dB).},
  keywords = {EAGE, acquisition, marine, simultaneous},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/wason2012EAGEode/wason2012EAGEode_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/wason2012EAGEode/wason2012EAGEode.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=58915}
}


@CONFERENCE{wason2011SEGsprsd,
  author = {Haneet Wason and Felix J. Herrmann and Tim T.Y. Lin},
  title = {Sparsity-promoting recovery from simultaneous data: a compressive sensing approach},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2011},
  month = {09},
  volume = {30},
  pages = {6-10},
  organization = {SEG},
  abstract = {Seismic data acquisition forms one of the main
                  bottlenecks in seismic imaging and inversion. The
                  high cost of acquisition work and collection of
                  massive data volumes compel the adoption of
                  simultaneous-source seismic data acquisition - an
                  emerging technology that is developing rapidly,
                  stimulating both geophysical research and commercial
                  efforts. Aimed at improving the performance of
                  marine- and land-acquisition crews, simultaneous
                  acquisition calls for development of a new set of
                  design principles and post-processing
                  tools. Leveraging developments from the field of
                  compressive sensing the focus here is on
                  simultaneous-acquisition design and
                  sequential-source data recovery. Apart from proper
                  compressive sensing sampling schemes, the recovery
                  from simultaneous simulations depends on a
                  sparsifying transform that compresses seismic data,
                  is fast, and reasonably incoherent with the
                  compressive-sampling matrix. Using the curvelet
                  transform, in which seismic data can be represented
                  parsimoniously, the recovery of the
                  sequential-source data volumes is achieved using the
                  sparsity-promoting program {\textemdash} SPGL1, a
                  solver based on projected spectral gradients. The
                  main outcome of this approach is a new technology
                  where acquisition related costs are no longer
                  determined by the stringent Nyquist sampling
                  criteria.},
  keywords = {Presentation, SEG, acquisition, compressive sensing},
  doi = {10.1190/1.3628174},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2011/wason11SEGsprsd/wason11SEGsprsd_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2011/wason11SEGsprsd/wason11SEGsprsd.pdf}
}


@CONFERENCE{yan2008SINBADwru,
  author = {Jiupeng Yan},
  title = {Wavefield reconstruction using simultaneous denoising interpolation vs. denoising after interpolation},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {This report represents and compares two methods of
                  wavefield reconstruction from noisy seismic data
                  with missing traces. The two methods are (i) First
                  interpolate incomplete noisy data to get complete
                  noisy data and then denoise, and (ii) Interpolate
                  and denoise the incomplete noisy data
                  simultaneously. A sample test of synthetic data will
                  be presented. The results of tests show that
                  denoising after interpolation is better than
                  simultaneous denoising and interpolation if the
                  parameter of the denoising problem is chosen
                  appropriately.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/yan2008SINBADwru/yan2008SINBADwru.pdf}
}


@CONFERENCE{yan2009SEGgpb,
  author = {Jiupeng Yan and Felix J. Herrmann},
  title = {Groundroll prediction by interferometry and separation by curvelet-domain matched filtering},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  month = {10},
  volume = {28},
  pages = {3297-3301},
  organization = {SEG},
  abstract = {The removal of groundroll in land based seismic data is
                  a critical step for seismic imaging. In this paper,
                  we introduce a work flow to predict the groundroll
                  by interferometry and then separate the groundroll
                  in the curvelet domain. Thus workflow is similar to
                  the workflow of surface-related multiple elimination
                  (SRME). By exploiting the adaptability and sparsity
                  of curvelets, we are able to significantly improve
                  the separation of groundroll in comparison to
                  results yielded by frequency-domain adaptive
                  subtraction methods. We provide synthetic data
                  example to illustrate our claim.},
  keywords = {Presentation, SEG},
  doi = {10.1190/1.3255544},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/yan09SEGgpb/yan09SEGgpb_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/yan09SEGgpb/yan09SEGgpb.pdf}
}


@CONFERENCE{yan2009SEGgpb2,
  author = {Jiupeng Yan and Felix J. Herrmann},
  title = {Groundroll prediction by interferometry and separation by curvelet-domain filtering},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  abstract = {The removal of groundroll in land based seismic data is
                  a critical step for seismic imaging. In this paper,
                  we introduce a work flow to predict the groundroll
                  by interferometry and then separate the groundroll
                  in the curvelet domain. Thus workflow is similar to
                  the workflow of surface-related multiple elimination
                  (SRME). By exploiting the adaptability and sparsity
                  of curvelets, we are able to significantly improve
                  the separation of groundroll in comparison to
                  results yielded by frequency-domain adaptive
                  subtraction methods. We provide synthetic data
                  example to illustrate our claim.},
  keywords = {Presentation},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/yan2009SEGgpb2/yan2009SEGgpb2.pdf}
}


@CONFERENCE{yarham2008SINBADbss,
  author = {Carson Yarham},
  title = {Bayesian signal separation applied to ground-roll removal},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {Accurate and adaptive noise removal is a critical part
                  in seismic processing. Recent developments in signal
                  separation methods have allowed a more flexible and
                  accurate framework in which to perform ground roll
                  and reflector separation. The use of a new Bayesian
                  separation scheme developed at the SLIM group that
                  contains control parameters to adjust for the
                  uniqueness of specific problems is used. The
                  sensitivity and variation of the control parameters
                  is examined and this method is applied to synthetic
                  and real data and the results are compared to
                  previous methods.},
  date-modified = {2008-08-22 12:42:58 -0700},
  keywords = {Presentation, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/yarham2008SINBADbss/yarham2008SINBADbss.pdf}
}


@CONFERENCE{yarham2007EAGEnsw,
  author = {Carson Yarham},
  title = {Nonlinear surface wave prediction and separation},
  booktitle = {EAGE 2007},
  year = {2007},
  abstract = {Removal of surface waves is an integral step in seismic
                  processing. There are many standard techniques for
                  removal of this type of coherent noise, such as f-k
                  filtering, but these methods are not always
                  effective. One of the common problems with removal
                  of surface waves is that they tend to be aliased in
                  the frequency domain. This can make removal
                  difficult and affect the frequency content of the
                  reflector signals, as this signals will not be
                  completely separated. As seen in (Hennenfent, G. and
                  F. Herrmann, 2006, Application of stable signal
                  recovery to seismic interpolation) interpolation can
                  be used effectively to resample the seismic record
                  thus dealiasing the surface waves. This separates
                  the signals in the frequency domain allowing for a
                  more precise and complete removal. The use of this
                  technique with curvelet based surface wave
                  predictions and an iterative L1 separation scheme
                  can be used to remove surface waves from shot
                  records more completely that with standard
                  techniques.},
  keywords = {Presentation, EAGE, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/yarham2007EAGEnsw/yarham2007EAGEnsw.pdf}
}


@CONFERENCE{yarham2006SEGcgrr,
  author = {Carson Yarham and Urs Boeniger and Felix J. Herrmann},
  title = {Curvelet-based ground roll removal},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2006},
  volume = {25},
  pages = {2777-2782},
  organization = {SEG},
  abstract = {We have effectively identified and removed ground roll
                  through a two-step process. The first step is to
                  identify the major components of the ground roll
                  through various methods including multiscale
                  separation, directional or frequency filtering or by
                  any other method that identifies the ground
                  roll. Given this estimate for ground roll, the
                  recorded signal is separated during the second step
                  through a block-coordinate relaxation method that
                  seeks the sparsest set for weighted curvelet
                  coefficients of the ground roll and the sought-after
                  reflectivity. The combination of these two methods
                  allows us to separate out the ground roll signal
                  while preserving the reflector information. Since
                  our method is iterative, we have control of the
                  separation process. We successfully tested our
                  algorithm on a real data set with a complex ground
                  roll and reflector structure.},
  keywords = {SEG},
  doi = {10.1190/1.2370101},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2006/yarham06SEGcgrr/yarham06SEGcgrr.pdf }
}


@CONFERENCE{yarham2007EAGEcai,
  author = {Carson Yarham and Gilles Hennenfent and Felix J. Herrmann},
  title = {Curvelet applications in surface wave removal},
  booktitle = {EAGE Workshop on Curvelets, contourlets, seislets, … in seismic data processing - where are we and where are we going?},
  year = {2007},
  month = {06},
  abstract = {Ground roll removal of seismic signals can be a
                  challenging prospect. Dealing with undersampleing
                  causing aliased waves amplitudes orders of magnitude
                  higher than reflector signals and low frequency loss
                  of information due to band ...},
  keywords = {SLIM, EAGE},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/yarham07EAGEcai/yarham07EAGEcai.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=7590}
}


@CONFERENCE{yarham2008SEGbgr,
  author = {Carson Yarham and Felix J. Herrmann},
  title = {Bayesian ground-roll seperation by curvelet-domain sparsity promotion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  pages = {2576-2580},
  organization = {SEG},
  abstract = {The removal of coherent noise generated by surface waves
                  in land based seismic is a prerequisite to imaging
                  the subsurface. These surface waves, termed as
                  ground roll, overlay important reflector information
                  in both the t-x and f-k domains. Standard techniques
                  of ground-roll removal commonly alter reflector
                  information. We propose the use of the curvelet
                  domain as a sparsifying transform in which to
                  preform signal-separation techniques that preserves
                  reflector information while increasing ground-roll
                  removal. We look at how this method preforms on
                  synthetic data for which we can build quantitative
                  results and a real field data set.},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.3063878},
  month = {11},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/yarham08SEGbgr/yarham08SEGbgr_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/yarham08SEGbgr/yarham08SEGbgr.pdf }
}


@CONFERENCE{yarham2004CSEGgrr,
  author = {Carson Yarham and Felix J. Herrmann and Daniel Trad},
  title = {Ground roll removal using non-separable wavelet transforms},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2004},
  keywords = {Presentation},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2004/Yarham04CSEGgrr/Yarham04CSEGgrr.pdf}
}


@CONFERENCE{yarham2004CSEGcpa,
  author = {Carson Yarham and Daniel Trad and Felix J. Herrmann},
  title = {Curvelet processing and imaging: adaptive ground roll removal},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2004},
  organization = {CSEG},
  abstract = {In this paper we present examples of ground roll
                  attenuation for synthetic and real data gathers by
                  using Contourlet and Curvelet transforms. These
                  non-separable wavelet transforms are locoalized both
                  (x,t)- and (k,f)-domains and allow for adaptive
                  seperation of signal and ground roll. Both linear
                  and non-linear filtering are discussed using the
                  unique properties of these basis that allow for
                  simultaneous localization in the both
                  domains. Eventhough, the linear filtering techniques
                  are encouraging the true added value of these
                  basis-function techniques becomes apparent when we
                  use these decompositions to adaptively substract
                  modeled ground roll from data using a non-linear
                  thesholding procedure. We show real and synthetic
                  examples and the results suggest that these
                  directional-selective basis functions provide a
                  usefull tool for the removal of coherent noise such
                  as ground roll.},
  keywords = {Presentation, SLIM, CSEG},
  month = {05},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2004/Yarham04CSEGcpa/Yarham04CSEGcpa_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2004/Yarham04CSEGcpa/yarham04csegcpa.pdf}
}


@CONFERENCE{yilmaz2008SINBADsse,
  author = {Ozgur Yilmaz},
  title = {Stable sparse expansions via non-convex optimization},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We present theoretical results pertaining to the ability
                  of p-(quasi)norm minimization to recover sparse and
                  compressible signals from incomplete and noisy
                  measurements. In particular, we extend the results
                  of Candes, Romberg and Tao for 1-norm to the p $\ll$
                  1 case. Our results indicate that depending on the
                  restricted isometry constants and the noise level,
                  p-norm minimization with certain values of p $\ll$ 1
                  provides better theoretical guarantees in terms of
                  stability and robustness compared to 1-norm
                  minimization. This is especially true when the
                  restricted isometry constants are relatively large,
                  or equivalently, when the data is significantly
                  undersampled.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/yilmaz2008SINBADsse/yilmaz2008SINBADsse.pdf}
}
