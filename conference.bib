%-----------------------------------2012------------------------



%----------------------------------------2011-------------------------------

@CONFERENCE{aravkin11SIAMfwi,
  author = {Aleksandr Y. Aravkin and Felix J. Herrmann and Tristan van Leeuwen
	and James V. Burke and Xiang Li},
  title = {Full Waveform Inversion with Compressive Updates},
  year = {2011},
  organization = {SIAM CS\&E 2011},
  publisher = {SIAM CS\&E 2011},
  abstract = {Full-waveform inversion relies on large multi-experiment data volumes.
	While improvements in acquisition and inversion have been extremely
	successful, the current push for higher quality models reveals fundamental
	shortcomings handling increasing problem sizes numerically. To address
	this fundamental issue, we propose a randomized dimensionality-reduction
	strategy motivated by recent developments in stochastic optimization
	and compressive sensing. In this formulation conventional Gauss-Newton
	iterations are replaced by dimensionality-reduced sparse recovery
	problems with source encodings.},
  presentation = {http://slim/Publications/Public/Presentations/2011/Aravkin2.28.2011.pdf}
}

@CONFERENCE{aravkin11ICIAMspfa,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and James Burke and
	Felix J. Herrmann},
  title = {Sparsity promoting formulations and algorithms for FWI. Presented
	at AMP Medical and Seismic Imaging, 2011, Vancouver BC.},
  year = {2011},
  organization = {ICIAM 2011},
  abstract = {Full Waveform Inversion (FWI) is a computational procedure to extract
	medium parameters from seismic data. FWI is typically formulated
	as a nonlinear least squares optimization problem, and various regularization
	techniques are used to guide the optimization because the problem
	is ill-posed. We propose a novel sparse regularization which exploits
	the ability of curvelets to efficiently represent geophysical images.
	We then formulate a corresponding sparsity promoting constrained
	optimization problem, which we solve using an open source algorithm.
	The techniques are applicable to any inverse problem where sparsity
	modeling is appropriate.
	
	We demonstrate the efficacy of the formulation on a toy example (stylized
	cross-well experiment) and on a realistic Seismic example (partial
	Marmoussi model). We also discuss the tradeoff between model fit
	and sparsity promotion, with a view to extend existing techniques
	for linear inverse problems to the case where the forward model is
	nonlinear. },
  date-added = {2011-07-15},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/aravkin2011AMP.pdf},
  url = {http://slim/Publications/Public/Presentations/2011/aravkin2011AMP.pdf}
}

@CONFERENCE{aravkin11ICIAMrfwis,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Robust FWI using Student's t-distribution. Presented at Waves 2011,
	Vancouver BC.Presented at Waves 2011, Vancouver BC.},
  year = {2011},
  organization = {ICIAM 2011},
  abstract = {Iterative inversion algorithms require repeated simulation of 3D time-dependent
	acoustic, elastic, or electromagnetic wave fields, extending hundreds
	of wavelengths and hundreds of periods. Also, seismic data is rich
	in information at every representable scale. Thus simulation-driven
	optimization approaches to inversion impose great demands on simulator
	efficiency and accuracy. While computer hardware advances have been
	of critical importance in bringing inversion closer to practical
	application, algorithmic advances in simulator methodology have been
	equally important. Speakers in this two-part session will address
	a variety of numerical issues arising in the wave simulation, and
	in its application to inversion. },
  date-added = {2011-07-20},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/aravkin2011WAVES.pdf},
  url = {http://slim/Publications/Public/Presentations/2011/aravkin2011WAVES.pdf}
}

@CONFERENCE{aravkin11EAGEnspf,
  author = {Aleksandr Y. Aravkin and James V. Burke and Felix J. Herrmann and
	Tristan van Leeuwen},
  title = {A Nonlinear Sparsity Promoting Formulation and Algorithm for Full
	Waveform Inversion},
  year = {2011},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {Full Waveform Inversion (FWI) is a computational procedure to extract
	medium parameters from seismic data. FWI is typically formulated
	as a nonlinear least squares optimization problem, and various regularization
	techniques are used to guide the optimization because the problem
	is illposed. In this paper, we propose a novel sparse regularization
	which exploits the ability of curvelets to efficiently represent
	geophysical images. We then formulate a corresponding sparsity promoting
	constrained optimization problem, which we call Nonlinear Basis Pursuit
	Denoise (NBPDN) and present an algorithm to solve this problem to
	recover medium parameters. The utility of the NBPDN formulation and
	efficacy of the algorithm are demonstrated on a stylized cross-well
	exper- iment, where a sparse velocity perturbation is recovered with
	higher quality than the standard FWI formulation (solved with LBFGS).
	The NBPDN formulation and algorithm can recover the sparse perturbation
	even when the data volume is compressed to 5 percent of the original
	size using random superposition.},
  file = {AravkinEAGE2011submit.pdf:http\://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/AravkinEAGE2011submit.pdf:PDF},
  keywords = {EAGE},
  presentations = {http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage11/aravkin2011eage.pdf
	},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/AravkinEAGE2011submit.pdf}
}

@CONFERENCE{aravkin11SEGrobust,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Robust full-waveform inversion using the Student's t-distribution},
  year = {2011},
  organization = {SEG},
  publisher = {SEG},
  abstract = {Full-waveform inversion (FWI) is a computational procedure to extract
	medium parameters from seismic data. Robust meth- ods for FWI are
	needed to overcome sensitivity to noise and in cases where modeling
	is particularly poor or far from the real data generating process.
	We survey previous robust methods from a statistical perspective,
	and use this perspective to derive a new robust method by assuming
	the random errors in our model arise from the Student's t-distribution.
	We show that in contrast to previous robust methods, the new method
	progres- sively down-weighs large outliers, effectively ignoring
	them once they are large enough. This suggests that the new method
	is more robust and suitable for situations with very poor data quality
	or modeling. Experiments show that the new method recovers as well
	or better than previous robust methods, and can recover models with
	quality comparable to standard meth- ods on noise-free data when
	some of the data is completely corrupted, and even when a marine
	acquisition mask is entirely ignored in the modeling. The ability
	to ignore a marine acqui- sition mask via robust FWI methods offers
	an opportunity for stochastic optimization methods in marine acquisition.},
  keywords = {SEG},
  timestamp = {2011-04-06 15:00:00 -0700},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/aravkin2011seg.pdf}
}

@CONFERENCE{herrmann11ICIAMconvexcompfwi,
  author = {Felix J. Herrmann and Aleksandr Y. Aravkin and Tristan van Leeuwen
	and Xiang Li},
  title = {FWI with sparse recovery: a convex-composite approach},
  year = {2011},
  organization = {ICIAM 2011},
  abstract = {Iterative inversion algorithms require repeated simulation of 3D time-dependent
	acoustic, elastic, or electromagnetic wave fields, extending hundreds
	of wavelengths and hundreds of periods. Also, seismic data is rich
	in information at every representable scale. Thus simulation-driven
	optimization approaches to inversion impose great demands on simulator
	efficiency and accuracy. While computer hardware advances have been
	of critical importance in bringing inversion closer to practical
	application, algorithmic advances in simulator methodology have been
	equally important. Speakers in this two-part session will address
	a variety of numerical issues arising in the wave simulation, and
	in its application to inversion. },
  date-added = {2011-07-20},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/herrmann2011ICIAM.pdf},
  url = {http://slim/Publications/Public/Presentations/2011/herrmann2011ICIAM.pdf}
}

@CONFERENCE{Herrmann11fwi,
  author = {Felix J. Herrmann and Aleksandr Y. Aravkin and Xiang Li and Tristan
	van Leeuwen},
  title = {Full Waveform Inversion with Compressive Updates},
  year = {2011},
  organization = {Sparse and Low Rank Approximation 2011},
  publisher = {Sparse and Low Rank Approximation 2011},
  abstract = {Full-waveform inversion relies on large multi-experiment data volumes.
	While improvements in acquisition and inversion have been extremely
	successful, the current push for higher quality models reveals fundamental
	shortcomings handling increasing problem sizes numerically. To address
	this fundamental issue, we propose a randomized dimensionality-reduction
	strategy motivated by recent developments in stochastic optimization
	and compressive sensing. In this formulation conventional Gauss-Newton
	iterations are replaced by dimensionality-reduced sparse recovery
	problems with source encodings.},
  presentation = {http://slim/Publications/Public/Presentations/2011/Herrmann2011css.pdf}
}

@CONFERENCE{Herrmann11EAGEefmsp,
  author = {Felix J. Herrmann and Xiang Li},
  title = {Efficient least-squares migration with sparsity promotion},
  year = {2011},
  organization = {EAGE},
  publisher = {EAGE Technical Program Expanded Abstracts},
  abstract = {Seismic imaging relies on the collection of multi-experimental data
	volumes in combination with a sophisticated back-end to create high-fidelity
	inversion results. While significant improve- ments have been made
	in linearized inversion, the current trend of incessantly pushing
	for higher quality models in increasingly complicated regions reveals
	fundamental shortcomings in handling increasing problem sizes numerically.
	The so-called “curse of dimensionality” is the main culprit because
	it leads to an exponential growth in the number of sources and the
	corresponding number of wavefield simulations required by ‘wave-equation’
	migration. We address this issue by reducing the number of sources
	by a randomized dimensionality reduction technique that combines
	recent developments in stochastic optimization and compressive sensing.
	As a result, we replace the cur- rent formulations of imaging that
	rely on all data by a sequence of smaller imaging problems that use
	the output of the previous inversion as input for the next. Empirically,
	we find speedups of at least one order-of-magnitude when each reduced
	experiment is considered theoretically as a separate compressive-sensing
	experiment.},
  file = {herrmann11eage.pdf:http\://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/herrmann11eage.pdf:PDF},
  keywords = {EAGE},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage11/herrmann2011eage.pdf},
  timestamp = {2011-01-14},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/herrmann11eage.pdf}
}


@CONFERENCE{herrmann11SLIMsummer1,
  author = {Felix J. Herrmann},
  title = {Gene Golub SIAM Summer School July 4 - 15, 2011},
  year = {2011},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/HerrmannLecture1.pdf},
  timestamp = {2011.08.05}
}

@CONFERENCE{herrmann11SLIMsummer2,
  author = {Felix J. Herrmann},
  title = {Lecture 2. Gene Golub SIAM Summer School July 4 - 15, 2011},
  year = {2011},
  owner = {shruti},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/HerrmannLecture2.pdf},
  timestamp = {2011.08.05}
}


@CONFERENCE{Jumah11SEGdrepsi,
  author = {Bander Jumah and Felix J. Herrmann},
  title = {Dimensionality-reduced estimation of primaries by sparse inversion},
  year = {2011},
  organization = {SEG},
  publisher = {SEG Technical Program Expanded Abstracts},
  abstract = {Data-driven methods—such as the estimation of primaries by sparse
	inversion—suffer from the ’curse of dimensionality’, which leads
	to disproportional growth in computational and storage demands when
	moving to realistic 3-D field data. To re- move this fundamental
	impediment, we propose a dimensional- ity reduction technique where
	the ’data matrix’ is approximated adaptively by a randomized low-rank
	approximation. Com- pared to conventional methods, our approach has
	the advantage that the cost of the low-rank approximation is reduced
	signif- icantly, which may lead to considerable reductions in storage
	and computational costs of the sparse inversion. Application of the
	proposed formalism to synthetic data shows that significant improvements
	are achievable at low computational overhead required to compute
	the low-rank approximations.},
  date-added = {2011-04-06 15:00:00 -0700},
  keywords = {SEG},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/jumah2011seg.pdf}
}


@CONFERENCE{li11EAGEfwirr,
  author = {Xiang Li and Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix
	J. Herrmann},
  title = {Full-waveform inversion with randomized L1 recovery for the model
	updates},
  year = {2011},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {Full-waveform inversion (FWI) is a data fitting procedure that relies
	on the collection of seis- mic data volumes and sophisticated computing
	to create high-resolution results. With the advent of FWI, the improvements
	in acquisition and inversion have been substantial, but these improvements
	come at a high cost because FWI involves extremely large multi-experiment
	data volumes. The main obstacle is the ‘curse of dimensionality’
	exemplified by Nyquist’s sampling criterion, which puts a disproportionate
	strain on current acquisition and processing systems as the size
	and desired resolution increases. In this paper, we address the ‘curse
	of dimensionality’ by randomized dimen- sionality reduction of the
	FWI problem adapted from the field of CS. We invert for model updates
	by replacing the Gauss-Newton linearized subproblem for subsampled
	FWI with a sparsity promoting formulation, and solve this formulation
	using the SPGl1 algorithm. We speed up the algorithm and avoid overfitting
	the data by solving for the linearized updates only approximately.
	Our approach is successful because it reduces the size of seismic
	data volumes without loss of information. With this reduction, we
	can compute a Newton-like update with the reduced data volume at
	the cost of roughly one gradient update for the fully sampled wavefield.},
  file = {li11eage.pdf:http\://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/li11eage.pdf:PDF},
  keywords = {EAGE},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage11/xiangli2011eage.pdf},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/li11eage.pdf}
}


@CONFERENCE{li11SBGFmgnsu,
  author = {Xiang Li and Felix J. Herrmann and Tristan van Leeuwen and Aleksandr
	Y. Aravkin},
  title = {Modified Gauss-Newton with Sparse Updates},
  year = {2011},
  organization = {SBGF},
  publisher = {Submitted to SBGF},
  abstract = {Full-waveform inversion (FWI) is a data fitting procedure that relies
	on the collection of seismic data volumes and sophisticated computing
	to create high-resolution models.With the advent of FWI, the improvements
	in acquisition and inversion have been substantial, but these improvements
	come at a high cost because FWI involves extremely large multi-experiment
	data volumes. The main obstacle is the {\textquoteleft}curse of dimensionality{\textquoteright}
	exemplified by Nyquist{\textquoteright}s sampling criterion, which
	puts a disproportionate strain on current acquisition and processing
	systems as the size and desired resolution increases. In this paper,
	we address the {\textquoteleft}curse of dimensionality{\textquoteright}
	by using randomized dimensionality reduction of the FWI problem,
	coupled with a modified Gauss-Newton (GN) method designed to promote
	curvelet-domain sparsity of model updates. We solve for these updates
	using the spectral projected gradient method, implemented in the
	SPGÔøø1 software package. Our approach is successful because it reduces
	the size of seismic data volumes without loss of information. With
	this reduction, we can compute Gauss-Newton updates with the reduced
	data volume at the cost of roughly one gradient update for the fully
	sampled wavefield},
  keywords = {SBGF},
  url = {http://slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2011/li2011sbgf.pdf}
}

@CONFERENCE{lin11EAGEepsic,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Estimating primaries by sparse inversion in a curvelet-like representation
	domain},
  year = {2011},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {We present an uplift in the fidelity and wavefront continuity of results
	obtained from the Estimation of Primaries by Sparse Inversion (EPSI)
	program by reconstructing the primary events in a hybrid wavelet-curvelet
	representation domain. EPSI is a multiple removal technique that
	belongs to the class of wavefield inversion methods, as an alternative
	to the traditional adaptive-subtraction process. The main assumption
	is that the correct primary events should be as sparsely-populated
	in time as possible. A convex reformulation of the original EPSI
	algorithm allows its convergence property to be preserved even when
	the solution wavefield is not formed in the physical domain. Since
	wavefronts and edge-type singularities are sparsely represented in
	the curvelet domain, sparse solutions formed in this domain will
	exhibit vastly improved continuity when compared to those formed
	in the physical domain, especially for the low-energy events at later
	arrival times. Further- more, a wavelet-type representation domain
	will preserve sparsity in the reflected events even if they originate
	from non-zero-order discontinuities in the subsurface, providing
	an additional level of robustness. This method does not require any
	changes in the underlying computational algorithm and does not explicitly
	impose continuity constraints on each update.},
  file = {lin11eage.pdf:http\://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/lin11eage.pdf:PDF},
  keywords = {EAGE},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage11/lin2011eage.pdf},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/lin11eage.pdf}
}

@CONFERENCE{lin11SEGrssde,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Robust source signature deconvolution and the estimation of primaries
	by sparse inversion},
  booktitle = {SEG},
  year = {2011},
  organization = {Dept. of Earth and Ocean Sciences, University of British Columbia},
  publisher = {Dept. of Earth and Ocean Sciences, University of British Columbia},
  keywords = {deconvolution, SEG, sparse inversion},
  url = {https://slimweb.eos.ubc.ca/sites/data/Papers/timseg2011.pdf}
}




@CONFERENCE{Mansour11SBGFcspsma,
  author = {Hassan Mansour and Haneet Wason and Tim T.Y. Lin and Felix J. Herrmann},
  title = {A compressive sensing perspective on simultaneous marine acquisition},
  year = {2011},
  organization = {SBGF},
  publisher = {SBGF},
  abstract = {The high cost of acquiring seismic data in Marine environments compels
	the adoption of simultaneous- source acquisition - an emerging technology
	that is stimulating both geophysical research and commercial efforts.
	In this paper, we discuss the properties of randomized simultaneous
	acquisition matrices and demonstrate that sparsity-promoting recovery
	improves the quality of the reconstructed seismic data volumes. Simultaneous
	Marine acquisition calls for the development of a new set of design
	principles and post-processing tools. Leveraging established findings
	from the field of compressed sensing, the recovery from simultaneous
	sources depends on a sparsifying transform that compresses seismic
	data, is fast, and reasonably incoherent with the compressive sampling
	matrix. To achieve this incoherence, we use random time dithering
	where sequential acquisition with a single airgun is replaced by
	continuous acquisition with multiple airguns firing at random times
	and at random locations. We demonstrate our results with simulations
	of simultaneous Marine acquisition using periodic and randomized
	time dithering.},
  keywords = {SBGF},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/mansour2011.pdf}
}


@CONFERENCE{vanleeuwen11SIAMGEOmawt,
  author = {Tristan van Leeuwen and Wim Mulder},
  title = {Multiscale aspects of waveform tomography},
  year = {2011},
  organization = {SIAM GeoSciences 2011},
  publisher = {SIAM GeoSciences 2011},
  abstract = {We consider the inference of medium velocity from transmitted acoustic
	waves. Typically, the measurements are done in a narrow frequency
	band. As a result the sensitivity of the data with respect to velocity
	perturbations varies dramatically with the scale of the perturbation.
	{\textquoteleft}Smooth{\textquoteright} perturbations will cause
	a phase shift, whereas perturbations that vary on the wavelength-scale
	cause amplitude variations. We investigate how to incorporate this
	scale dependent behavior in the formulation of the inverse problem.},
  presentation = {http://slim/Publications/Public/Presentations/2011/SIAMGS11_MS61_Leeuwen.pdf}
}


@CONFERENCE{tu11SEGmult,
  author = {Ning Tu and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Migration with surface-related multiples from incomplete seismic
	data},
  year = {2011},
  organization = {SEG},
  publisher = {SEG},
  abstract = {Seismic acquisition is confined by limited aperture that leads to
	finite illumination, which, together with other factors, hin- ders
	imaging of subsurface objects in complex geological set- tings such
	as salt structures. Conventional processing, includ- ing surface-related
	multiple elimination, further reduces the amount of information we
	can get from seismic data. With the growing consensus that multiples
	carry valuable informa- tion that is missing from primaries, we are
	motivated to exploit the extra illumination provided by multiples
	to image the sub- surface. In earlier research, we proposed such
	a method by combining primary estimation and sparsity-promoting migra-
	tion to invert for model perturbations directly from the total up-going
	wavefield. In this abstract, we focus on a particular case. By exploiting
	the extra illumination from surface-related multiples, we mitigate
	the effects caused by migrating from in- complete data with missing
	sources and missing near-offsets.},
  keywords = {SEG},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/tu2011seg.pdf}
}


@CONFERENCE{tu11EAGEspmsrm,
  author = {Ning Tu and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Sparsity-promoting migration with surface-related multiples},
  year = {2011},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {Multiples, especially the surface-related multiples, form a significant
	part of the total up-going wave- field. If not properly dealt with,
	they can lead to false reflectors in the final image. So conventionally
	practitioners remove them prior to migration. Recently research has
	revealed that multiples can actually provide extra illumination so
	different methods are proposed to address the issue that how to use
	multiples in seismic imaging, but with various kinds of limitations.
	In this abstract, we combine primary estimation and sparsity-promoting
	migration into one convex-optimization process to include information
	from multiples. Synthetic examples show that multiples do make active
	contributions to seismic migration. Also by this combination, we
	can benefit from better recoveries of the Green’s function by using
	sparsity-promoting algorithms since reflectivity is sparser than
	the Green’s function.},
  file = {tu11eage.pdf:http\://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/tu11eage.pdf:PDF},
  keywords = {EAGE},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage11/tu2011eage.pdf
	},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/tu11eage.pdf}
}


@CONFERENCE{vanleeuwen11AMPhsdmwi,
  author = {Tristan van Leeuwen and Mark Shmidt and Michael P. Friedlander and
	Felix J. Herrmann.},
  title = {A hybrid stocahstic-deterministic method for waveform inversion.Presented
	at AMP Medical and Seismic Imaging, 2011, Vancouver BC.},
  year = {2011},
  organization = {WAVES 2011},
  abstract = {A lot of seismic and medical imaging problems can be written as a
	least-squares data- fitting problem. In particular, we consider the
	case of multi-experiment data, where the data consists of a large
	number of ‘independent’ measurements. Solving the inverse prob- lem
	then involves repeatedly forward modeling the data for each of these
	experiments. In case the number of experiments is large and the modeling
	kernel expensive to apply, such an approach may be prohibitively
	expensive. We review techniques from stochastic opti- mization which
	aim at dramatically reducing the number of experiments that need
	to be modeled at each iteration. This reduction is typically achieved
	by randomly subsampling the data. Special care needs to be taken
	in the optimization to deal with the stochasticity that is introduced
	in this way. },
  date-added = {2011-07-15},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/AMPleeuwen2011b.pdf},
  url = {http://slim/Publications/Public/Presentations/2011/AMPleeuwen2011b.pdf}
}



@CONFERENCE{vanleeuwen11EAGEhsdomwi,
  author = {Tristan van Leeuwen and Felix J. Herrmann and Mark Schmidt and Michael
	P. Friedlander},
  title = {A hybrid stochastic-deterministic optimization method for waveform
	inversion},
  year = {2011},
  organization = {EAGE},
  publisher = {EAGE Technical Program Expanded Abstracts},
  abstract = {Present-day high quality 3D acquisition can give us lower frequencies
	and longer offsets with which to invert. However, the computational
	costs involved in handling this data explosion are tremendous. Therefore,
	recent developments in full-waveform inversion have been geared towards
	reducing the computational costs involved. A key aspect of several
	approaches that have been proposed is a dramatic reduction in the
	number of sources used in each iteration. A reduction in the number
	of sources directly translates to less PDE-solves and hence a lower
	computational cost. Re- cent attention has been drawn towards reducing
	the sources by randomly combining the sources in to a few supershots,
	but other strategies are also possible. In all cases, the full data
	misfit, which involves all the sequential sources, is replaced by
	a reduced misfit that is much cheaper to evaluate because it involves
	only a small number of sources (batchsize). The batchsize controls
	the accuracy with which the reduced misfit approximates the full
	misfit. The optimization of such an inaccurate, or noisy, misfit
	is the topic of stochastic optimization. In this paper, we propose
	an optimization strategy that borrows ideas from the field of stochastic
	optimization. The main idea is that in the early stage of the optimization,
	far from the true model, we do not need a very accurate misfit. The
	strategy consists of gradually increasing the batchsize as the iterations
	proceed. We test the proposed strategy on a synthetic dataset. We
	achieve a very reasonable inversion result at the cost of roughly
	13 evaluations of the full misfit. We observe a speed-up of roughly
	a factor 20.},
  keywords = {EAGE},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage11/vanleeuwen2011eage.pdf},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/vanleeuwen11eage.pdf}
}

@CONFERENCE{vanleeuwen11WAVESpeiv,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {Probing the extended image volume for seismic velocity inversion},
  year = {2011},
  organization = {Waves 2011},
  abstract = {In seismic velocity inversion one aims to reconstruct a kinematically
	correct subsurface velocity model that can be used as input for further
	processing and inversion of the data. An important tool in velocity
	inversion is the prestack image volume. This image volume can be
	defined as a cross- correlation of the source and receivers wavefields
	for non-zero space and time lags. If the background velocity is kinematically
	acceptable, this image volume will have its main contributions at
	zero lag, even for complex models. Thus, it is an ideal tool for
	wave-equation migration velocity analysis in the presence of strong
	lateral heterogeneity. In particular, it allows us to pose migration
	velocity analysis as a PDE- constrained optimization problem, where
	the goal is to minimize the energy in the image volume at non-zero
	lag subject to fitting the data approximately. However, it is computationally
	infeasible to explicitly form the whole image volume. In this paper,
	we discuss several ways to reduce the computational costs involved
	in computing the image volume and evaluating the focusing criterion.
	We reduce the costs for calculating the data by randomized source
	synthesis. We also present an efficient way to subsample the image
	volume. Finally, we propose an alternative optimization criterion
	and suggest a multiscale inversion strategy for wave-equation MVA.
	},
  date-added = {2011-07-29},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/waves11leeuwen.pdf},
  url = {http://slim/Publications/Public/Presentations/2011/waves11leeuwen.pdf}
}
@CONFERENCE{vanleeuwen11SEGext,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {Probing the extended image volume},
  year = {2011},
  organization = {SEG},
  publisher = {SEG},
  abstract = {The prestack image volume can be defined as a cross- correlation of
	the source and receivers wavefields for non-zero space and time lags.
	If the background velocity is kinemati- cally acceptable, this image
	volume will have its main contri- butions at zero lag, even for complex
	models. Thus, it is an ideal tool for wave-equation migration velocity
	analysis in the presence of strong lateral heterogeneity. In particular,
	it allows us to pose migration velocity analysis as a PDE-constrained
	optimization problem, where the goal is to minimize the en- ergy
	in the image volume at non-zero lag subject to fitting the data approximately.
	However, it is computationally infeasi- ble to explicitly form the
	whole image volume. In this paper, we discuss several ways to reduce
	the computational costs in- volved in computing the image volume
	and evaluating the fo- cusing criterion. We reduce the costs for
	calculating the data by randomized source synthesis. We also present
	an efficient way to subsample the image volume. Finally, we propose
	an alternative optimization criterion and suggest a multiscale in-
	version strategy for wave-equation MVA.},
  keywords = {SEG},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/vanleeuwen2011seg.pdf}
}

@CONFERENCE{vanleeuwen11ICIAMcbmcwe,
  author = {Tristan van Leeuwen},
  title = {A correlation-based misfit criterion for wave-equation traveltime
	tomography.Presented at ICIAM 2011, Vancouver BC.},
  year = {2011},
  organization = {ICIAM 2011},
  abstract = {The inference of subsurface medium parameters from seismic data can
	be posed as a PDE-constrained data-fitting procedure. This approach
	is successful in reconstructing medium perturbations that are in
	the order of the wavelength. In practice, the data lack low frequency
	content and this means that one needs a good initial guess of the
	slowly varying component of the medium. For a wrong starting model
	an iterative reconstruction procedure is likely to end up in a local
	minimum. We propose to use a different measure of the misfit that
	makes the optimization problem well-posed in terms of the slowly
	varying velocity structures. This procedure can be seen as a generalization
	of ray-based traveltime tomography. We discuss the theoretical underpinnings
	of the method and give some numerical examples. },
  date-added = {2011-07-19},
  file = {:http\://slim/Publications/Public/Presentations/2011/ICIAM11leeuwen.pdf:PDF},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/ICIAM11leeuwen.pdf},
  url = {http://slim/Publications/Public/Presentations/2011/ICIAM11leeuwen.pdf}
}


@CONFERENCE{wason11SEGsprsd,
  author = {Haneet Wason and Felix J. Herrmann and Tim T.Y. Lin},
  title = {Sparsity-promoting recovery from simultaneous data: a compressive
	sensing approach},
  year = {2011},
  organization = {SEG},
  publisher = {SEG},
  abstract = {Seismic data acquisition forms one of the main bottlenecks in seismic
	imaging and inversion. The high cost of acquisition work and collection
	of massive data volumes compel the adoption of simultaneous-source
	seismic data acquisition - an emerging technology that is developing
	rapidly, stimulating both geophysical research and commercial efforts.
	Aimed at improving the performance of marine- and land-acquisition
	crews, simultaneous acquisition calls for development of a new set
	of design principles and post-processing tools. Leveraging developments
	from the field of compressive sensing the focus here is on simultaneous-acquisition
	design and sequential-source data recovery. Apart from proper compressive
	sensing sampling schemes, the recovery from simultaneous simulations
	depends on a sparsifying transform that compresses seismic data,
	is fast, and reasonably incoherent with the compressive-sampling
	matrix. Using the curvelet transform, in which seismic data can be
	represented parsimoniously, the recovery of the sequential-source
	data volumes is achieved using the sparsity-promoting program {\textemdash}
	SPGL1, a solver based on projected spectral gradients. The main outcome
	of this approach is a new technology where acquisition related costs
	are no longer determined by the stringent Nyquist sampling criteria.},
  file = {:http\://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/wason2011seg.pdf:PDF},
  keywords = {SEG},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/wason2011seg.pdf}
}
%-------------------------------------------2010------------------------------------------

@CONFERENCE{frijlink10EAGEcos,
  author = {M.O. Frijlink and Reza Shahidi and Felix J. Herrmann and R.G. van
	Borselen},
  title = {Comparison of Standard Adaptive Subtraction and Primary-multiple
	Separation in the Curvelet Domain},
  year = {2010},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {In recent years, data-driven multiple prediction methods and wavefield
	extrapolation methods have proven to be powerful methods to attenuate
	multiples from data acquired in complex 3-D geologic environments.
	These methods make use of a two-stage approach, where first the multiples
	(surface-related and / or internal) multiples are predicted before
	they are subtracted from the original input data in an adaptively.
	The quality of these predicted multiples often raises high expectations
	for the adaptive subtraction techniques, but for various reasons
	these expectations are not always met in practice. Standard adaptive
	subtraction methods use the well-known minimum energy criterion,
	stating that the total energy after optimal multiple attenuation
	should be minimal. When primaries and multiples interfere , the minimum
	energy criterion is no longer appropriate. Also, when multiples of
	different orders interfere, adaptive energy minimization will lead
	to a compromise between different amplitudes corrections for the
	different orders of multiples. This paper investigates the performance
	of two multiple subtraction schemes for a real data set that exhibits
	both interference problems. Results from an adaptive subtraction
	in the real curvelet domain, separating primaries and multiples,
	are compared to those obtained using a more conventional adaptive
	subtraction method in the spatial domain.},
  file = {frijlink10csa.pdf:http\://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/frijlink10csa.pdf:PDF},
  keywords = {EAGE},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/frijlink10csa.pdf}
}

@CONFERENCE{herrmann10MATHIAScssr,
  author = {Felix J. Herrmann},
  title = {Compressive Sensing and Sparse Recovery in Exploration Seismology.
	Presented at MATHIAS 2010 organized by Total SA. Paris.},
  year = {2010},
  abstract = {During this presentation, I will talk about how recent results from
	compressive sensing and sparse recovery can be used to solve problems
	in exploration seismology where incomplete sampling is ubiquitous.
	I will also talk about how these ideas apply to dimensionality reduction
	of full-waveform inversion by randomly phase encoded sources.},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2010/herrmann2010total.pdf}
}

@CONFERENCE{herrmann10EAGErds,
  author = {Felix J. Herrmann and Xiang Li},
  title = {Randomized dimensionality reduction for full-waveform inversion},
  year = {2010},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {Full-waveform inversion relies on the collection of large multi-experiment
	data volumes in combination with a sophisticated back-end to create
	high-fidelity inversion results. While improvements in acquisition
	and inversion have been extremely successful, the current trend of
	incessantly pushing for higher quality models in increasingly complicated
	regions of the Earth continues to reveal fundamental shortcomings
	in our ability to handle the ever increasing problem size numerically.
	Two causes can be identified as the main culprits responsible for
	this barrier. First, there is the so-called {\textquoteleft}{\textquoteleft}curse
	of dimensionality{\textquoteright}{\textquoteright} exemplified by
	Nyquist{\textquoteright}s sampling criterion, which puts disproportionate
	strain on current acquisition and processing systems as the size
	and desired resolution of our survey areas continues to increase.
	Secondly, there is the recent {\textquoteleft}{\textquoteleft}departure
	from Moore{\textquoteright}s law{\textquoteright}{\textquoteright}
	that forces us to lower our expectations to compute ourselves out
	of this. In this paper, we address this situation by randomized dimensionality
	reduction, which we adapt from the field of compressive sensing.
	In this approach, we combine deliberate randomized subsampling with
	structure-exploiting transform-domain sparsity promotion. Our approach
	is successful because it reduces the size of seismic data volumes
	without loss of information. With this reduction, we compute Newton-like
	updates at the cost of roughly one gradient update for the fully-sampled
	wavefield.},
  keywords = {EAGE},
  presentation = {http://slim.eos.ubc.ca/Publications/public/presentations/eage/eage10/herrmannEAGE2010rdr.pdf
	},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/herrmann10rds.pdf}
}

@CONFERENCE{herrmann10EAGErss,
  author = {Felix J. Herrmann},
  title = {Randomized sampling strategies},
  year = {2010},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {Seismic exploration relies on the collection of massive data volumes
	that are subsequently mined for information during seismic processing.
	While this approach has been extremely successful in the past, the
	current trend towards higher quality images in increasingly complicated
	regions continues to reveal fundamental shortcomings in our workflows
	for high-dimensional data volumes. Two causes can be identified..
	First, there is the so-called {\textquoteleft}{\textquoteleft}curse
	of dimensionality{\textquoteright}{\textquoteright} exemplified by
	Nyquist{\textquoteright}s sampling criterion, which puts disproportionate
	strain on current acquisition and processing systems as the size
	and desired resolution of our survey areas continues to increase.
	Secondly, there is the recent {\textquoteleft}{\textquoteleft}departure
	from Moore{\textquoteright}s law{\textquoteright}{\textquoteright}
	that forces us to lower our expectations to compute ourselves out
	of this curse of dimensionality. In this paper, we offer a way out
	of this situation by a deliberate randomized subsampling combined
	with structure-exploiting transform-domain sparsity promotion. Our
	approach is successful because it reduces the size of seismic data
	volumes without loss of information. As such we end up with a new
	technology where the costs of acquisition and processing are no longer
	dictated by the size of the acquisition but by the transform-domain
	sparsity of the end-product.},
  keywords = {EAGE},
  presentation = {http://slim.eos.ubc.ca/Publications/public/presentations/eage/eage10/herrmannEAGE2010rss.pdf
	},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/herrmann10rss.pdf}
}



@CONFERENCE{herrmann10IRISsns,
  author = {Felix J. Herrmann},
  title = {Sub-Nyquist sampling and sparsity: getting more information from
	fewer samples. Presented at the IRIS Workshop},
  year = {2010},
  abstract = {Many seismic exploration techniques rely on the collection of massive
	data volumes. While this approach has been extremely successful in
	the past, current efforts toward higher resolution images in increasingly
	complicated regions of the Earth continue to reveal fundamental shortcomings
	in our workflows. Chiefly amongst these is the so-called {\textquoteleft}{\textquoteleft}curse
	of dimensionality{\textquoteright}{\textquoteright} exemplified by
	Nyquist{\textquoteright}s sampling criterion, which disproportionately
	strains current acquisition and processing systems as the size and
	desired resolution of our survey areas continues to increase. In
	this presentation, we offer an alternative sampling method leveraging
	recent insights from compressive sensing towards seismic acquisition
	and processing of severely under-sampled data. The main outcome of
	this approach is a new technology where acquisition and processing
	related costs are no longer determined by overly stringent sampling
	criteria, such as Nyquist. At the heart of our approach lies randomized
	incoherent sampling that breaks subsampling related interferences
	by turning them into harmless noise, which we subsequently remove
	by promoting transform-domain sparsity. Now, costs no longer grow
	significantly with resolution and dimensionality of the survey area,
	but instead depend on transform-domain sparsity only. Our contribution
	is twofold. First, we demonstrate by means of carefully designed
	numerical experiments that compressive sensing can successfully be
	adapted to seismic exploration. Second, we show that accurate recovery
	can be accomplished for compressively sampled data volumes sizes
	that exceed the size of conventional transform-domain data volumes
	by only a small factor. Because compressive sensing combines transformation
	and encoding by a single linear encoding step, this technology is
	directly applicable to acquisition and to dimensionality reduction
	during processing. In either case, sampling, storage, and processing
	costs scale with transform-domain sparsity. Many seismic exploration
	techniques rely on the collection of massive data volumes. While
	this approach has been extremely successful in the past, current
	efforts toward higher resolution images in increasingly complicated
	regions of the Earth continue to reveal fundamental shortcomings
	in our workflows. Chiefly amongst these is the so-called {\textquoteleft}{\textquoteleft}curse
	of dimensionality{\textquoteright}{\textquoteright} exemplified by
	Nyquist{\textquoteright}s sampling criterion, which disproportionately
	strains current acquisition and processing systems as the size and
	desired resolution of our survey areas continues to increase. In
	this presentation, we offer an alternative sampling method leveraging
	recent insights from compressive sensing towards seismic acquisition
	and processing of severely under-sampled data. The main outcome of
	this approach is a new technology where acquisition and processing
	related costs are no longer determined by overly stringent sampling
	criteria, such as Nyquist. At the heart of our approach lies randomized
	incoherent sampling that breaks subsampling related interferences
	by turning them into harmless noise, which we subsequently remove
	by promoting transform-domain sparsity. Now, costs no longer grow
	significantly with resolution and dimensionality of the survey area,
	but instead depend on transform-domain sparsity only. Our contribution
	is twofold. First, we demonstrate by means of carefully designed
	numerical experiments that compressive sensing can successfully be
	adapted to seismic exploration. Second, we show that accurate recovery
	can be accomplished for compressively sampled data volumes sizes
	that exceed the size of conventional transform-domain data volumes
	by only a small factor. Because compressive sensing combines transformation
	and encoding by a single linear encoding step, this technology is
	directly applicable to acquisition and to dimensionality reduction
	during processing. In either case, sampling, storage, and processing
	costs scale with transform-domain sparsity.},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2010/herrmann2010iris.pdf}
}

@CONFERENCE{johnson10EAGEeop,
  author = {James Johnson and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Estimation of primaries via sparse inversion with reciprocity},
  year = {2010},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {Accurate removal of surface related multiples is a key step in seismic
	data processing. The industry standard for removing multiples is
	SRME, which involves convolving the data with itself to predict the
	multiples, followed by an adaptive subtraction procedure to recover
	the primaries (Verschuur and Berkhout, 1997). Other methods involve
	multidimensional division of the up-going and down-going wavefields
	(Amundsen, 2001). However, this approach may suffer from stability
	problems. With the introduction of the {\textquoteleft}{\textquoteleft}estimation
	of primaries by sparse inversion{\textquoteright}{\textquoteright}(EPSI),
	van Groenestijn and Verschuur (2009) recentely reformulated SRME
	to jointly estimate the surface-free impulse response and the source
	signature directly from the data. The advantage of EPSI is that it
	recovers the primary response directly, and does not require a second
	processing step for the subtraction of estimated multiples from the
	original data. However, because it estimates both the primary impulse
	response and source signature from the data EPSI must be regularized.
	Motivated by recent successful application of the curvelet transform
	in seismic data processing (Herrmann et al., 2007), we formulate
	EPSI as a bi-convex optimization problem that seeks sparsity on the
	surface-free Green{\textquoteright}s function and Fourier-domain
	smoothness on the source wavelet. Our main contribution compared
	to previous work (Lin and Herrmann, 2009), and the contribution of
	that author to the proceedings of this meeting(Lin and Herrmann,
	2010), is that we employ the physical principle of as source-receiver
	reciprocity to improve the inversion.},
  keywords = {EAGE},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/johnson10eop.pdf}
}

@CONFERENCE{li10SEGfwi,
  author = {Xiang Li and Felix J. Herrmann},
  title = {Full-waveform inversion from compressively recovered updates},
  year = {2010},
  organization = {SEG},
  publisher = {SEG},
  abstract = {Full-waveform inversion relies on the collection of large multi-experiment
	data volumes in combination with a sophisticated back-end to create
	high-fidelity inversion results. While improvements in acquisition
	and inversion have been extremely successful, the current trend of
	incessantly pushing for higher quality models in increasingly complicated
	regions of the Earth reveals fundamental shortcomings in our ability
	to handle increasing problem size numerically. Two main culprits
	can be identified. First, there is the so-called {\textquoteleft}{\textquoteleft}curse
	of dimensionality{\textquoteright}{\textquoteright} exemplified by
	Nyquist{\textquoteright}s sampling criterion, which puts disproportionate
	strain on current acquisition and processing systems as the size
	and desired resolution increases. Secondly, there is the recent {\textquoteleft}{\textquoteleft}departure
	from Moore{\textquoteright}s law{\textquoteright}{\textquoteright}
	that forces us to lower our expectations to compute ourselves out
	of this. In this paper, we address this situation by randomized dimensionality
	reduction, which we adapt from the field of compressive sensing.
	In this approach, we combine deliberate randomized subsampling with
	structure-exploiting transform-domain sparsity promotion. Our approach
	is successful because it reduces the size of seismic data volumes
	without loss of information. With this reduction, we compute Newton-like
	updates at the cost of roughly one gradient update for the fully-sampled
	wavefield.},
  keywords = {SEG},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg10/Li2010fcr.pdf},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/li10fwi.pdf}
}


@CONFERENCE{lin10EAGEseo,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Stabilization of estimation of primaries via sparse inversion},
  year = {2010},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {Recent works on surface-related multiple removal include a direct
	estimation method proposed by van Groenestijn and Verschuur (2009),
	where under a sparsity assumption the primary impulse response is
	determined directly from a data-driven wavefield inversion process
	called Estimation of Primaries by Sparse Inversion (EPSI). The authors
	have shown that this approach is superior to traditional estimation
	subtraction processes such as SRME on shallow bottom marine data,
	where by expanding the model to simultaneously invert for the near-offset
	traces, which are not directly available in most situation but are
	observable in the data multiples, a large improvement over Radon
	interpolation is demonstrated. One of the major roadblocks to the
	widespread adoption of EPSI is that one must have precise knowledge
	of a time-window that contains multiple-free primaries during each
	update. There is some anecdotal evidence that the inversion result
	is unstable under errors in the time-window length, a behavior that
	runs contrary to the strengths of EPSI and diminishes its effectiveness
	for shallow-bottom marine data where multiples are closely spaced.
	Moreover, due to the nuances involved in regularizing the model impulse
	response in the inverse problem, the EPSI approach has an additional
	number of inversion parameters to choose and often also does not
	often lead to a stable solution under perturbations to these parameters.
	We show that the specific sparsity constraint on the EPSI updates
	lead to an inherently intractable problem, and that the time-window
	and other inversion variables arise as additional regularizations
	on the unknown towards a meaningful solution. We furthermore suggest
	a way to remove almost all of these parameters via a L0 to L1 convexification,
	which stabilizes the inversion while preserving the crucial sparsity
	assumption in the primary impulse response model.},
  keywords = {EAGE},
presentation ={http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage10/lin10eagesoe.pdf},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/lin10eage.pdf}
}

@CONFERENCE{lin10SEGspm,
  author = {Tim T.Y. Lin and Ning Tu and Felix J. Herrmann},
  title = {Sparsity-promoting migration from surface-related multiples},
  year = {2010},
  organization = {SEG},
  publisher = {SEG},
  abstract = {Seismic imaging typically begins with the removal of multiple energy
	in the data, out of fear that it may introduce erroneous structure.
	However, seismic multiples have effectively seen more of the earth{\textquoteright}s
	structure, and if treated correctly can potential supply more information
	to a seismic image compared to primaries. Past approaches to accomplish
	this leave ample room for improvement; they either require extensive
	modification to standard migration techniques, rely too much on prior
	information, require extensive pre-processing, or resort to full-waveform
	inversion. We take some valuable lessons from these efforts and present
	a new approach balanced in terms of ease of implementation, robustness,
	efficiency and well-posedness, involving a sparsity-promoting inversion
	procedure using standard Born migration and a data-driven multiple
	modeling approach based on the focal transform.},
  keywords = {SEG},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg10/Tu-0945.pdf
	},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/lin10spm.pdf}
}

@CONFERENCE{moghaddam10SEGrfw,
  author = {Peyman P. Moghaddam and Felix J. Herrmann},
  title = {Randomized full-waveform inversion: a dimenstionality-reduction approach},
  year = {2010},
  organization = {SEG},
  publisher = {SEG},
  abstract = {Full-waveform inversion relies on the collection of large multi-experiment
	data volumes in combination with a sophisticated back-end to create
	high-fidelity inversion results. While improvements in acquisition
	and inversion have been extremely successful, the current trend of
	incessantly pushing for higher quality models in increasingly complicated
	regions of the Earth reveals fundamental shortcomings in our ability
	to handle increasing problem sizes numerically. Two main culprits
	can be identified. First, there is the so-called {\textquoteleft}{\textquoteleft}curse
	of dimensionality{\textquoteright}{\textquoteright} exemplified by
	Nyquist{\textquoteright}s sampling criterion, which puts disproportionate
	strain on current acquisition and processing systems as the size
	and desired resolution increases. Secondly, there is the recent {\textquoteleft}{\textquoteleft}departure
	from Moore{\textquoteright}s law{\textquoteright}{\textquoteright}
	that forces us to develop algorithms that are amenable to parallelization.
	In this paper, we discuss different strategies that address these
	issues via randomized dimensionality reduction.},
  keywords = {SEG},
  presentation = {http://slim.eos.ubc.ca/Publications/public/presentations/seg/seg10/Herrmann10RFW.pdf
	},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/moghaddam10rfw.pdf}
}

%------------------------------2009---------------------------------------------
@CONFERENCE{erlangga09SEGfwi,
  author = {Yogi A. Erlangga and Felix J. Herrmann},
  title = {Full-Waveform Inversion with Gauss-Newton-Krylov Method},
  booktitle = {SEG},
  year = {2009},
  abstract = {This abstract discusses an implicit implementation of the Gauss-Newton
	method, used for the frequency-domain full-waveform inversion, where
	the inverse of the Hessian for the update is never formed explicitly.
	Instead, the inverse of the Hessian is computed approximately by
	a conjugate gradient (CG) method, which only requires the action
	of the Hessian on the CG search direction. This procedure avoids
	an excessive computer storage, usually needed for storing the Hessian,
	at the expense of extra computational work in CG. An effective preconditioner
	for the Hessian is important to improve the convergence of CG, and
	hence to reduce the overall computational work.},
  keywords = {Presentation, SEG, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/public/presentations/seg/seg09/erlangga09segfwi.pdf}
}

@CONFERENCE{Erlangga09EAGEmwi,
  author = {Yogi A. Erlangga and Felix J. Herrmann},
  title = {Migration with implicit solvers for the time-harmonic Helmholtz equation},
  year = {2009},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {From the measured seismic data, the location and the amplitude of
	reflectors can be determined via a migration algorithm. Classically,
	following Claerbout{\textquoteright}s imaging principle [2], a reflector
	is located at the position where the source{\textquoteright}s forward-propagated
	wavefield correlates with the backward-propagated wavefield of the
	receiver data. Lailly and Tarantola later showed that this imaging
	principle is an instance of inverse problems, with the associated
	migration operator formulated via a least-squares functional; see
	[6, 12, 13]. Furthermore, they showed that the migrated image is
	associated with the gradient of this functional with respect to the
	image. If the solution of the least-squares functional is done iteratively,
	the correlation-based image coincides up to a constant with the first
	iteration of a gradient method. In practice, this migration is done
	either in the time domain or in the frequency domain. In the frequency-domain
	migration, the main bottleneck thus far, which renders its full implementation
	to large scale problems, is the lack of efficient solvers for computing
	wavefields. Robust direct methods easily run into excessive memory
	requirements as the size of the problem increases. On the other hand,
	iterative methods, which are less demanding in terms of memory, suffered
	from lack of convergence. During the past years, however, progress
	has been made in the development of an efficient iterative method
	[4, 3] for the frequency-domain wavefield computations. In this paper,
	we will show the significance of this method (called MKMG) in the
	context of the frequency-domain migration, where multi-shot-frequency
	wavefields (of order of 10,000 related wavefields) need to be computed.},
  keywords = {EAGE migration},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/erlanggaEAGE2009.pdf
	},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2009/erlangga2009.pdf}
}

@CONFERENCE{erlangga09SEGswi,
  author = {Yogi A. Erlangga and Felix J. Herrmann},
  title = {Seismic waveform inversion with Gauss-Newton-Krylov method},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  number = {1},
  pages = {2357-2361},
  organization = {SEG},
  publisher = {SEG},
  abstract = {This abstract discusses an implicit implementation of the Gauss-Newton
	method, used for the frequency-domain full-waveform inversion, where
	the inverse of the Hessian for the update is never formed explicitly.
	Instead, the inverse of the Hessian is computed approximately by
	a conjugate gradient (CG) method, which only requires the action
	of the Hessian on the CG search direction. This procedure avoids
	an excessive computer storage, usually needed for storing the Hessian,
	at the expense of extra computational work in CG. An effective preconditioner
	for the Hessian is important to improve the convergence of CG, and
	hence to reduce the overall computational work.},
  keywords = {SEG},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/erlangga09segswi.pdf}
}

@CONFERENCE{friedlander09VIETcsgpa,
  author = {Michael P. Friedlander},
  title = {Computing sparse and group-sparse approximations},
  ogranization = {2009 High Performance Scientific Computing Conference},
  year = {2009},
  address = {Hanoi, Vietnam},
  keywords = {minimization, Presentation, SLIM}
}


@CONFERENCE{friedlander09NUalssr,
  author = {Michael P. Friedlander},
  title = {Algorithms for large-scale sparse reconstruction},
  organization = {IEMS Colloquim Speaker},
  year = {2009},
  address = {Northwestern University},
  keywords = {minimization, Presentation, SLIM}
}

@CONFERENCE{friedlander09SCAIMspot,
  author = {E. van den Berg and Michael P. Friedlander},
  title = {Spot: A linear-operator toolbox for Matlab},
  organization = {SCAIM Seminar},
  year = {2009},
  address = {University of British Columbia},
  keywords = {minimization, Presentation, SLIM},
presentation ={http://slim.eos.ubc.ca/Publications/Private/Presentations/sinbad/2010Fall/Thu-13-50-Friedlander.pdf }
}

@CONFERENCE{herrmann09SEGcib,
  author = {Felix J. Herrmann},
  title = {Compressive imaging by wavefield inversion with group sparsity},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  number = {1},
  pages = {2337-2341},
  organization = {SEG},
  publisher = {SEG},
  abstract = {Migration relies on multi-dimensional correlations between source-
	and residual wavefields. These multi-dimensional correlations are
	computationally expensive because they involve operations with explicit
	and full matrices that contain both wavefields. By leveraging recent
	insights from compressive sampling, we present an alternative method
	where linear correlation-based imaging is replaced by imaging via
	multidimensional deconvolutions of compressibly sampled wavefields.
	Even though this approach goes at the expense of having to solve
	a sparsity-promotion recovery program for the image, our wavefield
	inversion approach has the advantage of reducing the system size
	in accordance to transform-domain sparsity of the image. Because
	seismic images also exhibit a focusing of the energy towards zero
	offset, the compressive-wavefield inversion itself is carried out
	using a recent extension of one-norm solver technology towards matrix-valued
	problems. These so-called hybrid $(1,\,2)$-norm solvers allow us
	to penalize pre-stack energy away from zero offset while exploiting
	joint sparsity amongst near-offset images. Contrary to earlier work
	to reduce modeling and imaging costs through random phase-encoded
	sources, our method compressively samples wavefields in model space.
	This approach has several advantages amongst which improved system-size
	reduction, and more flexibility during subsequent inversions for
	subsurface properties.},
  keywords = {SEG},
  presentation = {http://slim.eos.ubc.ca/Publications/public/presentations/seg/seg09/herrmann09segciw.pdf
	},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09segciw.pdf}
}

@CONFERENCE{Herrmann09EAGEcsa,
  author = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title = {Compressive sensing applied to full-waveform inversion},
  year = {2009},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {With the recent resurgence of full-waveform inversion, the computational
	cost of solving forward modeling problems has become{\textendash}-aside
	from issues with non-uniqueness{\textendash}-one of the major impediments
	withstanding successful application of this technology to industry-size
	data volumes. To overcome this impediment, we argue that further
	improvements in this area will depend on a problem formulation with
	a computational complexity that is no longer strictly determined
	by the size of the discretization but by transform-domain sparsity
	of its solution. In this new paradigm, we bring computational costs
	in par with our ability to compress seismic data and images. This
	premise is related to two recent developments. First, there is the
	new field of compressive sensing (CS in short throughout the paper,
	Cand{\textquoteleft}es et al., 2006; Donoho, 2006){\textendash}-where
	the argument is made, and rigorously proven, that compressible signals
	can be recovered from severely sub-Nyquist sampling by solving a
	sparsity promoting program. Second, there is in the seismic community
	the recent resurgence of simultaneous-source acquisition (Beasley,
	2008; Krohn and Neelamani, 2008; Herrmann et al., 2009; Berkhout,
	2008; Neelamani et al., 2008), and continuing efforts to reduce the
	cost of seismic modeling, imaging, and inversion through phase encoding
	of simultaneous sources (Morton and Ober, 1998; Romero et al., 2000;
	Krohn and Neelamani, 2008; Herrmann et al., 2009), removal of subsets
	of angular frequencies (Sirgue and Pratt, 2004; Mulder and Plessix,
	2004; Lin et al., 2008) or plane waves (Vigh and Starr, 2008). By
	using CS principles, we remove sub-sampling interferences asocciated
	with these approaches through a combination of exploiting transform-domain
	sparsity, properties of certain sub-sampling schemes, and the existence
	of sparsity promoting solvers.},
  keywords = {EAGE},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Herrmann09eagecs.pdf
	},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2009/herrmann09eage.pdf}
}

@CONFERENCE{Herrmann09SAMPTAcws,
  author = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title = {Compressive-wavefield simulations},
  year = {2009},
  organization = {SAMPTA},
  publisher = {SAMPTA},
  abstract = {Full-waveform inversion{\textquoteright}s high demand on computational
	resources forms, along with the non-uniqueness problem, the major
	impediment withstanding its widespread use on industrial-size datasets.
	Turning modeling and inversion into a compressive sensing problem{\textendash}-where
	simulated data are recovered from a relatively small number of independent
	simultaneous sources{\textendash}-can effectively mitigate this high-cost
	impediment. The key is in showing that we can design a sub-sampling
	operator that commutes with the time-harmonic Helmholtz system. As
	in compressive sensing, this leads to a reduction in simulation cost.
	Moreover, this reduction is commensurate with the transform-domain
	sparsity of the solution, implying that computational costs are no
	longer determined by the size of the discretization but by transform-domain
	sparsity of the solution of the CS problem which forms our data.
	The combination of this sub-sampling strategy with our recent work
	on implicit solvers for the Helmholtz equation provides a viable
	alternative to full-waveform inversion schemes based on explicit
	finite-difference methods.},
  keywords = {SAMPTA},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SAMPTA/herrmann09sampta.pdf}
}

@CONFERENCE{herrmann09EAGEbnrs,
  author = {Felix J. Herrmann and Gang Tang and Reza Shahidi and Gilles Hennenfent
	and Tim T.Y. Lin},
  title = {Beating Nyquist by randomized sampling. Presented at the EAGE (workshop),
	Amsterdam},
  year = {2009},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Herrmann09eagews.pdf}
}


@CONFERENCE{herrmann09IAPcsisa,
  author = {Felix J. Herrmann and Yogi Erlangga and Tim T.Y. Lin},
  title = {Compressive seismic imaging with simultaneous acquisition presented
	at the IAP meeting, Vienna,},
  year = {2009},
  abstract = {The shear size of seismic data volumes forms one of the major impediments
	for the inversion of seismic data. Turning forward modeling and inversion
	into a compressive sensing (CS) problem - where simulated data are
	recovered from a relatively small number of independent sources -
	can effectively mitigate this high-cost impediment. Our key contribution
	lies in the design of a sub-sampling operator that commutes with
	the time-harmonic Helmholtz system. As in compressive sensing, this
	leads to a reduction of simulation cost. This reduction is commensurate
	with the transform-domain sparsity of the solution., implying that
	computational costs are no longer determined by the size of the discretization
	but by transform-domain sparsity of the solution of the CS problem
	that recovers the data. The combination of this sub-sampling strategy
	with our recent work on preconditioned implicit solvers for the time-harmonic
	Helmholtz equation provides a viable alternative to full-waveform
	inversion schemes based on explicit time-domain finite-difference
	methods.},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Herrmann09AIP1.pdf}
}

@CONFERENCE{herrmann09SEGsns,
  author = {Felix J. Herrmann},
  title = {Sub-Nyquist sampling and sparsity: getting more information from
	fewer samples},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  number = {1},
  pages = {3410-3415},
  organization = {SEG},
  publisher = {SEG},
  abstract = {Seismic exploration relies on the collection of massive data volumes
	that are subsequently mined for information during seismic processing.
	While this approach has been extremely successful in the past, the
	current trend of incessantly pushing for higher quality images in
	increasingly complicated regions of the Earth continues to reveal
	fundamental shortcomings in our workflows to handle massive high-dimensional
	data volumes. Two causes can be identified as the main culprits responsible
	for this barrier. First, there is the so-called {\textquoteleft}{\textquoteleft}curse
	of dimensionality{\textquoteright}{\textquoteright} exemplified by
	Nyquist{\textquoteright}s sampling criterion, which puts disproportionate
	strain on current acquisition and processing systems as the size
	and desired resolution of our survey areas continues to increase.
	Secondly, there is the recent {\textquoteleft}{\textquoteleft}departure
	from Moore{\textquoteright}s law{\textquoteright}{\textquoteright}
	that forces us to lower our expectations to compute ourselves out
	of this curse of dimensionality. In this paper, we offer a way out
	of this situation by a deliberate \emph{randomized} subsampling combined
	with structure-exploiting transform-domain sparsity promotion. Our
	approach is successful because it reduces the size of seismic data
	volumes without loss of information. Because of this size reduction
	both impediments are removed and we end up with a new technology
	where the costs of acquisition and processing are no longer dictated
	by the \emph{size of the acquisition} but by the transform-domain
	\emph{sparsity} of the end-product after processing.},
  keywords = {SEG},
  presentation = { http://slim.eos.ubc.ca/Publications/public/presentations/seg/seg09/herrmann09segsns.pdf},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09segsss.pdf}
}

@CONFERENCE{herrmann09PIMScssr3,
  author = {Felix J. Herrmann},
  title = {Compressed sensing and sparse recovery in exploration seismology.
	Lecture III presented at the PIMS Summer School on Seismic Imaging.
	Seattle.},
  year = {2009},
  abstract = {In this course, I will present how recent results from compressed
	sensing and sparse recovery apply to exploration seismology. During
	the first lecture, I will present the basic principles of compressive
	sensing; the importance of random jitter sampling and sparsifying
	transforms; and large-scale one-norm solvers. I will discuss the
	application of these techniques to missing trace interpolation. The
	second lecture will be devoted to coherent signal separation based
	on curveletdomain matched filtering and Bayesian separation with
	sparsity promotion. Applications of these techniques to the primary-multiple
	wavefield-separation problem on real data will be discussed as well.
	The third lecture will be devoted towards sparse recovery in seismic
	modeling and imaging and includes the problem of preconditioning
	the imaging operators, and the recovery from simultaneous source-acquired
	data.},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Herrmann09PIMS3.pdf}
}

@CONFERENCE{herrmann09PIMScssr2,
  author = {Felix J. Herrmann},
  title = {Compressed sensing and sparse recovery in exploration seismology.
	Lecture II presented at the PIMS Summer School on Seismic Imaging.
	Seattle.},
  year = {2009},
  abstract = {In this course, I will present how recent results from compressed
	sensing and sparse recovery apply to exploration seismology. During
	the first lecture, I will present the basic principles of compressive
	sensing; the importance of random jitter sampling and sparsifying
	transforms; and large-scale one-norm solvers. I will discuss the
	application of these techniques to missing trace interpolation. The
	second lecture will be devoted to coherent signal separation based
	on curveletdomain matched filtering and Bayesian separation with
	sparsity promotion. Applications of these techniques to the primary-multiple
	wavefield-separation problem on real data will be discussed as well.
	The third lecture will be devoted towards sparse recovery in seismic
	modeling and imaging and includes the problem of preconditioning
	the imaging operators, and the recovery from simultaneous source-acquired
	data.},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Herrmann09PIMS2.pdf}
}

@CONFERENCE{herrmann09PIMScssr1,
  author = {Felix J. Herrmann},
  title = {Compressed sensing and sparse recovery in exploration seismology.
	Lecture I presented at the PIMS Summer School on Seismic Imaging.
	Seattle.},
  year = {2009},
  abstract = {In this course, I will present how recent results from compressed
	sensing and sparse recovery apply to exploration seismology. During
	the first lecture, I will present the basic principles of compressive
	sensing; the importance of random jitter sampling and sparsifying
	transforms; and large-scale one-norm solvers. I will discuss the
	application of these techniques to missing trace interpolation. The
	second lecture will be devoted to coherent signal separation based
	on curveletdomain matched filtering and Bayesian separation with
	sparsity promotion. Applications of these techniques to the primary-multiple
	wavefield-separation problem on real data will be discussed as well.
	The third lecture will be devoted towards sparse recovery in seismic
	modeling and imaging and includes the problem of preconditioning
	the imaging operators, and the recovery from simultaneous source-acquired
	data},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Herrmann09PIMS1.pdf}
}

@CONFERENCE{herrmann09SEGrpl,
  author = {Felix J. Herrmann},
  title = {Reflector-preserved lithological upscaling},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  number = {1},
  pages = {3466-3470},
  organization = {SEG},
  publisher = {SEG},
  abstract = {By combining Percolation models with lithological smoothing, we arrive
	at method for upscaling rock elastic constants that preserves reflections.
	In this approach, the Percolation model predicts sharp onsets in
	the elastic moduli of sand-shale mixtures when the shales reach a
	critical volume fraction. At that point, the shale inclusions form
	a connected cluster, and the macroscopic rock properties change with
	the power-law growth of the cluster. This switch-like nonlinearity
	preserves singularities, and hence reflections, even if no sharp
	transition exists in the lithology or if they are smoothed out using
	standard upscaling procedures.},
  keywords = {SEG},
  presentation = {http://slim.eos.ubc.ca/Publications/public/presentations/seg/seg09/herrmann09segrpu.pdf},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09segrlu.pdf}
}

@CONFERENCE{kumar09SEGins,
  author = {Vishal Kumar and Felix J. Herrmann},
  title = {Incoherent noise suppression with curvelet-domain sparsity},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  number = {1},
  pages = {3356-3360},
  organization = {SEG},
  publisher = {SEG},
  abstract = {The separation of signal and noise is a key issue in seismic data
	processing. By noise we refer to the incoherent noise that is present
	in the data. We use the recently introduced multiscale and multidirectional
	curvelet transform for suppression of random noise. The curvelet
	transform decomposes data into directional plane waves that are local
	in nature. The coherent features of the data occupy the large coefficients
	in the curvelet domain, whereas the incoherent noise lives in the
	small coefficients. In other words, signal and noise have minimal
	overlap in the curvelet domain. This gives us a chance to use curvelets
	to suppress noise present in data.},
  keywords = {SEG},
  presentation = {http://slim.eos.ubc.ca/Publications/public/presentations/seg/seg09/kumar09segins.pdf
	},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/kumar09segins.pdf}
}

@CONFERENCE{Lin09SEGcsf,
  author = {Tim T.Y. Lin and Yogi A. Erlangga and Felix J. Herrmann},
  title = {Compressive simultaneous full-waveform simulation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  number = {1},
  pages = {2577},
  organization = {SEG},
  publisher = {SEG},
  abstract = {The fact that the computational complexity of wavefield simulation
	is proportional to the size of the discretized model and acquisition
	geometry, and not to the complexity of the simulated wavefield, is
	a major impediment within seismic imaging. By turning simulation
	into a compressive sensing problem{\textendash}where simulated data
	is recovered from a relatively small number of independent simultaneous
	sources{\textendash}we remove this impediment by showing that compressively
	sampling a simulation is equivalent to compressively sampling the
	sources, followed by solving a reduced system. As in compressive
	sensing, this allows for a reduction in sampling rate and hence in
	simulation costs. We demonstrate this principle for the time-harmonic
	Helmholtz solver. The solution is computed by inverting the reduced
	system, followed by a recovery of the full wavefield with a sparsity
	promoting program. Depending on the wavefield{\textquoteright}s sparsity,
	this approach can lead to a significant cost reduction, in particular
	when combined with the implicit preconditioned Helmholtz solver,
	which is known to converge even for decreasing mesh sizes and increasing
	angular frequencies. These properties make our scheme a viable alternative
	to explicit time-domain finite-difference.},
  keywords = {SEG},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/lin09segcss.pdf}
}

@CONFERENCE{lin09EAGEdsa,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Designing simultaneous acquisitions with compressive sensing},
  year = {2009},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {The goal of this paper is in designing a functional simultaneous acquisition
	scheme by applying the principles of compressive sensing. By framing
	the acquisition in a compressive sensing setting we immediately gain
	insight into not only how to choose the source signature and shot
	patterns, but also in how well we can hope to demultiplex the data
	when given a set amount of reduction in the number of sweeps. The
	principles of compressive sensing dictates that the quality of the
	demultiplexed data is closely related to the transform-domain sparsity
	of the solution. This means that, given an estimate in the complexity
	of the expectant data wavefield, it is possible to controllably reduce
	the number of shots that needs to be recorded in the field. We show
	a proof of concept by introducing an acquisition compatible with
	compressive sensing based on randomly phase-encoded vibroseis sweeps.},
  keywords = {EAGE},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/lin2009eagedsa.pdf
	},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2009/CompAq.pdf}
}

@CONFERENCE{lin09SEGucs,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Unified compressive sensing framework for simultaneous acquisition
	with primary estimation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  number = {1},
  pages = {3113-3117},
  organization = {SEG},
  publisher = {SEG},
  abstract = {The central promise of simultaneous acquisition is a vastly improved
	crew efficiency during acquisition at the cost of additional post-processing
	to obtain conventional source-separated data volumes. Using recent
	theories from the field of compressive sensing, we present a way
	to systematically model the effects of simultaneous acquisition.
	Our formulation form a new framework in the study of acquisition
	design and naturally leads to an inversion-based approach for the
	separation of shot records. Furthermore, we show how other inversion-based
	methods, such as a recently proposed method from van Groenestijn
	and Verschuur (2009) for primary estimation, can be processed together
	with the demultiplexing problem to achieve a better result compared
	to a separate treatment of these problems.},
  keywords = {SEG},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg09/lin09segucs.pdf
	},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/lin09segucf.pdf}
}

@CONFERENCE{lin09DELPHIrwi,
  author = {Tim T.Y. Lin and Felix J. Herrmann and Yogi A. Erlangga},
  title = {Randomized wavefield inversion presented at the DELPHI meeting.The
	Hague.},
  year = {2009},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Delphi2009.pdf}
}

@CONFERENCE{saab09SAMPTAnccs,
  author = {Rayan Saab and Ozgur Yilmaz},
  title = {A short note on non-convex compressed sensing},
  year = {2009},
  organization = {SAMPTA},
  publisher = {SAMPTA},
  abstract = {In this note, we summarize the results we recently proved in\cite{SY08}
	on the theoretical performance guarantees of the decoders $øÓ_p$.
	These decoders rely on $\ell^p$ minimization with $p {\i}n (0,1)$
	to recover estimates of sparse and compressible signals from incomplete
	and inaccurate measurements. Our guarantees generalize the results
	of \cite{CRT05} and \cite{Wojtaszczyk08} about decoding by $\ell_p$
	minimization with $p=1$, to the setting where $p {\i}n (0,1)$ and
	are obtained under weaker sufficient conditions. We also present
	novel extensions of our results in \cite{SY08} that follow from the
	recent work of DeVore et al. in \cite{DPW08}. Finally, we show some
	insightful numerical experiments displaying the trade-off in the
	choice of $p {\i}n (0,1]$ depending on certain properties of the
	input signal.},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SAMPTA/SY09sampta.pdf}
}

@CONFERENCE{shahidi09SEGcdm,
  author = {Reza Shahidi and Felix J. Herrmann},
  title = {Curvelet-domain matched filtering with frequency-domain regularization},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  number = {1},
  pages = {3645},
  organization = {SEG},
  publisher = {SEG},
  abstract = {In Herrmann et al. (2008), it is shown that zero-order pseudodifferential
	operators, which model the migration-demigration operator and the
	operator mapping the predicted multiples to the true multiples, can
	be represented by a diagonal weighting in the curvelet domain. In
	that paper, a smoothness constraint was introduced in the phase space
	of the operator in order to regularize the solution to make it unique.
	In this paper, we use recent results in Demanet and Ying (2008) on
	the discrete symbol calculus to impose a further smoothness constraint,
	this time in the frequency domain. It is found that with this additional
	constraint, faster convergence is realized. Results on a synthetic
	pseudodifferential operator as well as on an example of primary-multiple
	separation in seismic data are included, comparing the model with
	and without the new smoothness constraint, from which it is found
	that results of improved quality are also obtained.},
  keywords = {SEG},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/shahidi09segcmf.pdf}
}

@CONFERENCE{tang09SEGhdb,
  author = {Gang Tang and Reza Shahidi and Felix J. Herrmann and Jianwei Ma},
  title = {Higher dimensional blue-noise sampling schemes for curvelet-based
	seismic data recovery},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  number = {1},
  pages = {191-195},
  organization = {SEG},
  publisher = {SEG},
  abstract = {In combination with compressive sensing, a successful reconstruction
	scheme called Curvelet-based Recovery by Sparsity-promoting Inversion
	(CRSI) has been developed, and has proven to be useful for seismic
	data processing. One of the most important issues for CRSI is the
	sampling scheme, which can greatly affect the quality of reconstruction.
	Unlike usual regular undersampling, stochastic sampling can convert
	aliases to easy-to-eliminate noise. Some stochastic sampling methods
	have been developed for CRSI, e.g. jittered sampling, however most
	have only been applied to 1D sampling along a line. Seismic datasets
	are usually higher dimensional and very large, thus it is desirable
	and often necessary to develop higher dimensional sampling methods
	to deal with these data. For dimensions higher than one, few results
	have been reported, except uniform random sampling, which does not
	perform well. In the present paper, we explore 2D sampling methodologies
	for curvelet-based reconstruction, possessing sampling spectra with
	blue noise characteristics, such as Poisson Disk sampling, Farthest
	Point Sampling, and the 2D extension of jittered sampling. These
	sampling methods are shown to lead to better recovery and results
	are compared to the other more traditional sampling protocols.},
  keywords = {SEG},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/tang09seghdb.pdf}
}

@CONFERENCE{vandenberg08SLIMocf,
  author = {E. van den Berg and Mark Schmidt and Michael P. Friedlander and K.
	Murphy},
  title = {Optimizing Costly Functions with Simple Constraints: A Limited-Memory
	Projected Quasi-Newton Algorithm},
  year = {2009},
  volume = {12},
  series = {Twelfth International Conference on Artificial Intelligence and Statistics},
  month = {April},
  abstract = {An optimization algorithm for minimizing a smooth function over a
	convex set is described. Each iteration of the method computes a
	descent direction by minimizing, over the original constraints, a
	diagonal-plus low-rank quadratic approximation to the function. The
	quadratic approximation is constructed using a limited-memory quasi-Newton
	update. The method is suitable for large-scale problems where evaluation
	of the function is substan- tially more expensive than projection
	onto the constraint set. Numerical experiments on one- norm regularized
	test problems indicate that the proposed method is competitve with
	state- of-the-art methods such as bound-constrained L-BFGS and orthant-wise
	descent. We further show that the method generalizes to a wide class
	of problems, and substantially improves on state-of-the-art methods
	for problems such as learning the structure of Gaussian graphi- cal
	models (involving positive-definite matrix constraints) and Markov
	random fields (in- volving second-order cone constraints).},
  date-added = {2009-01-29 17:16:34 -0800},
  date-modified = {2009-01-29 17:16:34 -0800},
  keywords = {SLIM},
  pdf = {http://www.cs.ubc.ca/~mpf/public/group.pdf}
}

@CONFERENCE{yan09SEGgpb,
  author = {Jiupeng Yan and Felix J. Herrmann},
  title = {Groundroll prediction by interferometry and separation by curvelet-domain
	matched filtering},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  number = {1},
  pages = {3297-3301},
  organization = {SEG},
  publisher = {SEG},
  abstract = {The removal of groundroll in land based seismic data is a critical
	step for seismic imaging. In this paper, we introduce a work flow
	to predict the groundroll by interferometry and then separate the
	groundroll in the curvelet domain. Thus workflow is similar to the
	workflow of surface-related multiple elimination (SRME). By exploiting
	the adaptability and sparsity of curvelets, we are able to significantly
	improve the separation of groundroll in comparison to results yielded
	by frequency-domain adaptive subtraction methods. We provide synthetic
	data example to illustrate our claim.},
  keywords = {SEG},
  presentation = {http://slim.eos.ubc.ca/Publications/public/presentations/seg/seg09/yan09seggpi.pdf
	},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/yan09seggrp.pdf}
}

@CONFERENCE{yan09SEGgpb2,
  author = {Jiupeng Yan and Felix J. Herrmann},
  title = {Groundroll prediction by interferometry and separation by curvelet-domain
	filtering. Presented at the 79th SEG Meeting, Houston},
  year = {2009},
  abstract = {The removal of groundroll in land based seismic data is a critical
	step for seismic imaging. In this paper, we introduce a work flow
	to predict the groundroll by interferometry and then separate the
	groundroll in the curvelet domain. Thus workflow is similar to the
	workflow of surface-related multiple elimination (SRME). By exploiting
	the adaptability and sparsity of curvelets, we are able to significantly
	improve the separation of groundroll in comparison to results yielded
	by frequency-domain adaptive subtraction methods. We provide synthetic
	data example to illustrate our claim.},
  url = {http://slim.eos.ubc.ca/Publications/public/presentations/seg/seg09/yan09seggpi.pdf}
}

%--------------------------------------2008------------------------------
@CONFERENCE{Berg08SINBADsat,
  author = {E. van den Berg},
  title = {Sparco: A testing framework for sparse reconstruction},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {Sparco is a framework for testing and benchmarking algorithms for
	sparse reconstruction. It includes a large collection of sparse reconstruction
	problems drawn from the imaging, compressed sensing, and geophysics
	literature. Sparco is also a framework for implementing new test
	problems and can be used as a tool for reproducible research. We
	describe the software environment, and demonstrate its usefulness
	for testing and comparing solvers for sparse reconstruction.},
  date-modified = {2008-08-22 12:54:25 -0700},
  keywords = {SLIM, SINBAD, Presentation},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Ewout_Spar.pdf}
}

@CONFERENCE{erlangga08SEGaim,
  author = {Yogi A. Erlangga and Felix J. Herrmann},
  title = {An iterative multilevel method for computing wavefields in frequency-domain
	seismic inversion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  number = {1},
  pages = {1957-1960},
  month = {November},
  organization = {SEG},
  publisher = {SEG},
  abstract = {We describe an iterative multilevel method for solving linear systems
	representing forward modeling and back propagation of wavefields
	in frequency-domain seismic inversions. The workhorse of the method
	is the so-called multilevel Krylov method, applied to a multigrid-preconditioned
	linear system, and is called multigrid-multilevel Krylov (MKMG) method.
	Numerical experiments are presented for 2D Marmousi synthetic model
	for a range of frequencies. The convergence of the method is fast,
	and depends only mildly on frequency. The method can be considered
	as the first viable alternative to LU factorization, which is practically
	prohibitive for 3D seismic inversions.},
  keywords = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg08/erlangga08imm.pdf
	},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/erlangga08seg.pdf}
}

@CONFERENCE{erlangga08SINBADimf,
  author = {Yogi A. Erlangga and K. Vuik and K. Oosterlee and D. Riyanti and
	R. Nabben},
  title = {Iterative methods for 2D/3D Helmholtz operator},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We present an iterative method for solving the 2D/3D Helmholtz equation.
	The method is mainly based on a Krylov method, preconditioned by
	a special operator which represents a damped Helmholtz operator.
	The discretization of the preconditioning operator is then solved
	by one multigrid sweep. It can be shown that while the spectrum is
	bounded above by one, the smallest eigenvalue of the preconditioned
	system is of order $k^{-1}$. In this situation, the convergence of
	a Krylov method will be proportional to the frequency of the problem.
	Further convergence acceleration can be achieved if eigenvalues of
	order $k^{-1}$ are projected from the spectrum. This can be done
	by a projection operator, similar to but more stable than deflation.
	This projection operator has been the core of a new multilevel method,
	called multilevel Krylov method, proposed by Erlangga and Nabben
	only recently. Putting the preconditioned Helmholtz operator in this
	setting, a convergence which is independent of frequency can be obtained.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Erlangga_Ite.pdf}
}

@CONFERENCE{eso08SEGira,
  author = {R. A. Eso and S. Napier and Felix J. Herrmann and D. W. Oldenburg},
  title = {Iterative reconstruction algorithm for non-linear operators},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  number = {1},
  pages = {579-583},
  month = {November},
  organization = {SEG},
  publisher = {SEG},
  abstract = {Iterative soft thresholding of a models wavelet coefficients can be
	used to obtain models that are sparse with respect to a known basis
	function. We generate sparse models for non-linear forward operators
	by applying the soft thresholding operator to the model obtained
	through a Gauss-Newton iteration and apply the technique in a synthetic
	2.5D DC resistivity crosswell tomographic example.},
  keywords = {SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/eso08seg.pdf}
}

@CONFERENCE{friedlander08SINBADafl,
  author = {Michael P. Friedlander},
  title = {Algorithms for Large-Scale Sparse Reconstruction},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {Many signal processing applications seek to approximate a signal as
	a linear combination of only a few elementary atoms drawn from a
	large collection. This is known as sparse reconstruction, and the
	theory of compressed sensing allows us to pose it as a structured
	convex optimization problem. I will discuss the role of duality in
	revealing some unexpected and useful properties of these problems,
	and will show how they can lead to practical, large-scale algorithms.
	I will also describe some applications of these algorithms.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Michael_Alg.pdf}
}

@CONFERENCE{friedlander08SIAMasa,
  author = {Michael P. Friedlander},
  title = {Active-set Approaches to Basis Pursuit Denoising},
  booktitle = {SIAM Optimization},
  year = {2008},
  month = {May},
  organization = {SIAM Optimization},
  publisher = {SIAM Optimization},
  file = {:http\://www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf:PDF},
  keywords = {Presentation, SLIM},
  url = {http://www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf}
}

@CONFERENCE{friedlander08WCOMasm,
  author = {Michael P. Friedlander and M. A. Saunders},
  title = {Active-set methods for basis pursuit},
  organization = {West Coast Opitmization Meeting (WCOM)},
  year = {2008},
  month = {September},
  abstract = {Many imaging and compressed sensing applications seek sparse solutions
	to large under-determined least-squares problems. The basis pursuit
	(BP) approach minimizes the 1-norm of the solution, and the BP denoising
	(BPDN) approach balances it against the least-squares fit. The duals
	of these problems are conventional linear and quadratic programs.
	We introduce a modified parameterization of the BPDN problem and
	explore the effectiveness of active-set methods for solving its dual.
	Our basic algorithm for the BP dual unifies several existing algorithms
	and is applicable to large-scale examples.},
  file = {:http\://www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf:PDF},
  url = {http://www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf}
}

@CONFERENCE{hennenfent08SINBADnii2,
  author = {Gilles Hennenfent},
  title = {New insights into one-norm solvers from the Pareto curve},
  year = {2008},
  abstract = {Several geophysical ill-posed inverse problems are successfully solved
	by promoting sparsity using one-norm regularization. The practicality
	of this approach depends on the effectiveness of the one-norm solver
	used and on its robustness under limited number of iterations. We
	propose an approach to understand the behavior and evaluate the performance
	of one-norm solvers. The technique consists of tracking on a graph
	the data misfit versus the one norm of successive iterates. By comparing
	the solution paths to the Pareto curve, we are able to assess the
	performance of the solvers and the quality of the solutions. Such
	an assessment is particularly relevant given the renewed interest
	in one-norm regularization.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Gilles_New.pdf}
}

@CONFERENCE{hennenfent08SINBADsdw2,
  author = {Gilles Hennenfent},
  title = {Simply denoise: wavefield reconstruction via jittered undersampling},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We present a new discrete undersampling scheme designed to favor wavefield
	reconstruction by sparsity-promoting inversion with transform elements
	that are localized in the Fourier domain. Our work is motivated by
	empirical observations in the seismic community, corroborated by
	recent results from compressive sampling, which indicate favorable
	(wavefield) reconstructions from random as opposed to regular undersampling.
	As predicted by theory, random undersampling renders coherent aliases
	into harmless incoherent random noise, effectively turning the interpolation
	problem into a much simpler denoising problem. A practical requirement
	of wavefield reconstruction with localized sparsifying transforms
	is the control on the maximum gap size. Unfortunately, random undersampling
	does not provide such a control and the main purpose of this paper
	is to introduce a sampling scheme, coined jittered undersampling,
	that shares the benefits of random sampling, while offering control
	on the maximum gap size. Our contribution of jittered sub-Nyquist
	sampling proofs to be key in the formulation of a versatile wavefield
	sparsity-promoting recovery scheme that follows the principles of
	compressive sampling. After studying the behavior of the jittered-undersampling
	scheme in the Fourier domain, its performance is studied for curvelet
	recovery by sparsity-promoting inversion (CRSI). Our findings on
	synthetic and real seismic data indicate an improvement of several
	decibels over recovery from regularly-undersampled data for the same
	amount of data collected.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Gilles_jit.pdf}
}

@CONFERENCE{herrmann08SEGcdm,
 title = {Curvelet-domain matched filtering},
 author = {Felix J. Herrmann and Peyman P. Moghaddam and Deli Wang},
 month = {August},
 year = {2008},
 abstract = {Matching seismic wavefields and images lies at the heart
of many pre-/post-processing
	steps part of seismic imaging{\textendash}- whether one is matching
	predicted wavefield components, such as multiples, to the actual
	to-be-separated wavefield components present in the data or whether
	one is aiming to restore migration amplitudes by scaling, using an
	image-to-remigrated-image matching procedure to calculate the scaling
	coefficients. The success of these wavefield matching procedures
	depends on our ability to (i) control possible overfitting, which
	may lead to accidental removal of energy or to inaccurate image-amplitude
	corrections, (ii) handle data or images with nonunique dips, and
	(iii) apply subsequent wavefield separations or migraton amplitude
	corrections stably. In this paper, we show that the curvelet transform
	allows us to address all these issues by imposing smoothness in phase
	space, by using their capability to handle conflicting dips, and
	by leveraging their ability to represent seismic data and images
	sparsely. This latter property renders curvelet-domain sparsity promotion
	an effective prior.},
 keywords = {SLIM, SEG},
 number = {TR-2008-6},
organization={SEG},
 presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg08/herrmann08cmf.pdf},
 publisher = {UBC Earth and Ocean Sciences Department},
 url = {http://slim.eos.ubc.ca/Publications/Public/TechReports/herrmann08segmat.pdf}
}


@CONFERENCE{herrmann08SINBADacd2,
  author = {Felix J. Herrmann},
  title = {Adaptive curvelet-domain primary-multiple separation},
  organization = {SINBAD},
  year = {2008},
  note = {SINBAD 2008},
  abstract = {In many exploration areas, successful separation of primaries and
	multiples greatly determines the quality of seismic imaging. Despite
	major advances made by Surface-Related Multiple Elimination (SRME),
	amplitude errors in the predicted multiples remain a problem. When
	these errors vary for each type of multiple differently (as a function
	of offset, time and dip), these amplitude errors pose a serious challenge
	for conventional least-squares matching and for the recently introduced
	separation by curvelet-domain thresholding. We propose a data-adaptive
	method that corrects amplitude errors, which vary smoothly as a function
	of location, scale (frequency band) and angle. In that case, the
	amplitudes can be corrected by an element-wise curvelet-domain scaling
	of the predicted multiples. We show that this scaling leads to a
	successful estimation of the primaries, despite amplitude, sign,
	timing and phase errors in the predicted multiples. Our results on
	synthetic and real data show distinct improvements over conventional
	least-squares matching, in terms of better suppression of multiple
	energy and high-frequency clutter and better recovery of the estimated
	primaries.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Felix_Ada.pdf}
}

@CONFERENCE{herrmann08SEGbgs,
  author = {Felix J. Herrmann},
  title = {Bayesian ground-roll separation by curvelet-domain sparsity promotion},
  booktitle = {SEG},
  year = {2008},
  date-added = {2008-11-17 11:16:00 -0700},
  date-modified = {2008-11-17 11:16:00 -0700},
  keywords = {SLIM, SEG, Presentation},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg08/herrmann08bgs.pdf},
}

@CONFERENCE{Herrmann08SEGcdm3,
  author = {Felix J. Herrmann},
  title = {Curvelet-domain matched filtering},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  number = {1},
  pages = {3643-3647},
  month = {November},
  organization = {SEG},
  publisher = {SEG},
  abstract = {Matching seismic wavefields lies at the heart of seismic processing
	whether one is adaptively subtracting multiples predictions or groundroll.
	In both cases, the predictions are matched to the actual to-be-separated
	wavefield components in the observed data. The success of these wavefield
	matching procedures depends on our ability to (i) control possible
	overfitting, which may lead to accidental removal of primary energy,
	(ii) handle data with nonunique dips, and (iii) apply wavefield separation
	after matching stably. In this paper, we show that the curvelet transform
	allows us to address these issues by imposing smoothness in phase
	space, by using their capability to handle conflicting dips, and
	by leveraging their ability to represent seismic data sparsely.},
  keywords = {SEG, SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/herrmann08segws.pdf}
}

@CONFERENCE{herrmann08IONcsa,
  author = {Felix J. Herrmann},
  title = {Compressive sampling: a new paradigm for seismic data acquistion
	and processing?},
  booktitle = {ION},
  year = {2008},
  abstract = {Seismic data processing and imaging are firmly rooted in the well-established
	paradigm of regular Nyquist sampling. Faced with a typical uncooperative
	environment, practitioners of seismic data acquisition make all efforts
	to comply to this theory by creating regularly-sampled seismic-data
	volumes that are suitable for Fourier-based processing flows. The
	current advent of new alternative transform domains{\textendash}-
	such as the sparsifying curvelet domain, where seismic data is decomposed
	into localized, multiscale and multidirectional plane waves{\textendash}-
	opens the possibility to change this paradigm by no longer combating
	sampling irregularity but by embracing it. During this talk, we show
	that as long as seismic data volumes permit a compressible representation{\textendash}-i.e.,
	data can be represented as a superposition of relatively few number
	of elementary waveforms{\textendash}- Nyquist sampling is unnecessary
	pessimistic. So far, nothing new, we all know from the work on Fourier-
	or other transform-based seismic-data regularization methodologies
	that wavefields can be recovered accurately from sub-Nyquist samplings
	through some sort of optimization procedure. What is new, however,
	are recent insights from the field of "compressive sampling", which
	dictate the conditions that guarantee or, at least, in practice provide
	conditions that favor sparsity-promoting recovery from sub-Nyquist
	sampling. Random sub-sampling, or to be more precise, jitter sub-sampling
	creates favorable conditions for curvelet-based recovery. We explain
	this phenomenon by arguing that this type of sampling leads to noisy
	data, hence our slogan "Simply denoise: wavefield reconstruction
	via jittered undersampling", where we bank on separating incoherent
	sub-sampling noise with curvelet-domain sparsity promotion. During
	our presentation, we introduce you to what curvelets are, why random
	jitter sampling is important and why this opens a pathway towards
	a new paradigm of curvelet-domain seismic data processing. Our claims
	will be supported by examples on synthetic and field data. This is
	joint work with Gilles Hennenfent, PhD. student at SLIM.},
  keywords = {ION, Presentation, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2008/herrmann08ion_pres.pdf}
}

@CONFERENCE{herrmann08SINBADfwr,
  author = {Felix J. Herrmann},
  title = {(De)-Focused wavefield reconstructions},
  booktitle = {SINBAD 2008},
  year = {2008},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Felix_Dfo.pdf}
}

@CONFERENCE{herrmann08SEGgbu,
  author = {Felix J. Herrmann},
  title = {Seismic noise: the good, the bad, \& the ugly},
  booktitle = {SEG},
  year = {2008},
  keywords = {Presentation, SEG, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg08/herrmann08gbu.pdf}
}

@CONFERENCE{herrmann08SINBADpsm,
  author = {Felix J. Herrmann},
  title = {Phase-space matched filtering and migration preconditioning},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {During this talk, I will report on new phase-space regularization
	functionals defined in terms of splines. This spline representation
	reduces the dimensionality of estimating our phase-space matched
	filter. We will discuss how this filter can be used in migration
	preconditioning. This is joint work with Christiaan Stolk.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Felix_Pha.pdf}
}

@CONFERENCE{herrmann08SINBADs2c,
  author = {Felix J. Herrmann},
  title = {SINBAD 2008 Consortium meeting},
  booktitle = {SINBAD 2008},
  year = {2008},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Felix_Ope.pdf}
}

@CONFERENCE{Herrmann08SIAMcsm,
  author = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title = {Compressive sampling meets seismic imaging},
  booktitle = {SIAM},
  year = {2008},
  abstract = {Compressive sensing has led to fundamental new insights in the recovery
	of compressible signals from sub-Nyquist samplings. It is shown how
	jittered subsampling can be used to create favorable recovery conditions.
	Applications include mitigation of incomplete acquisitions and wavefield
	computations. While the former is a direct adaptation of compressive
	sampling, the latter application represents a new way of compressing
	wavefield extrapolation operators. Operators are not diagonalized
	but are compressively sampled reducing the computational costs.},
  keywords = {Presentation, SIAM, SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2008/herrmann08siam08.pdf}
}

@CONFERENCE{herrmann08SINBADitc,
  author = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin and Cody
	R. Brown},
  title = {Introduction to compressive (wavefield) computation},
  booktitle = {SINBAD 2008},
  year = {2008},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Felix_Int.pdf}
}

@CONFERENCE{herrmann08SEGswi,
  author = {Felix J. Herrmann and Deli Wang},
  title = {Seismic wavefield inversion with curvelet-domain sparsity promotion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  number = {1},
  pages = {2497-2501},
  month = {November},
  organization = {SEG},
  publisher = {SEG},
  abstract = {Inverting seismic wavefields lies at the heart of seismic data processing
	and imaging{\textendash}- whether one is applying {\textquoteleft}{\textquoteleft}a
	poor man{\textquoteright}s inverse{\textquoteright}{\textquoteright}
	by correlating wavefields during imaging or whether one inverts wavefields
	as part of a focal transform interferrometric deconvolution or as
	part of computing the {\textquoteright}data verse{\textquoteright}.
	The success of these wavefield inversions depends on the stability
	of the inverse with respect to data imperfections such as finite
	aperture, bandwidth limitation, and missing data. In this paper,
	we show how curvelet domain sparsity promotion can be used as a suitable
	prior to invert seismic wavefields. Examples include, seismic data
	regularization with the focused curvelet-based recovery by sparsity-promoting
	inversion (fCRSI), which involves the inversion of the primary-wavefield
	operator, the prediction of multiples by inverting the adjoint of
	the primary operator, and finally the inversion of the data itself
	{\textendash}- the so-called {\textquoteright}data inverse{\textquoteright}.
	In all cases, curvelet-domain sparsity leads to a stable inversion.},
  keywords = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg08/herrmann08swi.pdf
	},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/herrmann08segwav.pdf}
}

@CONFERENCE{johnson08SINBADsdi,
  author = {James Johnson and Gilles Hennenfent},
  title = {Seismic Data Interpolation with Symmetry},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {Due to the physics of reciprocity seismic data sets are symmetric
	in the source and receiver coordinates. Often seismic data sets are
	incomplete and the missing data must be interpolated. Typically,
	missing traces do not occur symmetrically. The purpose of this project
	is to extend the current formulation for solving the seismic interpolation
	problems in such a way that they enforce reciprocity. The method
	decomposes the seismic data volume into symmetric and antisymmetric
	parts. This decomposition leads to an augmented system of equations
	for the L1-solver that promotes sparsity in the curvelet domain.
	Interpolation is carried out on the entire system during which the
	asymmetric component of the volume is forced to zero, while the symmetric
	part of the data volume is matched to the measured data.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_James_Sei.pdf}
}

@CONFERENCE{kumar08SINBADcd,
  author = {Vishal Kumar},
  title = {Curvelet Denoising},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {The separation of signal and noise is an important issue in seismic
	data processing. By noise we refer to the incoherent noise which
	is present in the data. In our case, we showed curvelets concentrate
	seismic signal energy in few significant coefficients unlike noise
	energy that is spread all over the coefficients. The sparsity of
	seismic data in the curvelet domain makes curvelets an ideal choice
	for separating the noise from the seismic data. In our approach the
	denoising problem is framed as curvelet-regularized inversion problem.
	After initial processing, we applied the algorithm to the poststack
	data and compared our results with conventional wavelet denoising.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Vishal_Den.pdf}
}

@CONFERENCE{kumar08SINBADcrd,
  author = {Vishal Kumar},
  title = {Curvelet-Regularized Deconvolution},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {The removal of source signature from seismic data is an important
	step in seismic data processing. The Curvelet transform provides
	sparse representations for images that comprise smooth objects separated
	by piece-wise smooth discontinuities (e.g. seismic reflectivity).
	In this approach the sparseness of reflectivity in Curvelet domain
	is used as a prior to stabilize the inversion process. Our Curvelet-regularized
	deconvolution algorithm uses recently developed SPGL1 solver which
	does adaptive sampling of the trade-off curve. We applied the algorithm
	on a synthetic example and compared our results with that of Spiky
	deconvolution approach.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Vishal_Dec.pdf}
}

@CONFERENCE{kumar08CSEGcrs,
  author = {Vishal Kumar and Felix J. Herrmann},
  title = {Curvelet-regularized seismic deconvolution},
  booktitle = {CSEG Technical Program Expanded Abstracts},
  year = {2008},
  month = {May},
  organization = {CSEG},
  publisher = {CSEG},
  abstract = {There is an inherent continuity along reflectors of a seismic image.
	We use the recently introduced multiscale and multidirectional curvelet
	transform to exploit this continuity along reflectors for cases in
	which the assumption of spiky reflectivity may not hold. We show
	that such type of seismic reflectivity can be represented in the
	curvelet-domain by a vector whose entries decay rapidly. This curvelet-domain
	compression of reflectivity opens new perspectives towards solving
	classical problems in seismic processing including the deconvolution
	problem. In this paper, we present a formulation that seeks curvelet-domain
	sparsity for non-spiky reflectivity and we compare our results with
	those of spiky deconvolution.},
  keywords = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/cseg/kumar08crs.pdf
	},
  url = {http://slim.eos.ubc.ca/Publications/Private/Conferences/CSEG/2008/CSEG2008_Vishal.pdf}
}

@CONFERENCE{kumar08SEGdwc,
  author = {Vishal Kumar and Felix J. Herrmann},
  title = {Deconvolution with curvelet-domain sparsity},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  number = {1},
  pages = {1996-2000},
  month = {November},
  organization = {SEG},
  publisher = {SEG},
  abstract = {There is an inherent continuity along reflectors of a seismic image.
	We use the recently introduced multiscale and multidirectional curvelet
	transform to exploit this continuity along reflectors for cases in
	which the assumption of spiky reflectivity may not hold. We show
	that such type of seismic reflectivity can be represented in the
	curvelet-domain by a vector whose entries decay rapidly. This curvelet-domain
	compression of reflectivity opens new perspectives towards solving
	classical problems in seismic processing including the deconvolution
	problem. In this paper, we present a formulation that seeks curvelet-domain
	sparsity for non-spiky reflectivity and we compare our results with
	those of spiky deconvolution.},
  keywords = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg08/kumar08dcs.pdf
	},
  url = {http://slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2008/kumar08seg.pdf}
}

@CONFERENCE{lebed08SINBADaoc,
  author = {Evgeniy Lebed},
  title = {Curvelet / Surfacelet comparison},
  booktitle = {SINBAD},
  year = {2008},
  abstract = {Curvelets and Surfacelets are two transforms that aim to achieve a
	multiscale and a multidirectional decomposition of arbitrary N-dimensional
	($N>=2$) signals. While both transforms are Fourier-based, their
	construction is intrinsically different. In this talk we will give
	and overview of the construction of the two transforms, and explore
	their properties such as frequency domain / spatial domain coherence,
	sparsity, redundancy and computational complexity.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Evgeniy_Curv.pdf}
}

@CONFERENCE{lebed08SINBADaoc1,
  author = {Evgeniy Lebed},
  title = {Applications of Curvelets/Surfacelets to seismic data processing},
  booktitle = {SINBAD},
  year = {2008},
  abstract = {In this talk we explore several applications of the curvelet and surfacelet
	transforms to seismic data processing. The first application is stable
	signal recovery in the physical domain - seismic data acquisition
	is often limited by physical and economic constraints, and the goal
	is to interpolate the data from a given subset of seismic traces.
	The second application is signal recovery in a transform domain -
	we assume that our data comes in a form of a random subset of temporal
	frequencies and the goal is to recover the missing frequencies from
	this data. Since seismic signals are generally not bandwidth limited,
	this in fact becomes an anti-aliasing problem. In both these problems
	the recovery is resolved via a robust l_1 solver that exploits the
	sparsity of the signals in curvelet/surfacelet domains. In the last
	application we explore the problem of primary-multiple separation
	by simple thresholding.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Evgeniy_App.pdf}
}

@CONFERENCE{lin08SINBADcwe,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Compressed wavefield extrapolation},
  booktitle = {SINBAD},
  year = {2008},
  keywords = {Presentation, SINBAD, SLIM},
presentation ={ https://www.slim.eos.ubc.ca/sites/data/Papers/lin08cwe.pdf}
}

@CONFERENCE{lin08SEGiso,
  author = {Tim T.Y. Lin and Evgeniy Lebed and Yogi A. Erlangga and Felix J.
	Herrmann},
  title = {Interpolating solutions of the Helmholtz equation with compressed
	sensing},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  number = {1},
  pages = {2122-2126},
  month = {November},
  organization = {SEG},
  publisher = {SEG},
  abstract = {We present an algorithm which allows us to model wavefields with frequency-domain
	methods using a much smaller number of frequencies than that typically
	required by the classical sampling theory in order to obtain an alias-free
	result. The foundation of the algorithm is the recent results on
	the compressed sensing, which state that data can be successfully
	recovered from an incomplete measurement if the data is sufficiently
	sparse. Results from numerical experiment show that only 30\% of
	the total frequency spectrum is need to capture the full wavefield
	information when working in the hard 2D synthetic Marmousi model.},
  keywords = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg08/lin08ish.pdf
	},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/lin08seg.pdf}
}

@CONFERENCE{maysami08SEGlcf,
  author = {Mohammad Maysami and Felix J. Herrmann},
  title = {Lithological constraints from seismic waveforms: application to opal-A
	to opal-CT transition},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  number = {1},
  pages = {2011-2015},
  month = {November},
  organization = {SEG},
  publisher = {SEG},
  abstract = {n this paper, we present a new method for seismic waveform characterization
	whose aim is threefold, namely (i) extraction of detailed information
	on the sharpness of transitions in the subsurface from seismic waveforms,
	(ii) reflector modeling, based on binary-mixture and percolation
	theory, and (iii) establishment of well-seismic ties, through parameterizations
	of our waveform and critical reflector model. We test this methodology
	on the opal-A (Amorphous) to opal-CT (Cristobalite/Tridymite) transition
	imaged in a migrated section of North Sea field data West of the
	Shetlands.},
  keywords = {SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2008/mohammad08seg.pdf}
}

@CONFERENCE{modzelewski08SINBADdas,
  author = {Henryk Modzelewski},
  title = {Design and specifications for SLIM{\textquoteright}s software framework},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {The SLIM group is actively developing software for seismic imaging.
	This talk will give a general overview of the software development
	during SINBAD project with focus on the final release in February
	2008. The covered topics will include: 1) adopting Python for object-oriented
	programming, 2) including parallelism into the algorithms used in
	seismic imaging/modeling, 3) in-house algorithms for seismic imaging,
	and 4) contributions to Madagascar (RSF). The talk will serve as
	an introduction to the other presentations in the session "SINBAD
	Software releases".},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Henryk_Des.pdf}
}

@CONFERENCE{moghaddam08SINBADrtm,
  author = {Peyman P. Moghaddam},
  title = {Reverse-time Migration Amplitude Recovery with Curvelets},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We recover the amplitude of a seismic image by approximating the normal
	(demigration-migration) operator. In this approximation, we make
	use of the property that curvelets remain invariant under the action
	of the normal operator. We propose a seismic amplitude recovery method
	that employs an eigenvalue like decomposition for the normal operator
	using curvelets as eigenvectors. Subsequently, we propose an approximate
	nonlinear singularity-preserving solution to the least-squares seismic
	imaging problem with sparseness in the curvelet domain and spatial
	continuity constraints. Our method is tested with a reverse-time
	{\textquoteright}wave-equation{\textquoteright} migration code simulating
	the acoustic wave equation.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Peyman_Mig.pdf}
}

@CONFERENCE{moghaddam08SEGcbm,
  author = {Peyman P. Moghaddam and Cody R. Brown and Felix J. Herrmann},
  title = {Curvelet-based migration preconditioning},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  number = {1},
  pages = {2211-2215},
  month = {November},
  organization = {SEG},
  publisher = {SEG},
  abstract = {In this paper, we introduce a preconditioner for seismic imaging{\textendash}-i.e.,
	the inversion of the linearized Born scattering operator. This preconditioner
	approximately corrects for the {\textquoteleft}{\textquoteleft}square
	root{\textquoteright}{\textquoteright} of the normal{\textendash}-i.e.,
	the demigration-migration operator. This approach consists of three
	parts, namely (i) a left preconditoner, defined by a fractional time
	integration designed to make the migration operator zero order, and
	two right preconditioners that apply (ii) a scaling in the physical
	domain accounting for a spherical spreading, and (iii) a curvelet-domain
	scaling that corrects for spatial and reflector-dip dependent amplitude
	errors. We show that a combination of these preconditioners lead
	to a significant improvement of the convergence for iterative least-squares
	solutions to the seismic imaging problem based on reverse-time migration
	operators.},
  keywords = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg08/brown08cmp.pdf
	},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/moghaddam08seg.pdf}
}

@CONFERENCE{ross08SINBADsit,
  author = {Sean Ross-Ross},
  title = {Seismic inversion through operator overloading},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {Geophysical processing is dominated by many different out of core
	memory software environments (OOCE). Such environments include Madagascar
	and SU and are designed to handle data that can not be operated on
	in memory. Each base operation is created as a main program that
	reads data from disk and writes the result to disk. The main programs
	can also be chained together on stdin/out pipes using a shell only
	writing data to disk at the end. To be efficient, the algorithm using
	an OOCE must chain together the longest pipe to avoid disk I/O, as
	a result it is very difficult to use iterative techniques. The algorithms
	are written in shell scripts can be difficult to read and understand.
	SLIMpy is a software library that contains definitions of coordinate
	free vectors and linear operators. It allows the user to design and
	run algorithms with any out of core package, in a Matlab style interface
	while maintaining optimal efficiency and speed. SLIMpy looks at each
	main program of each OOCE as a Matrix vector operation or vector
	reduction/transformation operation. It uses operator overloading
	to generate an abstract syntax tree (AST) which can be optimized
	in many ways before executing its commands. The AST also provides
	a pathway for embarrassingly parallel applications by splitting the
	tree over different nodes and processors. SLIMpy provides an interface
	to these OOCE that allows for optimal construction of commands and
	allows for iterative techniques. It smoothes the transition from
	other languages such as Matlab and allows the algorithm designer
	to write readable and reusable code. SLIMpy also adds to OOCE by
	allowing for easy parallelization.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Sean_Sli.pdf}
}

@CONFERENCE{saab08SINBADcps,
  author = {Rayan Saab},
  title = {Curvelet-Based Primary-Multiple Separation from a Bayesian Perspective},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We present a novel primary-multiple separation scheme which makes
	use of the sparsity of both primaries and multiples in a transform
	domain, such as the curvelet transform, to provide estimates of each.
	The proposed algorithm utilizes seismic data as well as the output
	of a preliminary step that provides (possibly) erroneous predictions
	of the multiples. The algorithm separates the signal components,
	i.e., the primaries and multiples, by solving an optimization problem
	that assumes noisy input data and can be derived from a Bayesian
	perspective. More precisely, the optimization problem can be arrived
	at via an assumption of a weighted Laplacian distribution for the
	primary and multiple coefficients in the transform domain and of
	white Gaussian noise contaminating both the seismic data and the
	preliminary prediction of the multiples, which both serve as input
	to the algorithm.},
  date-modified = {2008-08-22 12:45:53 -0700},
  keywords = {SLIM, SINBAD, Presentation},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Rayan_Curv.pdf}
}

@CONFERENCE{vandenberg08IAMesr,
  author = {E. van den Berg},
  title = {Exact sparse reconstruction and neighbourly polytopes},
  booktitle = {IAM},
  year = {2008},
  bdsk-url-1 = {http://slim.eos.ubc.ca/Publications/Private/Presentations/2008/vandenberg08iam_pres.pdf},
  date-added = {2008-08-26 15:44:44 -0700},
  date-modified = {2008-08-26 15:45:58 -0700},
  keywords = {SLIM, IAM, Presentation},
  presentation = {http://slim.eos.ubc.ca/Publications/Private/Presentations/2008/vandenberg08iam_pres.pdf}
}


@CONFERENCE{wang08SINBADrri,
  author = {Deli Wang and Rayan Saab and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Recent results in curvelet-based primary-multiple separation},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We present a nonlinear curvelet-based sparsity-promoting formulation
	for the primary-multiple separation problem. We show that these coherent
	signal components can be separated robustly by explicitly exploiting
	the locality of curvelets in phase space (space-spatial frequency
	plane) and their ability to compress data volumes that contain wavefronts.
	This work is an extension of earlier results and the presented algorithms
	are shown to be stable under noise and moderately erroneous multiple
	predictions.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Deli_Rec.pdf}
}

@CONFERENCE{yan08SINBADwru,
  author = {Jiupeng Yan},
  title = {Wavefield Reconstruction Using Simultaneous Denoising Interpolation
	vs. Denoising after Interpolation},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {This report represents and compares two methods of wavefield reconstruction
	from noisy seismic data with missing traces. The two methods are
	(i) First interpolate incomplete noisy data to get complete noisy
	data and then denoise, and (ii) Interpolate and denoise the incomplete
	noisy data simultaneously. A sample test of synthetic data will be
	presented. The results of tests show that denoising after interpolation
	is better than simultaneous denoising and interpolation if the parameter
	of the denoising problem is chosen appropriately.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Jiapeng_Wav.pdf}
}

@CONFERENCE{yarham08SINBADbss,
  author = {Carson Yarham},
  title = {Bayesian signal separation applied to ground-roll removal},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {Accurate and adaptive noise removal is a critical part in seismic
	processing. Recent developments in signal separation methods have
	allowed a more flexible and accurate framework in which to perform
	ground roll and reflector separation. The use of a new Bayesian separation
	scheme developed at the SLIM group that contains control parameters
	to adjust for the uniqueness of specific problems is used. The sensitivity
	and variation of the control parameters is examined and this method
	is applied to synthetic and real data and the results are compared
	to previous methods.},
  date-modified = {2008-08-22 12:42:58 -0700},
  keywords = {SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Carson_Gro.pdf}
}

@CONFERENCE{yarham08SEGbgr,
  author = {Carson Yarham and Felix J. Herrmann},
  title = {Bayesian ground-roll seperation by curvelet-domain sparsity promotion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  number = {1},
  pages = {2576-2580},
  month = {November},
  organization = {SEG},
  publisher = {SEG},
  abstract = {The removal of coherent noise generated by surface waves in land based
	seismic is a prerequisite to imaging the subsurface. These surface
	waves, termed as ground roll, overlay important reflector information
	in both the t-x and f-k domains. Standard techniques of ground-roll
	removal commonly alter reflector information. We propose the use
	of the curvelet domain as a sparsifying transform in which to preform
	signal-separation techniques that preserves reflector information
	while increasing ground-roll removal. We look at how this method
	preforms on synthetic data for which we can build quantitative results
	and a real field data set.},
  keywords = {SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2008/yarham08seg.pdf}
}

@CONFERENCE{yilmaz08SINBADsse,
  author = {Ozgur Yilmaz},
  title = {Stable sparse expansions via non-convex optimization},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We present theoretical results pertaining to the ability of p-(quasi)norm
	minimization to recover sparse and compressible signals from incomplete
	and noisy measurements. In particular, we extend the results of Candes,
	Romberg and Tao for 1-norm to the p<1 case. Our results indicate
	that depending on the restricted isometry constants and the noise
	level, p-norm minimization with certain values of p<1 provides better
	theoretical guarantees in terms of stability and robustness compared
	to 1-norm minimization. This is especially true when the restricted
	isometry constants are relatively large, or equivalently, when the
	data is significantly undersampled.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Ozgur_Sta.pdf}
}

%----------------------------2007---------------------

@CONFERENCE{challa07EAGEsrf,
  author = {Sastry S. Challa and Gilles Hennenfent and Felix J. Herrmann},
  title = {Signal reconstruction from incomplete and misplaced measurements},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  month = {June},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {Constrained by practical and economical considerations, one often
	uses seismic data with missing traces. The use of such data results
	in image artifacts and poor spatial resolution. Sometimes due to
	practical limitations, measurements may be available on a perturbed
	grid, instead of on the designated grid. Due to algorithmic requirements,
	when such measurements are viewed as those on the designated grid,
	the recovery procedures may result in additional artifacts. This
	paper interpolates incomplete data onto regular grid via the Fourier
	domain, using a recently developed greedy algorithm. The basic objective
	is to study experimentally as to what could be the size of the perturbation
	in measurement coordinates that allows for the measurements on the
	perturbed grid to be considered as on the designated grid for faithful
	recovery. Our experimental work shows that for compressible signals,
	a uniformly distributed perturbation can be offset with slightly
	more number of measurements.},
  keywords = {SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/sastry07.pdf}
}

@CONFERENCE{hennenfent07SINBADjdn,
  author = {Gilles Hennenfent},
  title = {Just denoise. Nonlinear recovery from randomly sampled data},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {In this talk, we turn the interpolation problem of coarsely-sampled
	data into a denoising problem. From this point of view, we illustrate
	the benefit of random sampling at sub-Nyquist rate over regular sampling
	at the same rate. We show that, using nonlinear sparsity-promoting
	optimization, coarse random sampling may actually lead to significantly
	better wavefield reconstruction than equivalent regularly sampled
	data.},
  keywords = {Presentation, SINBAD, SLIM}
}

@CONFERENCE{hennenfent07EAGEcrw,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Curvelet reconstruction with sparsity-promoting inversion: successes
	an challenges},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  month = {June},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {In this overview of the recent Curvelet Reconstruction with Sparsity-promoting
	Inver- sion (CRSI) method, we present our latest 2-D and 3-D interpolation
	results on both synthetic and real datasets. We compare these results
	to interpolated data using other ex- isting methods. Finally, we
	discuss the challenges related to sparsity-promoting solvers for
	the large-scale problems the industry faces.},
  keywords = {SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07eage_workshop.pdf}
}

@CONFERENCE{hennenfent07EAGEisf,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Irregular sampling: from aliasing to noise},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  month = {June},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {Seismic data is often irregularly and/or sparsely sampled along spatial
	coordinates. We show that these acquisition geometries are not necessarily
	a source of adversity in order to accurately reconstruct adequately-sampled
	data. We use two examples to illustrate that it may actually be better
	than equivalent regularly subsampled data. This comment was already
	made in earlier works by other authors. We explain this behavior
	by two key observations. Firstly, a noise-free underdetermined problem
	can be seen as a noisy well-determined problem. Secondly, regularly
	subsampling creates strong coherent acquisition noise (aliasing)
	difficult to remove unlike the noise created by irregularly subsampling
	that is typically weaker and Gaussian-like.},
  keywords = {SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07eage.pdf}
}

@CONFERENCE{hennenfent07SINBADrii,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Recent insights in $L_1$ solvers},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {During this talk, an overview is given on our work on norm-one solvers
	as part of the DNOISE project. Gilles will explain the ins and outs
	of our iterative thresholding solver based on log cooling while Felix
	will present the work of Michael Friedlander "A Newton root-finding
	algorithms for large-scale basis pursuit denoise". Both approaches
	involve the solution of the basis pursuit problem that seeks a minimum
	one-norm solution of an underdetermined least-squares problem. Basis
	pursuit denoise (BPDN) fits the least-squares problem only approximately,
	and a single parameter determines a curve that traces the trade-off
	between the least-squares fit and the one-norm of the solution. In
	the work of Friedlander, it is shown show that the function that
	describes this curve is convex and continuously differentiable over
	all points of interest. They describe an efficient procedure for
	evaluating this function and its derivatives. As a result, they can
	compute arbitrary points on this curve. Their method is suitable
	for large-scale problems. Only matrix-vector operations are required.
	This is joint work with Ewout van der Berg and Michael P. Friedlander},
  keywords = {Presentation, SINBAD, SLIM}
}

@CONFERENCE{hennenfent07SEGrsn,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Random sampling: New insights into the reconstruction of coarsely
	sampled wavefields},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  number = {1},
  pages = {2575-2579},
  organization = {SEG},
  publisher = {SEG},
  abstract = {In this paper, we turn the interpolation problem of coarsely-sampled
	data into a denoising problem. From this point of view, we illustrate
	the benefit of random sampling at sub-Nyquist rate over regular sampling
	at the same rate. We show that, using nonlinear sparsity-promoting
	optimization, coarse random sampling may actually lead to significantly
	better wavefield reconstruction than equivalent regularly sampled
	data. {\copyright}2007 Society of Exploration Geophysicists},
  doi = {10.1190/1.2793002},
  keywords = {SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/hennenfent07seg.pdf}
}

@CONFERENCE{herrmann07AMScsi,
  author = {Felix J. Herrmann},
  title = {Compressive seismic imaging},
  booktitle = {AMS Von Neumann},
  year = {2007},
  abstract = {Seismic imaging involves the solution of an inverse-scattering problem
	during which the energy of (extremely) large data volumes is collapsed
	onto the Earth{\textquoteright}s reflectors. We show how the ideas
	from {\textquoteright}compressive sampling{\textquoteright} can alleviate
	this task by exploiting the curvelet transform{\textquoteright}s
	{\textquoteright}wavefront-set detection{\textquoteright} capability
	and {\textquoteright}invariance{\textquoteright} property under wave
	propagation. First, a wavelet-vaguellete technique is reviewed, where
	seismic amplitudes are recovered from complete data by diagonalizing
	the Gramm matrix of the linearized scattering problem. Next, we show
	how the recovery of seismic wavefields from incomplete data can be
	cast into a compressive sampling problem, followed by a proposal
	to compress wavefield extrapolation operators via compressive sampling
	in the modal domain. During the latter approach, we explicitly exploit
	the mutual incoherence between the eigenfunctions of the Helmholtz
	operator and the curvelet frame elements that compress the extrapolated
	wavefield. This is joint work with Gilles Hennenfent, Peyman Moghaddam,
	Tim Lin, Chris Stolk and Deli Wang.},
  keywords = {AMS Von Neumann, Presentation, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07Neumann.pdf}
}

@CONFERENCE{herrmann07PIMScsm,
  author = {Felix J. Herrmann},
  title = {Compressive sampling meets seismic imaging},
  booktitle = {PIMS},
  year = {2007},
  keywords = {PIMS, Presentation, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07PIMS.pdf}
}

@CONFERENCE{herrmann07SINBADcwe,
  author = {Felix J. Herrmann},
  title = {Compressed wavefield extrapolation},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {An explicit algorithm for the extrapolation of one-way wavefields
	is proposed which combines recent developments in information theory
	and theoretical signal processing with the physics of wave propagation.
	Because of excessive memory requirements, explicit formulations for
	wave propagation have proven to be a challenge in 3-D. By using ideas
	from {\textquoteleft}{\textquoteleft}compressed sensing{\textquoteright}{\textquoteright},
	we are able to formulate the (inverse) wavefield extrapolation problem
	on small subsets of the data volume, thereby reducing the size of
	the operators. According to compressed sensing theory, signals can
	successfully be recovered from an incomplete set of measurements
	when the measurement basis is incoherent with the representation
	in which the wavefield is sparse. In this new approach, the eigenfunctions
	of the Helmholtz operator are recognized as a basis that is incoherent
	with curvelets that are known to compress seismic wavefields. By
	casting the wavefield extrapolation problem in this framework, wavefields
	can successfully be extrapolated in the modal domain via a computationally
	cheaper operation. A proof of principle for the {\textquoteleft}{\textquoteleft}compressed
	sensing{\textquoteright}{\textquoteright} method is given for wavefield
	extrapolation in 2-D. The results show that our method is stable
	and produces identical results compared to the direct application
	of the full extrapolation operator. This is joint work with Tim Lin.},
  keywords = {Presentation, SINBAD, SLIM},
url={http://slim.eos.ubc.ca/Publications/Public/Journals/CompressedExtrap.pdf}
}

@CONFERENCE{herrmann07SINBADfrw,
  author = {Felix J. Herrmann},
  title = {Focused recovery with the curvelet transform},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {Incomplete data represents a major challenge for a successful prediction
	and subsequent removal of multiples. In this paper, a new method
	will be represented that tackles this challenge in a two-step approach.
	During the first step, the recently developed curvelet-based recovery
	by sparsity-promoting inversion (CRSI) is applied to the data, followed
	by a prediction of the primaries. During the second high-resolution
	step, the estimated primaries are used to improve the frequency content
	of the recovered data by combining the focal transform, defined in
	terms of the estimated primaries, with the curvelet transform. This
	focused curvelet transform leads to an improved recovery, which can
	subsequently be used as input for a second stage of multiple prediction
	and primary-multiple separation. This is joint work with Deli Wang
	and Gilles Hennenfent.},
  keywords = {Presentation, SINBAD, SLIM},
presentation ={https://www.slim.eos.ubc.ca/sites/data/Papers/Herrmann2007SINBADfoc.pdf }
}

@CONFERENCE{herrmann07SLIMfsd,
  author = {Felix J. Herrmann},
  title = {From seismic data to the composition of rocks: an interdisciplinary
	and multiscale approach to exploration seismology},
  booktitle = {Berkhout{\textquoteright}s valedictory address: the conceptual approach
	of understanding},
  year = {2007},
  abstract = {In this essay, a nonlinear and multidisciplinary approach is presented
	that takes seismic data to the composition of rocks. The presented
	work has deep roots in the {\textquoteleft}gedachtengoed{\textquoteright}
	(philosophy) of Delphi spearheaded by Guus Berkhout. Central themes
	are multiscale, object-orientation and a multidisciplinary approach.},
  keywords = {SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/Misc/herrmann07fsd.pdf}
}

@CONFERENCE{herrmann07COIPpti,
  author = {Felix J. Herrmann},
  title = {Phase transitions in explorations seismology: statistical mechanics
	meets information theory},
  booktitle = {COIP},
  year = {2007},
  abstract = {In this paper, two different applications of phase transitions to
	exploration seismology will be discussed. The first application concerns
	a phase diagram ruling the recovery conditions for seismic data volumes
	from incomplete and noisy data while the second phase transition
	describes the behavior of bi-compositional mixtures as a function
	of the volume fraction. In both cases, the phase transitions are
	the result of randomness in large system of equations in combination
	with nonlinearity. The seismic recovery problem from incomplete data
	involves the inversion of a rectangular matrix. Recent results from
	the field of "compressive sensing" provide the conditions for a successful
	recovery of functions that are sparse in some basis (wavelet) or
	frame (curvelet) representation, by means of a sparsity ($\ell_1$-norm)
	promoting nonlinear program. The conditions for a successful recovery
	depend on a certain randomness of the matrix and on two parameters
	that express the matrix{\textquoteright} aspect ratio and the ratio
	of the number of nonzero entries in the coefficient vector for the
	sparse signal representation over the number of measurements. It
	appears that the ensemble average for the success rate for the recovery
	of the sparse transformed data vector by a nonlinear sparsity promoting
	program, can be described by a phase transition, demarcating the
	regions for the two ratios for which recovery of the sparse entries
	is likely to be successful or likely to fail. Consistent with other
	phase transition phenomena, the larger the system the sharper the
	transition. The randomness in this example is related to the construction
	of the matrix, which for the recovery of spike trains corresponds
	to the randomly restricted Fourier matrix. It is shown, that these
	ideas can be extended to the curvelet recovery by sparsity-promoting
	inversion (CRSI) . The second application of phase transitions in
	exploration seismology concerns the upscaling problem. To counter
	the intrinsic smoothing of singularities by conventional equivalent
	medium upscaling theory, a percolation-based nonlinear switch model
	is proposed. In this model, the transport properties of bi-compositional
	mixture models for rocks undergo a sudden change in the macroscopic
	transport properties as soon as the volume fraction of the stronger
	material reaches a critical point. At this critical point, the stronger
	material forms a connected cluster, which leads to the creation of
	a cusp-like singularity in the elastic moduli, which in turn give
	rise to specular reflections. In this model, the reflectivity is
	no longer explicitly due to singularities in the rocks composition.
	Instead, singularities are created whenever the volume fraction exceeds
	the critical point. We will show that this concept can be used for
	a singularity-preserved lithological upscaling.},
  keywords = {Presentation, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07COIP.pdf}
}

@CONFERENCE{herrmann07EAGErdi,
  author = {Felix J. Herrmann},
  title = {Recent developments in curvelet-based seismic processing},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  month = {June},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {Combinations of parsimonious signal representations with nonlinear
	sparsity promoting programs hold the key to the next-generation of
	seismic data processing algorithms ... Since they allow for a formulation
	that is stable w.r.t. noise \& incomplete data do not require prior
	information on the velocity or locations and dips of the events},
  keywords = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07EAGEWSDEV.pdf}
}

@CONFERENCE{herrmann07SINBADrdi2,
  author = {Felix J. Herrmann},
  title = {Recent developments in primary-multiple separation},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {In this talk, we present a novel primary-multiple separation scheme
	which makes use of the sparsity of both primaries and multiples in
	a transform domain, such as the curvelet transform, to provide estimates
	of each. The proposed algorithm utilizes seismic data as well as
	the output of a preliminary step that provides (possibly) erroneous
	predictions of the multiples. The algorithm separates the signal
	components, i.e., the primaries and multiples, by solving an optimization
	problem that assumes noisy input data and can be derived from a Bayesian
	perspective. More precisely, the optimization problem can be arrived
	at via an assumption of a weighted Laplacian distribution for the
	primary and multiple coefficients in the transform domain and of
	white Gaussian noise contaminating both the seismic data and the
	preliminary prediction of the multiples, which both serve as input
	to the algorithm. Time permitted, we will also briefly discuss a
	propasal for adaptive curvelet-domain matched filtering. This is
	joint work with Deli Wang, Rayan Saaba, {\o}zgur Yilmaz and Eric
	Verschuur.},
  keywords = {Presentation, SINBAD, SLIM},
presentation={http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg07/herrmann07rdi.pdf}
}

@CONFERENCE{herrmann07SINBADsia2,
  author = {Felix J. Herrmann},
  title = {Seismic image amplitude recovery},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {In this talk, we recover the amplitude of a seismic image by approximating
	the normal (demigration-migration) operator. In this approximation,
	we make use of the property that curvelets remain invariant under
	the action of the normal operator. We propose a seismic amplitude
	recovery method that employs an eigenvalue like decomposition for
	the normal operator using curvelets as eigen-vectors. Subsequently,
	we propose an approximate nonlinear singularity-preserving solution
	to the least-squares seismic imaging problem with sparseness in the
	curvelet domain and spatial continuity constraints. Our method is
	tested with a reverse-time {\textquoteright}wave-equation{\textquoteright}
	migration code simulating the acoustic wave equation on the SEG-AA
	salt model. This is joint work with Peyman Moghaddam and Chris Stolk
	(University of Twente)},
  keywords = {Presentation, SINBAD, SLIM}
}

@CONFERENCE{herrmann07AIPsit,
  author = {Felix J. Herrmann},
  title = {Seismic inversion through operator overloading},
  booktitle = {AIP},
  year = {2007},
  abstract = {Inverse problems in (exploration) seismology are known for their large
	to very large scale. For instance, certain sparsity-promoting inversion
	techniques involve vectors that easily exceed 230 unknowns while
	seismic imaging involves the construction and application of matrix-free
	discretized operators where single matrix-vector evaluations may
	require hours, days or even weeks on large compute clusters. For
	these reasons, software development in this field has remained the
	domain of highly technical codes programmed in low-level languages
	with little eye for easy development, code reuse and integration
	with (nonlinear) programs that solve inverse problems. Following
	ideas from the Symes{\textquoteright} Rice Vector Library and Bartlett{\textquoteright}s
	C++ object-oriented interface, Thyra, and Reduction/Transformation
	operators (both part of the Trilinos software package), we developed
	a software-development environment based on overloading. This environment
	provides a pathway from in-core prototype development to out-of-core
	and MPI {\textquoteright}production{\textquoteright} code with a
	high level of code reuse. This code reuse is accomplished by integrating
	the out-of-core and MPI functionality into the dynamic object-oriented
	programming language Python. This integration is implemented through
	operator overloading and allows for the development of a coordinate-free
	solver framework that (i) promotes code reuse; (ii) analyses the
	statements in an abstract syntax tree and (iii) generates executable
	statements. In the current implementation, we developed an interface
	to generate executable statements for the out-of-core unix-pipe based
	(seismic) processing package RSF-Madagascar (rsf.sf.net). The modular
	design allows for interfaces to other seismic processing packages
	and to in-core Python packages such as numpy. So far, the implementation
	overloads linear operators and elementwise reduction/transformation
	operators. We are planning extensions towards nonlinear operators
	and integration with existing (parallel) solver frameworks such as
	Trilinos.},
  keywords = {AIP, Presentation, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07AIP2.pdf}
}

@CONFERENCE{herrmann07CYBERsmc,
  author = {Felix J. Herrmann},
  title = {Seismology meets compressive sampling presented at the joint NSF-IPAM
	meeting. Los Angeles. October, 2007.},
  booktitle = {Cyber},
  year = {2007},
  abstract = {Presented at Cyber-Enabled Discovery and Innovation: Knowledge Extraction
	as a success story lecture. See for more detail https://www.ipam.ucla.edu/programs/cdi2007/},
  keywords = {Cyber, Presentation, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07Cyber.pdf}
}

@CONFERENCE{herrmann07EAGEsrm,
  author = {Felix J. Herrmann},
  title = {Surface related multiple prediction from incomplete data},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  month = {June},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {Incomplete data, unknown source-receiver signatures and free-surface
	reflectivity represent challenges for a successful prediction and
	subsequent removal of multiples. In this paper, a new method will
	be represented that tackles these challenges by combining what we
	know about wavefield (de-)focussing, by weighted convolutions/correlations,
	and recently developed curvelet-based recovery by sparsity-promoting
	inversion (CRSI). With this combination, we are able to leverage
	recent insights from wave physics to- wards a nonlinear formulation
	for the multiple-prediction problem that works for incomplete data
	and without detailed knowledge on the surface effects.},
  keywords = {SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/herrmann07a.pdf}
}

@CONFERENCE{herrmann07EAGEsrm1,
  author = {Felix J. Herrmann},
  title = {Surface related multiple prediction from incomplete data},
  booktitle = {EAGE},
  year = {2007},
  keywords = {EAGE, Presentation, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07EAGPRED.pdf}
}

@CONFERENCE{herrmann07AIPssd,
  author = {Felix J. Herrmann},
  title = {Stable seismic data recovery},
  booktitle = {AIP},
  year = {2007},
  abstract = {In this talk, directional frames, known as curvelets, are used to
	recover seismic data and images from noisy and incomplete data. Sparsity
	and invariance properties of curvelets are exploited to formulate
	the recovery by a {\textquoteleft}1-norm promoting program. It is
	shown that our data recovery approach is closely linked to the recent
	theory of {\textquoteleft}{\textquoteleft}compressive sensing{\textquoteright}{\textquoteright}
	and can be seen as a first step towards a nonlinear sampling theory
	for wavefields. The second problem that will be discussed concerns
	the recovery of the amplitudes of seismic images in clutter. There,
	the invariance of curvelets is used to approximately invert the Gramm
	operator of seismic imaging. In the high-frequency limit, this Gramm
	matrix corresponds to a pseudo-differential operator, which is near
	diagonal in the curvelet domain.In this talk, directional frames,
	known as curvelets, are used to recover seismic data and images from
	noisy and incomplete data. Sparsity and invariance properties of
	curvelets are exploited to formulate the recovery by a l1-norm promoting
	program. It is shown that our data recovery approach is closely linked
	to the recent theory of {\textquoteleft}{\textquoteleft}compressive
	sensing{\textquoteright}{\textquoteright} and can be seen as a first
	step towards a nonlinear sampling theory for wavefields. The second
	problem that will be discussed concerns the recovery of the amplitudes
	of seismic images in clutter. There, the invariance of curvelets
	is used to approximately invert the Gramm operator of seismic imaging.
	In the high-frequency limit, this Gramm matrix corresponds to a pseudo-differential
	operator, which is near diagonal in the curvelet domain.},
  keywords = {AIP, Presentation, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07AIP.pdf}
}

@CONFERENCE{herrmann07EAGEsia,
  author = {Felix J. Herrmann and Gilles Hennenfent and Peyman P. Moghaddam},
  title = {Seismic imaging and processing with curvelets},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  month = {June},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {In this paper, we present a nonlinear curvelet-based sparsity-promoting
	formulation for three problems in seismic processing and imaging
	namely, seismic data regularization from data with large percentages
	of traces missing; seismic amplitude recovery for sub-salt images
	obtained by reverse-time migration and primary-multiple separation,
	given an inaccurate multiple prediction. We argue why these nonlinear
	formulations are beneficial.},
  keywords = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07EAGEIMPROC.pdf},
  url = {http://slim.eos.ubc.ca/Publications/Private/Conferences/EAGE/2007/herrmann07b.pdf}
}

@CONFERENCE{herrmann07EAGEjda,
  author = {Felix J. Herrmann and Peyman P. Moghaddam},
  title = {Just diagonalize: a curvelet-based approach to seismic amplitude
	recovery},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  month = {June},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {In this presentation we present a nonlinear curvelet-based sparsity-promoting
	formulation for the recovery of seismic amplitudes. We show that
	the curvelet{\textquoteright}s wavefront detection capability and
	invariance under wave propagation lead to a formulation of this recovery
	problem that is stable under noise and missing data.},
  keywords = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07EAGEWSIM.pdf}
}

@CONFERENCE{herrmann07SEGmpf,
  author = {Felix J. Herrmann and Deli Wang and Gilles Hennenfent},
  title = {Multiple prediction from incomplete data with the focused curvelet
	transform},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  number = {1},
  pages = {2505-2509},
  organization = {SEG},
  publisher = {SEG},
  abstract = {Incomplete data represents a major challenge for a successful prediction
	and subsequent removal of multiples. In this paper, a new method
	will be represented that tackles this challenge in a two-step approach.
	During the first step, the recenly developed curvelet-based recovery
	by sparsity-promoting inversion (CRSI) is applied to the data, followed
	by a prediction of the primaries. During the second high-resolution
	step, the estimated primaries are used to improve the frequency content
	of the recovered data by combining the focal transform, defined in
	terms of the estimated primaries, with the curvelet transform. This
	focused curvelet transform leads to an improved recovery, which can
	subsequently be used as input for a second stage of multiple prediction
	and primary-multiple separation. {\copyright}2007 Society of Exploration
	Geophysicists},
  doi = {10.1190/1.2792987},
  keywords = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07SEGPRED.pdf},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/herrmann07segb.pdf}
}

@CONFERENCE{herrmann07SEGsdp,
  author = {Felix J. Herrmann and Deli Wang and Gilles Hennenfent and Peyman
	P. Moghaddam},
  title = {Seismic data processing with curvelets: a multiscale and nonlinear
	approach},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  number = {1},
  pages = {2220-2224},
  organization = {SEG},
  publisher = {SEG},
  abstract = {In this abstract, we present a nonlinear curvelet-based sparsity-promoting
	formulation of a seismic processing flow, consisting of the following
	steps: seismic data regularization and the restoration of migration
	amplitudes. We show that the curvelet{\textquoteright}s wavefront
	detection capability and invariance under the migration-demigration
	operator lead to a formulation that is stable under noise and missing
	data. {\copyright}2007 Society of Exploration Geophysicists},
  doi = {10.1190/1.2792927},
  keywords = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07SEGPROC.pdf},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/herrmann07sega.pdf}
}

@CONFERENCE{herrmann07SEGsnt,
  author = {Felix J. Herrmann and D. Wilkinson},
  title = {Seismic noise: the good, the bad and the ugly},
  booktitle = {SEG Summer Research Workshop: Seismic Noise: Origins Preventions,
	Mitigation, Utilization},
  year = {2007},
  note = {Presented at SEG Summer Research Workshop: Seismic Noise: Origins,
	Prevention, Mitigation, Utilization},
  abstract = {In this paper, we present a nonlinear curvelet-based sparsity-promoting
	formulation for three problems related to seismic noise, namely the
	{\textquoteright}good{\textquoteright}, corresponding to noise generated
	by random sampling; the {\textquoteright}bad{\textquoteright}, corresponding
	to coherent noise for which (inaccurate) predictions exist and the
	{\textquoteright}ugly{\textquoteright} for which no predictions exist.
	We will show that the compressive capabilities of curvelets on seismic
	data and images can be used to tackle these three categories of noise-related
	problems.},
  keywords = {SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/herrmann07segws.pdf}
}


@CONFERENCE{lin07SEGcwe1,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Compressed wavefield extrapolation with curvelets},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  number = {1},
  pages = {1997-2001},
  organization = {SEG},
  publisher = {SEG},
  abstract = {An explicit algorithm for the extrapolation of one-way wavefields
	is proposed which combines recent developments in information theory
	and theoretical signal processing with the physics of wave propagation.
	Because of excessive memory requirements, explicit formulations for
	wave propagation have proven to be a challenge in 3-D. By using ideas
	from {\textquoteleft}{\textquoteleft}compressed sensing{\textquoteright}{\textquoteright},
	we are able to formulate the (inverse) wavefield extrapolation problem
	on small subsets of the data volume, thereby reducing the size of
	the operators. According to compressed sensing theory, signals can
	successfully be recovered from an imcomplete set of measurements
	when the measurement basis is incoherent with the representation
	in which the wavefield is sparse. In this new approach, the eigenfunctions
	of the Helmholtz operator are recognized as a basis that is incoherent
	with curvelets that are known to compress seismic wavefields. By
	casting the wavefield extrapolation problem in this framework, wavefields
	can successfully be extrapolated in the modal domain via a computationally
	cheaper operation. A proof of principle for the {\textquoteleft}{\textquoteleft}compressed
	sensing{\textquoteright}{\textquoteright} method is given for wavefield
	extrapolation in 2-D. The results show that our method is stable
	and produces identical results compared to the direct application
	of the full extrapolation operator. {\copyright}2007 Society of Exploration
	Geophysicists},
  doi = {10.1190/1.2792882},
  keywords = {SLIM},
presentation ={http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/lin07SEG.pdf },
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/lin2007seg.pdf}
}


@CONFERENCE{maysami07EAGEsrc,
  author = {Mohammad Maysami and Felix J. Herrmann},
  title = {Seismic reflector characterization by a multiscale detection-estimation
	method},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  month = {June},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {Seismic transitions of the subsurface are typically considered as
	zero-order singularities (step functions). According to this model,
	the conventional deconvolution problem aims at recovering the seismic
	reflectivity as a sparse spike train. However, recent multiscale
	analysis on sedimentary records revealed the existence of accumulations
	of varying order singularities in the subsurface, which give rise
	to fractional-order discontinuities. This observation not only calls
	for a richer class of seismic reflection waveforms, but it also requires
	a different methodology to detect and characterize these reflection
	events. For instance, the assumptions underlying conventional deconvolution
	no longer hold. Because of the bandwidth limitation of seismic data,
	multiscale analysis methods based on the decay rate of wavelet coefficients
	may yield ambiguous results. We avoid this problem by formulating
	the estimation of the singularity orders by a parametric nonlinear
	inversion method.},
  keywords = {SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/maysami07eage.pdf}
presentation={http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/maysami07SINBADsrc.pdf }
}

@CONFERENCE{moghaddam07CSEGmar,
  author = {Peyman P. Moghaddam and Felix J. Herrmann and C. C. Stolk},
  title = {Migration amplitude recovery using curvelets},
  booktitle = {CSEG Technical Program Expanded Abstracts},
  year = {2007},
  month = {May},
  organization = {CSEG},
  publisher = {CSEG},
  abstract = {In this paper, we recover the amplitude of a seismic image by approximating
	the normal operator and subsequently inverting it. Normal operator
	(migration followed by modeling) is an example of pseudo-differential.
	curvelets are proven to be invariant under the action of pseudo-differential
	operators under certain conditions. Subsequently, curvelets are forming
	as eigen-vectors for such an operator. We propose a seismic amplitude
	recovery method that employs an eigen-value decomposition for normal
	operator using curvelets as eigen-vectors and to be estimated eigenvalues.
	A post-stack reverse-time, wave-equation migration is used for evaluation
	of the proposed method.},
  file = {:http\\\://www.cseg.ca/conventions/abstracts/2007/2007abstracts/168S0131.pdf:PDF},
  keywords = {SLIM},
  url = {http\://www.cseg.ca/conventions/abstracts/2007/2007abstracts/168S0131.pdf}
}

@CONFERENCE{moghaddam07SEGrsi,
  author = {Peyman P. Moghaddam and Felix J. Herrmann and C. C. Stolk},
  title = {Robust seismic-images amplitude recovery using curvelets},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  number = {1},
  pages = {2225-2229},
  organization = {SEG},
  publisher = {SEG},
  abstract = {In this paper, we recover the amplitude of a seismic image by approximating
	the normal (demigration-migration) operator. In this approximation,
	we make use of the property that curvelets remain invariant under
	the action of the normal operator. We propose a seismic amplitude
	recovery method that employs an eigenvalue like decomposition for
	the normal operator using curvelets as eigen-vectors. Subsequently,
	we propose an approximate non-linear singularity-preserving solution
	to the least-squares seismic imaging problem with sparseness in the
	curvelet domain and spatial continuity constraints. Our method is
	tested with a reverse-time {\textquoteleft}wave-equation{\textquoteright}
	migration code simulating the acoustic wave equation on the SEG-AA
	salt model. {\copyright}2007 Society of Exploration Geophysicists},
  doi = {10.1190/1.2792928},
  keywords = {SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/moghaddam07seg.pdf},
presentation={http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg07/moghaddam07rsi.pdf}
}

@CONFERENCE{moghaddam07CSEGsac,
  author = {Peyman P. Moghaddam and Felix J. Herrmann and C. C. Stolk},
  title = {Sparsity and continuity enhancing seismic imaging},
  booktitle = {CSEG Technical Program Expanded Abstracts},
  year = {2007},
  month = {May},
  organization = {CSEG},
  publisher = {CSEG},
  abstract = {A non-linear singularity-preserving solution to the least-squares
	seismic imaging problem with sparseness and continuity constraints
	is proposed. The applied formalism explores curvelets as a directional
	frame that, by their sparsity on the image, and their invariance
	under the imaging operators, allows for a stable recovery of the
	amplitudes. Our method is based on the estimation of the normal operator
	in the form of an {\textquoteright}eigenvalue{\textquoteright} decompsoition
	with curvelets as the eigenvectors{\textquoteright}. Subsequently,
	we propose an inversion method that derives from estimation of the
	normal operator and is formulated as a convex optimization problem.
	Sparsity in the curvelet domain as well as continuity along the reflectors
	in the image domain are promoted as part of this optimization. Our
	method is tested with a reverse-time {\textquoteright}wave-equation{\textquoteright}
	migration code simulating the acoustic wave equation.},
  file = {:http\\\://www.cseg.ca/conventions/abstracts/2007/2007abstracts/091S0130.pdf:PDF},
  keywords = {SLIM},
  url = {http\://www.cseg.ca/conventions/abstracts/2007/2007abstracts/091S0130.pdf}
}

@CONFERENCE{moghaddam07EAGEsar,
  author = {Peyman P. Moghaddam and Felix J. Herrmann and C. C. Stolk},
  title = {Seismic amplitude recovery with curvelets},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  month = {June},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {A non-linear singularity-preserving solution to the least-squares
	seismic imaging problem with sparseness and continuity constraints
	is proposed. The applied formalism explores curvelets as a directional
	frame that, by their sparsity on the image, and their invariance
	under the imaging operators, allows for a stable recovery of the
	amplitudes. Our method is based on the estimation of the normal operator
	in the form of an {\textquoteright}eigenvalue{\textquoteright} decompsoition
	with curvelets as the {\textquoteright}eigenvectors{\textquoteright}.
	Subsequently, we propose an inversion method that derives from estimation
	of the normal operator and is formulated as a convex optimization
	problem. Sparsity in the curvelet domain as well as continuity along
	the reflectors in the image domain are promoted as part of this optimization.
	Our method is tested with a reverse-time {\textquoteright}wave-equation{\textquoteright}
	migration code simulating the acoustic wave equation.},
  keywords = {SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/moghaddam07.pdf}
}

@CONFERENCE{saab07SEGcbp,
  author = {Rayan Saab and Deli Wang and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Curvelet-based primary-multiple separation from a {B}ayesian perspective},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  number = {1},
  pages = {2510-2514},
  organization = {SEG},
  publisher = {SEG},
  abstract = {In this abstract, we present a novel primary-multiple separation scheme
	which makes use of the sparsity of both primaries and multiples in
	a transform domain, such as the curvelet transform, to provide estimates
	of each. The proposed algorithm utilizes seismic data as well as
	the output of a preliminary step that provides (possibly) erroneous
	predictions of the multiples. The algorithm separates the signal
	components, i.e., the primaries and multiples, by solving an optimization
	problem that assumes noisy input data and can be derived from a Bayesian
	perspective. More precisely, the optimization problem can be arrived
	at via an assumption of a weighted Laplacian distribution for the
	primary and multiple coefficients in the transform domain and of
	white Gaussian noise contaminating both the seismic data and the
	preliminary prediction of the multiples, which both serve as input
	to the algorithm. {\copyright}2007 Society of Exploration Geophysicists},
  doi = {10.1190/1.2792988},
  keywords = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg07/saab07cbp.pdf
	},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/saab07seg.pdf}
}

@CONFERENCE{sastry07SINBADnor,
  author = {Challa S. Sastry},
  title = {Norm-one recovery from irregular sampled data},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {Seismic traces are sampled irregularly and insufficiently due to practical
	and economical limitations. The use of such data in seismic imaging
	results in image artifacts and poor spatial resolution. Therefore,
	before being used, the measurements are to be interpolated onto a
	regular grid. One of the methods achieving this objective is based
	on the Fourier reconstruction, which deals with the under-determined
	system of equations. The recent pursuit techniques (namely, basis
	pursuit, matching pursuit etc) admit certain promising features such
	as faster and simpler implementation even in large scale settings.
	The presentation discusses the application of the pursuit algorithms
	to the Fourier-based interpolation problem for the signals that have
	sparse Fourier spectra. In particular, the objective of the presentation
	includes: 1). studying the performance of the algorithm if, and how
	far, the measurement coordinates can be shifted from uniform distribution
	on the continuous interval. 2). studying what could be the allowable
	misplacement in the measurement coordinates that does not alter the
	quality of the reconstruction process},
  keywords = {SLIM, SINBAD, Presentation}
}

@CONFERENCE{vandenberg07SINBADipo1,
  author = {E. van den Berg and Michael P. Friedlander},
  title = {In Pursuit of a Root},
  booktitle = {2007 Von Neumann Symposium},
  year = {2007},
  keywords = {minimization, Presentation, SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Private/Presentations/sinbad/2007/friedlander07ipo.pdf}
}

@CONFERENCE{verschuur07SEGmmp,
  author = {D. J. Verschuur and Deli Wang and Felix J. Herrmann},
  title = {Multiterm multiple prediction using separated reflections and diffractions
	combined with curvelet-based subtraction},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  number = {1},
  pages = {2535-2539},
  organization = {SEG},
  publisher = {SEG},
  abstract = {The surface-related multiple elimination (SRME) method has proven
	to be successful on a large number of data cases. Most of the applications
	are still 2D, as the full 3D implementation is still expensive and
	under development. However, the earth is a 3D medium, such that 3D
	effects are difficult to avoid. Most of the 3D effects come from
	diffractive structures, whereas the specular reflections normally
	have less of a 3D behavior. By separating the seismic data in a specular
	reflecting and a diffractive part, multiple prediction can be carried
	out with these different subsets of the input data, resulting in
	several categories of predicted multiples. Because each category
	of predicted multiples can be subtracted from the input data with
	different adaptation filters, a more flexible SRME procedure is obtained.
	Based on some initial results from a Gulf of Mexico dataset, the
	potential of this approach is investigated. {\copyright}2007 Society
	of Exploration Geophysicists},
  doi = {10.1190/1.2792993},
  keywords = {SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/verschuur07seg.pdf}
}

@CONFERENCE{wang07SEGrri,
  author = {Deli Wang and Rayan Saab and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Recent results in curvelet-based primary-multiple separation: application
	to real data},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  number = {1},
  pages = {2500-2504},
  organization = {SEG},
  publisher = {SEG},
  abstract = {In this abstract, we present a nonlinear curvelet-based sparsity-promoting
	formulation for the primary-multiple separation problem. We show
	that these coherent signal components can be separated robustly by
	explicitly exploting the locality of curvelets in phase space (space-spatial
	frequency plane) and their ability to compress data volumes that
	contain wavefronts. This work is an extension of earlier results
	and the presented algorithms are shown to be stable under noise and
	moderately erroneous multiple predictions. {\copyright}2007 Society
	of Exploration Geophysicists},
  doi = {10.1190/1.2792986},
  keywords = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg07/wang07rri.pdf
	},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/wang07seg.pdf}
}

@CONFERENCE{yarham07SINBADnsw,
  author = {Carson Yarham},
  title = {Nonlinear surface wave prediction and separation},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {Removal of surface waves is an integral step in seismic processing.
	There are many standard techniques for removal of this type of coherent
	noise, such as f-k filtering, but these methods are not always effective.
	One of the common problems with removal of surface waves is that
	they tend to be aliased in the frequency domain. This can make removal
	difficult and affect the frequency content of the reflector signals,
	as this signals will not be completely separated. As seen in (Hennenfent,
	G. and F. Herrmann, 2006, Application of stable signal recovery to
	seismic interpolation) interpolation can be used effectively to resample
	the seismic record thus dealiasing the surface waves. This separates
	the signals in the frequency domain allowing for a more precise and
	complete removal. The use of this technique with curvelet based surface
	wave predictions and an iterative L1 separation scheme can be used
	to remove surface waves from shot records more completely that with
	standard techniques.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Private/Conferences/EAGE/2007/yarham07eage.pdf}
}

@CONFERENCE{yarham07EAGEcai,
  author = {Carson Yarham and Gilles Hennenfent and Felix J. Herrmann},
  title = {Curvelet applications in surface wave removal},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  month = {June},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {Ground roll removal of seismic signals can be a challenging prospect.
	Dealing with undersampleing causing aliased waves amplitudes orders
	of magnitude higher than reflector signals and low frequency loss
	of information due to band ...},
  keywords = {SLIM}
}


%---------------------------------2006--------------------------------------

@CONFERENCE{hennenfent06SINBADapo,
  author = {Gilles Hennenfent},
  title = {A primer on stable signal recovery},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {During this presentation an introduction will be given on the method
	of stable recovery from noisy and incomplete data. Strong recovery
	conditions that guarantee the recovery for arbitrary acquisition
	geometries will be reviewed and numerical recovery examples will
	be presented.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/4-Felix3.pdf}
}

@CONFERENCE{hennenfent06SINBADros,
  author = {Gilles Hennenfent},
  title = {Recovery of seismic data: practical considerations},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {We propose a method for seismic data interpolation based on 1) the
	reformulation of the problem as a stable signal recovery problem
	and 2) the fact that seismic data is sparsely represented by curvelets.
	This method does not require information on the seismic velocities.
	Most importantly, this formulation potentially leads to an explicit
	recovery condition. We also propose a large-scale problem solver
	for the l1-regularization minimization involved in the recovery and
	successfully illustrate the performance of our algorithm on 2D synthetic
	and real examples.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/5-Gilles2.pdf}
}

@CONFERENCE{hennenfent06SINBADtnf,
  author = {Gilles Hennenfent},
  title = {The Nonuniform Fast Discrete Curvelet Transform (NFDCT)},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {The authors present an extension of the fast discrete curvelet transform
	(FDCT) to nonuniformly sampled data. This extension not only restores
	curvelet compression rates for nonuniformly sampled data but also
	removes noise and maps the data to a regular grid.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/10-Gilles3.pdf}
}

@CONFERENCE{hennenfent06SEGaos,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Application of stable signal recovery to seismic data interpolation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2006},
  volume = {25},
  number = {1},
  pages = {2797-2801},
  organization = {SEG},
  publisher = {SEG},
  abstract = {We propose a method for seismic data interpolation based on 1) the
	reformulation of the problem as a stable signal recovery problem
	and 2) the fact that seismic data is sparsely represented by curvelets.
	This method does not require information on the seismic velocities.
	Most importantly, this formulation potentially leads to an explicit
	recovery condition. We also propose a large-scale problem solver
	for the 1-regularization minimization involved in the recovery and
	successfully illustrate the performance of our algorithm on 2D synthetic
	and real examples. {\copyright}2006 Society of Exploration Geophysicists},
  doi = {10.1190/1.2370105},
  keywords = {SLIM, curvelets, interpolation, seismic data, regularization minimization, iterative thresholding, amplitude, continuity, fast transform},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2006/hennenfent06seg.pdf},
url2 = {http://dx.doi.org/10.1190/1.2370105 }
}

@CONFERENCE{herrmann06SINBADapo1,
  author = {Felix J. Herrmann},
  title = {A primer on sparsity transforms: curvelets and wave atoms},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {During this presentation an overview will be given on the different
	sparsity transforms that are used at SLIM. Emphasis will be on two
	directional and multiscale wavelet transforms, namely the curvelet
	and the recently introduced wave-atom transforms. The main properties
	of these transforms will be listed and their performance on seismic
	data will be discussed.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/4-Felix3.pdf}
}

@CONFERENCE{herrmann06SINBADapow,
  author = {Felix J. Herrmann},
  title = {A primer on weak conditions for stable recovery},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {During this presentation an introduction will be given on the method
	of stable recovery from noisy and incomplete data. Weak recovery
	conditions that guarantee the recovery for typical acquisition geometries
	will be reviewed and numerical recovery examples will be presented.
	The advantage of these weak conditions is that they are less pessimistic
	and {\textquoteleft}verifiable{\textquoteright} or very large-scale
	acquisition geometries.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/6-Felix4.pdf}
}

@CONFERENCE{herrmann06SINBADmpf,
  author = {Felix J. Herrmann},
  title = {Multiple prediction from incomplete data},
  booktitle = {SINBAD 2006},
  year = {2006},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/16-Felix7.pdf}
}

@CONFERENCE{herrmann06SINBADom,
  author = {Felix J. Herrmann},
  title = {Opening meeting},
  booktitle = {SINBAD 2006},
  year = {2006},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/1-Felix1.pdf}
}

@CONFERENCE{herrmann06SINBADpms,
  author = {Felix J. Herrmann and Urs Boeniger and Dirk-Jacob Verschuur},
  title = {Primary-multiple separation by curvelet frames},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {Predictive multiple suppression methods consist of two main steps:
	a prediction step, during which multiples are predicted from seismic
	data, and a primary-multiple separation step, during which the predicted
	multiples are {\textquoteright}matched{\textquoteright} with the
	true multiples in the data and subsequently removed. The last step
	is crucial in practice: an incorrect separation will cause residual
	multiple energy in the result or may lead to a distortion of the
	primaries, or both. To reduce these adverse effects, a new transformed-domain
	method is proposed where primaries and multiples are separated rather
	than matched. This separation is carried out on the basis of differences
	in the multiscale and multidirectional characteristics of these two
	signal components. Our method uses the curvelet transform, which
	maps multidimensional data volumes into almost orthogonal localized
	multidimensional prototype waveforms that vary in directional and
	spatio-temporal content. Primaries-only and multiples-only signal
	components are recovered from the total data volume by a nonlinear
	optimization scheme that is stable under noisy input data. During
	the optimization, the two signal components are separated by enhancing
	sparseness (through weighted l1-norms) in the transformed domain
	subject to fitting the observed data as the sum of the separated
	components to within a user-defined tolerance level. Whenever the
	prediction for the two signal components in the transformed domain
	correlate, the recovery is suppressed while for regions where the
	correlation is small the method seeks the sparsest set of coefficients
	that represent each signal component. Our algorithm does not seek
	a matched filter and as such it differs fundamentally from traditional
	adaptive subtraction methods. The method derives its stability from
	the sparseness obtained by a non-parametric multiscale and multidirectional
	overcomplete signal representation. This sparsity serves as prior
	information and allows for a Bayesian interpretation of our method
	during which the log-likelihood function is minimized while the two
	signal components are assumed to be given by a superposition of prototype
	waveforms, drawn independently from a probability function that is
	weighted by the predicted primaries and multiples. In this paper,
	the predictions are based on the data-driven surface-related multiple
	elimination (SRME) method. Synthetic and field data examples show
	a clean separation leading to a considerable improvement in multiple
	suppression compared to the conventional method of adaptive matched
	filtering. This improved separation translates into an improved stack.},
  keywords = {Presentation, SINBAD, SLIM},
doi={10.1111/j.1365-246X.2007.03360.x},
Volume={170},
pages={781-799},
organization={Geophysical Journal International},
  url = {http://slim.eos.ubc.ca/Publications/Public/Journals/herrmann07npm.pdf}

}

@CONFERENCE{herrmann06SINBADsac,
  author = {Felix J. Herrmann},
  title = {Sparsity- and continuity-promoting seismic image recovery with curvelets},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {A nonlinear singularity-preserving solution to seismic image recovery
	with sparseness and continuity constraints is proposed. The method
	explicitly explores the curvelet transform as a directional frame
	expansion that, by virtue of its sparsity on seismic images and its
	invariance under the Hessian of the linearized imaging problem, allows
	for a stable recovery of the migration amplitudes from noisy data.
	The method corresponds to a preconditioning that corrects the amplitudes
	during a post-processing step. The solution is formulated as a nonlinear
	optimization problem where sparsity in the curvelet domain as well
	as continuity along the imaged reflectors are jointly promoted. To
	enhance sparsity, the l1-norm on the curvelet coefficients is minimized
	while continuity is promoted by minimizing an anisotropic diffusion
	norm on the image. The performance of the recovery scheme is evaluated
	with {\textquoteright}wave-equation{\textquoteright} migration code
	on a synthetic dataset. This is joint work with Peyman Moghaddam.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/14-Felix6.pdf}
}

@CONFERENCE{herrmann06SINBADsra,
  author = {Felix J. Herrmann},
  title = {Stable recovery and separation of seismic data},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {During this presentation an overview will be given on how seismic
	data regularization and separation problems can be cast into the
	framework of stable signal recovery. It is shown that the successful
	solution of these two problems depends on the existence of signal
	expansions that are compressible. Preliminary examples will be shown.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/3-Felix2.pdf}
}

@CONFERENCE{lin06SINBADci,
  author = {Tim T.Y. Lin},
  title = {Compressed imaging},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {In 1998 Grimbergen et. al. introduced a new method for computing wavefield
	propagation which improved on the previously employed local explicit
	operator method in that it exhibited no dip limitation, accurately
	handled laterally varying background ground velocity models, and
	is unconditionally stable. These desirable properties are mainly
	attributed to bringing the propagation problem into an eigenvector
	basis that diagonalizes the propagation operators. This modal-transform
	method, however, requires at each depth-level the solution of a large-scale
	sparse eigenvalue problem to compute the square-root of the Helmholtz
	operator. By using recent results from compressed sensing, we hope
	to reduce these computational costs that typically involve the synthesizes
	of the imaging operators and the cost of matrix-vector products.
	To reduce these costs, we compress the extrapolation operators by
	using only a fraction of the positive eigenvalues and temporal frequencies.
	This reduction not only leads to smaller matrices but also to reduced
	synthesis costs. These reductions go at the expense of solving a
	recovery problem from incomplete data. During the presentation, we
	show that wavefields can accurately be extrapolated with a compressed
	operators and competitive costs.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/17-Tim1.pdf}
}


@CONFERENCE{maysami06SINBADrro,
  author = {Mohammad Maysami},
  title = {Recent results on seismic deconvolution},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {One of the important steps in seismic imaging is to provide suitable
	information about boundaries. Sharp variation of physical properties
	at a layer boundary cause reflection the wavefield. In previous work
	done by C. M. Dupuis, seismic signal characterization is divided
	into two steps: detection and estimation. In the detection phase,
	the goal is to find all singularities in a seismic section regardless
	of their order and then to categorize the data to different events
	by windowing each singularity. In the estimation step, we determine
	the order of singularity more precisely by using a rough estimate
	based on the detection phase. Traditionally, a redundant dictionary
	method is employed for the detection part. However, we attempt to
	instead use a new L1-solver developed by D.L. Donoho: the Stagewise
	Orthogonal Matching Pursuit (StOMP). It approximates the solution
	to inverse problems while promoting the sparsity in the solution
	vector. This algorithm will allow us to experimentally confirm the
	recent analysis by S. Mallat on spiky deconvolution limits, which
	imposes a required minimum distance between spikes. This required
	minimum distance between different spikes is dependent on the number
	of spikes as well as the width of the chosen source wavelet used
	in convolution with the train. These results allow for the design
	of more robust and accurate detection schemes for seismic signal
	characterization.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2006/maysami06SINBADrrs.pdf}
}


@CONFERENCE{modzelewski06SINBADdas,
  author = {Henryk Modzelewski},
  title = {Design and specifications for SLIM{\textquoteright}s software framework},
  booktitle = {SINBAD 2006},
  year = {2006},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/19-Henryk1.pdf}
}

@CONFERENCE{moghaddam06SINBADioa,
  author = {Peyman P. Moghaddam},
  title = {Imaging operator approximation using Curvelets},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {In this presentation, the normal (demigation-migration) operator is
	studied in terms of a pseudo-differential operator. The invariance
	of curvelets under this operator and their sparsity on the seismic
	images is used to precondition the migration operator. A brief overview
	will be given on some of the theory from micro-local analysis which
	proofs that curvelets remain approximately invariant under the operator.
	The proper setting for which a diagonal approximation in the curvelet
	domain is accurate is discussed together with different methods that
	estimate this diagonal from of-the-shelf migration operators. This
	is joint work with Chris Stolk.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/15-Peyman2.pdf}
}

@CONFERENCE{moghaddam06SINBADsac,
  author = {Peyman P. Moghaddam},
  title = {Sparsity- and continuity-promoting norms for seismic images},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {During this presentation, the importance of sparsity and continuity
	enhancing energy norms is emphasized for seismic imaging and inversion.
	The continuity promoting energy norm is justified by the apparent
	smoothness of reflectors in the direction along and the oscillatory
	behavior across the interfaces. This energy norm is called anisotropic
	diffusion and will be defined mathematically. Denoising examples
	will be given during which seismic images are recovered from the
	noise by a joint norm-one and continuity promoting minimization.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/13-Peyman1.pdf}
}

@CONFERENCE{sastry07SINBADrfu,
  author = {Challa S. Sastry and Gilles Hennenfent and Felix J. Herrmann},
  title = {Recovery from unstructured data},
  booktitle = {SINBAD 2006},
  year = {2006},
  keywords = {SLIM, SINBAD, Presentation},
  presentation= {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/11-Sastry1.pdf}
}

@CONFERENCE{thomson06SINBADlss,
  author = {Darren Thomson},
  title = {Large-scale seismic data recovery by the Parallel Windowed Curvelet
	Transform},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {We propose using overlapping, tapered windows to process seismic data
	in parallel. This method consists of numerically tight linear operators
	and adjoints that are suitable for use in iterative algorithms. This
	method is also highly scalable and makes parallelprocessing of large
	seismic data sets feasible. We use this scheme to define the Parallel
	Windowed Fast Discrete Curvelet Transform (PWFDCT), which we have
	applied to a seismic data interpolation algorithm. Some preliminary
	results will be shown. Henryk Modzeleweski: Design and specifications
	for SLIMPy's software framework The SLIM group is actively developing
	software for seismic imaging. This talk will give a general overview
	of the software development philosophy adopted by SLIM. The covered
	topics will include: 1) adopting Python for object-oriented programming,
	2) including parallelism into the algorithms used in seismic imaging/modeling,
	3) in-house algorithms for seismic imaging, and 4) contributions
	to Madagascar (RSF). The talk will serve as an introduction to the
	other presentations in the session ``SINBAD Software releases".},
  keywords = {SLIM, SINBAD, Presentation},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/18-Darren1.pdf}
}


@CONFERENCE{thomson06SINBADppe,
  author = {Darren Thomson},
  title = {(P)SLIMPy: parallel extension},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {The parallel extensions to the SLIMpy environment enable pipe-based
	processing of large data sets in an MPI-based parallel environment.
	Parallel processing can be done by straightforward slicing of data,
	or by using an overlapping domain decomposition that requires communication
	between different processors. The principal aim of the parallel extensions
	is to leave abstract numerical algorithms (ANA's) and applications
	programmed for use in SLIMpy untouched when moving to parallel processing.
	The object-oriented functionality of Python makes this possible.},
  keywords = {SLIM, SINBAD, Presentation},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/21-Darren2.pdf}
}





%------------------------Haneet's Section ------------------ 2005-2001-------------------------------------------------------------------%      



@CONFERENCE{Beyreuther05SEGcot,
  author       = {Moritz Beyreuther and Jamin Cristall and Felix J. Herrmann},
  title        = {Computation of time-lapse differences with {3-D} directional frames},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2005},
  volume       = {24},
  number       = {1},
  pages        = {2488-2491},
  organization = {SEG},
  publisher    = {SEG},
  abstract     = {We present an alternative method of extracting production related
        differences from time-lapse seismic data sets. Our method is not
        based on the actual subtraction of the two data sets, risking the
        enhancement of noise and introduction of artifacts due to local phase
        rotation and slightly misaligned events. Rather, it mutes events
        of the monitor survey with respect to the baseline survey based on
        the magnitudes of coefficients in a sparse and local atomic decomposition.
        Our technique is demonstrated to be an effective tool for enhancing
        the time-lapse signal from surveys which have been cross-equalized.
        {\copyright}2005 Society of Exploration Geophysicists},
  doi          = {10.1190/1.2148227},
  keywords     = {SLIM},
  url          = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/seg4D2005.pdf},
  url2         = {http://dx.doi.org/10.1190/1.2148227}
}




@CONFERENCE{Hennenfent05SEGscd,
  author       = {Gilles Hennenfent and Felix J. Herrmann},
  title        = {Sparseness-constrained data continuation with frames: applications
                  to missing traces and aliased signals in {2/3-D}},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2005},
  volume       = {24},
  number       = {1},
  pages        = {2162-2165},
  organization = {SEG},
  publisher    = {SEG},
  abstract     = {We present a robust iterative sparseness-constrained interpolation
        algorithm using 2-/3-D curvelet frames and Fourier-like transforms
        that exploits continuity along reflectors in seismic data. By choosing
        generic transforms, we circumvent the necessity to make parametric
        assumptions (e.g. through linear/parabolic Radon or demigration)
        regarding the shape of events in seismic data. Simulation and real
        data examples for data with moderately sized gaps demonstrate that
        our algorithm provides interpolated traces that accurately reproduce
        the wavelet shape as well as the AVO behavior. Our method also shows
        good results for de-aliasing judged by the behavior of the ($f-k$)-spectrum
        before and after regularization. {\copyright}2005 Society of Exploration
        Geophysicists},
  doi          = {10.1190/1.2148142},
  keywords     = {SLIM},
  url          = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/SEG2005_Data_Cont.pdf}
}




@CONFERENCE{Hennenfent05CSEGscs,
  author       = {Gilles Hennenfent and Felix J. Herrmann and R. Neelamani},
  title        = {Sparseness-constrained seismic deconvolution with curvelets},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2005},
  month        = {May},
  organization = {CSEG},
  publisher    = {CSEG},
  abstract     = {Continuity along reflectors in seismic images is used via Curvelet
        representation to stabilize the convolution operator inversion. The
        Curvelet transform is a new multiscale transform that provides sparse
        representations for images that comprise smooth objects separated
        by piece-wise smooth discontinuities (e.g. seismic images). Our iterative
        Curvelet-regularized deconvolution algorithm combines conjugate gradient-based
        inversion with noise regularization performed using non-linear Curvelet
        coefficient thresholding. The thresholding operation enhances the
        sparsity of Curvelet representations. We show on a synthetic example
        that our algorithm provides improved resolution and continuity along
        reflectors as well as reduced ringing effect compared to the iterative
        Wiener-based deconvolution approach.},
  keywords     = {SLIM},
  url          = {http://slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/Hennenfent_G_Sparseness_Constrained_Deconvolutionw_Curvelets.doc}
}




@CONFERENCE{Hennenfent05EAGEsdr,
  author       = {Gilles Hennenfent and R. Neelamani and Felix J. Herrmann},
  title        = {Seismic deconvolution revisited with curvelet frames},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2005},
  month        = {June},
  organization = {EAGE},
  publisher    = {EAGE},
  abstract     = {We propose an efficient iterative curvelet-regularized deconvolution
        algorithm that exploits continuity along reflectors in seismic images.
        Curvelets are a new multiscale transform that provides sparse representations
        for images (such as seismic images) that comprise smooth objects
        separated by piece-wise smooth discontinuities. Our technique combines
        conjugate gradient-based convolution operator inversion with noise
        regularization that is performed using non-linear curvelet coefficient
        shrinkage (thresholding). The shrinkage operation leverages the sparsity
        of curvelets representations. Simulations demonstrate that our algorithm
        provides improved resolution compared to the traditional Wiener-based
        deconvolution approach.},
  keywords     = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/hennenfent05eage_poster.pdf}
}




@CONFERENCE{Herrmann05CSEGnld,
  author       = {Felix J. Herrmann and Gilles Hennenfent},
  title        = {Non-linear data continuation with redundant frames},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2005},
  month        = {May},
  organization = {CSEG},
  publisher    = {CSEG},
  abstract     = {We propose an efficient iterative data interpolation method using
        continuity along reflectors in seismic images via curvelet and discrete
        cosine transforms. The curvelet transform is a new multiscale transform
        that provides sparse representations for images that comprise smooth
    The latter is motivated by the successful data continuation with
        the discrete Fourier transform. By choosing generic basis functions
        we circumvent the necessity to make parametric assumptions (e.g.
        through linear/parabolic Radon or demigration) regarding the shape
        of events in seismic data. Synthetic and real data examples demonstrate
        that our algorithm provides interpolated traces that accurately reproduce
        the wavelet shape as well as the AVO behavior along events in shot
        gathers.},
  keywords     = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/cseg/herrmann05nld.pdf}
}




@CONFERENCE{Herrmann05CSEGnlr,
  author       = {Felix J. Herrmann and Peyman P. Moghaddam},
  title        = {Non-linear regularization in seismic imaging},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2005},
  month        = {May},
  organization = {CSEG},
  publisher    = {CSEG},
  abstract     = {Two complementary solution strategies to the least-squares imaging
        problem with sparseness \& continuity continuity constraints are
        proposed. The applied formalism explores the sparseness of curvelets
        coefficients of the reflectivity and their invariance under the demigration-migration
        operator. We achieve the solution by jointly minimizing a weighted
        l1-norm on the curvelet coefficients and an anisotropic difussion
        or total variation norm on the imaged reflectivity model. The l1-norm
        exploits the sparsenss of the reflectivity in the curvelet domain
        whereas the anisotropic norm enhances the continuity along the reflections
        while removing artifacts residing in between reflectors. While the
        two optimization methods (convex versus non-convex) share the same
        type of regularization, they differ in flexibility how to handle
        additional constraints on the coefficients of the imaged reflectivity
        and in computational expense. A brief sketch of the theory is provided
        along with a number of synthetic examples.},
  keywords     = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/cseg/herrmann05nlr.pdf}
}




@CONFERENCE{Herrmann05EAGEosf,
  author       = {Felix J. Herrmann and Peyman P. Moghaddam and R. Kirlin},
  title        = {Optimization strategies for sparseness- and continuity-enhanced imaging:
                  theory},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2005},
  month        = {June},
  organization = {EAGE},
  publisher    = {EAGE},
  abstract     = {Two complementary solution strategies to the least-squares migration
        problem with sparseness- and continuity constraints are proposed.
        The applied formalism explores the sparseness of curvelets on the
        reflectivity and their invariance under the demigration-migration
        operator. Sparseness is enhanced by (approximately) minimizing a
        (weighted) l1-norm on the curvelet coefficients. Continuity along
        imaged reflectors is brought out by minimizing the anisotropic diffusion
        or total variation norm which penalizes variations along and in between
        reflectors. A brief sketch of the theory is provided as well as a
        number of synthetic examples. Technical details on the implementation
        of the optimization strategies are deferred to an accompanying paper:
        implementation.},
  keywords     = {SLIM},
  url          = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/EAGEIM12005.pdf}
}




@CONFERENCE{Herrmann05EAGErcd,
  author       = {Felix J. Herrmann and Gilles Hennenfent},
  title        = {Robust curvelet-domain data continuation with sparseness constraints},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2005},
  month        = {June},
  organization = {EAGE},
  publisher    = {EAGE},
  abstract     = {A robust data interpolation method using curvelets frames is presented.
        The advantage of this method is that curvelets arguably provide an
        optimal sparse representation for solutions of wave equations with
        smooth coefficients. As such curvelets frames circum- vent {\textendash}
        besides the assumption of caustic-free data {\textendash} the necessity
        to make parametric assumptions (e.g. through linear/parabolic Radon
        or demigration) regarding the shape of events in seismic data. A
        brief sketch of the theory is provided as well as a number of examples
        on synthetic and real data.},
  keywords     = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage05/herrmann05rcd.pdf}
}




@CONFERENCE{Herrmann05EAGErcd1,
  author       = {Felix J. Herrmann and D. J. Verschuur},
  title        = {Robust curvelet-domain primary-multiple separation with sparseness
                  constraints},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2005},
  month        = {June},
  organization = {EAGE},
  publisher    = {EAGE},
  abstract     = {A non-linear primary-multiple separation method using curvelets frames
        is presented. The advantage of this method is that curvelets arguably
        provide an optimal sparse representation for both primaries and multiples.
        As such curvelets frames are ideal candidates to separate primaries
        from multiples given inaccurate predictions for these two data components.
        The method derives its robustness regarding the presence of noise;
        errors in the prediction and missing data from the curvelet frame{\textquoteright}s
        ability (i) to represent both signal components with a limited number
        of multi-scale and directional basis functions; (ii) to separate
        the components on the basis of differences in location, orientation
        and scales and (iii) to minimize correlations between the coefficients
        of the two components. A brief sketch of the theory is provided as
        well as a number of examples on synthetic and real data.},
  keywords     = {SLIM},
  url          = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/EAGEM2005.pdf}
}




@CONFERENCE{Beyreuther04EAGEcdo,
  author       = {Moritz Beyreuther and Felix J. Herrmann and Jamin Cristall},
  title        = {Curvelet denoising of {4-D} seismic},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2004},
  month        = {June},
  organization = {EAGE},
  publisher    = {EAGE},
  abstract     = {With burgeoning world demand and a limited rate of discovery of new
        reserves, there is increasing impetus upon the industry to optimize
        recovery from already existing fields. 4D, or time-lapse, seismic
        imaging is an emerging technology that holds great promise to better
        monitor and optimise reservoir production. The basic idea behind
        4D seismic is that when multiple 3D surveys are acquired at separate
        calendar times over a producing field, the reservoir geology will
        not change from survey to survey but the state of the reservoir fluids
        will change. Thus, taking the difference between two 3D surveys should
        remove the static geologic contribution to the data and isolate the
        time- varying fluid flow component. However, a major challenge in
        4D seismic is that acquisition and processing differences between
        3D surveys often overshadow the changes caused by fluid flow. This
        problem is compounded when 4D effects are sought to be derived from
        vintage 3D data sets that were not originally acquired with 4D in
        mind. The goal of this study is to remove the acquisition and imaging
        artefacts from a 4D seismic difference cube using Curveket processing techniques.},
  keywords     = {SLIM},
  url          = {http://slim.eos.ubc.ca/~felix/public/EAGE4D2004.pdf},
  url2         = {https://circle.ubc.ca/bitstream/handle/2429/453/EAGE4D2004.pdf?sequence=1}
}




@CONFERENCE{Cristall04CSEGcpa,
  author       = {Jamin Cristall and Moritz Beyreuther and Felix J. Herrmann},
  title        = {Curvelet processing and imaging: {4-D} adaptive subtraction},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2004},
  month        = {May},
  organization = {CSEG},
  publisher    = {CSEG},
  abstract     = {With burgeoning world demand and a limited rate of discovery of new
        reserves, there is increasing impetus upon the industry to optimize
        recovery from already existing fields. 4D, or time-lapse, seismic
        imaging holds great promise to better monitor and optimise reservoir
        production. The basic idea behind 4D seismic is that when multiple
        3D surveys are acquired at separate calendar times over a producing
        field, the reservoir geology will not change from survey to survey
        but the state of the reservoir fluids will change. Thus, taking the
        difference between two 3D surveys should remove the static geologic
        contribution to the data and isolate the time-varying fluid flow
        component. However, a major challenge in 4D seismic is that acquisition
        and processing differences between 3D surveys often overshadow the
        changes caused by fluid flow. This problem is compounded when 4D
        effects are sought to be derived from legacy 3D data sets that were
        not originally acquired with 4D in mind. The goal of this study is
        to remove the acquisition and imaging artefacts from a 4D seismic
        difference cube using Curvelet processing techniques.},
  keywords     = {SLIM},
  url          = {http://www.cseg.ca/conventions/abstracts/2004/2004abstracts/059S0201-Cristall_J_Curvelet_4D.pdf}
}




@CONFERENCE{Hennenfent04SEGtta,
  author       = {Gilles Hennenfent and Felix J. Herrmann},
  title        = {Three-term amplitude-versus-offset ({AVO}) inversion revisited by
                  curvelet and wavelet transforms},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2004},
  volume       = {23},
  number       = {1},
  pages        = {211-214},
  organization = {SEG},
  publisher    = {SEG},
  abstract     = {We present a new method to stabilize the three-term AVO inversion
        using Curvelet and Wavelet transforms. Curvelets are basis functions
        that effectively represent otherwise smooth objects having discontinuities
        along smooth curves. The applied formalism explores them to make
        the most of the continuity along reflectors in seismic images. Combined
        with Wavelets, Curvelets are used to denoise the data by penalizing
        high frequencies and small contributions in the AVO-cube. This approach
        is based on the idea that rapid amplitude changes along the ray-parameter
        axis are most likely due to noise. The AVO-inverse problem is linearized,
        formulated and solved for all (x, z) at once. Using densities and
        velocities of the Marmousi model to define the fluctuations in the
        elastic properties, the performance of the proposed method is studied
        and compared with the smoothing along the ray-parameter direction
        only. We show that our method better approximates the true data after
        the denoising step, especially when noise level increases. {\copyright}2004
        Society of Exploration Geophysicists},
  doi          = {10.1190/1.1851201},
  keywords     = {SLIM},
  url          = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/hennenfent04seg.pdf}
}




@CONFERENCE{Hennenfent04SEGtta1,
  author       = {Gilles Hennenfent},
  title        = {Three-term amplitude-versus-offset ({AVO}) inversion revisited by
                  curvelet and wavelet transforms},
  booktitle    = {SEG 2004},
  year         = {2004},
  keywords     = {Presentation, SEG, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg04/hennenfent04tta.pdf}
}




@CONFERENCE{Herrmann04SEGcbn,
  author       = {Felix J. Herrmann and Peyman P. Moghaddam},
  title        = {Curvelet-based non-linear adaptive subtraction with sparseness constraints},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2004},
  volume       = {23},
  number       = {1},
  pages        = {1977-1980},
  organization = {SEG},
  publisher    = {SEG},
  abstract     = {In this paper an overview is given on the application of directional
        basis functions, known under the name Curvelets/Contourlets, to various
        aspects of seismic processing and imaging, which involve adaptive
        subtraction. Key concepts in the approach are the use of directional
        basis functions that localize in both domains (e.g. space and angle);
        non-linear estimation, which corresponds to localized muting on the
        coefficients, possibly supplemented by constrained optimization.
        We will discuss applications that include multiple, ground-roll removal
        and migration denoising. {\copyright}2004 Society of Exploration
        Geophysicists},
  doi          = {10.1190/1.1851181},
  keywords     = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg04/herrmann04cbn.pdf},
  url          = {http://slim.eos.ubc.ca/~felix/public/SEGAD2004.pdf}
}




@CONFERENCE{Herrmann04EAGEcdp,
  author       = {Felix J. Herrmann and Peyman P. Moghaddam},
  title        = {Curvelet-domain preconditioned 'wave-equation' depth-migration with sparseness and illumination constraints},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2004},
  month        = {June},
  organization = {EAGE},
  publisher    = {EAGE},
  abstract     = {A non-linear edge-preserving solution to the least-squares migration
        problem with sparseness constraints is introduced. The applied formalism
        explores Curvelets as basis functions that, by virtue of their sparseness
        and locality, not only allow for a reduction of the dimensionality
        of the imaging problem but which also naturally lead to a non-linear
        solution with significantly improved signal- to-noise ratio. Additional
        conditions on the image are imposed by solving a constrained optimization
        problem on the estimated Curvelet coefficients initialized by thresholding.
        This optimization is designed to also restore the amplitudes by (approximately)
        inverting the normal operator, which is like-wise the (de)-migration
        operators, almost diagonalized by the Curvelet transform.},
  keywords     = {SLIM},
  url          = {http://slim.eos.ubc.ca/Publications/Public/TechReports/herrmann04mpw.pdf}
}




@CONFERENCE{Herrmann04CSEGcdl,
  author       = {Felix J. Herrmann and Peyman P. Moghaddam},
  title        = {Curvelet-domain least-squares migration with sparseness constraints},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2004},
  month        = {May},
  organization = {CSEG},
  publisher    = {CSEG},
  abstract     = {A non-linear edge-preserving solution to the least-squares migration
        problem with sparseness constraints is introduced. The applied formalism
        explores Curvelets as basis functions that, by virtue of their sparseness
        and locality, not only allow for a reduction of the dimensionality
        of the imaging problem but which also naturally lead to a non-linear
        solution with significantly improved signal-to-noise ratio. Additional
        conditions on the image are imposed by solving a constrained optimization
        problem on the estimated Curvelet coefficients initialized by thresholding.
        This optimization is designed to also restore the amplitudes by (approximately)
        inverting the normal operator, which is like-wise the (de)-migration
        operators, almost diagonalized by the Curvelet transform.},
  keywords     = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage04/herrmann04cdl.pdf}
}




@CONFERENCE{Herrmann04SEGcdm,
  author       = {Felix J. Herrmann and D. J. Verschuur},
  title        = {Curvelet-domain multiple elimination with sparseness constraints},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2004},
  volume       = {23},
  number       = {1},
  pages        = {1333-1336},
  organization = {SEG},
  publisher    = {SEG},
  abstract     = {Predictive multiple suppression methods consist of two main steps:
        a prediction step, in which multiples are predicted from the seismic
        data, and a subtraction step, in which the predicted multiples are
        matched with the true multiples in the data. The last step appears
        crucial in practice: an incorrect adaptive subtraction method will
        cause multiples to be sub-optimally subtracted or primaries being
        distorted, or both. Therefore, we propose a new domain for separation
        of primaries and multiples via the Curvelet transform. This transform
        maps the data into almost orthogonal localized events with a directional
        and spatial-temporal component. The multiples are suppressed by thresholding
        the input data at those Curvelet components where the predicted multiples
        have large amplitudes. In this way the more traditional filtering
        of predicted multiples to fit the input data is avoided. An initial
        field data example shows a considerable improvement in multiple suppression.
        {\copyright}2004 Society of Exploration Geophysicists},
  doi          = {10.1190/1.1851110},
  keywords     = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg04/herrmann04cdm.pdf},
  url          = {http://slim.eos.ubc.ca/~felix/public/SEGM2004.pdf}
}




@CONFERENCE{Herrmann04CSEGcia,
  author       = {Felix J. Herrmann},
  title        = {Curvelet imaging and processing: an overview},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2004},
  month        = {May},
  organization = {CSEG},
  publisher    = {CSEG},
  abstract     = {In this paper an overview is given on the application of directional
        basis functions, known under the name Curvelets/Contourlets, to various
        aspects of seismic processing and imaging. Key conceps in the approach
        are the use of (i) directional basis functions that localize in both
        domains (e.g. space and angle); (ii) non-linear estimation, which
        corresponds to localized muting on the coefficients, possibly supplemented
        by constrained optimization (iii) invariance of the basis functions
        under the imaging operators. We will discuss applications that include
        multiple and ground roll removal; sparseness-constrained least-squares
        migration and the computation of 4-D difference cubes.},
  keywords     = {SLIM},
  url          = {http://slim.eos.ubc.ca/~felix/public/Herrmann_F_Curvelet_overview.doc}
}




@CONFERENCE{Herrmann04CSEGcia1,
  author       = {Felix J. Herrmann and D. J. Verschuur},
  title        = {Curvelet imaging and processing: adaptive multiple elimination},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2004},
  month        = {May},
  organization = {CSEG},
  publisher    = {CSEG},
  abstract     = {Predictive multiple suppression methods consist of two main steps:
        a prediction step, in which multiples are predicted from the seismic
        data, and a subtraction step, in which the predicted multiples are
        matched with the true multiples in the data. The last step appears
        crucial in practice: an incorrect adaptive subtraction method will
        cause multiples to be sub-optimally subtracted or primaries being
        distorted, or both. Therefore, we propose a new domain for separation
        of primaries and multiples via the Curvelet transform. This transform
        maps the data into almost orthogonal localized events with a directional
        and spatial-temporal component. The multiples are suppressed by thresholding
        the input data at those Curvelet components where the predicted multiples
        have large amplitudes. In this way the more traditional filtering
        of predicted multiples to fit the input data is avoided. An initial
        field data example shows a considerable improvement in multiple suppression.},
  keywords     = {SLIM},
  url          = {http://slim.eos.ubc.ca/~felix/public/Herrmann_F_Curvelet_multiple.doc}
}




@CONFERENCE{Herrmann04CSEGcia2,
  author       = {Felix J. Herrmann and Peyman P. Moghaddam},
  title        = {Curvelet imaging and processing: sparseness-constrained least-squares
        migration},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2004},
  month        = {May},
  organization = {CSEG},
  publisher    = {CSEG},
  abstract     = {A non-linear edge-preserving solution to the least-squares migration
        problem with sparseness constraints is introduced. The applied formalism
        explores Curvelets as basis functions that, by virtue of their sparseness
        and locality, not only allow for a reduction of the dimensionality
        of the imaging problem but which also naturally lead to a non-linear
        solution with significantly improved signal-to-noise ratio. Additional
        conditions on the image are imposed by solving a constrained optimization
        problem on the estimated Curvelet coefficients initialized by thresholding.
        This optimization is designed to also restore the amplitudes by (approximately)
        inverting the normal operator, which is, like-wise to the (de)-migration
        operators, almost diagonalized by the Curvelet transform.},
  keywords     = {SLIM},
  url          = {http://slim.eos.ubc.ca/~felix/public/Herrmann_F_Curvelet_imaging.doc}
}




@CONFERENCE{Herrmann04EAGEsop,
  author       = {Felix J. Herrmann and D. J. Verschuur},
  title        = {Separation of primaries and multiples by non-linear estimation in
        the curvelet domain},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2004},
  month        = {June},
  organization = {EAGE},
  publisher    = {EAGE},
  abstract     = {Predictive multiple suppression methods consist of two main steps:
        a prediction step, in which multiples are predicted from the seismic
        data, and a subtraction step, in which the predicted multiples are
        matched with the true multiples in the data. The last step appears
        crucial in practice: an incorrect adaptive subtraction method will
        cause multiples to be sub-optimally subtracted or primaries being
        distorted, or both. Therefore, we propose a new domain for separation
        of primaries and multiples via the Curvelet transform. This transform
        maps the data into almost orthogonal localized events with a directional
        and spatial-temporal component. The multiples are suppressed by thresholding
        the input data at those Curvelet components where the predicted multiples
        have large amplitudes. In this way the more traditional filtering
        of predicted multiples to fit the input data is avoided. An initial
        field data example shows a considerable improvement in multiple suppression.},
  keywords     = {SLIM},
  url          = {http://slim.eos.ubc.ca/~felix/public/EAGEM2004.pdf}
}




@CONFERENCE{Moghaddam04SEGmpw,
  author       = {Peyman P. Moghaddam and Felix J. Herrmann},
  title        = {Migration preconditioning with curvelets},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2004},
  volume       = {23},
  number       = {1},
  pages        = {2204-2207},
  organization = {SEG},
  publisher    = {SEG},
  abstract     = {In this paper, the property of Curvelet transforms for preconditioning
        the migration and normal operators is investigated. These operators
        belong to the class of Fourier integral operators and pseudo-differential
        operator, respectively. The effect of this pre-conditioner is shown
        in term of improvement of sparsity, convergence rate, number of iteration
        for the Krylov-subspace solver and clustering of singular(eigen)
        values. The migration operator, which we employed in this work is
        the common-offset Kirchoff-Born migration. {\copyright}2004 Society
        of Exploration Geophysicists},
  doi          = {10.1190/1.1845213},
  keywords     = {SLIM},
  url          = {http://slim.eos.ubc.ca/~felix/public/SEGLAN2004.pdf}
}




@CONFERENCE{Yarham04CSEGcpa,
  author       = {Carson Yarham and Daniel Trad and Felix J. Herrmann},
  title        = {Curvelet processing and imaging: adaptive ground roll removal},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2004},
  month        = {May},
  organization = {CSEG},
  publisher    = {CSEG},
  abstract     = {In this paper we present examples of ground roll attenuation for synthetic
        and real data gathers by using Contourlet and Curvelet transforms.
        These non-separable wavelet transforms are locoalized both (x,t)-
        and (k,f)-domains and allow for adaptive seperation of signal and
        ground roll. Both linear and non-linear filtering are discussed using
        the unique properties of these basis that allow for simultaneous
        localization in the both domains. Eventhough, the linear filtering
        techniques are encouraging the true added value of these basis-function
        techniques becomes apparent when we use these decompositions to adaptively
        substract modeled ground roll from data using a non-linear thesholding
        procedure. We show real and synthetic examples and the results suggest
        that these directional-selective basis functions provide a usefull
        tool for the removal of coherent noise such as ground roll.},
  keywords     = {SLIM},
  url          = {http://slim.eos.ubc.ca/~felix/public/Yarham_C_Curvelet_Ground_roll.doc}
}




@CONFERENCE{Herrmann03SEGoiw,
  author       = {Felix J. Herrmann},
  title        = {"Optimal" imaging with curvelets},
  booktitle    = {2003 SEG},
  year         = {2003},
  keywords     = {Presentation, SEG, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg03/herrmann03oiw.pdf}
}




@CONFERENCE{Herrmann03SPIEmsa,
  author       = {Felix J. Herrmann},
  title        = {Multifractional splines: application to seismic imaging},
  booktitle    = {Proceedings of SPIE Technical Conference on Wavelets: Applications
        in Signal and Image Processing X},
  year         = {2003},
  editor       = {Michael A. Unser and Akram Aldroubi and Andrew F. Laine},
  volume       = {5207},
  pages        = {240-258},
  organization = {SPIE},
  publisher    = {SPIE},
  abstract     = {Seismic imaging commits itself to locating singularities in the elastic
        properties of the Earth{\textquoteright}s subsurface. Using the high-frequency
        ray-Born approximation for scattering from non-intersecting smooth
        interfaces, seismic data can be represented by a generalized Radon
        transform mapping the singularities in the medium to seismic data.
        Even though seismic data are bandwidth limited, signatures of the
        singularities in the medium carry through this transform and its
        inverse and this mapping property presents us with the possibility
        to develop new imaging techniques that preserve and characterize
        the singularities from incomplete, bandwidth-limited and noisy data.
        In this paper we propose a non-adaptive Curvelet/Contourlet technique
        to image and preserve the singularities and a data-adaptive Matching
        Pursuit method to characterize these imaged singularities by Multi-fractional
        Splines. This first technique borrows from the ideas within the Wavelet-Vaguelette/Quasi-SVD
        approach. We use the almost diagonalization of the scattering operator
        to approximately compensate for (i) the coloring of the noise and
        hence facilitate estimation; (ii) the normal operator itself. Results
        of applying these techniques to seismic imaging are encouraging although
        many open fundamental questions remain.},
  keywords     = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/herrmann03msa.pdf},
  url          = {http://www.eos.ubc.ca/~felix/Preprint/SPIE03DEF.pdf}

}




@CONFERENCE{Herrmann01EAGEsas,
  author       = {Felix J. Herrmann},
  title        = {Scaling and seismic reflectivity: implications of scaling on {AVO}},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2001},
  month        = {June},
  organization = {EAGE},
  publisher    = {EAGE},
  abstract     = {AVO analysis of seismic data is based on the assumption that transitions
        in the earth consist of jump discontinuities only. Generalization
        of these transitions to more realistic transitions shows a drastic
        change in observed AVO behavior, especially for the large angles
        currently attained by increasing cable lengths. We propose a simple
ities. After renormalization, the inverted
        fluctuations regain their relative magnitudes which, due to the scaling,
        may have been significantly distorted.},
  keywords     = {SLIM},
%  url          = {NOT FOUND}                                                                                                 
}

@
