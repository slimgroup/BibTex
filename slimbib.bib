% This file was created with JabRef 2.9.
% Encoding: ISO8859_1

%-----2021-----%

@CONFERENCE{herrmann2021Microsoftrla,
  author = {Felix J. Herrmann and Mathias Louboutin and Ali Siahkoohi},
  title = {ML@scale using randomized linear algebra},
  booktitle = {Microsoft},
  year = {2021},
  month = {03},
  abstract = {Deep Learning for large-scale applications such as video encoding or seismic segmentation are challenged by the excessive amounts of memory that is required for training networks via backpropagation. In this talk, I will discuss how techniques from randomized linear algebra can be used to address these bottle necks and bring down the memory footprint of training CNNs by up to a factor of O(N) (where N is number of pixels) without increasing computational cost. Additionally, I will illustrate how the seemingly disparate technologies of deep learning and large-scale PDE-constrained optimization share important similarities that can be taken advantage of in the development of next-generation deep learning technologies, with possible applications in scientific computing and sustainability.},
  keywords = {Randomized linear algebra, deep learning, uncertainty quantification},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/Microsoft/2021/herrmann2021Microsoftrla/herrmann2021Microsoftrla_pres.pdf},
  note = {Talk at Microsoft}
}

@CONFERENCE{herrmann2021KAUSTdbi,
  author = {Ali Siahkoohi and Gabrio Rizzuti and Mathias Louboutin and Philipp A. Witte and Felix J. Herrmann},
  title = {Deep Bayesian Inference for Task-based Seismic Imaging},
  booktitle = {KAUST},
  year = {2021},
  month = {03},
  abstract = {High dimensionality, complex physics, and lack of access to the ground truth make seismic imaging one of the most challenging inversion problems in the computational imaging sciences. If these challenges were not bad enough, modern applications of computational imaging increasingly call for the assessment of uncertainty on the image itself and on subsequent tasks conducted on these images. By making use of a Markov-chain Monte Carlo (McMC) sampling technique, we demonstrate how uncertainty in the data can be propagated to the task of seismic horizon tracking. While this example shows that uncertainty can in principle be handled in a systematic manner by using neural nets as deep priors, it unfortunately also reveals that the McMC method fundamentally lacks ability to scale to relevant problem sizes. To overcome this shortcoming, we introduce a variational formulation based on normalizing flows. In this approach, invertible neural networks are trained to generate samples from the posterior. There are strong indications that this approach has the capability to scale better to problems where the forward operator is expensive to evaluate.},
  keywords = {Inverse problems, Bayesian inference, seismic imaging, Uncertainty quantification},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/KAUST/2021/herrmann2021KAUSTdbi/Kaust.pdf},
  note = {Talk at KAUST}
}

@CONFERENCE{siahkoohi2021AABIpto,
  author = {Ali Siahkoohi and Gabrio Rizzuti and Mathias Louboutin and Philipp A. Witte and Felix J. Herrmann},
  title = {Preconditioned training of normalizing flows for variational inference in inverse problems},
  booktitle = {3rd Symposium on Advances in Approximate Bayesian Inference},
  year = {2021},
  month = {01},
  abstract = {Obtaining samples from the posterior distribution of inverse problems with
expensive forward operators is challenging especially when the unknowns
involve the strongly heterogeneous Earth. To meet these challenges, we
propose a preconditioning scheme involving a conditional normalizing flow
(NF) capable of sampling from a low-fidelity posterior distribution directly.
This conditional NF is used to speed up the training of the high-fidelity
objective involving minimization of the Kullback-Leibler divergence between
the predicted and the desired high-fidelity posterior density for indirect
measurements at hand. To minimize costs associated with the forward operator,
we initialize the high-fidelity NF with the weights of the pretrained
low-fidelity NF, which is trained beforehand on available model and data
pairs. Our numerical experiments, including a 2D toy and a seismic compressed
sensing example, demonstrate that thanks to the preconditioning considerable
speed-ups are achievable compared to training NFs from scratch.},
  keywords = {Inverse problems, Normalizing flows, Uncertainty quantification},
  note = {AABI},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/AABI/2021/siahkoohi2021AABIpto/siahkoohi2020ABIfab.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/AABI/2021/siahkoohi2021AABIpto/siahkoohi2021AABIpto_pres.pdf},
  url2 = {https://openreview.net/forum?id=P9m1sMaNQ8T},
  software = {https://github.com/slimgroup/Software.siahkoohi2021AABIpto}
}

%-----2020-----%

@CONFERENCE{louboutin2020SIAMTXLAtdw,
  author = {Mathias Louboutin and Gabrio Rizzuti and Felix J. Herrmann},
  title = {Time-Domain Wavefield Reconstruction Inversion In Tilted Transverse
Isostropic Media},
  booktitle = {SIAM Texas-Louisiana},
  year = {2020},
  month = {10},
  abstract = {We introduce a generalization of time-domain wavefield
reconstruction inversion to anisotropic acoustic modeling. Wavefield
reconstruction inversion has been extensively researched in recent years for
its ability to mitigate cycle skipping. The original method was formulated in
the frequency domain with acoustic isotropic physics. However,
frequency-domain modeling requires sophisticated iterative solvers that are
difficult to scale to industrial-size problems and more realistic physical
assumptions, such as tilted transverse isotropy, object of this study. The
work presented here is based on a recently proposed dual formulation of
wavefield reconstruction inversion, which allows time-domain propagator that
are suitable to both large scales and more accurate physics.},
  keywords = {Time-domain, WRI, FWI},
  note = {SIAMTXLA},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SIAMTXLA/2020/louboutin2020SIAMTXLAtdw/louboutin2020SIAMTXLAtdw_pres.pdf}
}

@CONFERENCE{siahkoohi2020SIAMTXLAudg,
  author = {Ali Siahkoohi and Gabrio Rizzuti and Mathias Louboutin and Felix J.
Herrmann},
  title = {Unsupervised data-guided uncertainty analysis in imaging and horizon
tracking},
  booktitle = {SIAM Texas-Louisiana},
  year = {2020},
  month = {10},
  abstract = {Imaging typically is the first stage of a sequential workflow, and
uncertainty quantification becomes more relevant when applied to subsequent
tasks. We propose a Bayesian approach to horizon tracking uncertainty
analysis, where we deploy a deep prior instead of adhering to handcrafted
priors. By passing samples from the posterior distribution obtained via
stochastic gradient Langevin dynamics to an automatic horizon tracker, we are
able to incorporate the uncertainty on model parameters into horizon
tracking.},
  keywords = {uncertainty quantification, deep learning, MCMC, horizon tracking},
  note = {SIAMTXLA},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SIAMTXLA/2020/siahkoohi2020SIAMTXLAudg/siahkoohi2020SIAMTXLAudg_pres.pdf}
}

@CONFERENCE{siahkoohi2020SIAMISsiu,
  author = {Ali Siahkoohi and Philipp A. Witte and Mathias Louboutin and Felix J.
Herrmann and Gabrio Rizzuti},
  title = {Seismic Imaging with Uncertainty Quantification: Sampling from the
Posterior with Generative Networks},
  booktitle = {SIAM Conference on Imaging Science},
  year = {2020},
  month = {07},
  abstract = {n/a},
  keywords = {Imaging, Uncertainty quantification, MCMC, Invertible networks, IS20},
  note = {IS20},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SIAMIS/2020/siahkoohi2020SIAMISsiu/siahkoohi2020SIAMISsiu_pres.pdf}
}

@CONFERENCE{rizzuti2020SEGuqavp,
  author = {Gabrio Rizzuti and Ali Siahkoohi and Philipp A. Witte and Felix J. Herrmann},
  title = {Parameterizing uncertainty by deep invertible networks, an application
to reservoir characterization},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2020},
  month = {09},
  pages = {1541-1545},
  abstract = {Uncertainty quantification for full-waveform inversion provides a
probabilistic characterization of the ill-conditioning of the problem,
comprising the sensitivity of the solution with respect to the starting model
and data noise. This analysis allows to assess the confidence in the
candidate solution and how it is reflected in the tasks that are typically
performed after imaging (e.g., stratigraphic segmentation following reservoir
characterization). Classically, uncertainty comes in the form of a
probability distribution formulated from Bayesian principles, from which we
seek to obtain samples. A popular solution involves Monte Carlo sampling.
Here, we propose instead an approach characterized by training a deep network
that "pushes forward" Gaussian random inputs into the model space
(representing, for example, density or velocity) as if they were sampled from
the actual posterior distribution. Such network is designed to solve a
variational optimization problem based on the Kullback-Leibler divergence
between the posterior and the network output distributions. This work is
fundamentally rooted in recent developments for invertible networks. Special
invertible architectures, besides being computational advantageous with
respect to traditional networks, do also enable analytic computation of the
output density function. Therefore, after training, these networks can be
readily used as a new prior for a related inversion problem. This stands in
stark contrast with Monte-Carlo methods, which only produce samples. We
validate these ideas with an application to angle-versus-ray parameter
analysis for reservoir characterization.},
  keywords = {Full-waveform inversion, Uncertainty quantification, SEG},
  note = {(SEG, virtual)},
  doi = {10.1190/segam2020-3428150.1},
   url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2020/rizzuti2020SEGuqavp/rizzuti2020SEGuqavp.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2020/rizzuti2020SEGuqavp/rizzuti2020SEGuqavp_pres.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2020/rizzuti2020SEGuqavp/rizzuti2020SEGuqavp_pres.mp4},
  software = {https://github.com/slimgroup/Software.SEG2020}
}

@CONFERENCE{siahkoohi2020SEGwdp,
  author = {Ali Siahkoohi and Gabrio Rizzuti and Felix J. Herrmann},
  title = {Weak deep priors for seismic imaging},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2020},
  month = {09},
  pages = {2998-3002},
  abstract = {Incorporating prior knowledge on model unknowns of interest is
essential when dealing with ill-posed inverse problems due to the
nonuniqueness of the solution and data noise. Unfortunately, it is not
trivial to fully describe our priors in a convenient and analytical way.
Parameterizing the unknowns with a convolutional neural network (CNN), and
assuming an uninformative Gaussian prior on its weights, leads to a
variational prior on the output space that favors "natural" images and
excludes noisy artifacts, as long as overfitting is prevented. This is the
so-called deep-prior approach. In seismic imaging, however, evaluating the
forward operator is computationally expensive, and training a randomly
initialized CNN becomes infeasible. We propose, instead, a weak version of
deep priors, which consists of relaxing the requirement that reflectivity
models must lie in the network range, and letting the unknowns deviate from
the network output according to a Gaussian distribution. Finally, we jointly
solve for the reflectivity model and CNN weights. The chief advantage of this
approach is that the updates for the CNN weights do not involve the modeling
operator, and become relatively cheap. Our synthetic numerical experiments
demonstrate that the weak deep prior is more robust with respect to noise
than conventional least-squares imaging approaches, with roughly twice the
computational cost of reverse-time migration, which is the affordable
computational budget in large-scale imaging problems.},
  keywords = {machine learning, deep prior, seismic imaging, SEG},
  note = {(SEG, virtual)},
  doi = {10.1190/segam2020-3417568.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2020/siahkoohi2020SEGwdp/siahkoohi2020SEGwdp.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2020/siahkoohi2020SEGwdp/siahkoohi2020SEGwdp_pres.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2020/siahkoohi2020SEGwdp/siahkoohi2020SEGwdp_pres.mp4},
  software = {https://github.com/slimgroup/Software.SEG2020}
}

@CONFERENCE{siahkoohi2020SEGuqi,
  author = {Ali Siahkoohi and Gabrio Rizzuti and Felix J. Herrmann},
  title = {Uncertainty quantification in imaging and automatic horizon
tracking—a Bayesian deep-prior based approach},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2020},
  month = {09},
  pages = {1636-1640},
  abstract = {In inverse problems, uncertainty quantification (UQ) deals with a
probabilistic description of the solution nonuniqueness and data noise
sensitivity. Setting seismic imaging into a Bayesian framework allows for a
principled way of studying uncertainty by solving for the model posterior
distribution. Imaging, however, typically constitutes only the first stage of
a sequential workflow, and UQ becomes even more relevant when applied to
subsequent tasks that are highly sensitive to the inversion outcome. In this
paper, we focus on how UQ trickles down to horizon tracking for the
determination of stratigraphic models and investigate its sensitivity with
respect to the imaging result. As such, the main contribution of this work
consists in a data-guided approach to horizon tracking uncertainty analysis.
This work is fundamentally based on a special reparameterization of
reflectivity, known as "deep prior". Feasible models are restricted to the
output of a convolutional neural network with a fixed input, while weights
and biases are Gaussian random variables. Given a deep prior model, the
network parameters are sampled from the posterior distribution via a Markov
chain Monte Carlo method, from which the conditional mean and point-wise
standard deviation of the inferred reflectivities are approximated. For each
sample of the posterior distribution, a reflectivity is generated, and the
horizons are tracked automatically. In this way, uncertainty on model
parameters naturally translates to horizon tracking. As part of the
validation for the proposed approach, we verified that the estimated
confidence intervals for the horizon tracking coincide with geologically
complex regions, such as faults.},
  keywords = {machine learning, uncertainty quantification, imaging, horizon
picking, SEG},
  note = {(SEG, virtual)},
  doi = {10.1190/segam2020-3417560.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2020/siahkoohi2020SEGuqi/siahkoohi2020SEGuqi.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2020/siahkoohi2020SEGuqi/siahkoohi2020SEGuqi_pres.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2020/siahkoohi2020SEGuqi/siahkoohi2020SEGuqi_pres.mp4},
  software = {https://github.com/slimgroup/Software.SEG2020}
}


@CONFERENCE{louboutin2020SEGWSspl,
  author = {Mathias Louboutin and Ziyi Yin and Yijun Zhang and Felix J. Herrmann},
  title = {Sparsity promoting least-squares migration for long offset sparse OBN},
  booktitle = {SEG Workshop on Promises and Challenges with Sparse Node Ultra-long Offset OBN Acquisition in Imaging and Earth Model Building; virtual},
  year = {2020},
  month = {10},
  keywords = {SEG, workshop, multiple, OBN, born, deblending},
  note = {(SEG Workshop, virtual)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2020/louboutin2020SEGWSspl/louboutin2020SEGWSspl_pres.pdf}
}

@CONFERENCE{yin2020SEGesi,
  author = {Ziyi Yin and Rafael Orozco and Philipp A. Witte and Mathias Louboutin and Gabrio
Rizzuti and Felix J. Herrmann},
  title = {Extended source imaging –- a unifying framework for seismic &
medical imaging},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2020},
  month = {09},
  pages = {3502-3506},
  abstract = {We present three imaging modalities that live on the crossroads of
seismic and medical imaging. Through the lens of extended source imaging, we
can draw deep connections among the fields of wave-equation based seismic and
medical imaging, despite first appearances. From the seismic perspective, we
underline the importance to work with the correct physics and spatially
varying velocity fields. Medical imaging, on the other hand, opens the
possibility for new imaging modalities where outside stimuli, such as laser
or radar pulses, can not only be used to identify endogenous optical or
thermal contrasts but that these sources can also be used to insonify the
medium so that images of the whole specimen can in principle be created.},
  keywords = {seismic imaging, medical imaging, variable projection, SEG},
  note = {(SEG, virtual)},
  doi = {10.1190/segam2020-3426999.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2020/yin2020SEGesi/yin2020SEGesi.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2020/yin2020SEGesi/yin2020SEGesi_pres.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2020/yin2020SEGesi/yin2020SEGesi_pres.mp4},
  software = {https://github.com/slimgroup/Software.SEG2020}
}

@CONFERENCE{zhang2020SEGtli,
  author = {Mi Zhang and Ali Siahkoohi and Felix J. Herrmann},
  title = {Transfer learning in large-scale ocean bottom seismic wavefield
reconstruction},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2020},
  month = {09},
  pages = {1666-1670},
  abstract = {Achieving desirable receiver sampling in ocean bottom acquisition
is often not possible because of cost considerations. Assuming adequate
source sampling is available, which is achievable by virtue of reciprocity
and the use of modern randomized (simultaneous-source) marine acquisition
technology, we are in a position to train  convolutional neural networks
(CNNs) to bring the receiver sampling to the same spatial grid as the dense
source sampling. To accomplish this task, we form training pairs consisting
of densely sampled data and artificially subsampled data using a reciprocity
argument and the assumption that the source-site sampling is dense. While
this approach has successfully been used on the recovery monochromatic
frequency slices, its application in practice calls for wavefield
reconstruction of time-domain data. Despite having the option to parallelize,
the overall costs of this approach can become prohibitive if we decide to
carry out the training and recovery independently for each frequency. Because
different frequency slices share information, we propose the use the method
of transfer training to make our approach computationally more efficient by
warm starting the training with CNN weights obtained from a neighboring
frequency slices. If the two neighboring frequency slices share information,
we would expect the training to improve and converge faster. Our aim is to
prove this principle by carrying a series of carefully selected experiments
on a relatively large-scale five-dimensional data synthetic data volume
associated with wide-azimuth 3D ocean bottom node acquisition. From these
experiments, we observe that by transfer training we are able t significantly
speedup in the training, specially at relatively higher frequencies where
consecutive frequency slices are more correlated.},
  keywords = {transfer learning, wavefield reconstruction, SEG},
  note = {(SEG, virtual)},
  doi = {10.1190/segam2020-3427882.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2020/zhang2020SEGtli/zhang2020SEGtli.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2020/zhang2020SEGtli/zhang2020SEGtli_pres.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2020/zhang2020SEGtli/zhang2020SEGtli_pres.mp4},
  software = {https://github.com/slimgroup/Software.SEG2020}
}

@CONFERENCE{zhang2020SEGwrw,
  author = {Yijun Zhang and Shashin Sharan and Oscar Lopez and Felix J. Herrmann},
  title = {Wavefield recovery with limited-subspace weighted matrix
factorizations},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2020},
  month = {09},
  pages = {2858-2862},
  abstract = {Modern-day seismic imaging and monitoring technology increasingly
rely on dense full-azimuth sampling. Unfortunately, the costs of acquiring
densely sampled data rapidly become prohibitive and we need to look for ways
to sparsely collect data, e.g. from sparsely distributed ocean bottom nodes,
from which we then derive densely sampled surveys through the method of
wavefield reconstruction. Because of their relatively cheap and simple
calculations, wavefield reconstruction via matrix factorizations has proven
to be a viable and scalable alternative to the more generally used
transform-based methods. While this method is capable of processing all full
azimuth data frequency by frequency slice, its performance degrades at higher
frequencies because monochromatic data at these frequencies is not as well
approximated by low-rank factorizations. We address this problem by proposing
a recursive recovery technique, which involves weighted matrix factorizations
where recovered wavefields at the lower frequencies serve as prior
information for the recovery of the higher frequencies. To limit the adverse
effects of potential overfitting, we propose a limited-subspace recursively
weighted matrix factorization approach where the size of the row and column
subspaces to construct the weight matrices is constrained. We apply our
method to data collected from the Gulf of Suez, and our results show that our
limited-subspace weighted recovery method significantly improves the recovery
quality.},
  keywords = {algorithm, data reconstruction, frequency-domain, interpolation,
Processing, SEG},
  note = {(SEG, virtual)},
  doi = {10.1190/segam2020-3428365.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2020/zhang2020SEGwrw/zhang2020SEGwrw.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2020/zhang2020SEGwrw/zhang2020SEGwrw_pres.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2020/zhang2020SEGwrw/zhang2020SEGwrw_pres.mp4},
  software = {https://github.com/slimgroup/Software.SEG2020}
}

@CONFERENCE{rizzuti2020EAGEtwri,
  author = {Gabrio Rizzuti and Mathias Louboutin and Rongrong Wang and Felix J. Herrmann},
  title = {Time-domain wavefield reconstruction inversion for large-scale
seismics},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2020},
  month = {1},
  abstract = {Wavefield reconstruction inversion is an imaging technique akin to
full-waveform inversion, albeit based on a relaxed version of the wave
equation. This relaxation aims to beat the multimodality typical of
full-waveform inversion. However it prevents the use of time-marching solvers
for the augmented equation and, as a consequence, cannot be straightforwardly
employed to large 3D problems. In this work, we formulate a dual version of
wavefield reconstruction inversion amenable to explicit time-domain solvers,
yielding a robust and scalable inversion technique.},
  keywords = {3D, Full-waveform inversion, EAGE, Time-domain},
  note = {Accepted in EAGE, withdrawn due to logistical challenge},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2020/rizzuti2020EAGEtwri/rizzuti2020EAGEtwri.html}
}

@CONFERENCE{siahkoohi2020EAGEdlb,
  author = {Ali Siahkoohi and Gabrio Rizzuti and Felix J. Herrmann},
  title = {A deep-learning based Bayesian approach to seismic imaging and
uncertainty quantification},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2020},
  month = {1},
  abstract = {Uncertainty quantification is essential when dealing with ill-conditioned
inverse problems due to the inherent nonuniqueness of the solution. Bayesian
approaches allow us to determine how likely an estimation of the unknown
parameters is via formulating the posterior distribution. Unfortunately, it
is often not possible to formulate a prior distribution that precisely
encodes our prior knowledge about the unknown. Furthermore, adherence to
handcrafted priors may greatly bias the outcome of the Bayesian analysis. To
address this issue, we propose to use the functional form of a randomly
initialized convolutional neural network as an implicit structured prior,
which is shown to promote natural images and excludes images with unnatural
noise. In order to incorporate the model uncertainty into the final estimate,
we sample the posterior distribution using stochastic gradient Langevin
dynamics and perform Bayesian model averaging on the obtained samples. Our
synthetic numerical experiment verifies that deep priors combined with
Bayesian model averaging are able to partially circumvent imaging artifacts
and reduce the risk of overfitting in the presence of extreme noise. Finally,
we present pointwise variance of the estimates as a measure of uncertainty,
which coincides with regions that are difficult to image.},
  keywords = {seismic imaging, uncertainty quantification, stochastic gradient
Langevin dynamics, deep learning, EAGE},
  note = {Accepted in EAGE},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2020/siahkoohi2020EAGEdlb/siahkoohi2020EAGEdlb.html}
}

%-----2019-----%

@CONFERENCE{herrmann2019NIPSliwcuc,
  author = {Felix J. Herrmann and Ali Siahkoohi and Gabrio Rizzuti},
  title = {Learned imaging with constraints and uncertainty quantification},
  booktitle = {Neural Information Processing Systems (NeurIPS)},
  year = {2019},
  month = {12},
  abstract = {We outline new approaches to incorporate ideas from convolutional
networks into wave-based least-squares imaging. The aim is to combine
hand-crafted constraints with deep convolutional networks allowing us to
directly train a network capable of generating samples from the posterior. The main contributions include combination of weak deep priors with hard handcrafted constraints and a possible new way to sample the posterior.},
  keywords = {imaging, deep learning, uncertainty quantification, constraint},
  note = {Accepted on October 1, 2019},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/NIPS/2019/herrmann2019NIPSliwcuc/herrmann2019NIPSliwcuc.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/NIPS/2019/herrmann2019NIPSliwcuc/herrmann2019NIPSliwcuc_pres.pdf},
  url2 = {https://openreview.net/pdf?id=Hyet2Q29IS}
}

@CONFERENCE{herrmann2019EAGEHPCaii,
  author = {Felix J. Herrmann and Charles Jones and Gerard Gorman and Jan Hückelheim and Keegan Lensink and Paul H. J. Kelly and Navjot Kukreja and Henryk Modzelewski and Michael Lange and Mathias Louboutin and Fabio Luporini and James Selvages and Philipp A. Witte},
  title = {Accelerating ideation & innovation cheaply in the Cloud the power of abstraction, collaboration & reproducibility},
  booktitle = {4th EAGE Workshop on High-performance Computing},
  year = {2019},
  month = {10},
  keywords = {EAGE, HPC, Devito, JUDI, Cloud},
  note = {(EAGE HPC Workshop, Dubai)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGEHPC/2019/herrmann2019EAGEHPCaii/herrmann2019EAGEHPCaii_pres.pdf}
}

@CONFERENCE{rizzuti2019EAGElis,
  author = {Gabrio Rizzuti and Ali Siahkoohi and Felix J. Herrmann},
  title = {Learned iterative solvers for the Helmholtz equation},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2019},
  month = {06},
  abstract = {We propose a ‘learned’ iterative solver for the Helmholtz equation, by combining traditional Krylov-based solvers with machine learning. The method is, in principle, able to circumvent the shortcomings of classical iterative solvers, and has clear advantages over purely data-driven ap- proaches. We demonstrate the effectiveness of this approach under a 1.5-D assumption, when ade- quate a priori information about the velocity distribution is known.},
  keywords = {EAGE, Helmholtz, Iterative, Machine learning},
  note = {(EAGE, Copenhagen)},
  doi = {10.3997/2214-4609.201901542},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2019/rizzuti2019EAGElis/rizzuti2019EAGElis.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2019/rizzuti2019EAGElis/rizzuti2019EAGElis_pres.pdf}
}

@CONFERENCE{peters2019SEGans,
  author = {Bas Peters and Felix J. Herrmann},
  title = {A numerical solver for least-squares sub-problems in 3D wavefield reconstruction inversion and related problem formulations},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2019},
  month = {09},
  pages = {1536-1540},
  abstract = {Recent years saw a surge of interest in seismic waveform inversion approaches based on quadratic-penalty or augmented-Lagrangian methods, including Wavefield Reconstruction Inversion. These methods typically need to solve a least-squares sub-problem that contains a discretization of the Helmholtz equation. Memory requirements for direct solvers are often prohibitively large in three dimensions, and this limited the examples in the literature to two dimensions. We present an algorithm that uses iterative Helmholtz solvers as a black-box to solve the least-squares problem corresponding to 3D grids. This algorithm enables Wavefield Reconstruction Inversion and related formulations, in three dimensions. Our new algorithm also includes a root-finding method to convert a penalty into a constraint on the data-misfit without additional computational cost, by reusing precomputed quantities. Numerical experiments show that the cost of parallel communication and other computations are small compared to the main cost of solving one Helmholtz problem per source and one per receiver.},
  keywords = {SEG, Least-squares, numerical linear algebra, penalty method, quadratic constraints, full-waveform inversion, root finding},
  note = {(SEG, San Antonio)},
  doi = {10.1190/segam2019-3216638.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/peters2019SEGans/peters2019SEGans.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/peters2019SEGans/peters2019SEGans_pres.pdf}
}  

@CONFERENCE{rizzuti2019SEGadf,
  author = {Gabrio Rizzuti and Mathias Louboutin and Rongrong Wang and Emmanouil Daskalakis and Felix J. Herrmann},
  title = {A dual formulation for time-domain wavefield reconstruction inversion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2019},
  month = {09},
  pages = {1480-1485},
  abstract = {We illustrate a dual formulation for full-waveform inversion potentially apt to large 3-D problems. It is based on the optimization of the wave equation compliance, under the constraint of data misfit not exceeding a prescribed noise level. In the Lagrangian formulation, model and wavefield state variables are complemented with multipliers having the same dimension of data ("dual data" variables). Analogously to classical wavefield reconstruction inversion, the wavefield unknowns can be projected out in closed form, by solving a version of the augmented wave equation. This leads to a saddle-point problem whose variables are only model and dual data. As such, this formulation represents a model extension, and is potentially robust against local minima. The classical downsides of model extension methods and wavefield reconstruction inversion are here effectively mitigated: storage of the dual variables is affordable, the augmented wave equation is amenable to time-marching finite-difference schemes, and no continuation strategy for penalty parameters is needed, with the prospect of 3-D applications.},
  keywords = {SEG, full-waveform inversion, time-domain, 3D},
  note = {(SEG, San Antonio)},
  doi = {10.1190/segam2019-3216760.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/rizzuti2019SEGadf/rizzuti2019SEGadf.html},
  software = {https://github.com/slimgroup/Software.rizzuti2019SEGadf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/rizzuti2019SEGadf/rizzuti2019SEGadf_pres.pdf}
}

@CONFERENCE{siahkoohi2019SEGdlb,
  author = {Ali Siahkoohi and Rajiv Kumar and Felix J. Herrmann},
  title = {Deep-learning based ocean bottom seismic wavefield recovery},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2019},
  month = {09},
  pages = {2232-2237},
  abstract = {Ocean bottom surveys usually suffer from having very sparse receivers. Assuming a desirable source sampling, achievable by existing methods such as (simultaneous-source) randomized marine acquisition, we propose a deep-learning based scheme to bring the receivers to the same spatial grid as sources using a convolutional neural network. By exploiting source-receiver reciprocity, we construct training pairs by artificially subsampling the fully-sampled single-receiver frequency slices using a random training mask and later, we deploy the trained neural network to fill-in the gaps in single-source frequency slices. Our experiments show that a random training mask is essential for successful wavefield recovery, even when receivers are on a periodic gird. No external training data is required and experiments on a 3D synthetic data set demonstrate that we are able to recover receivers for up to 90 % missing receivers, missing either randomly or periodically, with a better recovery for random case, at low to midrange frequencies.},
  keywords = {SEG, obn, reconstruction, machine learning, reciprocity},
  note = {(SEG, San Antonio)},
  doi = {10.1190/segam2019-3216632.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/siahkoohi2019SEGdlb/siahkoohi2019SEGdlb.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/siahkoohi2019SEGdlb/siahkoohi2019SEGdlb_pres.pdf}
}

@CONFERENCE{siahkoohi2019SEGsrm,
  author = {Ali Siahkoohi and Dirk J. Verschuur and Felix J. Herrmann},
  title = {Surface-related multiple elimination with deep learning},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2019},
  month = {09},
  pages = {4629-4634},
  abstract = {We explore the potential of neural networks in approximating the action of the computationally expensive Estimation of Primaries by Sparse Inversion (EPSI) algorithm, applied to real data, via a supervised learning algorithm. We show that given suitable training data, consisting of a relatively cheap prediction of multiples and pairs of shot records with and without surface-related multiples, obtained via EPSI, a well-trained neural network is capable of providing an approximation to the action of the EPSI algorithm. We perform our numerical experiment on the field Nelson data set. Our results demonstrate that the quality of the multiple elimination via our neural network improves compared to the case where we only feed the network with shot records with surface-related multiples. We take these benefits by supplying the neural network with a relatively poor prediction of the multiples, e.g. obtained by a relatively cheap single step of Surface-Related Multiple Elimination.},
  keywords = {SEG, SRME, EPSI, machine learning},
  note = {(SEG, San Antonio)},
  doi = {10.1190/segam2019-3216723.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/siahkoohi2019SEGsrm/siahkoohi2019SEGsrm.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/siahkoohi2019SEGsrm/siahkoohi2019SEGsrm_pres.pdf}
}

@CONFERENCE{witte2019SEGedw,
  author = {Philipp A. Witte and Mathias Louboutin and Henryk Modzelewski and Charles Jones and James Selvage and Felix J. Herrmann},
  title = {Event-driven workflows for large-scale seismic imaging in the cloud},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2019},
  month = {09},
  pages = {3984-3988},
  abstract = {Cloud computing has seen a large rise in popularity in recent years and is becoming a cost effective alternative to on-premise computing, with theoretically unlimited scalability. However, so far little progress has been made in adapting the cloud for high performance computing (HPC) tasks, such as seismic imaging and inversion. As the cloud does not provide the same type of fast and reliable connections as conventional HPC clusters, porting legacy codes developed for HPC environments to the cloud is ineffective and misses out on an opportunity to take advantage of new technologies presented by the cloud. We present a novel approach of bringing seismic imaging and inversion workflows to the cloud, which does not rely on a traditional HPC environment, but is based on serverless and event-driven computations. Computational resources are assigned dynamically in response to events, thus minimizing idle time and providing resilience to hardware failures. We test our workflow on two large-scale imaging examples and demonstrate that cost-effective HPC in the cloud is possible, but requires careful reconsiderations of how to bring software to the cloud.},
  keywords = {SEG, cloud, large-scale, imaging, RTM, LS-RTM, workflow},
  note = {(SEG, San Antonio)},
  doi = {10.1190/segam2019-3215069.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/witte2019SEGedw/witte2019SEGedw.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/witte2019SEGedw/witte2019SEGedw_pres.pdf}
}

@CONFERENCE{witte2019RHPCssi,
  author = {Philipp A. Witte and Mathias Louboutin and Charles Jones and Felix J. Herrmann},
  title = {Serverless seismic imaging in the cloud},
  booktitle = {Rice Oil and Gas High Performance Computing Conference 2020},
  year = {2020},
  month = {03},
  abstract = {This talk presents a serverless approach to seismic imaging in the
cloud based on high-throughput containerized batch processing, event-driven
computations and a  domain-specific language compiler for solving the
underlying wave equations. A 3D case study on Azure demonstrates that this
approach allows reducing the operating cost of up to a factor of 6, making
the cloud a viable alternative to on-premise HPC clusters for seismic
imaging.},
  keywords = {cloud, hpc, reverse-time migration, serverless},
  note = {Rice Oil and Gas High Performance Computing Conference 2020},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/RHPC/2019/witte2019RHPCssi/witte2019RHPCssi.html},
  url2 = {https://www.youtube.com/watch?v=epilCpa078A&list=PLcsG4X8Zn_UA3m7tgIMbOdXD8eOEPiUYD&index=17},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/RHPC/2019/witte2019RHPCssi/witte2019RHPCssi_pres.pdf}
}

@CONFERENCE{yang2019SEGlrr,
  author = {Mengmeng Yang and Marie Graff and Rajiv Kumar and Felix J. Herrmann},
  title = {Low-rank representation of subsurface extended image volumes with power iterations},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2019},
  month = {09},
  pages = {4470-4474},
  abstract = {Full subsurface extended image volumes (EIVs) contain abundant information such as subsurface image gathers used in imaging, interpretation of rock properties or velocity analysis, but are expensive in term of computational cost and storage requirement due to their large size. Yet, due to their redundant information, monochromatic EIVs exhibit a low-rank structure that allows us to get a good approximation at a computational cost proportional to the rank k , while conventional techniques require at least the number of sources. Such low-rank factorized EIVs are computed thanks to the randomized singular values decomposition (SVD) algorithm. However, recent developments on low-rank approximations of EIVs raise two major questions. First, monochromatic EIVs rely on time-harmonic wave equation solvers, which do not scale well to realistic 3D models. Second, the rank of the monochromatic EIVs increases with the frequency, which yields increasing computational costs and storage. Here, we propose a construction approach based on a time-domain finite-difference wave-equation solver with time stepping, which is combined to power iteration schemes in the randomized SVD algorithm to accelerate the decay of the singular values. We compare the performances of simultaneous iterations, block-Krylov iterations and the original randomized SVD on the computation of the EIV for a small section of the Marmousi model. Then, we show further results on the full Marmousi model to validate our approach.},
  keywords = {SEG, full extended image volumes, time domain, randomized SVD, power iterations},
  note = {(SEG, San Antonio)},
  doi = {10.1190/segam2019-3215961.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/yang2019SEGlrr/yang2019SEGlrr.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/yang2019SEGlrr/yang2019SEGlrr_pres.pdf}
}

@CONFERENCE{zhang2019SEGhfw,
  author = {Yijun Zhang and Shashin Sharan and Felix J. Herrmann},
  title = {High-frequency wavefield recovery with weighted matrix factorizations},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2019},
  month = {09},
  pages = {3959-3963},
  abstract = {Acquired seismic data is normally not the fully sampled data we would like to work with since traces are missing due to physical constraints or budget limitations. Rank minimization is an effective way to recovering the missing trace data. Unfortunately, this technique's performance may deteriorate at higher frequency because high-frequency data can not necessarily be captured accurately by low-rank matrix factorizations albeit remedies exist such as hierarchical semi-separable matrices. As a result, recovered data often suffers from low signal to noise ratio S/Rs at the higher frequencies. To deal with this situation, we propose a weighted recovery method that improves the performance at the high frequencies by recursively using information from matrix factorizations at neighboring lower frequencies. Essentially, we include prior information from previously reconstructed frequency slices during the wavefield reconstruction. We apply our method to data collected from the Gulf of Suez, which shows that our method performs well compared to the traditional method without weighting.},
  keywords = {SEG, algorithm, data reconstruction, interpolation, processing, frequency-domain},
  note = {(SEG, San Antonio)},
  doi = {10.1190/segam2019-3215103.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/zhang2019SEGhfw/zhang2019SEGhfw.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2019/zhang2019SEGhfw/zhang2019SEGhfw_pres.pdf}
}

@CONFERENCE{peters2019SIAMGEOgmrip,
  author = {Bas Peters and Felix J. Herrmann},
  title = {Generalized Minkowski sets for the regularization of inverse problems},
  booktitle = {SIAM Conference on Mathematical and Computational Issues in the
Geosciences},
  year = {2019},
  month = {05},
  abstract = {We present a new algorithm to compute projections onto the intersection of constraint sets, designed particularly for multiple sets because we exploit similarities between constraint sets. When we do not know projections onto the individual sets in closed form, as is the case for total-variation constraints, our algorithm does not need other optimization algorithms to solve sub-problems. This a significant advantage in terms of computational cost and number of tuning parameters and stopping conditions, compared to classical algorithms to compute projections onto the intersection, such as Dykstra's algorithm.
  The proposed algorithm is suitable for problems with a large number of model parameters such as full-waveform inversion because it exploits coarse and fine-grained parallelism, and we also present a multilevel accelerated version. The corresponding software is open-source and implemented in Julia.
  We present strategies to use projections onto multiple constraints to regularize full-waveform inversion for models with salt domes or sedimentary geology.},
  keywords = {Projections, Intersections, Sets, Minkowski set, Inverse problems},
  note = {(SIAM)},
  url = {https://www.pathlms.com/siam/courses/11267/sections/14618/video_presentations/128671},
  url2 = {https://meetings.siam.org/sess/dsp_talk.cfm?p=97656}
}

@CONFERENCE{rizzuti2019SIAMGStwri,
  author = {Gabrio Rizzuti and Mathias Louboutin and Rongrong Wang and Emmanouil Daskalakis and Felix J. Herrmann},
  title = {A dual formulation for time-domain wavefield reconstruction inversion},
  booktitle = {SIAM Conference on Mathematical and Computational Issues in the Geosciences},
  year = {2019},
  month = {03},
  abstract = {Wavefield reconstruction inversion (WRI) is based on the combination of data misfit and PDE-residual penalty terms, and it aims at the reconstruction of the optimal property model and field pair. A full-space optimization would require the storage of field variables for every frequency or time sample (and every source). Remedies via variable projection are well studied, but they entail the solution of a modified wave equation, which is costly both in frequency and time domain. Here, we explore an efficient alternative to WRI, where properties are complemented, instead, by elements of the dual data space. The starting point of our proposal is the denoising reformulation of WRI: the PDE compliance is optimized under the constraint of a given data fit. Since the field variable can be eliminated from the associated saddle-point problem, we obtain a new objective functional of only properties and Lagrange multipliers. Its evaluation, and gradient computation, involves the solution of the original wave equation, which is amenable to time-domain discretization. The optimization scheme of choice can now leverage on affordable storage of the iterates.},
  keywords = {WRI},
  note = {(SIAM GS)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SIAMGS/2019/rizzuti2019SIAMGStwri/rizzuti2019SIAMGStwri.pdf}
}

@CONFERENCE{witte2019SIAMCSEclsmft,
  author = {Philipp A. Witte and Mathias Louboutin and Fabio Luporini and Gerard J. Gorman and Felix J. Herrmann},
  title = {Compressive least squares migration with on-the-fly Fourier transforms},
  booktitle = {SIAM Conference on Computational Science and Engineering},
  year = {2019},
  month = {03},
  abstract = {Least-squares seismic imaging is an inversion-based approach for accurately imaging the earth's subsurface. However, in the time-domain, the computational cost and memory requirements of this approach scale with the size and recording length of the seismic experiment, thus making this approach often prohibitively expensive in practice. To overcome these issues, we borrow ideas from compressive sensing and signal processing and introduce an algorithm for sparsity-promoting seismic imaging using on-the-fly Fourier transforms. By computing gradients and functions values for random subsets of source locations and frequencies, we considerably limit the number of wave equation solves, while on-the-fly Fourier transforms allow computing an arbitrary number of monochromatic frequency-domain wavefields with a time-domain modeling code and without having to solve large-scale Helmholtz equations. The memory requirements of this approach are independent of the number of time steps and solely depend on the number of frequencies, which determine the amount of crosstalk and subsampling artifacts in the image. We show the application of our approach to several large-scale open source data sets and compare the results to a conventional time-domain approach with optimal checkpointing.},
  keywords = {least-squares, migration, imaging, on-the-fly, Fourier},
  note = {(SIAM CSE)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SIAMCSE/2019/witte2019SIAMCSEclsmft/witte2019SIAMCSEclsmft.pdf}
}


%-----2018-----%

@CONFERENCE{peters2018CMSWMissrip,
  author = {Bas Peters and Felix J. Herrmann},
  title = {Intersections and sums of sets for the regularization of inverse problems},
  booktitle = {Canadian Mathematical Society Winter Meeting},
  year = {2018},
  month = {12},
  abstract = {We present new algorithms to compute projections onto the intersection of constraint sets. We focus on problems with multiple sets for which there is no simple and closed-form projection. Different from more classical methods such as Dykstra's algorithm, we do not need other algorithms to solve sub-problems.
Our algorithms are based on the alternating direction method of multipliers and apply to models/images/video on small 2D and large 3D grids because we exploit computational similarity between constraint sets, coarse and fine-grained parallelism, and we also present a multilevel accelerated version.
To obtain more flexible descriptions of prior knowledge, we introduce a formulation that allows constraint sets to be the sum of intersections of sets, which is essentially an extension of a Minkowski set. This formulation builds on the success of additive image descriptions that are usually based on penalty methods, such as cartoon-texture decomposition and robust principal component analysis.
We show applications where we use multiple constraint sets to regularize partial-differential-equation based parameter estimation problems such as seismic waveform inversion, as well as various image and video processing and segmentation tasks.},
  keywords = {Sets, Intersections, Projections, Inverse problems},
  note = {(CMSWM)},
  url = {https://cms.math.ca/Events/winter18/abs/ipi#bp}
}

@CONFERENCE{alfaraj2018SEGswi,
  author = {Ali M. Alfaraj and Rajiv Kumar and Felix J. Herrmann},
  title = {Seismic waveform inversion using decomposed one-way wavefields},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2018},
  month = {10},
  pages = {1379-1383},
  abstract = {Conventional seismic full waveform inversion (FWI) estimates a velocity model by matching the two-way predicted data with the observed data. The non-linearity of the problem and the complexity of the Earth's subsurface, which results in complex wave propagation may lead to unsatisfactory inversion results. Moreover, having inaccurate overburden models, i.e water velocity model in the ocean bottom acquisition setting, may lead to erroneous velocity models. In order to obtain better velocity models, we simplify the FWI problem by decomposing the total two-way observed and predicted wavefields into one-way wavefields at the receiver locations using acoustic wavefield decomposition. We then propose to fit the less-complex one-way wavefield to obtain a velocity model that contains valuable information about the overburden. We use this inverted velocity model as a better initial model for the inversion using the other more-complex one-way wavefield. We demonstrate our proposed algorithm on acoustic non-inverse crime synthetic data produced from the Marmousi2 model. The proposed method provides improved inversion results compared with conventional FWI due to properly accounting for separate model updates from the up- and down-going wavefields.},
  keywords = {SEG, fwi, one-way, decomposition},
  note = {(SEG, Anaheim)},
  doi = {10.1190/segam2018-2998162.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2018/alfaraj2018SEGswi/alfaraj2018SEGswi.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2018/alfaraj2018SEGswi/alfaraj2018SEGswi_pres.pdf}
}

@CONFERENCE{herrmann2018SMCtpa,
  author = {Felix J. Herrmann and Gerard J. Gorman and Jan Hückelheim and Keegan Lensink and Paul H. J. Kelly and Navjot Kukreja and Henryk Modzelewski and Michael Lange and Mathias Louboutin and Fabio Luporini and Ali Siahkoohi and Philipp A. Witte},
  title = {The power of abstraction in Computational Exploration Seismology},
  booktitle = {Smoky Mountains Computational Sciences and Engineering Conference},
  year = {2018},
  month = {08},
  abstract = {The field of Computational Exploration Seismology has over the years benefited tremendously from developments in HPC. Back in the 80’s and early 90’s, the oil & gas industry was among the main drivers of HPC technology with a resurgence about ten years ago with the advent of full-waveform inversion-i.e., an adjoint-state method for inverse problems that involve the wave equation. 
While the combination of hardware developments and tedious manual coding efforts have resulted in major improvements, the rate of innovations is slow in part because of the inherent complexities of the mostly monolithic code bases. As a result, innovations make it too slowly into these optimized codes. This stifles innovation and could prevent adaptation of important developments such as machine learning with deep convolutional neural networks into existing workflows.
In comparison, the technological advances in the academic as well as commercial machine learning communities have been much faster in part because of robust and well abstracted code bases such as PyTorch and TensorFlow. Since both machine learning and adjoint-state methods rely on back propagation strong similarities exist ready to be exploited.
During my talk, I will discuss lessons we learned in finding the appropriate abstractions, read Domain Specific Language, for time-stepping stencil-based finite differences in Devito, and how this is relevant for machine learning with deep networks. I will also share a vision on how ideas from machine learning can be merged into the development of the next-generation of wave-equation based imaging codes.
In particular, I will demonstrate the remarkable ability of deep convolutional networks to map low-fidelity dispersed numerical wave simulations to high-fidelity ones. I feel that this apparently generalizable capability of neural nets opens a complete new approach to tackle fundamental problems in computational exploration seismology.},
  keywords = {SMC},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SMC/2018/herrmann2018SMCtpa/herrmann2018SMCtpa_pres.pdf}
}

@CONFERENCE{kadu2018SEGMSFWI,
  author = {Ajinkya Kadu and Rajiv Kumar and Tristan van Leeuwen},
  title = {Full-waveform inversion with Mumford-Shah regularization},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2018},
  month = {10},
  pages = {1258-1262},
  abstract = {Full-waveform inversion (FWI) is a non-linear procedure to es- timate subsurface rock parameters from surface measurements of induced seismic waves. This procedure is ill-posed in nature and hence, requires regularization to enhance some structure depending on the prior information. Recently, Total-variation (TV) regularization has gained popularity due to its ability to produce blocky structures. Contrary to this, the earth behave more like a piecewise smooth function. TV regularization fails to enforce this prior information into FWI. We propose a Mumford-Shah functional to incorporate the piecewise smooth spatial structure in the FWI procedure. The resulting opti- mization problem is solved by a splitting method. We show the improvement in results against TV regularization on two synthetic camembert examples.},
  keywords = {SEG, FWI, Mumford-Shah, Regularization},
  note = {(SEG, Anaheim)},
  doi = {10.1190/segam2018-2997224.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2018/kadu2018SEGMSFWI/kadu2018SEGMSFWI.pdf}
}

@CONFERENCE{kumar2018SEGlreiv,
  author = {Rajiv Kumar and Marie Graff-Kray and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Low-rank representation of extended image volumes––applications to imaging and velocity continuation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2018},
  month = {10},
  pages = {4418-4422},
  abstract = {In this paper, we present a cost-effective low-rank factorized form of computing the full subsurface extended image volumes without explicitly calculating the receiver wavefields.  The propose approach is computationally feasible, which exploits the low-rank structure of full subsurface extended image volumes organized as a matrix, thus avoiding the customary loop over sources.  Using carefully selected stylized examples, we show how conventional migration images can be extracted from the low-rank factorized form. We also present a velocity continuation procedure that uses the same low-rank form and allows us to map the extended image for one velocity model to another velocity model without recomputing all the source wavefields.},
  keywords = {SEG, low-rank, image volume, velocity continuation},
  note = {(SEG, Anaheim)},
  doi = {10.1190/segam2018-2998404.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2018/kumar2018SEGlreiv/kumar2018SEGlreiv.html}
}

@CONFERENCE{louboutin2018SEGeow,
  author = {Mathias Louboutin and Philipp A. Witte and Felix J. Herrmann},
  title = {Effects of wrong adjoints for RTM in TTI media},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2018},
  month = {10},
  pages = {331-335},
  abstract = {In order to obtain accurate images of the subsurface, anisotropic modeling and imaging is necessary. However, the twenty-one parameter complete wave-equation is too computationally expensive to be of use in this case. The transverse tilted isotropic wave-equation is then the next best feasible representation of the physics to use for imaging. The main complexity arising from transverse tilted isotropic imaging is to model the receiver wavefield (back propagation of the data or data residual) for the imaging condition. Unlike the isotropic or the full physics wave-equations, the transverse tilted isotropic wave-equation is not not self-adjoint. This difference means that time-reversal will not model the correct receiver wavefield and this can lead to incorrect subsurface images. In this work, we derive and implement the adjoint wave-equation to demonstrate the necessity of exact adjoint modeling for anisotropic modeling and compare our result with adjoint-free time-reversed imaging.},
  keywords = {SEG, TTI, Imaging, RTM, Adjoint, Finite-differences, Anisotropy},
  note = {(SEG, Anaheim)},
  doi = {10.1190/segam2018-2996274.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2018/louboutin2018SEGeow/louboutin2018SEGeow.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2018/louboutin2018SEGeow/louboutin2018SEGeow_pres.pdf}
}

@CONFERENCE{moldoveanu2018SEGcsbdl,
  author = {Nick Moldoveanu and Philip Bilsby and John Quigley and Rajiv Kumar and Felix J. Herrmann},
  title = {Compressive sensing based design for land and OBS surveys: The noise issue},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2018},
  month = {10},
  pages = {102-106},
  abstract = {Two important developments in seismic acquisition in the last decade were introduction of simultaneous shooting and reconstruction via inversion of the complete seismic wavefield. As the seismic wavefield is typically under sampled in at least one of the four spatial coordinates, both developments could contribute to improve the sampling and, in addition, to increase acquisition efficiency.
Herein, we look at the impact of seismic noise when simultaneous shooting and seismic wavefield reconstruction are implemented for a land or ocean bottom seismic survey.},
  keywords = {SEG},
  note = {(SEG, Anaheim)},
  doi = {10.1190/segam2018-2997759.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2018/moldoveanu2018SEGcsbdl/moldoveanu2018SEGcsbdl.pdf}
}

@CONFERENCE{sharan2018IEEEIUSspoa,
  author = {Shashin Sharan and Rajiv Kumar and Diego S. Dumani and Mathias Louboutin and Rongrong Wang and Stanislav Emelianov and Felix J. Herrmann},
  booktitle={2018 IEEE International Ultrasonics Symposium (IUS)}, 
  title = {Sparsity-promoting photoacoustic imaging with source estimation},
  year = {2018},
  month = {10},
  pages = {206-212},
  abstract = {Photoacoustics has emerged as a high-contrast imaging modality that
provides optical absorption maps inside of tissues, therefore
complementing morphological information of conventional ultrasound. The
laser-generated photoacoustic waves are usually envelope-detected, thus
disregarding the specific waveforms generated by each photoabsorber.
Here we propose a sparsity-promoting image reconstruction method that
allows the estimation of each photoabsorber's source-time function.
Preliminary studies showed the ability to reconstruct the optical
absorption map of an in silico vessel phantom. By using a
sparsity-promoting imaging method, absorption maps and source-time
functions can still be recovered even in situations where the number of
transducers is decreased by a factor of six. Moreover, the recovery is
able to attain higher resolution than conventional beamforming methods.
Because our method recovers the source-time function of the
absorbers, it could potentially also be used to distinguish different
types of photoabsorbers, or the degree of aggregation of exogenous
agents, under the assumption that these would generate different
source-time functions at the moment of laser irradiation.},
  keywords = {IEEE, Transducers, Photoacoustic imaging, Image reconstruction, Acceleration, Estimation, Phantoms, sparsity promoting inversion, source-time function, wave equation, photoacoustic imaging},
  note = {(IEEE IUS, Kobe)},
  doi={10.1109/ULTSYM.2018.8580037}, 
  url = {https://slim.gatech.edu/Publications/Public/Conferences/IEEEIUS/2018/sharan2018IEEEIUSspoa/sharan2018IEEEIUSspoa.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IEEEIUS/2018/sharan2018IEEEIUSspoa/sharan2018IEEEIUSspoa_pres.pdf}
}

@CONFERENCE{sharan2018SEGada,
  author = {Shashin Sharan and Rajiv Kumar and Rongrong Wang and Felix J. Herrmann},
  title = {A debiasing approach to microseismic},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2018},
  month = {10},
  pages = {2942-2946},
  abstract = {Microseismic data is often used to locate fracture locations and their origin in time created by fracking. Although surface microseismic data can have large apertures and is easier to acquire than the borehole data, it often suffers from poor signal to noise ratio (S/R). Poor S/R poses a challenge in terms of estimating the correct location and source-time function of a microseismic source. In this work, we propose a denoising step in combination with a computationally cheap debiasing based approach to locate microseismic sources and to estimate their source-time functions with correct amplitude from extremely noisy data. Through numerical experiments, we demonstrate that our method can work with closely spaced microseismic sources with source-time functions of different peak amplitudes and frequencies. We have also shown the ability of our method with the smooth velocity model.},
  keywords = {SEG, full-waveform inversion, microseismic, imaging, noise, wave equation},
  note = {(SEG, Anaheim)},
  doi = {10.1190/segam2018-2997252.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2018/sharan2018SEGada/sharan2018SEGada.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2018/sharan2018SEGada/sharan2018SEGada_pres.pdf}
}

@CONFERENCE{siahkoohi2018SEGcnn,
  author = {Ali Siahkoohi and Mathias Louboutin and Rajiv Kumar and Felix J. Herrmann},
  title = {Deep Convolutional Neural Networks in prestack seismic---two exploratory examples},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2018},
  month = {10},
  pages = {2196-2200},
  abstract = {Deep convolutional neural networks are having quite an impact and have resulted in step changes in the capabilities of image and voice recognition systems and there are indications they may have a similar impact on post-stack seismic images. While these developments are certainly important, we are after all dealing with an imaging problem involving an unknown earth and not completely understood physics. When the imaging step itself is not handled properly, this may possibly offset gains offered by deep learning. Motivated by our work on deep convolutional neural networks in seismic data reconstruction, we discuss how generative networks can be employed in prestack problems ranging from the relatively mondane removal of the effects of the free surface to dealing with the complex effects of numerical dispersion in time-domain finite differences. While the results we present are preliminary, there are strong indications that generative deep convolutional neural networks can have a major impact on complex tasks in prestack seismic data processing and modeling for inversion.},
  keywords = {SEG, Machine Learning, SRME, Dispersion, Modeling, Processing},
  note = {(SEG, Anaheim)},
  doi = {10.1190/segam2018-2998599.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2018/siahkoohi2018SEGcnn/siahkoohi2018SEGcnn.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2018/siahkoohi2018SEGcnn/siahkoohi2018SEGcnn_pres.pdf}
}

@CONFERENCE{yang2018SEGrde,
  author = {Mengmeng Yang and Rajiv Kumar and Rongrong Wang and Felix J. Herrmann},
  title = {Removing density effects in LS-RTM with a low-rank matched filter},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2018},
  month = {10},
  pages = {4251-4255},
  abstract = {Least-squares reverse-time migration faces difficulties when it inverts the data containing strong components related to density variation with velocity-only Born modeling operator. The strong density perturbation will be inverted as strong dummy velocity perturbations, which influence the amplitudes and phase of the velocity perturbations in the inverted model. The traditional method is to invert the additional density variations by developing Born operator with respect to both density and velocity or modify the image condition. In this work, we develop a matched-filter based LS-RTM for velocity-only Born modeling operator, which removes the artifacts in the imaging created by the strong density variation. This method doesn't call for extra work of finite difference stencil and is more general. In the experiment part, we use a complex discontinuous layered medium with strong density variations at the ocean bottom, and show the efficacy of the propose formulation.},
  keywords = {SEG, Least square, low rank, density effect},
  note = {(SEG, Anaheim)},
  doi = {10.1190/segam2018-2997814.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2018/yang2018SEGrde/yang2018SEGrde.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2018/yang2018SEGrde/yang2018SEGrde_pres.pdf}
}

@CONFERENCE{alfaraj2018EAGEasa,
  author = {Ali M. Alfaraj and Rajiv Kumar and Felix J. Herrmann},
  title = {Automatic statics and residual statics correction with low-rank approximation},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2018},
  month = {06},
  abstract = {Seismic data, mainly on land, suffers from long-wavelength statics
due to the laterally varying and heterogeneous nature of the near-surface
weathering layers. We propose an automatic, data-driven and computationally efficient
statics correction method based on low-rank approximation to correct for such
statics. The method does not require a model to estimate static time shifts,
which is the case for other static correction methods; rather it applies the
appropriate static corrections on the data such that it becomes low rank in a
certain domain. As of now, the method is applicable to data that has been
corrected for elevation statics. Due to the near-surface irregularities and
due to approximations used by static correction methods that lead to not fully
correcting for statics, an iterative residual statics correction becomes
necessary. Our proposed method corrects for residual statics without the
necessity of the surface consistency assumption and a multi-iterate process.
Additional benefits of the method include artifacts and noise suppression. We
demonstrate the successful application of our method on several synthetic
data examples.},
  keywords = {EAGE, statics, residual statics, rank, land},
  note = {(EAGE, Copenhagen)},
  doi = {10.3997/2214-4609.201801107},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2018/alfaraj2018EAGEasa/alfaraj2018EAGEasa.html},
 presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2018/alfaraj2018EAGEasa/alfaraj2018EAGEasa_pres.pdf}
}

@CONFERENCE{kadu2018EAGEdfwi,
  author = {Ajinkya Kadu and Rajiv Kumar},
  title = {Decentralized full-waveform inversion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2018},
  month = {06},
  abstract = {With the advent of efficient seismic data acquisition, we are
having a surplus of seismic data, which is improving the imaging of the earth using
full-waveform inversion. However, such inversion suffers from many issues,
including (i) substantial network waiting time due to repeated communications of
function and gradient values in the distributed environment, and (ii)
requirement of the sophisticated optimizer to solve an optimization problem
involving non-smooth regularizers. To circumvent these issues, we propose a
decentralized full-waveform inversion, a scheme where connected agents in a
network optimize their objectives locally while being in consensus. The
proposed formulation can be solved using the ADMM method efficiently. We demonstrate
using the standard marmousi model that such scheme can decouple the
regularization from data fitting and reduce the network waiting time.},
  keywords = {EAGE},
  note = {(EAGE, Copenhagen)},
  doi = {10.3997/2214-4609.201801230},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2018/kadu2018EAGEdfwi/kadu2018EAGEdfwi.pdf}
}

@CONFERENCE{kumar2018EAGEcsb,
  author = {Rajiv Kumar and Shashin Sharan and Nick Moldoveanu and Felix J. Herrmann},
  title = {Compressed sensing based land simultaneous acquisition using encoded sweeps},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2018},
  month = {06},
  abstract = {Simultaneous shooting methods using encoded sweeps can enhance the
productivity of land acquisition in situations where deployment of many
vibrators and larger receiver spread is not possible in the field due to
obstructions or permit limitations. However, the existing framework requires
shooting the full sequence of encoded sweeps on each shot point to
reconstruct the complete frequency bandwidth Green's function. Although this simultaneous
shooting method reduces the sweeping time vs conventional sequential
shooting, the gain in efficiency is limited. To further reduce the sweeping time, we
propose to acquire randomly selected subsets of the encoded sweeps sequences
followed by a rank-minimization based joint source separation and spectral
interpolation framework to reconstruct the full bandwidth deblended Green's
function. We demonstrate the advantages of proposed sampling and
reconstruction framework using a synthetic seismic line simulated using SEG-SEAM Phase II
land velocity and density model.},
  keywords = {EAGE, compressed sensing, source separation, spectral interpolation, Land, SEAM},
  note = {(EAGE, Copenhagen)},
  doi = {10.3997/2214-4609.201800643},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2018/kumar2018EAGEcsb/kumar2018EAGEcsb.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2018/kumar2018EAGEcsb/kumar2018EAGEcsb_pres.pdf}
}

@CONFERENCE{siahkoohi2018EAGEsdr,
  author = {Ali Siahkoohi and Rajiv Kumar and Felix J. Herrmann},
  title = {Seismic data reconstruction with Generative Adversarial Networks},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2018},
  month = {06},
  abstract = {A main challenge in seismic imaging is acquiring densely sampled data. Compressed Sensing has provided theoretical foundations upon which desired sampling rate can be achieved by applying a sparsity promoting algorithm on sub-sampled data. The key point in successful recovery is to deploy a randomized sampling scheme. In this paper, we propose a novel deep learning-based method for fast and accurate reconstruction of heavily under-sampled seismic data, regardless of type of sampling. A neural network learns to do reconstruction directly from data via an adversarial process. Once trained, the reconstruction can be done by just feeding the frequency slice with missing data into the neural network. This adaptive nonlinear model makes the algorithm extremely flexible, applicable to data with arbitrarily type of sampling. With the assumption that we have access to training data, the quality of reconstructed slice is superior even for extremely low sampling rate (as low as 10%) due to the data-driven nature of the method.},
  keywords = {EAGE, data reconstruction, machine learning, generative adversarial networks},
  note = {(EAGE, Copenhagen)},
  doi = {10.3997/2214-4609.201801393},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2018/siahkoohi2018EAGEsdr/siahkoohi2018EAGEsdr.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2018/siahkoohi2018EAGEsdr/siahkoohi2018EAGEsdr_pres.pdf}
}

%-----2017-----%

@CONFERENCE{alfaraj2017SEGrsw,
  author = {Ali M. Alfaraj and Rajiv Kumar and Felix J. Herrmann},
  title = {Reconstruction of {S-waves} from low-cost randomized and simultaneous acquisition by joint sparse inversion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2017},
  month = {09},
  pages = {2533-2538},
  abstract = {Spatial Nyquist sampling rate, which is proportional to
                  the apparent subsurface velocity, is the key
                  deciding cost factor for conventional seismic
                  acquisition. Due to the lower shear wave velocity
                  compared with compressional waves, finer spatial
                  sampling is required to properly record the earlier,
                  which increases the acquisition costs. This is one
                  of the reasons why shear waves are usually not
                  considered in practice. To avoid having the Nyquist
                  criterion as the deciding cost factor and to utilize
                  multicomponent data to its available full extent, we
                  propose acquiring randomly undersampled ocean bottom
                  seismic data. Each component can then be
                  interpolated separately, followed by elastic
                  decomposition to recover up- and down-going
                  S-waves. Instead, we jointly interpolate and
                  decompose the recorded multicomponent data by
                  solving one sparsity promoting optimization
                  problem. This way we ensure that the relative
                  amplitudes of the multicomponent data is
                  preserved. We compare two sparsifying transforms:
                  the curvelet transform and the frequency-wavenumber
                  transform. Another key cost deciding factor for
                  seismic acquisition is the efficiency of acquiring
                  data. This calls for simultaneous acquisition, which
                  requires a source separation step. Similarly,
                  instead of taking a two-step approach, we perform a
                  sparsity-promoting joint source separation
                  decomposition. Results on economically and
                  efficiently acquired synthetic data of both joint
                  methods show their ability of reconstructing
                  accurate up- and down-going S-waves.},
  keywords = {SEG, elastic, shear wave, randomized acquisition, interpolation, source separation, decomposition, sparse},
  note = {(SEG, Houston)},
  doi = {10.1190/segam2017-17681376.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/alfaraj2017SEGrsw/alfaraj2017SEGrsw.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/alfaraj2017SEGrsw/alfaraj2017SEGrsw_pres.pdf}
}


@CONFERENCE{daskalakis2017SEGdds,
  author = {Emmanouil Daskalakis and Rachel Kuske and Felix J. Herrmann},
  title = {Developments in the direction of solving extremly large problems in {Geophysics}},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2017},
  month = {09},
  pages = {4375-4378},
  abstract = {Often in exploration Geophysics we are forced to work
                  with extremely large problems. Acquisitions via
                  dense grids of receivers translate into very large
                  mathematical systems. Usually, depending on the
                  acquisition, the size of the matrix of the system to
                  be solved, can be measured in the millions. The
                  proper way to address this problem is by
                  subsampling. Even though subsampling can reduce the
                  computational efforts required, it can not address
                  stability problems caused by preconditioning and/or
                  instrumental response errors. In this abstract, we
                  introduce a modification of the linearized Bregman
                  solver for these large problems that resolves
                  stability issues.},
  keywords = {SEG, linearized Bregman, cycling, weighted increment, least-squares migration},
  note = {(SEG, Houston)},
  doi = {10.1190/segam2017-17795188.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/daskalakis2017SEGdds/daskalakis2017SEGdds.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/daskalakis2017SEGdds/daskalakis2017SEGdds_pres.pdf}
}


@CONFERENCE{sharan2017SEGhrf,
  author = {Shashin Sharan and Rongrong Wang and Felix J. Herrmann},
  title = {High resolution fast microseismic source collocation and source time function estimation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2017},
  month = {09},
  pages = {2778-2783},
  abstract = {Sparsity promotion based joint microseismic source
                  collocation and source time function estimation,
                  using Linearized Bregman algorithm, although simple
                  to implement, suffers from slow convergence. This is
                  due to the fact that Linearized Bregman algorithm
                  has only first order of convergence. In this work,
                  we propose to accelerate the existing Linearized
                  Bregman algorithm using the L-BFGS
                  algorithm. Without any initial guess for the source
                  location or source time function, our method is able
                  to estimate the source location and source time
                  function for kinematically correct smooth velocity
                  model. We demonstrate the effectiveness of our
                  method for multiple sources spaced within half a
                  wavelength. We also compare our results with
                  Linearized Bregman based method in ``2.5``D instead
                  of ``2``D.},
  keywords = {SEG, algorithm, full-waveform inversion, microseismic, passive imaging, wave equation},
  note = {(SEG, Houston)},
  doi = {10.1190/segam2017-17787749.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/sharan2017SEGhrf/sharan2017SEGhrf.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/sharan2017SEGhrf/sharan2017SEGhrf_pres.pdf}
}


@CONFERENCE{wang2017SEGdff,
  author = {Rongrong Wang and Felix J. Herrmann},
  title = {A denoising formulation of full-waveform inversion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2017},
  month = {09},
  pages = {1594-1598},
  abstract = {We propose a wave-equation-based subsurface inversion
                  method that in many cases is more robust than the
                  conventional Full-Waveform Inversion. The new
                  formulation is written in a denoising form that
                  allows the synthetic data to match the observed ones
                  up to a small error. Compared to the Full-Waveform
                  Inversion, our method treats the noise arising from
                  the data measuring/recording process and that from
                  the synthetic modelling process
                  separately. Comparing to the Wavefields
                  Reconstruction Inversion, the new formulation
                  mitigates the difficulty of choosing the penalty
                  parameter $\lambda$. To solve the proposed
                  optimization problem, we develop an efficient
                  frequency domain algorithm that alternatively
                  updates the model and the data. Numerical
                  experiments confirm strong stability of the proposed
                  method by comparisons between the results of our
                  algorithm with that from both plain FWI and a
                  weighted formulation of the FWI.},
  keywords = {SEG, FWI, denoising},
  note = {(SEG, Houston)},
  doi = {10.1190/segam2017-17794690.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/wang2017SEGdff/wang2017SEGdff.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/wang2017SEGdff/wang2017SEGdff_pres.pdf}
}


@CONFERENCE{yang2017SEGfsp,
  author = {Mengmeng Yang and Emmanouil Daskalakis and Felix J. Herrmann},
  title = {Fast sparsity-promoting least-squares migration with multiples in time domain},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2017},
  month = {09},
  pages = {4828-4832},
  abstract = {Based on the latest developments of research in
                  inversion technology with optimization, researchers
                  have made significant progress in the implementation
                  of least-squares reverse-time migration (LS-RTM) of
                  primaries. In Marine data however, these
                  applications rely on the success of a pre-imaging
                  separation of primaries and multiples, which can be
                  modeled as a multi-dimensional convolution between
                  the vertical derivative of the surface-free Green’s
                  function and the down-going receiver
                  wavefield. Instead of imaging the primaries and
                  multiples separately, we implement the LS-RTM of the
                  total down-going wavefield by combining areal source
                  injection and linearized Born modelling, where
                  strong surface related multiples are generated from
                  a strong density variation at the ocean bottom. The
                  advantage including surface related multiples in
                  LS-RTM is the extra illumination we obtain from
                  these multiples without incurring additional
                  computational costs related to carrying out
                  multi-dimensional convolutions part of conventional
                  multiple prediction procedures. Even though we are
                  able to avert these computational costs, our
                  approach shares the large costs of LSRTM. We reduce
                  these costs by combining randomized source
                  subsampling with our sparsity-promoting imaging
                  technology, which produces artifact-free,
                  high-resolution images, with the surface-related
                  multiples migrated properly.},
  keywords = {SEG, sparsity, least-squares RTM, multiples, time domain},
  note = {(SEG, Houston)},
  doi = {10.1190/segam2017-17741608.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/yang2017SEGfsp/yang2017SEGfsp.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/yang2017SEGfsp/yang2017SEGfsp_pres.pdf}
}


@CONFERENCE{yang2017SEGWStds,
  author = {Mengmeng Yang and Felix J. Herrmann},
  title = {Time domain sparsity promoting LSRTM with surface-related multiples in shallow-water case},
  booktitle = {SEG Workshop on Multiples: Status, Progress, Challenges and Open issues; Houston},
  year = {2017},
  month = {09},
  abstract = {In the SRME relation, multiples are expressed as the multi- dimensional convolution between the the vertical derivative of the surface-free Green’s function and the down-going receiver wavefield. This relation leads to the methods that separate the surface-related multiples and primaries. Instead of imaging separated multiples trivially, we introduced the SRME relation into wave equation by areal source injection, which costs nothing extra to involve multiples into the forward wavefields. The related image contains the phantoms components from cross-correlation between different-orders of events. Especially in shallow water, due to the strong magnitudes of multiples and their overlap with primaries, the artifacts contain strong phantoms not only of ocean bottom but also subsurface layer interfaces. We use sparsity-promoting inversions where the nicely curved structure of the subsurface models are detected to help cleaning up the phantoms. We reduce the costs by combin- ing randomized source subsampling with linearized Bregman method and get cleaned image with one data pass.},
  keywords = {SEG, workshop, multiple},
  note = {(SEG Workshop, Houston)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/yang2017SEGWStds/yang2017SEGWStds.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/yang2017SEGWStds/yang2017SEGWStds_post.pdf}
}


@CONFERENCE{zhang2017SEGmsd,
  author = {Yiming Zhang and Curt Da Silva and Rajiv Kumar and Felix J. Herrmann},
  title = {Massive {3D} seismic data compression and inversion with hierarchical {Tucker}},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2017},
  month = {09},
  pages = {1347-1352},
  abstract = {Modern-day oil and gas exploration, especially in areas
                  of complex geology such as fault belts and sub-salt
                  areas, is an increasingly expensive and risky
                  endeavour. Typically long-offset and dense sampling
                  seismic data are required for subsequent shot based
                  processing procedures, e.g. wave-equation based
                  inversion (WEI) and surface-related multiple
                  elimination (SRME). However, these strict
                  requirements result in an exponential growth in data
                  volume size and prohibitive demands on computational
                  resources, given the multidimensional nature of the
                  data volumes. Moreover the physical constraints and
                  cost limitations impose restrictions on acquiring
                  fully sampled data. In this work, we propose to
                  invert our large-scale data from a set of subsampled
                  measurements, resulting in an estimate of the true
                  volume in a compressed low-rank tensor
                  format. Rather than expanding the data to its
                  fully-sampled form for later downstream processes,
                  we demonstrate how to use this compressed data
                  directly via on-the-fly common shot or receiver
                  gathers extraction. The combination of massive
                  compression and fast on demand data reconstruction
                  of 3D shot or receiver gathers leads to a
                  substantial reduction in memory costs but with
                  minimal effects on results in the subsequent
                  processing procedures. We demonstrate the effective
                  implementation of our proposed framework on
                  full-waveform inversion on a 3D seismic synthetic
                  data set generated from a Overthrust model.},
  keywords = {SEG, 3D, tensor algebra, FWI},
  note = {(SEG, Houston)},
  doi = {10.1190/segam2017-17742951.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/zhang2017SEGmsd/zhang2017SEGmsd.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2017/zhang2017SEGmsd/zhang2017SEGmsd_pres.pdf}
}


@CONFERENCE{alfaraj2017EAGEswr,
  author = {Ali M. Alfaraj and Rajiv Kumar and Felix J. Herrmann},
  title = {Shear wave reconstruction from low cost randomized acquisition},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2017},
  month = {06},
  abstract = {Shear waves travel in the subsurface at a lower speed
                  compared with compressional waves. Therefore, much
                  finer spatial sampling is required to properly
                  record the shear waves. This leads to higher
                  acquisition costs which are typically avoided by
                  designing surveys geared towards only compressional
                  waves imaging. We propose using randomly
                  under-sampled ocean bottom acquisition for recording
                  both compressional and shear waves. The recorded
                  multicomponent data is then interpolated using an
                  SVD-free low rank interpolation scheme that is
                  feasible for large scale seismic data volumes to
                  obtain finely sampled data. Following that, we
                  perform elastic wavefield decomposition at the ocean
                  bottom to recover accurate up- and dow-going
                  S-waves. Synthetic data results indicate that using
                  randomized under-sampled acquisition, we can recover
                  accurate S-waves with an economical cost compared
                  with conventional acquisition designs.},
  keywords = {EAGE, shear waves, rank minimization, interpolation, randomized acquisition},
  note = {(EAGE, Paris)},
  doi = {10.3997/2214-4609.201700594},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2017/alfaraj2017EAGEswr/alfaraj2017EAGEswr.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2017/alfaraj2017EAGEswr/alfaraj2017EAGEswr_pres.pdf}
}


@CONFERENCE{fang2017AIPepm,
  author = {Zhilong Fang and Curt Da Silva and Felix J. Herrmann},
  title = {An efficient penalty method for {PDE}-constrained optimization problem with source estimation and stochastic optimization},
  booktitle = {Applied Inverse Problems Annual Conference Proceedings},
  year = {2017},
  month = {05-06},
  pages = {40},
  abstract = {Many inverse problems in applied science and engineering
                  can be written as PDE-constrained optimization
                  problems. Typically, one is interested in estimating
                  the unknown medium parameters of the PDE describing
                  the system, given multi-source measurements of the
                  fields in the true medium. The objective function is
                  often very oscillatory, which gives rise to
                  problematic local minima, and merely evaluating one
                  or more of its derivatives requires solving multiple
                  PDEs, which are expensive. In many applications such
                  as seismic exploration, only the source location is
                  available and the source signature itself is
                  unknown, which results in erroneous estimated
                  parameters. In this work, we propose a penalty
                  method to solve the PDE-constrained optimization
                  problem, whereby we replace the PDE constraints by a
                  two-norm penalty term, which leads to a bi-linear
                  optimization problem with a much less oscillatory
                  objective function. By applying the variable
                  projection method, we develop an efficient method to
                  invert the unknown parameters and source signatures
                  simultaneously. To reduce the computational cost, we
                  randomly select a subset of all data to update the
                  parameters at each iteration. Numerical examples
                  demonstrate that this method is less prone to
                  becoming stuck in harmful local minima and is able
                  to successfully invert the unknown parameters and
                  source signature with less computational cost
                  compared to using all the data.},
  keywords = {optimization, source estimation, stochastic optimization, penalty method},
  note = {(AIP, Hangzhou)},
  url = {http://aip2017.csp.escience.cn/dct/page/1},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/AIP/2017/fang2017AIPepm/fang2017AIPepm_pres.pdf}
}


@CONFERENCE{fang2017WAVESuqf,
  author = {Zhilong Fang and Curt Da Silva and Rachel Kuske and Felix J. Herrmann},
  title = {Uncertainty quantification for inverse problems with a weak wave-equation constraint},
  booktitle = {WAVES 2017 --- 13th International Conference on Mathematical and Numerical Aspects of Wave Propagation},
  year = {2017},
  month = {05},
  pages = {127-128},
  abstract = {In this work, we present a new posterior distribution to
                  quantify uncertainties in solutions of wave-equation
                  based inverse problems. By introducing an auxiliary
                  variable for the wavefields, we weaken the strict
                  wave-equation constraint used by conventional
                  Bayesian approaches. With this weak constraint, the
                  new posterior distribution is a bi-Gaussian
                  distribution with respect to both model parameters
                  and wavefields, which can be directly sampled by the
                  Gibbs sampling method.},
  keywords = {uncertainty, wave equation, constraint, Gibbs sampling},
  note = {(WAVES, Minneapolis)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/WAVES/2017/fang2017WAVESuqf/fang2017WAVESuqf.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/WAVES/2017/fang2017WAVESuqf/fang2017WAVESuqf_pres.pdf}
}


@CONFERENCE{herrmann2017SIAMdsa,
  author = {Felix J. Herrmann and Curt Da Silva},
  title = {Domain-specific abstractions for full-waveform inversion},
  booktitle = {SIAM Conference on Computational Science and Engineering},
  year = {2017},
  month = {02-03},
  keywords = {SIAM, FWI, inverse problems, software},
  note = {(SIAM Conference on Computational Science and Engineering, Atlanta)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2017/herrmann2017SIAMdsa/herrmann2017SIAMdsa_pres.pdf}
}


@CONFERENCE{kukreja2016OGHPClsm,
  author = {Navjot Kukreja and Mathias Louboutin and Michael Lange and Fabio Luporini and Gerard Gorman},
  title = {Leveraging symbolic math for rapid development of applications for seismic modeling},
  year = {2017},
  month = {03},
  booktitle = {OGHPC},
  abstract = {Wave propagation kernels are the core of many commonly
                  used algorithms for inverse problems in exploration
                  geophysics. While they are easy to write and analyze
                  for the simplied cases, the code quickly becomes
                  complex when the physics needs to be made more
                  precise or the performance of these codes is to be
                  optimized. Signicant eort is repeated every time new
                  forms of physics need to be implemented, or a new
                  computing platform to be supported. The use of
                  symbolic mathematics as a domain specic language
                  (DSL) for input, combined with automatic generation
                  of high performance code customized for the target
                  hardware platform is a promising approach to
                  maximize code reuse. Devito is a DSL for nite
                  dierence that uses symbolic mathematics to generate
                  optimized code for wave propagation based on a
                  provided wave equation. It enables rapid application
                  development in a eld where the average time spent on
                  development has historically been in weeks and
                  months. The Devito DSL system is completely wrapped
                  within the Python programming language and the fact
                  that the running code is in C is completely
                  transparent, making it simple to include Devito as
                  part of a larger work ow including multiple
                  applications over a large cluster.},
  keywords = {OGHPC, finite differences, HPC, modelling, time domain},
  note = {(Oil and Gas HPC Conference, Rice University)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/OGHPC/2017/kukreja2016OGHPClsm/kukreja2016OGHPClsm.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/OGHPC/2017/kukreja2016OGHPClsm/kukreja2016OGHPClsm_pres.pdf}
}


@CONFERENCE{kukreja2017SIAMdsm,
  author = {Navjot Kukreja and Michael Lange and Mathias Louboutin and Fabio Luporini and Gerard Gorman},
  title = {Devito: symbolic math for automated fast finite difference computations},
  booktitle = {SIAM Conference on Computational Science and Engineering},
  year = {2017},
  month = {02-03},
  keywords = {SIAM, devito, finite differences, high performance computing, modelling, acoustic, compiler, stencil},
  note = {(SIAM Conference on Computational Science and Engineering, Atlanta)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2017/kukreja2017SIAMdsm/kukreja2017SIAMdsm_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2017/kukreja2017SIAMdsm/kukreja2017SIAMdsm_pres.mov}
}


@CONFERENCE{kumar2017EAGEdha,
  author = {Rajiv Kumar and Nick Moldoveanu and Felix J. Herrmann},
  title = {Denoising high-amplitude cross-flow noise using curvelet-based stable principle component pursuit},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2017},
  month = {06},
  abstract = {Removal of high-amplitude cross-flow noise in marine
                  towed-streamer acquisition is of great interest
                  because cross-flow noise hinders the success of
                  subsequent processing (e.g. EPSI) and
                  migration. However, the removal of cross-flow noise
                  is a challenging process because cross-flow noise
                  dominates steep angles and low-frequency components
                  of the signal. As a result, applying a simple
                  high-pass filter can result in a loss of coherent
                  diving waves and reflected energy. We propose a
                  stable curvelet-based principle-component pursuit
                  approach that does not suffer from this shortcoming
                  because it uses angle- and scale-adaptivity of the
                  curvelet transform in combination with the low-rank
                  property of cross-flow noise. As long as the
                  cross-flow noise exhibits low-rank in the curvelet
                  domain, our method successfully separates this
                  signal component from the diving waves and seismic
                  reflectivity, which is well-know to be sparse in the
                  curvelet domain. Experimental results on a
                  common-shot gather extracted from a coil shooting
                  survey in the Gulf of Mexico shows the potential of
                  our approach.},
  keywords = {EAGE, denoising, coil data, cross-flow noise, curvelet, SPCP},
  note = {(EAGE, Paris)},
  doi = {10.3997/2214-4609.201701055},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2017/kumar2017EAGEdha/kumar2017EAGEdha.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2017/kumar2017EAGEdha/kumar2017EAGEdha_poster.pdf}
}


@CONFERENCE{lange2017SCIPYofd,
  author = {Michael Lange and Navjot Kukreja and Fabio Luporini and Mathias Louboutin and Charles Yount and Jan H\"uckelheim and Gerard Gorman},
  title = {Optimised finite difference computation from symbolic equations},
  booktitle = {Python in Science Conference Proceedings},
  year = {2017},
  month = {07},
  pages = {89-96},
  abstract = {Domain-specific high-productivity environments are
                  playing an increasingly important role in scientific
                  computing due to the levels of abstraction and
                  automation they provide. In this paper we introduce
                  Devito, an open-source domain-specific framework for
                  solving partial differential equations from symbolic
                  problem definitions by the finite difference
                  method. We highlight the generation and automated
                  execution of highly optimized stencil code from only
                  a few lines of high-level symbolic Python for a set
                  of scientific equations, before exploring the use of
                  Devito operators in seismic inversion problems.},
  keywords = {finite difference, domain-specific languages, symbolic Python},
  note = {(SciPy, Texas)},
  url = {http://conference.scipy.org/proceedings/scipy2017/michael_lange.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SciPy/2017/lange2017SCIPYofd/lange2017SCIPYofd_pres.pdf}
}


@CONFERENCE{louboutin2017EAGEdns,
  author = {Mathias Louboutin and Llu\'{i}s Guasch and Felix J. Herrmann},
  title = {Data normalization strategies for full-waveform inversion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2017},
  month = {06},
  abstract = {Amplitude mismatch is an inherent problem in seismic
                  inversion. Most of the source estimation techniques
                  are associated with amplitude uncertainty due to
                  incomplete representation of the physics or
                  estimation method parameters. Rewriting the
                  inversion problem in an amplitude free formulation
                  allows to mitigate the amplitude ambiguity and help
                  the inversion process to converge. We present in
                  this work two different strategies to lessen
                  amplitude effects in seismic inversion, derive the
                  corresponding update directions and show how we
                  handle scaling error correctly in both the objective
                  function and the gradient.},
  keywords = {EAGE, FWI, normalization, inversion, nonlinear},
  note = {(EAGE, Paris)},
  doi = {10.3997/2214-4609.201700720},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2017/louboutin2017EAGEdns/louboutin2017EAGEdns.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2017/louboutin2017EAGEdns/louboutin2017EAGEdns_poster.pdf}
}


@CONFERENCE{louboutin2017EAGEess,
  author = {Mathias Louboutin and Felix J. Herrmann},
  title = {Extending the search space of time-domain adjoint-state {FWI} with randomized implicit time shifts},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2017},
  month = {06},
  abstract = {Adjoint-state full-waveform inversion aims to obtain
                  subsurface properties such as velocity, density or
                  anisotropy parameters, from surface recorded
                  data. As with any (non-stochastic) gradient based
                  optimization procedure, the solution of this
                  inversion procedure is to a large extend determined
                  by the quality of the starting model. If this
                  starting model is too far from the true model, these
                  derivative-based optimizations will likely end up in
                  local minima and erroneous inversion results. In
                  certain cases, extension of the search space,
                  e.g. by making the wavefields or focused matched
                  sources additional unknowns, has removed some of
                  these non-uniqueness issues but these rely on
                  time-harmonic formulations. Here, we follow a
                  different approach by combining an implicit
                  extension of the velocity model, time compression
                  techniques and recent results on stochastic sampling
                  in non-smooth/non-convex optimization},
  keywords = {EAGE, time domain, FWI, cycle skipping, inversion, nonconvex},
  note = {(EAGE, Paris)},
  doi = {10.3997/2214-4609.201700831},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2017/louboutin2017EAGEess/louboutin2017EAGEess.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2017/louboutin2017EAGEess/louboutin2017EAGEess_pres.pdf}
}


@CONFERENCE{louboutin2017SIAMras,
  author = {Mathias Louboutin and Michael Lange and Navjot Kukreja and Fabio Luporini and Felix J. Herrmann and Gerard Gorman},
  title = {Raising the abstraction to separate concerns: enabling different physics for geophysical exploration},
  booktitle = {SIAM Conference on Computational Science and Engineering},
  year = {2017},
  month = {02-03},
  abstract = {Full-waveform inversion is a PDE-constrained
                  optimisation problem involving massive amounts of
                  data (petabytes) and large numbers of unknowns
                  (O(10^9)). This well known compute-intensive and
                  data-intensive is extremely challenging for several
                  reasons. First, there is the complexity of having to
                  handle extremely large data volumes with metadata
                  related to experimental details in the field, and
                  the discretization of the unknown earth parameters
                  and approximate physics. Second, reduced or
                  adjoint-state methods call for computationally
                  intensive PDE solves for each source experiment (of
                  which there are thousands) for each iteration of a
                  gradient-based optimization scheme. The talks will
                  give an overview how carefully chosen layers of
                  abstraction can help manage both the complexity and
                  scale of inversion while still achieving the high
                  degree of computational performance required to make
                  full-waveform a practical tool. Specifically, the
                  presentations will focus on domain specific stencil
                  language for time-stepping methods to solve various
                  types of wave equations and on abstracts for
                  large-scale parallel optimization frameworks.},
  keywords = {SIAM, time domain, modelling, finite differences, HPC},
  note = {(SIAM Conference on Computational Science and Engineering, Atlanta)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2017/louboutin2017SIAMras/louboutin2017SIAMras_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2017/louboutin2017SIAMras/louboutin2017SIAMras_pres.mov}
}


@CONFERENCE{oghenekohwo2017EAGEitl,
  author = {Felix Oghenekohwo and Felix J. Herrmann},
  title = {Improved time-lapse data repeatability with randomized sampling and distributed compressive sensing},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2017},
  month = {06},
  abstract = {Recently, new ideas on randomized sampling for
                  time-lapse seismic acquisition have been proposed to
                  address some of the challenges of replicating
                  time-lapse surveys. These ideas, which stem from
                  distributed compressed sensing (DCS) led to the
                  birth of a joint recovery model (JRM) for processing
                  time-lapse data (noise-free) acquired from
                  non-replicated acquisition geometries. However, when
                  the earth does not change---i.e. no time-lapse—the
                  recovered vintages from two non-replicated surveys
                  should show high repeatability measured in terms of
                  normalized RMS, which is a standard metric for
                  quantifying time-lapse data repeatability. Under
                  this assumption of no time-lapse change, we
                  demonstrate improved repeatability (with JRM) of the
                  recovered data from non-replicated random samplings,
                  first with noisy data and secondly in situations
                  where there are calibration errors i.e. where the
                  acquisition parameters such as source/receiver
                  coordinates are not precise.},
  keywords = {EAGE, repeatability, time lapse, compressive sensing, calibration, noise},
  note = {(EAGE, Paris)},
  doi = {10.3997/2214-4609.201701389},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2017/oghenekohwo2017EAGEitl/oghenekohwo2017EAGEitl.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2017/oghenekohwo2017EAGEitl/oghenekohwo2017EAGEitl_poster.pdf}
}


@CONFERENCE{witte2017EAGEspl,
  author = {Philipp A. Witte and Mengmeng Yang and Felix J. Herrmann},
  title = {Sparsity-promoting least-squares migration with the linearized inverse scattering imaging condition},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2017},
  month = {06},
  abstract = {Reverse-time migration (RTM) with the conventional
                  cross-correlation imaging condition suffers from
                  low-frequency artifacts that result from
                  backscattered energy in the background velocity
                  models. This problem translates to least-squares
                  reverse-time migration (LS-RTM), where these
                  artifacts slow down the convergence, as many of the
                  initial iterations are spent on removing them. In
                  RTM, this problem has been successfully addressed by
                  the introduction of the so-called inverse scattering
                  imaging condition, which naturally removes these
                  artifacts. In this work, we derive the corresponding
                  linearized forward operator of the inverse
                  scattering imaging operator and incorporate this
                  forward/adjoint operator pair into a
                  sparsity-promoting (SPLS-RTM) workflow. We
                  demonstrate on a challenging salt model, that LS-RTM
                  with the inverse scattering imaging condition is far
                  less prone to low-frequency artifacts than the
                  conventional cross-correlation imaging condition,
                  improves the convergence and does not require any
                  type of additional image filters within the
                  inversion. Through source subsampling and sparsity
                  promotion, we reduce the computational cost in terms
                  of PDE solves to a number comparable to conventional
                  RTM, making our workflow applicable to large-scale
                  problems.},
  keywords = {EAGE, least-squares migration, imaging condition, linearized Bregman},
  note = {(EAGE, Paris)},
  doi = {10.3997/2214-4609.201701125},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2017/witte2017EAGEspl/witte2017EAGEspl.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2017/witte2017EAGEspl/witte2017EAGEspl_poster.pdf}
}


@CONFERENCE{witte2017SIAMlsw,
  author = {Philipp A. Witte and Mathias Louboutin and Felix J. Herrmann},
  title = {Large-scale workflows for wave-equation based inversion in {Julia}},
  booktitle = {SIAM Conference on Computational Science and Engineering},
  year = {2017},
  month = {02-03},
  abstract = {Full-waveform inversion is a PDE-constrained
                  optimisation problem involving massive amounts of
                  data (petabytes) and large numbers of unknowns
                  (O(10^9)). This well known compute-intensive and
                  data-intensive is extremely challenging for several
                  reasons. First, there is the complexity of having to
                  handle extremely large data volumes with metadata
                  related to experimental details in the field, and
                  the discretization of the unknown earth parameters
                  and approximate physics. Second, reduced or
                  adjoint-state methods call for computationally
                  intensive PDE solves for each source experiment (of
                  which there are thousands) for each iteration of a
                  gradient-based optimization scheme. The talks will
                  give an overview how carefully chosen layers of
                  abstraction can help manage both the complexity and
                  scale of inversion while still achieving the high
                  degree of computational performance required to make
                  full-waveform a practical tool. Specifically, the
                  presentations will focus on domain specific stencil
                  language for time-stepping methods to solve various
                  types of wave equations and on abstracts for
                  large-scale parallel optimization frameworks.},
  keywords = {SIAM, Julia, full-waveform inversion, least-squares migration, large scale},
  note = {(SIAM Conference on Computational Science and Engineering, Atlanta)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2017/witte2017SIAMlsw/witte2017SIAMlsw_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2017/witte2017SIAMlsw/witte2017SIAMlsw_pres.mov}
}


%-----2016-----%

@CONFERENCE{bougher2016SEGava,
  author = {Ben B. Bougher and Felix J. Herrmann},
  title = {AVA classification as an unsupervised machine-learning problem},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2016},
  month = {10},
  pages = {553-556},
  abstract = {Much of AVA analysis relies on characterizing background
                  trends and anomalies in pre-stack seismic
                  data. Analysts reduce a seismic section into a small
                  number of these trends and anomalies, suggesting
                  that a low-dimensional structure can be inferred
                  from the data. We describe AVA-attribute
                  characterization as an unsupervised-learning
                  problem, where AVA classes are learned directly from
                  the data without any prior assumptions on physics
                  and geological settings. The method is demonstrated
                  on the Marmousi II elastic model, where a gas
                  reservoir was successfully delineated from a
                  background trend in a depth migrated image.},
  keywords = {AVA, machine learning, SEG},
  note = {(SEG, Dallas)},
  doi = {10.1190/segam2016-13874419.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/bougher2016SEGava/bougher2016SEGava.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/bougher2016SEGava/bougher2016SEGava_pres.pdf}
}


@CONFERENCE{chai2016SEGlbm,
  author = {Xintao Chai and Mengmeng Yang and Philipp A. Witte and Rongrong Wang and Zhilong Fang and Felix J. Herrmann},
  title = {A linearized {Bregman} method for compressive waveform inversion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2016},
  month = {10},
  pages = {1449-1454},
  abstract = {We present an implementation of a recently developed and
                  relatively simple linearized Bregman method to solve
                  the large-scale $\ell_1$-norm sparsity-promoting
                  Gauss-Newton (GN) full-waveform inversion (FWI)
                  problem. Numerical experiments demonstrate that: the
                  much simpler linearized Bregman method does a
                  comparable and even superior job compared to a
                  state-of-the-art $\ell_1$-norm solver SPGL1, which
                  is previously used in the modified Gauss-Newton FWI;
                  the linearized Bregman method is also more efficient
                  (faster) than SPGL1; the FWI result with the
                  linearized Bregman method solving $\ell_1$-norm
                  sparsity-promoting problems to get the model updates
                  is better than that obtained by solving
                  $\ell_2$-norm least-squares problems to get the
                  model updates.},
  keywords = {Bregman method, waveform inversion, SEG},
  note = {(SEG, Dallas)},
  doi = {10.1190/segam2016-13848105.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/chai2016SEGlbm/chai2016SEGlbm.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/chai2016SEGlbm/chai2016SEGlbm_pres.pdf}
}


@CONFERENCE{dasilva2016SEGuse,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {A unified {2D/3D} software environment for large scale time-harmonic full waveform inversion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2016},
  month = {10},
  pages = {1169-1173},
  abstract = {Full-Waveform Inversion is a costly and complex
                  procedure for realistically sized 3D seismic
                  data. The performance-critical nature of this
                  problem often results in software environments that
                  are written entirely in low-level languages, making
                  them hard to understand, maintain, improve, and
                  extend. The unreadability of such codes can stymie
                  research developments, where the translation from
                  higher level mathematical ideas to high performance
                  codes can be lost, inhibiting the uptake of new
                  ideas in production-level codebases. We propose a
                  new software organization paradigm for Full-Waveform
                  Inversion and other PDE-constrained optimization
                  problems that is flexible, efficient, scalable, and
                  demonstrably correct. We decompose the various
                  structural components of FWI in to its constituent
                  components, from parallel computation to assembling
                  objective functions and gradients to lower level
                  matrix-vector multiplications. This decomposition
                  allows us to create a framework where individual
                  components can be easily swapped out to suit a
                  particular user's existing software environment. The
                  ease of applying high-level algorithms to the FWI
                  problem allows us to easily implement stochastic FWI
                  and demonstrate its effectiveness on a large scale
                  3D problem.},
  keywords = {software, full-waveform inversion, large scale, numerical analysis, SEG},
  note = {(SEG, Dallas)},
  doi = {10.1190/segam2016-13869051.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/dasilva2016SEGuse/dasilva2016SEGuse.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/dasilva2016SEGuse/dasilva2016SEGuse_pres.pdf}
}


@CONFERENCE{deaguiar2016SCdff,
  author = {Marcos de Aguiar and Gerard Gorman and Felix J. Herrmann and Navjot Kukreja and Michael Lange and Mathias Louboutin and Felippe Vieira Zacarias},
  title = {DeVito: fast finite difference computation},
  booktitle = {Super Computing (SC16)},
  year = {2016},
  month = {11},
  abstract = {Seismic imaging, used in energy exploration, is arguably
                  the most compute and data intensive application in
                  the private sector. The commonly used methods
                  involve solving the wave equations numerically using
                  finite difference formulations. Writing optimized
                  code for these applications involves multiple
                  man-years of effort that need to be repeated every
                  time a new development needs to be factored in – for
                  every target platform. DeVito is a new tool for
                  performing optimized Finite Difference (FD)
                  computation from high-level symbolic problem
                  definitions. The application developer needs to
                  provide a differential equation in symbolic
                  form. DeVito performs automated code generation and
                  Just-In-Time (JIT) compilation based on this
                  symbolic equation to create and execute highly
                  optimized Finite Difference kernels on multiple
                  computer platforms. DeVito has been designed to be
                  used as part of complex workflows involving data
                  flows across multiple applications over different
                  nodes of a cluster.},
  keywords = {finite differences, high performance computing, full-waveform inversion},
  note = {(Super Computing, Utah)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SC/2016/deaguiar2016SCdff/deaguiar2016SCdff_poster.pdf},
  url2 = {http://sc16.supercomputing.org/sc-archive/tech_poster/tech_poster_pages/post155.html}
}


@CONFERENCE{fang2016SEGuqw,
  author = {Zhilong Fang and Chia Ying Lee and Curt Da Silva and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Uncertainty quantification for {Wavefield} {Reconstruction} {Inversion} using a {PDE} free semidefinite {Hessian} and randomize-then-optimize method},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2016},
  month = {10},
  pages = {1390-1394},
  abstract = {The data of full-waveform inversion often contains
                  noise, which induces uncertainties in the inversion
                  results. Ideally, one would like to run a number of
                  independent inversions with different realizations
                  of the noise and assess model-side uncertainties
                  from the resulting models, however this is not
                  feasible because we collect the data only once. To
                  circumvent this restriction, various sampling
                  schemes have been devised to generate an ensemble of
                  models that fit the data to within the noise
                  level. Such sampling schemes typically involve
                  running multiple inversions or evaluating the
                  Hessian of the cost function, both of which are
                  computationally expensive. In this work, we propose
                  a new method to quantify uncertainties based on a
                  novel formulation of the full-waveform inversion
                  problem – wavefield reconstruction inversion. Based
                  on this formulation, we formulate a semidefinite
                  approximation of the corresponding Hessian
                  matrix. By precomputing certain quantities, we are
                  able to apply this Hessian to given input vectors
                  without additional solutions of the underlying
                  partial differential equations. To generate a
                  sample, we solve an auxiliary stochastic
                  optimization problem involving this Hessian. The
                  result is a computationally feasible method that,
                  with little overhead, can generate as many samples
                  as required at small additional cost. We test our
                  method on the synthetic BG Compass model and compare
                  the results to a direct-sampling approach. The
                  results show the feasibility of applying our method
                  to computing statistical quantities such as the mean
                  and standard deviation in the context of wavefield
                  reconstruction inversion.},
  keywords = {WRI, uncertainty quantification, SEG},
  note = {(SEG, Dallas)},
  doi = {10.1190/segam2016-13879108.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/fang2016SEGuqw/fang2016SEGuqw.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/fang2016SEGuqw/fang2016SEGuqw_pres.pdf}
}


@CONFERENCE{herrmann2016SEGWSctl,
  author = {Felix J. Herrmann and Rajiv Kumar and Felix Oghenekohwo and Shashin Sharan and Haneet Wason},
  title = {Compressive time-lapse marine acquisition},
  booktitle = {SEG Workshop on Low cost geophysics: How to be creative in a cost-challenged environment; Dallas},
  year = {2016},
  month = {10},
  keywords = {SEG, workshop, acquisition},
  note = {(SEG Workshop, Dallas)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/herrmann2016SEGWSctl/herrmann2016SEGWSctl_pres.pdf}
}


@CONFERENCE{herrmann2016SEGWScvp,
  author = {Felix J. Herrmann and Bas Peters},
  title = {Constraints versus penalties for edge-preserving full-waveform inversion},
  booktitle = {SEG Workshop on Where are we heading with FWI; Dallas},
  year = {2016},
  month = {10},
  abstract = {Full-waveform inversion is arguably one of the most
                  challenging inverse problems out there. For this
                  reason, it is remarkable that so much progress has
                  been made over the years. While we can look back
                  with confidence at an increasing number of success
                  stories, the application of full-waveform
                  technologies to deeper targets and complex geologies
                  remains challenging making it an expensive
                  proposition. So far, efforts to overcome some of
                  these challenges have mainly been directed towards
                  extended formulations such as Symes's and Biondi's
                  subsurface offset/rau-parameters, Mike Warner/Lluis
                  Guasch's Wiener filters in their adaptive
                  full-waveform inversion, and van Leeuwen/Herrmann's
                  Wavefield Reconstruction Inversion, where the
                  wave-equation appears as a least-squares term in the
                  objective. While this recent work --- including
                  other approaches such as different data-misfit
                  objective functions --- has removed some of FWI
                  cycle-skip issues it does, with the exception
                  perhaps of box constraints, not incorporate
                  rudimentary information on the geology on the
                  unknown model. Following recent work by Ernie Esser,
                  we present a general framework how to incorporate
                  this type of information in the form of
                  constraints. Contrary to Tikhonov-like
                  regularization or gradient filtering, imposing
                  constraints has several key advantages, namely (i)
                  they can be expressed in simple human understandable
                  terms such as lower and upper limits for the
                  velocity without the necessity to introduce
                  additional parameters and assumptions on the
                  probability distribution of the model; (ii) they can
                  consist of several constraints, e.g. box and
                  smoothness constraints, as long as their
                  intersection in non-empty; and (iii) they can be
                  imposed in a separate inner loop, which does not
                  affect gradients and Hessians of full-waveform
                  inversion itself. By means of examples, we will
                  demonstrate the advocacy of constrained formulations
                  as they apply to full-waveform inversion. This is
                  joint work with Bas Peters.},
  keywords = {SEG, workshop, full-waveform inversion},
  note = {(SEG Workshop, Dallas)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/herrmann2016SEGWScvp/herrmann2016SEGWScvp_pres.pdf}
}


@CONFERENCE{kukreja2016WOLFHPCdaf,
  author = {Navjot Kukreja and Mathias Louboutin and Felippe Vieira Zacarias and Fabio Luporini and Michael Lange and Gerard Gorman},
  title = {Devito: automated fast finite difference computation},
  booktitle = {WOLFHPC 2016 Workshop (Super Computing)},
  year = {2016},
  month = {11},
  abstract = {Domain specific languages have successfully been used in
                  a variety of fields to cleanly express scientific
                  problems as well as to simplify implementation and
                  performance optimization on different computer
                  architectures. Although a large number of stencil
                  languages are available, finite difference domain
                  specific languages have proved challenging to design
                  because most practical use cases require additional
                  features that fall outside the finite difference
                  abstraction. Inspired by the complexity of
                  real-world seismic imaging problems, we introduce
                  Devito, a domain specific language in which high
                  level equations are expressed using symbolic
                  expressions from the SymPy package. Complex
                  equations are automatically manipulated, optimized,
                  and translated into highly optimized C code that
                  aims to perform comparably or better than hand-tuned
                  code. All this is transparent to users, who only see
                  concise symbolic mathematical expressions.},
  keywords = {finite differences, high performance computing, modelling, acoustic, compiler, stencil},
  note = {(WOLFHPC Workshop (SC16), Utah)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SC/2016/WOLFHPC/kukreja2016WOLFHPCdaf/kukreja2016WOLFHPCdaf.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SC/2016/WOLFHPC/kukreja2016WOLFHPCdaf/kukreja2016WOLFHPCdaf_pres.pdf}
}


@CONFERENCE{kumar2016SEGtjm,
  author = {Rajiv Kumar and Shashin Sharan and Haneet Wason and Felix J. Herrmann},
  title = {Time-jittered marine acquisition---a rank-minimization approach for {5D} source separation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2016},
  month = {10},
  pages = {119-123},
  abstract = {Simultaneous source marine acquisition has been
                  recognized as an economic way of improving spatial
                  sampling and speedup acquisition time, where a
                  single- (or multiple-) source vessel fires at
                  jittered source locations and time
                  instances. Consequently, the acquired simultaneous
                  data volume is processed to separate the overlapping
                  shot records resulting in densely sampled data
                  volume. It has been shown in the past that the
                  simultaneous source acquisition design and source
                  separation process can be setup as a compressed
                  sensing problem, where conventional seismic data is
                  reconstructed from simultaneous data via a
                  sparsity-promoting optimization formulation. While
                  the recovery quality of separated data is reasonably
                  well, the recovery process can be computationally
                  expensive due to transform-domain redundancy. In
                  this paper, we present a computationally tractable
                  rank-minimization algorithm to separate simultaneous
                  data volumes. The proposed algorithm is suitable for
                  large-scale seismic data, since it avoids
                  singular-value decompositions and uses a low-rank
                  based factorized formulation instead. Results are
                  illustrated for simulations of simultaneous
                  time-jittered continuous recording for a 3D
                  ocean-bottom cable survey.},
  keywords = {5D, marine, time-jittered acquisition, source separation, SEG},
  note = {(SEG, Dallas)},
  doi = {10.1190/segam2016-13878249.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/kumar2016SEGtjm/kumar2016SEGtjm.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/kumar2016SEGtjm/kumar2016SEGtjm_pres.pdf}
}


@CONFERENCE{lange2016dtg,
  author = {Michael Lange and Navjot Kukreja and Mathias Louboutin and Fabio Luporini and Felippe Vieira Zacarias and Vincenzo Pandolfo and Paulius Velesko and Paulius Kazakas and Gerard Gorman},
  title = {Devito: {Towards} a generic finite difference {DSL} using symbolic python},
  booktitle = {6th Workshop on Python for High-Performance and Scientific Computing},
  year = {2016},
  month = {11},
  pages = {67-75},
  abstract = {Domain specific languages (DSL) have been used in a
                  variety of fields to express complex scientific
                  problems in a concise manner and provide automated
                  performance optimization for a range of
                  computational architectures. As such DSLs provide a
                  powerful mechanism to speed up scientific Python
                  computation that goes beyond traditional
                  vectorization and pre-compilation approaches, while
                  allowing domain scientists to build applications
                  within the comforts of the Python software
                  ecosystem. In this paper we present Devito, a new
                  finite difference DSL that provides optimized
                  stencil computation from high-level problem
                  specifications based on symbolic Python
                  expressions. We demonstrate Devito's symbolic API
                  and performance advantages over traditional Python
                  acceleration methods before highlighting its use in
                  the scientific context of seismic inversion
                  problems.},
  keywords = {HPC, python, finite differences, acoustic, modelling, inversion, software optimization},
  note = {(PyHPC, Utah)},
  doi = {10.1109/PyHPC.2016.9},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/PyHPC/2016/lange2016dtg/lange2016dtg.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/PyHPC/2016/lange2016dtg/lange2016dtg_pres.pdf}
}


@CONFERENCE{lopez2016CMSogl,
  author = {Oscar Lopez and Rajiv Kumar and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Off-the-grid low-rank matrix recovery: seismic data reconstruction},
  booktitle = {Canadian Mathematical Society Summer Meeting},
  year = {2016},
  month = {06},
  abstract = {This talk discusses a modified low-rank matrix recovery
                  work-flow that admits unstructured observations. By
                  incorporating a regularization operator which
                  accurately maps structured data to unstructured
                  data, into the nuclear-norm minimization problem,
                  this approach is able to compensate for data
                  irregularity. Furthermore, by construction this
                  formulation yields output that is supported on a
                  structured grid. Recovery error bounds are
                  established for the methodology with matrix sensing
                  and matrix completion numerical experiments
                  including applications to seismic trace
                  interpolation to demonstrate the potential of the
                  approach.},
  keywords = {CMS, matrix completion, matrix sensing, nonuniform discrete Fourier transform, nuclear-norm, seismic data interpolation},
  note = {(CMS, Edmonton, Alberta)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CMS/2016/lopez2016CMSogl/lopez2016CMSogl_pres.pdf}
}


@CONFERENCE{peters2016SEGprs,
  author = {Bas Peters and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Parallel reformulation of the sequential adjoint-state method},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2016},
  month = {10},
  pages = {1411-1415},
  abstract = {This abstract is about reducing the total computation
                  time to solve full-waveform inversion problems. A
                  common problem formulation is a nonlinear
                  least-squares problem where the gradient is computed
                  using the adjoint-state algorithm. This formulation
                  offers parallelism for the computation of gradients
                  for different sources and frequencies. The
                  adjoint-state algorithm itself is sequential however
                  and this is a limiting factor when a lot of compute
                  nodes are available and only a few wavefields need
                  to be computed. This situation occurs when
                  stochastic optimization strategies are used to
                  minimize the objective function. We present a
                  parallel reformulation of the sequential
                  adjoint-state algorithm, which allows the forward-
                  and adjoint wavefields to be computed in
                  parallel. Both algorithms are mathematically
                  equivalent but the parallel version is twice as fast
                  in run time. An important characteristic of the
                  proposed algorithm is that one wavefield needs to be
                  computed per source and one per receiver. These
                  fields can be used to apply the (inverse)
                  Gauss-Newton Hessian to a vector without recomputing
                  wavefields. A 2D example shows that good
                  full-waveform inversion results are obtained, even
                  when a small number of sources and receivers is
                  used.},
  keywords = {numerical linear algebra, PDE-constrained optimization, FWI, parallel computing, SEG},
  note = {(SEG, Dallas)},
  doi = {10.1190/segam2016-13966771.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/peters2016SEGprs/peters2016SEGprs.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/peters2016SEGprs/peters2016SEGprs_pres.pdf}
}


@CONFERENCE{sharan2016SEGspj,
  author = {Shashin Sharan and Rongrong Wang and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Sparsity-promoting joint microseismic source collocation and source-time function estimation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2016},
  month = {10},
  pages = {2574-2579},
  abstract = {In this work, we propose a new method to simultaneously
                  locate microseismic events (e.g., induced by
                  hydraulic fracturing) and estimate the source
                  signature of these events. We use the method of
                  linearized Bregman. This algorithm focuses unknown
                  sources at their true locations by promoting
                  sparsity along space and at the same time keeping
                  the energy along time in check. We are particularly
                  interested in situations where the microseismic data
                  is noisy, sources have different signatures and we
                  only have access to the smooth background-velocity
                  model. We perform numerical experiments to
                  demonstrate the usability of the proposed method. We
                  also compare our results with full-waveform
                  inversion based microseismic event collocation
                  methods. Our method gives flexibility to
                  simultaneously get a more accurate source image
                  along with an estimate of the source-time function,
                  which carries important information on the rupturing
                  process and source mechanism.},
  keywords = {microseismic, sparse, sources, wave equation, noise, SEG},
  note = {(SEG, Dallas)},
  doi = {10.1190/segam2016-13871022.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/sharan2016SEGspj/sharan2016SEGspj.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/sharan2016SEGspj/sharan2016SEGspj_pres.pdf}
}


@CONFERENCE{wang2016SEGfde,
  author = {Rongrong Wang and Felix J. Herrmann},
  title = {Frequency down-extrapolation with {TV} norm minimization},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2016},
  month = {10},
  pages = {1380-1384},
  abstract = {In this work, we present a total-variation (TV)-norm
                  minimization method to perform model-free
                  low-frequency data extrapolation for the purpose of
                  assisting full-waveform inversion. Low-frequency
                  extrapolation is the process of extending reliable
                  frequency bands of the raw data towards the lower
                  end of the spectrum. To this end, we propose to
                  solve a TV-norm based convex optimization problem
                  that has a global minimum and is equipped with a
                  fast solver. The approach takes into account both
                  the sparsity of the reflectivity series associated
                  with a single trace, as well as the inter-trace
                  correlations. A favorable byproduct of this approach
                  is that it allows one to work with coarsely sampled
                  trace along time (as coarse as 0.02s per sample),
                  hence substantially reduces the size of the proposed
                  optimization problem. Last, we show the
                  effectiveness of the method for frequency-domain FWI
                  on the Marmousi model.},
  keywords = {extrapolation, sparse, FWI, SEG},
  note = {(SEG, Dallas)},
  doi = {10.1190/segam2016-13879674.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/wang2016SEGfde/wang2016SEGfde.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/wang2016SEGfde/wang2016SEGfde_pres.pdf}
}


@CONFERENCE{witte2016SEGpve,
  author = {Philipp A. Witte and Christiaan C. Stolk and Felix J. Herrmann},
  title = {Phase velocity error minimizing scheme for the anisotropic pure {P-wave} equation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2016},
  month = {10},
  pages = {452-457},
  abstract = {Pure P-wave equations for acoustic modeling in
                  transverse isotropic media are derived by
                  approximating the exact pure Pwave dispersion
                  relation. In this work, we present an alternative
                  approach to the approximate dispersion relation of
                  Etgen and Brandsberg-Dahl, in which we approximate
                  the exact dispersion relation through a polynomial
                  expansion and determine its coefficients by solving
                  a linear least squares problem that minimizes the
                  phase velocity error over the entire range of phase
                  angles. The coefficients are also optimized over a
                  pre-defined range of Thomsen parameters, so that the
                  phase error is small for models with spatially
                  varying anisotropy. Phase velocity error analysis
                  shows that the optimized pure P-wave equation is up
                  to one order of magnitude more accurate than other
                  popular pure P-wave equations, even for highly
                  non-elliptic anisotropy. The optimized equation can
                  be easily turned into a time-domain forward modeling
                  scheme and comparisons of the modeled waveforms with
                  analytical travel times once more illustrate its
                  high accuracy. We also provide an efficient
                  implementation of our approach for 3D tilted TI
                  media that limits the count of fast Fourier
                  transforms per time step to a number that is
                  comparable to other pure P-wave equations.},
  keywords = {anisotropy, modelling, TTI, least squares, SEG},
  note = {(SEG, Dallas)},
  doi = {10.1190/segam2016-13844850.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/witte2016SEGpve/witte2016SEGpve.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/witte2016SEGpve/witte2016SEGpve_poster.pdf}
}


@CONFERENCE{yang2016SEGtds,
  author = {Mengmeng Yang and Philipp A. Witte and Zhilong Fang and Felix J. Herrmann},
  title = {Time-domain sparsity-promoting least-squares migration with source estimation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2016},
  month = {10},
  pages = {4225-4229},
  abstract = {Traditional reverse-time migration (RTM) gives images
                  with wrong amplitudes and low
                  resolution. Least-squares RTM (LS-RTM) on the other
                  hand, is capable of obtaining true-amplitude images
                  as solutions of $\ell_2$-norm norm minimization
                  problems by fitting the synthetic and observed
                  reflection data. The shortcoming of this approach is
                  that solutions of these $\ell_2$ problems are
                  typically smoothed, tend to be overfitted, and
                  computationally too expensive because it requires
                  compared to standard RTM too many iterations. By
                  working with randomized subsets of data only, the
                  computational costs of LS-RTM can be brought down to
                  an acceptable level while producing artifact-free
                  high-resolution images without overfitting the
                  data. While initial results of these "compressive
                  imaging" methods were encouraging various open
                  issues remain including guaranteed convergence,
                  algorithmic complexity of the solver, and lack of
                  on-the-fly source estimation for LS-RTMs with
                  wave-equation solvers based on time-stepping. By
                  including on-the-fly source-time function estimation
                  into the method of Linearized Bregman (LB), on which
                  we reported before, we tackle all these issues
                  resulting in a easy-to-implement algorithm that
                  offers flexibility in the trade-off between the
                  number of iterations and the number of wave-equation
                  solves per iteration for a fixed total number of
                  wave-equation solves. Application of our algorithm
                  on a 2D synthetic shows that we are able to obtain
                  high-resolution images, including accurate estimates
                  of the wavelet, for a single pass through the
                  data. The produced image, which is by virtue of the
                  inversion deconvolved with respect to the wavelet,
                  is roughly of the same quality as the image obtained
                  given the correct source function.},
  keywords = {time domain, least squares, RTM, source estimation, SEG},
  note = {(SEG, Dallas)},
  doi = {10.1190/segam2016-13850609.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/yang2016SEGtds/yang2016SEGtds.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2016/yang2016SEGtds/yang2016SEGtds_pres.pdf}
}



%-----2015-----%

@CONFERENCE{bougher2015CSEGpsu,
  author = {Ben B. Bougher and Felix J. Herrmann},
  title = {Prediction of stratigraphic units from spectral co-occurance coefficients of well logs},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2015},
  month = {05},
  abstract = {Well logging is the process of making physical
                  measurements down bore holes in order to
                  characterize geological and structural
                  properties. Logs are visually interpreted and
                  correlated to classify regions that are similar in
                  structure, a process that can be modelled with
                  machine learning. This project applies supervised
                  learning methods to labelled well logs from the
                  Trenton Black River data set in order to classify
                  major stratigraphic units. Spectral co-occurance
                  coefficients were used for feature extraction, and a
                  k-nearest-neighbours approach was used for
                  classification. This novel approach was applied to
                  real field data in a high-impact domain, yielding
                  promising results for future research.},
  keywords = {CSEG, scattering transform, well logs, machine learning},
  note = {(CSEG, Calgary)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2015/bougher2015CSEGpsu/bougher2015CSEGpsu.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2015/bougher2015CSEGpsu/bougher2015CSEGpsu_pres.pdf}
}


@CONFERENCE{dasilva2015EAGEogt,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Off the grid tensor completion for seismic data interpolation},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2015},
  month = {06},
  abstract = {The practical realities of acquiring seismic data in a
                  realistic survey are often at odds with the
                  stringent requirements of Shannon-Nyquist-based
                  sampling theory. The unpredictable movement of the
                  ocean`s currents can be detrimental in acquiring
                  exactly equally-spaced samples while sampling at
                  Nyquist-rates are expensive, given the huge
                  dimensionality and size of the data volume. Recent
                  work in matrix and tensor completion for seismic
                  data interpolation aim to alleviate such stringent
                  Nyquist-based sampling requirements but are
                  fundamentally posed on a regularly-spaced grid. In
                  this work, we extend our previous results in using
                  the so-called Hierarchical Tucker (HT) tensor format
                  for recovering seismic data to the irregularly
                  sampled case. We introduce an interpolation operator
                  that resamples our tensor from a regular grid (in
                  which we impose our low-rank constraints) to our
                  irregular sampling grid. Our framework is very
                  flexible and efficient, depending primarily on the
                  computational costs of this operator. We demonstrate
                  the superiority of this approach on a realistic BG
                  data set compared to using low-rank tensor methods
                  that merely use binning.},
  keywords = {EAGE, hierarchical tucker, structured tensor, tensor interpolation, off the grid, irregular sampling},
  note = {(EAGE, Madrid)},
  doi = {10.3997/2214-4609.201412978},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/dasilva2015EAGEogt/dasilva2015EAGEogt.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/dasilva2015EAGEogt/dasilva2015EAGEogt_pres.pdf}
}


@CONFERENCE{dasilva2015WSLRigt,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Irregular grid tensor completion},
  booktitle = {Workshop on Low-rank Optimization and Applications},
  year = {2015},
  month = {06},
  abstract = {Low rank tensor completion has recently garnered the
                  attention of researchers owing to the ubiquity of
                  tensors in the sciences and the theoretical and
                  numerical challenges compared to matrix
                  completion. Here we consider a tensor completion
                  scheme where the data arises from sampling a
                  continuous function. When the sampling grid is not
                  periodic, the resulting tensor may not be low rank
                  in the Hierarchical Tucker sense, which can
                  adversely affect reconstruction quality when there
                  are missing samples. In order to compensate for this
                  off-grid sampling, we introduce a resampling
                  operator (here, the non-uniform Fourier transform)
                  that accounts for this non-uniformity moreso than
                  merely treating the sampling grid as
                  periodic. Numerical experiments demonstrate that
                  this approach can improve reconstruction quality
                  when there is relatively little data.},
  keywords = {hierarchical tucker, structured tensor, tensor interpolation, off the grid, irregular sampling},
  note = {(Workshop on Low-rank Optimization and Applications, University of Bonn, Germany)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/WS_LR/2015/dasilva2015WSLRigt/dasilva2015WSLRigt_poster.pdf}
}


@CONFERENCE{esser2015EAGElcs,
  author = {Ernie Esser and Tim T.Y. Lin and Rongrong Wang and Felix J. Herrmann},
  title = {A lifted $\ell_1/\ell_2$ constraint for sparse blind deconvolution},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2015},
  month = {06},
  abstract = {We propose a modification to a sparsity constraint based
                  on the ratio of $\ell_1$ and $\ell_2$ norms for
                  solving blind seismic deconvolution problems in
                  which the data consist of linear convolutions of
                  different sparse reflectivities with the same source
                  wavelet. We also extend the approach to the
                  Estimation of Primaries by Sparse Inversion (EPSI)
                  model, which includes surface related multiples.
                  Minimizing the ratio of $\ell_1$ and $\ell_2$ norms
                  has been previously shown to promote sparsity in a
                  variety of applications including blind
                  deconvolution. Most existing implementations are
                  heuristic or require smoothing the $\ell_1/\ell_2$
                  penalty. Lifted versions of $\ell_1/\ell_2$
                  constraints have also been proposed but are
                  challenging to implement. Inspired by the lifting
                  approach, we propose to split the sparse signals
                  into positive and negative components and apply an
                  $\ell_1/\ell_2$ constraint to the difference,
                  thereby obtaining a constraint that is easy to
                  implement without smoothing the $\ell_1$ or $\ell_2$
                  norms. We show that a method of multipliers
                  implementation of the resulting model can recover
                  source wavelets that are not necessarily minimum
                  phase and approximately reconstruct the sparse
                  reflectivities. Numerical experiments demonstrate
                  robustness to the initialization as well as to noise
                  in the data.},
  keywords = {EAGE, blind deconvolution, EPSI},
  note = {(EAGE, Madrid)},
  doi = {10.3997/2214-4609.201413420},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/esser2015EAGElcs/esser2015EAGElcs.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/esser2015EAGElcs/esser2015EAGElcs_poster.pdf}
}


@CONFERENCE{esser2015SEGasd,
  author = {Ernie Esser and Llu\'{i}s Guasch and Tristan van Leeuwen and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Automatic salt delineation --- {Wavefield} {Reconstruction} {Inversion} with convex constraints},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2015},
  month = {10},
  pages = {1337-1343},
  abstract = {We extend full-waveform inversion by Wavefield
                  Reconstruction Inversion by including convex
                  constraints on the model. Contrary to the
                  conventional adjoint-state formulations, Wavefield
                  Reconstruction Inversion has the advantage that the
                  Gauss-Newton Hessian is well approximated by a
                  diagonal scaling, which allows us to add convex
                  constraints, such as the box- and the
                  edge-preserving total-variation constraint, on the
                  square slowness without incurring significant
                  increases in computational costs. As the examples
                  demonstrate, including these constraints yields far
                  superior results in complex geological areas that
                  contain high-velocity high-contrast bodies
                  (e.g. salt or basalt). Without these convex
                  constraints, adjoint-state and Wavefield
                  Reconstruction Inversion get trapped in local minima
                  for poor starting models.},
  keywords = {SEG, full-waveform inversion, convex constraints, total-variation norm, salt},
  note = {(SEG, New Orleans)},
  doi = {10.1190/segam2015-5877995.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2015/esser2015SEGasd/esser2015SEGasd.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2015/esser2015SEGasd/esser2015SEGasd_pres.pdf}
}


@CONFERENCE{esser2015SEGWSasf,
  author = {Ernie Esser and Llu\'{i}s Guasch and Felix J. Herrmann},
  title = {Automatic salt flooding with hinged {FWI}},
  booktitle = {SEG Workshop on Subsalt model buiding long to short wavelengths; New Orleans},
  year = {2015},
  month = {10},
  keywords = {SEG, workshop, waveform inversion},
  note = {(SEG, New Orleans)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2015/esser2015SEGWSasf/esser2015SEGWSasf_pres.pdf}
}


@CONFERENCE{esser2015CAMSAPrsa,
  author = {Ernie Esser and Rongrong Wang and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Resolving scaling ambiguities with the $\ell_1/\ell_2$ norm in a blind deconvolution problem with feedback},
  booktitle = {IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing},
  year = {2015},
  month = {12},
  pages = {365-368},
  abstract = {Compared to more mundane blind deconvolution problems,
                  blind deconvolution in seismic applications involves
                  a feedback mechanism related to the free
                  surface. The presence of this feedback mechanism
                  gives us an unique opportunity to remove ambiguities
                  that have plagued blind deconvolution for a long
                  time. While beneficial, this feedback by itself is
                  insufficient to remove the ambiguities even with
                  $\ell_1$ constraints. However, when paired with an
                  $\ell_1/\ell_2$ constraint the feedback allows us to
                  resolve the scaling ambiguity under relatively mild
                  assumptions. Inspired by lifting approaches, we
                  propose to split the sparse signal into positive and
                  negative components and apply an $\ell_1/\ell_2$
                  constraint to the difference, thereby obtaining a
                  constraint that is easy to implement. Numerical
                  experiments demonstrate robustness to the
                  initialization as well as to noise in the data.},
  keywords = {CAMSAP, blind deconvolution, $\ell_1/\ell_2$, lifting, method of multipliers},
  note = {(IEEE CAMSAP Workshop, Canc\'{u}n, Mexico)},
  doi = {10.1109/CAMSAP.2015.7383812},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CAMSAP/2015/esser2015CAMSAPrsa/esser2015CAMSAPrsa.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/CAMSAP/2015/esser2015CAMSAPrsa/esser2015CAMSAPrsa_ext.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CAMSAP/2015/esser2015CAMSAPrsa/esser2015CAMSAPrsa_poster.pdf}
}


@CONFERENCE{fang2015EAGEsew,
  author = {Zhilong Fang and Felix J. Herrmann},
  title = {Source estimation for {Wavefield} {Reconstruction} {Inversion}},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2015},
  month = {06},
  abstract = {Wavefield reconstruction inversion is a new approach to
                  waveform based inversion that helps overcome the
                  `cycle skipping' problem. However, like most
                  waveform based inversion methods, wavefield
                  reconstruction inversion also requires good source
                  wavelets. Without correct source wavelets,
                  wavefields cannot be reconstructed correctly and the
                  velocity model cannot be updated correctly
                  neither. In this work, we propose a source
                  estimation method for wavefield reconstruction
                  inversion based on the variable projection method.
                  In this method, we reconstruct wavefields and
                  estimate source wavelets simultaneously by solving
                  an extended least-squares problem, which contains
                  source wavelets. This approach does not increase the
                  computational cost compared to conventional
                  wavefield reconstruction inversion. Numerical
                  results illustrates with our source estimation
                  method we are able to recover source wavelets and
                  obtain inversion results that are comparable to
                  results obtained with true source wavelets.},
  keywords = {EAGE, source estimation, WRI},
  note = {(EAGE, Madrid)},
  doi = {10.3997/2214-4609.201412588},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/fang2015EAGEsew/fang2015EAGEsew.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/fang2015EAGEsew/fang2015EAGEsew_pres.pdf}
}


@CONFERENCE{fang2015EAGEuqw,
  author = {Zhilong Fang and Chia Ying Lee and Curt Da Silva and Felix J. Herrmann and Rachel Kuske},
  title = {Uncertainty quantification for {Wavefield} {Reconstruction} {Inversion}},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2015},
  month = {06},
  abstract = {In this work, we propose a method to quantify the
                  uncertainty of wavefield reconstruction inversion
                  under the framework of Bayesian inference. Unlike
                  the conventional method using the wave equation as
                  the forward mapping, we involve the wave equation
                  misfit in the posterior distribution and propose a
                  new posterior distribution. The negative
                  log-likelihood of the new distribution is less
                  oscillatory than that of the conventional posterior
                  distribution, and its Gauss-Newton Hessian is a
                  diagonal matrix that can be generated without any
                  additional computational cost. We use the diagonal
                  Gauss-Newton Hessian to derive an approximate
                  Gaussian distribution at the maximum likelihood
                  point to quantify the uncertainty. This method makes
                  the uncertainty quantification for WRI
                  computationally tractable and is able to provide
                  reasonable uncertainty analysis based on our
                  numerical results.},
  keywords = {EAGE, UQ, WRI, FWI},
  note = {(EAGE, Madrid)},
  doi = {10.3997/2214-4609.201413198},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/fang2015EAGEuqw/fang2015EAGEuqw.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/fang2015EAGEuqw/fang2015EAGEuqw_pres.pdf}
}


@CONFERENCE{fang2015IIPFWIsoa,
  author = {Zhilong Fang and Tim Burgess and Felix J. Herrmann},
  title = {Stochastic optimization and its application to seismic inversion},
  booktitle = {Inaugural Full-Waveform Inversion Workshop},
  year = {2015},
  month = {08-09},
  keywords = {FWI, stochastic, optimization, inversion},
  note = {(Natal, Brazil)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IIPFWI/fang2015IIPFWIsoa/fang2015IIPFWIsoa_pres.pdf}
}


@CONFERENCE{fang2015IIPFWIuqw,
  author = {Zhilong Fang and Chia Ying Lee and Curt Da Silva and Felix J. Herrmann and Rachel Kuske},
  title = {Uncertainty quantification for {Wavefield} {Reconstruction} {Inversion}},
  booktitle = {Inaugural Full-Waveform Inversion Workshop},
  year = {2015},
  month = {08-09},
  abstract = {In this work, we propose a method to quantify the
                  uncertainty of wavefield reconstruction inversion
                  under the framework of Bayesian inference. Unlike
                  the conventional method using the wave equation as
                  the forward mapping, we involve the wave equation
                  misfit in the posterior distribution and propose a
                  new posterior distribution. The negative
                  log-likelihood of the new distribution is less
                  oscillatory than that of the conventional posterior
                  distribution, and its Gauss-Newton Hessian is a
                  diagonal matrix that can be generated without any
                  additional computational cost. We use the diagonal
                  Gauss-Newton Hessian to derive an approximate
                  Gaussian distribution at the maximum likelihood
                  point to quantify the uncertainty. This method makes
                  the uncertainty quantification for WRI
                  computationally tractable and is able to provide
                  reasonable uncertainty analysis based on our
                  numerical results.},
  keywords = {FWI, WRI, uncertainity},
  note = {(Natal, Brazil)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IIPFWI/fang2015IIPFWIuqw/fang2015IIPFWIuqw_pres.pdf}
}


@CONFERENCE{fang2015IIPFWIwri,
  author = {Zhilong Fang and Felix J. Herrmann},
  title = {Wavefield {Reconstruction} {Inversion} with source estimation and its application to 2014 {Chevron} synthetic blind test dataset},
  booktitle = {Inaugural Full-Waveform Inversion Workshop},
  year = {2015},
  month = {08-09},
  abstract = {We present a robust wavefield reconstruction inversion
                  with source estimation. The source wavelet is
                  estimated with the reconstruction of wavefield
                  simultaneously by solving an extended data
                  augmentation problem. We apply this method to the
                  2014 Chevron synthetic blind test dataset and show
                  the robustness of our method.},
  keywords = {FWI, WRI, source estimation},
  note = {(Natal, Brazil)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IIPFWI/fang2015IIPFWIwri/fang2015IIPFWIwri_pres.pdf}
}


@CONFERENCE{herrmann2015ASEGrae,
  author = {Felix J. Herrmann},
  title = {Randomized algorithms in exploration seismology},
  booktitle = {ASEG Annual Conference Proceedings},
  year = {2015},
  month = {02},
  abstract = {As in several other industries, progress in exploration
                  seismology relies on the collection and processing
                  of massive data volumes that grow exponentially in
                  size as the survey area and desired resolution
                  increase. This exponential growth --- in combination
                  with the increased complexity of the next-generation
                  of iterative wave-equation based inversion
                  algorithms --- puts strains on our acquisition
                  systems and computational back ends impeding
                  progress in our field. During this talk, I will
                  review how recent randomized algorithms from
                  Compressive Sensing and Machine learning can be used
                  to overcome some of these challenges by
                  fundamentally rethinking how we sample and process
                  seismic data. The key idea here is to reduce
                  acquisition and computational costs by deliberately
                  working on small randomized subsets of the data at a
                  desired accuracy. I will illustrate these concepts
                  using a variety of compelling examples on realistic
                  synthetics and field data.},
  keywords = {ASEG, compressed sensing, seismology, acquisition, imaging, FWI},
  note = {(ASEG, Perth)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ASEG/2015/herrmann2015ASEGrae/herrmann2015ASEGrae.pdf}
}


@CONFERENCE{herrmann2015EAGEfom,
  author = {Felix J. Herrmann and Ning Tu and Ernie Esser},
  title = {Fast "online" migration with {Compressive} {Sensing}},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2015},
  month = {06},
  abstract = {We present a novel adaptation of a recently developed
                  relatively simple iterative algorithm to solve
                  large-scale sparsity-promoting optimization
                  problems. Our algorithm is particularly suitable to
                  large-scale geophysical inversion problems, such as
                  sparse least-squares reverse-time migration or
                  Kirchoff migration since it allows for a tradeoff
                  between parallel computations, memory allocation,
                  and turnaround times, by working on subsets of the
                  data with different sizes. Comparison of the
                  proposed method for sparse least-squares imaging
                  shows a performance that rivals and even exceeds the
                  performance of state-of-the art one-norm solvers
                  that are able to carry out least-squares migration
                  at the cost of a single migration with all data.},
  keywords = {EAGE, LSRTM},
  note = {(EAGE, Madrid)},
  doi = {10.3997/2214-4609.201412942},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/herrmann2015EAGEfom/herrmann2015EAGEfom.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/herrmann2015EAGEfom/herrmann2015EAGEfom_pres.pdf}
}


@CONFERENCE{herrmann2015IIPFWIras,
  author = {Felix J. Herrmann},
  title = {Randomized algorithms in seismic imaging},
  booktitle = {Inaugural Full-Waveform Inversion Workshop},
  year = {2015},
  month = {08-09},
  abstract = {As in several other industries, progress in exploration
                  seismology relies on the collection and processing
                  of massive data volumes that grow exponentially in
                  size as the survey area and desired resolution
                  increase. This exponential growth – in combination
                  with the increased complexity of the next-generation
                  of iterative wave-equation based inversion
                  algorithms – puts strains on our acquisition systems
                  and computational back ends impeding progress in our
                  field. During this talk, I will review how recent
                  randomized algorithms from Compressive Sensing and
                  Machine learning can be used to overcome some of
                  these challenges by fundamentally rethinking how we
                  sample and process seismic data. The key idea here
                  is to reduce acquisition and computational costs by
                  deliberately working on small randomized subsets of
                  the data at a desired accuracy. I will illustrate
                  these concepts using a variety of compelling
                  examples on realistic synthetics and field data.},
  keywords = {FWI, randomization, seismic, imaging},
  note = {(Natal, Brazil)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IIPFWI/herrmann2015IIPFWIras/herrmann2015IIPFWIras_pres.pdf}
}


@CONFERENCE{herrmann2015IIPFWIwri,
  author = {Felix J. Herrmann},
  title = {Wavefield-{Reconstruction} {Inversion} — {WRI}},
  booktitle = {Inaugural Full-Waveform Inversion Workshop},
  year = {2015},
  month = {08-09},
  abstract = {We discuss a recently proposed novel method for waveform
                  inversion: Wavefield Reconstruction Inversion
                  (WRI). As opposed to conventional FWI — which
                  attempts to minimize the error between observed and
                  predicted data obtained by solving a wave equation –
                  WRI reconstructs a wave-field from the data and
                  extracts a model-update from this wavefield by
                  minimizing the wave-equation residual. The method
                  does not require explicit computation of an adjoint
                  wavefield as all the necessary information is
                  contained in the reconstructed wavefield. We show
                  how the corresponding model updates can be
                  interpreted physically analogously to the
                  conventional imaging-condition-based approach.},
  keywords = {FWI, WRI, inversion},
  note = {(Natal, Brazil)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IIPFWI/herrmann2015IIPFWIwri/herrmann2015IIPFWIwri_pres.pdf}
}


@CONFERENCE{herrmann2015IIPFWIwcc,
  author = {Felix J. Herrmann and Ernie Esser and Llu\'{i}s Guasch},
  title = {Wavefield {Reconstruction} {Inversion} with convex constraints},
  booktitle = {Inaugural Full-Waveform Inversion Workshop},
  year = {2015},
  month = {08-09},
  abstract = {During this talk, we discuss how to exploit the special
                  structure of Wavefield Reconstruction Inversion
                  (WRI) to include convex bound and total-variation
                  constraints in a computationally feasible
                  manner. The resulting method shows promising results
                  on challenging models that include high-velocity
                  high-contrast inclusions such as salt or
                  basalt. This is joint work with Ernie Esser who
                  passed away and who is dearly missed.},
  keywords = {FWI, regularization, convex},
  note = {(Natal, Brazil)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IIPFWI/herrmann2015IIPFWIwcc/herrmann2015IIPFWIwcc_pres.pdf}
}


@CONFERENCE{herrmann2015SIAMpcf,
  author = {Felix J. Herrmann and Bas Peters},
  title = {Pros and cons of full- and reduced-space methods for {Wavefield} {Reconstruction} {Inversion}},
  booktitle = {SIAM Conference on Mathematical and Computational Issues in the Geosciences},
  year = {2015},
  month = {06-07},
  abstract = {By insisting on fitting observed data, Wavefield
                  Reconstruction Inversion (WRI) is no longer cycle
                  skipped and therefore less reliant on the accuracy
                  of starting models. While extending the search space
                  mitigates local minima, there are challenges scaling
                  to 3D seismic when using reduced-space methods that
                  require accurate solves. Conversely, full-space
                  methods allow for inaccurate solves but require
                  storage of all wavefields. We weigh pros and cons of
                  these two approaches in the seismic context.},
  keywords = {SIAM, WRI, FWI},
  note = {(SIAM Conference on Mathematical and Computational Issues in the Geosciences, Stanford University, California)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2015/herrmann2015SIAMpcf/herrmann2015SIAMpcf_pres.pdf}
}


@CONFERENCE{herrmann2015SIAMwri,
  author = {Felix J. Herrmann},
  title = {Wavefield {Reconstruction} {Inversion} – reaping the benefits from extending the search space},
  booktitle = {SIAM Conference on Mathematical and Computational Issues in the Geosciences},
  year = {2015},
  month = {06-07},
  keywords = {SIAM, WRI},
  note = {(SIAM Conference on Mathematical and Computational Issues in the Geosciences, Stanford University, California)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2015/herrmann2015SIAMwri/herrmann2015SIAMwri_pres.pdf}
}


@CONFERENCE{herrmann2015SEGWSasf,
  author = {Felix J. Herrmann},
  title = {Automatic salt flooding with constrained wavefield reconstruction inversion},
  booktitle = {SEG Summer Research Workshop on FWI Applications from Imaging to Reservoir Characterization; Houston},
  year = {2015},
  month = {07},
  keywords = {SEG, workshop, wavefield reconstruction inversion},
  note = {(SEG Summer Workshop, Houston)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2015/herrmann2015SEGWSasf/herrmann2015SEGWSasf_pres.pdf}
}


@CONFERENCE{herrmann2015SEGWScse,
  author = {Felix J. Herrmann},
  title = {Compressive {Sensing} in {Exploration} {Seismology} - where we came from, where we are now, and where we need to go},
  booktitle = {SEG Geophysical Compressed Sensing Workshop; Beijing, China},
  year = {2015},
  month = {12},
  keywords = {SEG, workshop, compressed sensing},
  note = {(SEG Workshop, Beijing, China)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2015/herrmann2015SEGWScse/herrmann2015SEGWScse_pres.pdf}
}


@CONFERENCE{kumar2015EAGEmcu,
  author = {Rajiv Kumar and Oscar Lopez and Ernie Esser and Felix J. Herrmann},
  title = {Matrix completion on unstructured grids : {2-D} seismic data regularization and interpolation},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2015},
  month = {06},
  abstract = {Seismic data interpolation via rank-minimization
                  techniques has been recently introduced in the
                  seismic community. All the existing
                  rank-minimization techniques assume the recording
                  locations to be on a regular grid, e.g., sampled
                  periodically, but seismic data are typically
                  irregularly sampled along spatial axes. Other than
                  the irregularity of the sampled grid, we often have
                  missing data. In this paper, we study the effect of
                  grid irregularity to conduct matrix completion on a
                  regular grid for unstructured data. We propose an
                  improvement of existing rank-minimization techniques
                  to do regularization. We also demonstrate that we
                  can perform seismic data regularization and
                  interpolation simultaneously. We illustrate the
                  advantages of the modification using a real seismic
                  line from the Gulf of Suez to obtain high quality
                  results for regularization and interpolation, a key
                  application in exploration geophysics.},
  keywords = {EAGE, regularization, interpolation, matrix completion, NFFT},
  note = {(EAGE, Madrid)},
  doi = {10.3997/2214-4609.201413448},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/kumar2015EAGEmcu/kumar2015EAGEmcu.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/kumar2015EAGEmcu/kumar2015EAGEmcu_poster.pdf}
}


@CONFERENCE{kumar2015CSEGlse,
  author = {Rajiv Kumar and Ning Tu and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Least-squares extended imaging with surface-related multiples},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2015},
  month = {05},
  abstract = {Common image gathers are used in building velocity
                  models, inverting anisotropy parameters, and
                  analyzing reservoir attributes. Often primary
                  reflections are used to form image gathers and
                  multiples are typically attenuated in processing to
                  remove strong coherent artifacts generated by
                  multiples that interfere with the imaged
                  reflectors. However, researchers have shown that, if
                  correctly used, multiples can actually provide extra
                  illumination of the subsurface in seismic imaging,
                  especially for delineating the near-surface
                  features. In this work, we borrow ideas from
                  literatures on imaging with surface-related
                  multiples, and apply these ideas to extended
                  imaging. This way we save the massive computation
                  cost in separating multiples from the data before
                  using them during the formation of image
                  gathers. Also, we mitigate the strong coherent
                  artifacts generated by multiples which can send the
                  migration velocity analysis type algorithms in wrong
                  direction. Synthetic examples on a three-layer model
                  show the efficacy of the proposed formulation.},
  keywords = {CSEG, image-gather, surface-related multiples},
  note = {(CSEG, Calgary)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2015/kumar2015CSEGlse/kumar2015CSEGlse.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2015/kumar2015CSEGlse/kumar2015CSEGlse_pres.pdf}
}


@CONFERENCE{kumar2015IIPFWIrmb,
  author = {Rajiv Kumar and Curt Da Silva and Oscar Lopez and Aleksandr Y. Aravkin and Hassan Mansour and Haneet Wason and Ernie Esser and Felix J. Herrmann},
  title = {Rank minimization based seismic data processing and inversion},
  booktitle = {Inaugural Full-Waveform Inversion Workshop},
  year = {2015},
  month = {08-09},
  abstract = {We present the low-rank extensions of full-waveform
                  inversion to mitigate the local-minima issues. The
                  purpose formulation is computationally tractable and
                  does not involve any extra computational cost
                  compared to the adjoint-state method.},
  keywords = {FWI, rank minimization, processing, inversion},
  note = {(Natal, Brazil)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IIPFWI/kumar2015IIPFWIrmb/kumar2015IIPFWIrmb_pres.pdf}
}


@CONFERENCE{lin2015IIPFWIsws,
  author = {Tim T.Y. Lin and Curt Da Silva and Felix J. Herrmann},
  title = {Software and workflows at {SLIM}/{SINBAD}},
  booktitle = {Inaugural Full-Waveform Inversion Workshop},
  year = {2015},
  month = {08-09},
  keywords = {FWI, SLIM, software},
  note = {(Natal, Brazil)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IIPFWI/lin2015IIPFWIsws/lin2015IIPFWIsws_pres.pdf}
}


@CONFERENCE{lin2015IIPFWIsdh,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {The student-driven {HPC} environment at {SLIM}},
  booktitle = {Inaugural Full-Waveform Inversion Workshop},
  year = {2015},
  month = {08-09},
  abstract = {A major role of academic environments is to provide
                  learning experiences for students to critically
                  analyze and develop methods, both at a high-level of
                  mathematical rigour, and at a low-enough level of
                  implementation in order to yield experimental
                  results on real datasets. Often these two goals are
                  in conflict with each other, in terms of both
                  learning time and attention. At SLIM, we strive to
                  strike a balance between the two by abstracting away
                  many of the low-level aspects of distributed HPC
                  under a framework that matches syntactically with
                  the mathematics of our field, while exposing enough
                  control parameters for tuning performance
                  characteristics. This talk will touch on the history
                  of our efforts at SLIM, and culminate in an overview
                  of our current method of interacting with the
                  in-house compute cluster.},
  keywords = {FWI, HPC},
  note = {(Natal, Brazil)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IIPFWI/lin2015IIPFWIsdh/lin2015IIPFWIsdh_pres.pdf}
}


@CONFERENCE{lopez2015EAGErma,
  author = {Oscar Lopez and Rajiv Kumar and Felix J. Herrmann},
  title = {Rank minimization via alternating optimization: seismic data interpolation},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2015},
  month = {06},
  abstract = {Low-rank matrix completion techniques have recently
                  become an effective tool for seismic trace
                  interpolation problems. In this talk, we consider an
                  alternating optimization scheme for nuclear norm
                  minimization and discuss the applications to large
                  scale wave field reconstruction. By adopting a
                  factorization approach to the rank minimization
                  problem we write our low-rank matrix in bi-linear
                  form, and modify this workflow by alternating our
                  optimization to handle a single matrix factor at a
                  time. This allows for a more tractable procedure
                  that can robustly handle large scale, highly
                  oscillatory and critically subsampled seismic data
                  sets. We demonstrate the potential of this approach
                  with several numerical experiments on a seismic line
                  from the Nelson 2D data set and a frequency slice
                  from the Gulf of Mexico data set.},
  keywords = {EAGE, matrix completion, low-rank, interpolation},
  note = {(EAGE, Madrid)},
  doi = {10.3997/2214-4609.201413453},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/lopez2015EAGErma/lopez2015EAGErma.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/lopez2015EAGErma/lopez2015EAGErma_poster.pdf}
}


@CONFERENCE{lopez2015SEGWSsfm,
  author = {Oscar Lopez and Haneet Wason and Curt Da Silva and Rajiv Kumar and Felix J. Herrmann},
  title = {{SVD}-free matrix completion in randomized marine acquisition},
  booktitle = {SEG Workshop on Rank-Reduction and Other Sparse Transform Methods with Application to Data Reconstruction, De-Noising, De-Blending and Imaging; New Orleans},
  year = {2015},
  month = {10},
  abstract = {During this presentation, we will illustrate how
                  insights from matrix-free matrix completion can be
                  used to solve problems in (on- and off-the-grid)
                  missing trace interpolation and time-jittered
                  (simultaneous) marine acquisition. In particular, we
                  will demonstrate the importance of data organization
                  to favor recovery by rank minimization and the use
                  of deterministic spectral properties to predict the
                  recovery. We also make make comparisons between
                  wavefield reconstructions based on transform-domain
                  sparsity promotion and matrix completion.},
  keywords = {SEG, workshop, matrix completion, rank reduction, interpolation, acquisition},
  note = {(SEG, New Orleans)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2015/lopez2015SEGWSsfm/lopez2015SEGWSsfm_pres.pdf}
}


@CONFERENCE{louboutin2015IIPFWIrwi,
  author = {Mathias Louboutin and Bas Peters and Brendan R. Smithyman and Felix J. Herrmann},
  title = {Regularizing waveform inversion by projections onto convex sets},
  booktitle = {Inaugural Full-Waveform Inversion Workshop},
  year = {2015},
  month = {08-09},
  abstract = {A framework is proposed for regularizing the waveform
                  inversion problem by projections onto intersections
                  of convex sets. Multiple pieces of prior information
                  about the geology are represented by multiple convex
                  sets, for example limits on the velocity or minimum
                  smoothness conditions on the model. The data-misfit
                  is then minimized, such that the estimated model is
                  always in the intersection of the convex
                  sets. Therefore, it is clear what properties the
                  estimated model will have at each iteration. This
                  approach does not require any quadratic penalties to
                  be used and thus avoids the known problems and
                  limitations of those types of penalties. It is shown
                  that by formulating waveform inversion as a
                  constrained problem, regularization ideas such as
                  Tikhonov regularization and gradient filtering can
                  be incorporated into one framework. The algorithm is
                  generally applicable, in the sense that it works
                  with any (differentiable) objective function and
                  does not require significant additional
                  computation. The method is demonstrated on the
                  inversion of the 2D marine isotropic elastic
                  synthetic seismic benchmark by Chevron using an
                  acoustic modeling code. To highlight the effect of
                  the projections, we apply no data
                  pre-processing. This is joint work with Bas Peters.},
  keywords = {FWI, regularization, convex},
  note = {(Natal, Brazil)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IIPFWI/louboutin2015IIPFWIrwi/louboutin2015IIPFWIrwi_pres.pdf}
}


@CONFERENCE{louboutin2015SEGtcs,
  author = {Mathias Louboutin and Felix J. Herrmann},
  title = {Time compressively sampled full-waveform inversion with stochastic optimization},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2015},
  month = {10},
  pages = {5153-5157},
  abstract = {Time-domain Full-Waveform Inversion (FWI) aims to image
                  the subsurface of the earth accurately from field
                  recorded data and can be solved via the reduced
                  adjoint-state method. However, this method requires
                  access to the forward and adjoint wavefields that
                  are meet when computing gradient updates. The
                  challenge here is that the adjoint wavefield is
                  computed in reverse order during time stepping and
                  therefore requires storage or other type of
                  mitigation because storing the full time history of
                  the forward wavefield is too expensive in realistic
                  3D settings. To overcome this challenge, we propose
                  an approximate adjoint-state method where the
                  wavefields are subsampled randomly, which
                  drastically the amount of storage needed. By using
                  techniques from stochastic optimization, we control
                  the errors induced by the subsampling. Examples of
                  the proposed technique on a synthetic but realistic
                  2D model show that the subsampling-related artifacts
                  can be reduced significantly by changing the
                  sampling for each source after each model
                  update. Combination of this gradient approximation
                  with a quasi-Newton method shows virtually artifact
                  free inversion results requiring only 5% of storage
                  compared to saving the history at Nyquist. In
                  addition, we avoid having to recompute the
                  wavefields as is required by checkpointing.},
  keywords = {SEG, Full-waveform inversion, Acoustic, Subsampling, Time-domain, Inversion, Stochastic optimization},
  note = {(SEG, New Orleans)},
  doi = {10.1190/segam2015-5924937.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2015/louboutin2015SEGtcs/louboutin2015SEGtcs.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2015/louboutin2015SEGtcs/louboutin2015SEGtcs_poster.pdf}
}


@CONFERENCE{oghenekohwo2015EAGEuci,
  author = {Felix Oghenekohwo and Rajiv Kumar and Ernie Esser and Felix J. Herrmann},
  title = {Using common information in compressive time-lapse full-waveform inversion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2015},
  month = {06},
  abstract = {The use of time-lapse seismic data to monitor changes in
                  the subsurface has become standard practice in
                  industry. In addition, full-waveform inversion has
                  also been extended to time-lapse seismic to obtain
                  useful time-lapse information. The computational
                  cost of this method are becoming more pronounced as
                  the volume of data increases. Therefore, it is
                  necessary to develop fast inversion algorithms that
                  can also give improved time-lapse results. Rather
                  than following existing joint inversion algorithms,
                  we are motivated by a joint recovery model which
                  exploits the common information among the baseline
                  and monitor data. We propose a joint inversion
                  framework, leveraging ideas from distributed
                  compressive sensing and the modified Gauss-Newton
                  method for full-waveform inversion, by using the
                  shared information in the time-lapse data. Our
                  results on a realistic synthetic example highlight
                  the benefits of our joint inversion approach over a
                  parallel inversion method that does not exploit the
                  shared information. Preliminary results also
                  indicate that our formulation can address time-lapse
                  data with inconsistent acquisition geometries.},
  keywords = {EAGE, time-lapse, FWI},
  note = {(EAGE, Madrid)},
  doi = {10.3997/2214-4609.201413086},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/oghenekohwo2015EAGEuci/oghenekohwo2015EAGEuci.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/oghenekohwo2015EAGEuci/oghenekohwo2015EAGEuci_poster.pdf}
}


@CONFERENCE{oghenekohwo2015IIPFWItlf,
  author = {Felix Oghenekohwo and Rajiv Kumar and Ernie Esser and Felix J. Herrmann},
  title = {Time-lapse {FWI} with distributed compressed sensing},
  booktitle = {Inaugural Full-Waveform Inversion Workshop},
  year = {2015},
  month = {08-09},
  abstract = {In this talk, I will discuss our most recent application
                  of full-waveform inversion (FWI) to time-lapse
                  seismic data inversion. Specifically, I will
                  illustrate how a joint recovery model (JRM) can be
                  used in a joint inversion framework, leveraging
                  ideas from distributed compressed sensing (DCS). The
                  key idea involves exploiting the common information
                  in the baseline and monitor gradients during the
                  inversion.},
  keywords = {FWI, time lapse, distributed compressed sensing},
  note = {(Natal, Brazil)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IIPFWI/oghenekohwo2015IIPFWItlf/oghenekohwo2015IIPFWItlf_pres.pdf}
}


@CONFERENCE{oghenekohwo2015PIMSntc,
  author = {Felix Oghenekohwo and Felix J. Herrmann},
  title = {A new take on compressive time-lapse seismic acquisition, imaging and inversion},
  booktitle = {PIMS Workshop on Advances in Seismic Imaging and Inversion},
  year = {2015},
  month = {05},
  abstract = {Compressive sensing (CS), a sampling paradigm that is
                  changing how data is acquired and processed in many
                  fields, has attracted tremendous attention in recent
                  years. In exploration seismology, particularly in 3D
                  seismic technology, there have been published work
                  on the application of CS to seismic data
                  acquisition, processing and inversion. However, very
                  little is known about the implications of CS in
                  time-lapse (4D) seismic. Leveraging ideas from
                  distributed CS where more than one signal is
                  acquired and processed, as in 4D seismic, we present
                  a joint recovery algorithm for time-lapse seismic
                  data acquisition, imaging and inversion. The
                  algorithm exploits the common information in the
                  data sets (baseline and monitor) as part of an
                  inversion procedure. Application of our algorithm to
                  realistic synthetic examples highlight its
                  advantages over other conventional approaches for
                  processing time-lapse seismic data.},
  keywords = {PIMS, workshop, time-lapse},
  note = {(PIMS Workshop on Advances in Seismic Imaging and Inversion, University of Alberta, Edmonton)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/PIMS/2015/oghenekohwo2015PIMSntc/oghenekohwo2015PIMSntc_pres.pdf}
}


@CONFERENCE{oghenekohwo2015CSEGctl,
  author = {Felix Oghenekohwo and Felix J. Herrmann},
  title = {Compressive time-lapse seismic data processing using shared information},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2015},
  month = {05},
  abstract = {Time-lapse images void of acquisition and processing
                  artifacts can provide more useful information about
                  subsurface changes compared to those with
                  acquisition footprints and other unwanted
                  anomalies. Although, several pre-processing
                  techniques are being developed and used to mitigate
                  these unwanted artifacts, these operations can be
                  very expensive, challenging and data
                  dependent. Migration, as a processing tool, using a
                  sparsity constraint has been shown to reduce
                  artifacts drastically but little is known about the
                  significance for compressed time-lapse seismic
                  data. Leveraging ideas from distributed compressed
                  sensing, and motivated by our earlier work on
                  recovery of densely sampled time-lapse data from
                  compressively sampled measurements, we present a
                  sparsity-constrained migration for time-lapse data
                  that uses a common component shared by the baseline
                  and monitor data. Our algorithm tested on a
                  synthetic example highlights the advantages of
                  exploiting the common information, compared to ad
                  hoc methods that involve parallel processing of the
                  time-lapse data before differencing.},
  keywords = {CSEG, time-lapse},
  note = {(CSEG, Calgary)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2015/oghenekohwo2015CSEGctl/oghenekohwo2015CSEGctl.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2015/oghenekohwo2015CSEGctl/oghenekohwo2015CSEGctl_pres.pdf}
}


@CONFERENCE{peters2015AIPwri,
  author = {Bas Peters and Felix J. Herrmann},
  title = {Wavefield-reconstruction inversion},
  booktitle = {Conference on Applied Inverse Problems},
  year = {2015},
  month = {05},
  abstract = {Wavefield Reconstruction Inversion is a method for
                  PDE-constrained optimization, which revolves around
                  the estimation of fields using the PDE as well as
                  the observed data in a least-squares sense. The
                  method is quadratic penalty based, which offers some
                  interesting possibilities for the construction of
                  algorithms, compared to the Lagrangian form. One of
                  the main benefits of the method is when the initial
                  guess is far from the global minimizer.
                  Reduced-space and full-space algorithms are
                  discussed, including illustrative examples. The
                  method was developed with seismic applications in
                  mind, but applies to other PDE-constrained
                  optimization problems as well.},
  keywords = {AIP, Waveform reconstruction inversion, penalty method, optimization, full-waveform inversion},
  note = {(AIP, Helsinki)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/AIP/2015/peters2015AIPwri/peters2015AIPwri.pdf}
}


@CONFERENCE{peters2015PRECONasl,
  author = {Bas Peters and Chen Greif and Felix J. Herrmann},
  title = {An algorithm for solving least-squares problems with a {Helmholtz} block and multiple right-hand-sides},
  booktitle = {International Conference On Preconditioning Techniques For Scientific And Industrial Applications},
  year = {2015},
  month = {06},
  keywords = {least-squares, matrix-free, CGMN, randomized linear algebra, Helmholtz},
  note = {(PRECON, The Netherlands)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/PRECON/2015/peters2015PRECONasl/peters2015PRECONasl_pres.pdf}
}


@CONFERENCE{peters2015SEGWSsew,
  author = {Bas Peters and Felix J. Herrmann},
  title = {Simultaneous estimation of wavefields and medium parameters - reduced-space versus full-space waveform inversion},
  booktitle = {SEG Workshop on The Limit of FWI in Subsurface Parameter Recovery; New Orleans},
  year = {2015},
  month = {10},
  keywords = {SEG, workshop, waveform inversion, full-space, PDE-constrained optimization, quadratic penalty method},
  note = {(SEG, New Orleans)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2015/peters2015SEGWSsew/peters2015SEGWSsew_pres.pdf}
}


@CONFERENCE{peters2015SIAMmfq,
  author = {Bas Peters and Felix J. Herrmann and Chen Greif},
  title = {Matrix-free quadratic-penalty methods for {PDE}-constrained optimization},
  booktitle = {SIAM Conference on Computational Science and Engineering},
  year = {2015},
  month = {03},
  abstract = {The large scale of seismic waveform inversion makes
                  matrix free implementations essential. We show how
                  to exploit the quadratic penalty structure to
                  construct matrix free reduced-space and full-space
                  algorithms, which have some advantages over the
                  commonly used Lagrangian based methods for
                  PDE-constrained optimization. This includes the
                  construction of effective and sparse Hessian
                  approximations and reduced sensitivity to the
                  initial guess. A computational bottleneck is the
                  need to solve a large least squares problem with a
                  PDE block. When direct solvers are not available, we
                  propose a fast matrix free iterative approach with
                  reasonable memory requirements. It takes advantage
                  of the structure of the least squares problem with a
                  combination of preconditioning, low rank
                  decomposition and deflation.},
  keywords = {quadratic-penalty, least-squares, low-rank, waveform inversion, PDE-constrained optimization},
  note = {(SIAM, Salt Lake City)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2015/peters2015SIAMmfq/peters2015SIAMmfq.pdf}
}


@CONFERENCE{smithyman2015EAGEcwi,
  author = {Brendan R. Smithyman and Bas Peters and Felix J. Herrmann},
  title = {Constrained waveform inversion of colocated {VSP} and surface seismic data},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2015},
  month = {06},
  abstract = {Constrained Full-Waveform Inversion (FWI) is applied to
                  produce a high-resolution velocity model from both
                  Vertical Seismic Profiling (VSP) and surface seismic
                  data. The case study comes from the Permian Basin in
                  Texas, USA. This dataset motivates and tests several
                  new developments in methodology that enable recovery
                  of model results that sit within multiple a priori
                  constraint sets. These constraints are imposed
                  through a Projected Quasi-Newton (PQN) approach,
                  wherein the projection set is the intersection of
                  physical property bounds and anisotropic wavenumber
                  filtering. This enables the method to recover
                  geologically-reasonable models while preserving the
                  fast model convergence offered by a quasi-Newton
                  optimization scheme like l-BFGS. In the Permian
                  Basin example, low-frequency data from both arrays
                  are inverted together and regularized by this
                  projection approach. Careful choice of the
                  constraint sets is possible without requiring
                  tradeoff parameters as in a quadratic penalty
                  approach to regularization. Multiple 2D FWI results
                  are combined to produce an interpolated 3D model
                  that is consistent with the models from migration
                  velocity analysis and VSP processing, while offering
                  improved resolution and illumination of features
                  from both datasets.},
  keywords = {EAGE, waveform inversion, VSP},
  note = {(EAGE, Madrid)},
  doi = {10.3997/2214-4609.201412906},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/smithyman2015EAGEcwi/smithyman2015EAGEcwi.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/smithyman2015EAGEcwi/smithyman2015EAGEcwi_pres.pdf}
}


@CONFERENCE{tu2015PIMSsls,
  author = {Ning Tu and Felix J. Herrmann},
  title = {Sparse least-squares seismic imaging with source estimation utilizing multiples},
  booktitle = {PIMS Workshop on Advances in Seismic Imaging and Inversion},
  year = {2015},
  month = {05},
  abstract = {We present an least-squares seismic imaging method that
                  (1) is efficient by subsampling the monochromatic
                  source experiments; (2) does not require the prior
                  knowledge of the source wavelet, by inverting for
                  the seismic image and the source wavelet
                  simultaneously using variable projection; and (3)
                  makes active use of surface-related multiples to
                  increase the illumination and to resolve the scaling
                  ambiguity in source estimation, by tightly
                  integrating the well-established
                  surface-related-multiple-prediction into the
                  wave-equation based modelling. As a result, we are
                  able to obtain ambiguity-resolved, high-fidelity
                  least-squares inverted seismic images with
                  computational costs that are comparable to
                  conventional reverse-time migrations with all the
                  data.  We demonstrate the efficacy of the proposed
                  method using synthetic examples.},
  keywords = {PIMS, workshop, least-squares migration, source estimation, multiples, inversion, seismic imaging},
  note = {(PIMS Workshop on Advances in Seismic Imaging and Inversion, University of Alberta, Edmonton)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/PIMS/2015/tu2015PIMSsls/tu2015PIMSsls_pres.pdf}
}


@CONFERENCE{vanleeuwen2015EAGEafs,
  author = {Tristan van Leeuwen and Rajiv Kumar and Felix J. Herrmann},
  title = {Affordable full subsurface image volume---an application to {WEMVA}},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2015},
  month = {06},
  abstract = {Common image gathers are used in building velocity
                  models, inverting for anisotropy parameters, and
                  analyzing reservoir attributes. In this paper, we
                  offer a new perspective on image gathers, where we
                  glean information from the image volume via
                  efficient matrix-vector products. The proposed
                  formulation make the computation of full subsurface
                  image volume feasible. We illustrate how this
                  matrix-vector product can be used to construct
                  objective functions for automatic MVA.},
  keywords = {EAGE, MVA, wave-equation, randomized trace estimation},
  note = {(EAGE Workshop on Wave Equation based Migration Velocity Analysis, Madrid)},
  doi = {10.3997/2214-4609.201413498},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/vanleeuwen2015EAGEafs/vanleeuwen2015EAGEafs.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/vanleeuwen2015EAGEafs/vanleeuwen2015EAGEafs_pres.pdf}
}


@CONFERENCE{wang2015SIAMwds,
  author = {Rongrong Wang and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Wavefield-denoising and source encoding},
  booktitle = {SIAM Conference on Mathematical and Computational Issues in the Geosciences},
  year = {2015},
  month = {06-07},
  keywords = {SIAM, WRI, source encoding},
  note = {(SIAM Conference on Mathematical and Computational Issues in the Geosciences, Stanford University, California)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2015/wang2015SIAMwds/wang2015SIAMwds_pres.pdf}
}


@CONFERENCE{wason2015EAGEcsm,
  author = {Haneet Wason and Felix Oghenekohwo and Felix J. Herrmann},
  title = {Compressed sensing in {4-D marine}---recovery of dense time-lapse data from subsampled data without repetition},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2015},
  month = {06},
  abstract = {We present an extension of our time-jittered marine
                  acquisition for time-lapse surveys by working on
                  more realistic field acquisition scenarios by
                  incorporating irregular spatial grids without
                  insisting on repeatability between the
                  surveys. Since we are always subsampled in both the
                  baseline and monitor surveys, we are interested in
                  recovering the densely sampled baseline and monitor,
                  and then the (complete) 4-D difference from
                  subsampled/incomplete baseline and monitor data.},
  keywords = {EAGE, simultaneous acquisition, time-lapse, off-the-grid, NFFT},
  note = {(EAGE, Madrid)},
  doi = {10.3997/2214-4609.201413088},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/wason2015EAGEcsm/wason2015EAGEcsm.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2015/wason2015EAGEcsm/wason2015EAGEcsm_poster.pdf}
}


@CONFERENCE{witte2015IIPFWIspl,
  author = {Philipp A. Witte and Ning Tu and Ernie Esser and Mengmeng Yang and Mathias Louboutin and Felix J. Herrmann},
  title = {Sparsity-promoting least-square migration with linearized {Bregman} and compressive sensing},
  booktitle = {Inaugural Full-Waveform Inversion Workshop},
  year = {2015},
  month = {08-09},
  abstract = {We present a novel adaptation of a recently developed
                  relatively simple iterative algorithm to solve
                  large-scale sparsity-promoting optimization
                  problems. Our algorithm is particularly suitable to
                  large-scale geophysical inversion problems, such as
                  sparse least-squares reverse-time migration or
                  Kirchoff migration since it allows for a tradeoff
                  between parallel computations, memory allocation,
                  and turnaround times, by working on subsets of the
                  data with different sizes. Comparison of the
                  proposed method for sparse least-squares imaging
                  shows a performance that rivals and even exceeds the
                  performance of state-of-the art one-norm solvers
                  that are able to carry out least-squares migration
                  at the cost of a single migration with all data.},
  keywords = {FWI, sparsity promotion, migration, Bregman, compressed sensing, time domain},
  note = {(Natal, Brazil)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IIPFWI/witte2015IIPFWIspl/witte2015IIPFWIspl_pres.pdf}
}


@CONFERENCE{witte2015IIPFWItdf,
  author = {Philipp A. Witte and Mathias Louboutin and Felix J. Herrmann},
  title = {Time-domain {FWI} in {TTI} media},
  booktitle = {Inaugural Full-Waveform Inversion Workshop},
  year = {2015},
  month = {08-09},
  abstract = {We develop an inversion workflow for tilted transverse
                  isotropic (TTI) media using a purely acoustic
                  formulation of the wave equation. The anisotropic
                  modeling kernel is used for the forward modeling
                  operator, as well as for the adjoint Jacobian to
                  back propagate the data residual, thus providing the
                  true gradient of the FWI objective function.},
  keywords = {FWI, TTI, time domain},
  note = {(Natal, Brazil)},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IIPFWI/witte2015IIPFWItdf/witte2015IIPFWItdf_pres.pdf}
}


%-----2014-----%

@CONFERENCE{herrmann2014SEGWSfli,
  author = {Ning Tu and Felix J. Herrmann},
  title = {Fast linearized inversion with surface-related multiples with source estimation},
  booktitle = {SEG Workshop on Using Multiples as Signal for Imaging; Denver},
  year = {2014},
  month = {10},
  abstract = {In the well-known SRME relation, multiples are expressed
                  as the multi-dimensional convolution between the
                  subsurface Green’s function and the downgoing
                  receiver wavefield. Therefore these multiples can be
                  considered as the response of the subsurface to an
                  "areal" source term given by the same downgoing
                  receiver wavefield. This relation can be used to
                  treat these multiples as signals instead of
                  considering them as noise that must removed before
                  seismic imaging. However, when we use conventional
                  reverse-time migration to image these multiple
                  events, it results in acausal imaging artifacts
                  caused by cross-correlations between wrong pairs of
                  up- and downgoing wavefields. We find that these
                  artifacts can be removed by adopting a
                  sparsity-promoting least-squares inversion
                  approach. However, iterative inversions go at the
                  expense of excessive computational costs. By
                  combining the SRME relation and wave-equation based
                  linearized modelling, we are able to significantly
                  reduce the costs by avoiding the dense matrix-matrix
                  multiplications of SRME and the number of
                  wave-equation solves via source subsampling. As a
                  result, we arrive at a method with a cost comparable
                  to that of a single reverse-time migration with all
                  shots.},
  keywords = {SEG, workshop, multiples, inversion},
  note = {(SEG Workshop, Denver)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/herrmann2014SEGWSfli/herrmann2014SEGWSfli.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/herrmann2014SEGWSfli/herrmann2014SEGWSfli_pres.pdf}
}


@CONFERENCE{herrmann2014SEGWS,
  author = {Felix J. Herrmann and Ernie Esser and Tristan van Leeuwen and Bas Peters},
  title = {Wavefield {Reconstruction} {Inversion} ({WRI}) – a new take on wave-equation based inversion},
  booktitle = {SEG Workshop on Full Waveform Inversion - Elastic Approaches and Issues with Anisotropy, Nonshallow Inversion, Poor Starting Model; Denver},
  year = {2014},
  month = {10},
  keywords = {SEG, workshop, WRI, FWI},
  note = {(SEG Workshop, Denver)},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/herrmann2014SEGWSwri/herrmann2014SEGWSwri_pres.pdf}
}


@CONFERENCE{herrmann2014SEGAGUWSiii,
  author = {Felix J. Herrmann},
  title = {Imaging/{Inversion} with irregular/random sampled spatial arrays},
  booktitle = {SEG-AGU Workshop on Advances in Active + Passive "Full Wavefield" Seismic Imaging: From Reservoirs to Plate Tectonics},
  year = {2014},
  month = {07},
  keywords = {SEG-AGU, workshop, imaging, inversion, randomization},
  note = {(SEG-AGU Workshop, 22-24 July, Vancouver)}
}


@CONFERENCE{miao2014SEGrhss,
  author = {Lina Miao and Polina Zheglova and Felix J. Herrmann},
  title = {Randomized {HSS} acceleration for full-wave-equation depth stepping migration},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2014},
  month = {10},
  pages = {3752-3756},
  abstract = {In this work we propose to use the spectral projector
                  (Kenney and Laub, 1995) and randomized HSS technique
                  (Chandrasekaran et al., 2006) to achieve a stable
                  and affordable two-way wave equation depth stepping
                  migration algorithm.},
  keywords = {SEG, acoustic, randomized SVD, spectral projector, full wave equation migration, depth extrapolation},
  note = {(SEG)},
  doi = {http://dx.doi.org/10.1190/segam2014-1417.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/miao2014SEGrhss/miao2014SEGrhss.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/miao2014SEGrhss/miao2014SEGrhss_pres.pdf}
}


@CONFERENCE{wason2014SEGsss,
  author = {Haneet Wason and Rajiv Kumar and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Source separation via {SVD}-free rank minimization in the hierarchical semi-separable representation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2014},
  month = {10},
  pages = {120-126},
  abstract = {Recent developments in matrix rank optimization have
                  allowed for new computational approaches in the
                  field of source separation or deblending. In this
                  paper, we propose a source separation algorithm for
                  blended marine acquisition, where two sources are
                  deployed at different depths (over/under
                  acquisition). The separation method incorporates the
                  Hierarchical Semi-Separable structure (HSS) inside
                  rank-regularized least-squares formulations. The
                  proposed approach is suitable for large scale
                  problems, since it avoids SVD computations and uses
                  a low-rank factorized formulation instead. We
                  illustrate the performance of the new HSS-based
                  deblending approach by simulating an over/under
                  blended acquisition, wherein uniformly random time
                  delays (of < 1 second) are applied to one of the
                  sources.},
  keywords = {SEG, source separation, deblending, marine, acquisition, rank, HSS},
  note = {(SEG)},
  doi = {http://dx.doi.org/10.1190/segam2014-1583.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/wason2014SEGsss/wason2014SEGsss.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/wason2014SEGsss/wason2014SEGsss_pres.pdf}
}


@CONFERENCE{wason2014SEGrrt,
  author = {Haneet Wason and Felix Oghenekohwo and Felix J. Herrmann},
  title = {Randomization and repeatability in time-lapse marine acquisition},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2014},
  month = {10},
  pages = {46-51},
  abstract = {We present an extension of our time-jittered
                  simultaneous marine acquisition to time-lapse
                  surveys where the requirement for repeatability in
                  acquisition can be waived provided we know the
                  acquisition geometry afterwards. Our method, which
                  does not require repetition, gives 4-D signals
                  comparable to conventional methods where
                  repeatability is key to their success.},
  keywords = {SEG, marine, acquisition, time-laspe, deblending},
  note = {(SEG)},
  doi = {http://dx.doi.org/10.1190/segam2014-1677.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/wason2014SEGrrt/wason2014SEGrrt.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/wason2014SEGrrt/wason2014SEGrrt_pres.pdf}
}


@CONFERENCE{peters2014SEGsrh,
  author = {Bas Peters and Felix J. Herrmann},
  title = {A sparse reduced Hessian approximation for multi-parameter wavefield reconstruction inversion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2014},
  month = {10},
  pages = {1206-1210},
  abstract = {Multi-Parameter full-waveform inversion is a challenging
                  problem, because the unknown parameters appear in
                  the same wave equation and the magnitude of the
                  parameters can vary many orders of magnitude. This
                  makes accurate estimation of multiple-parameters
                  very difficult. To mitigate the problems, sequential
                  strategies, regularization methods and scalings of
                  gradients and quasi-Newton Hessians have been
                  proposed. All of these require design, fine-tuning
                  and adaptation to different waveform inversion
                  problems. We propose to use a sparse approximation
                  to the Hessian derived from a penalty-formulation of
                  the objective function. Sparseness allows to have
                  the Hessian in memory and compute update directions
                  at very low cost. This results in decent
                  reconstruction of the multiple parameters at very
                  low additional memory and computational expense.},
  keywords = {SEG, full-waveform inversion, optimization, Hessian, penalty method},
  note = {(SEG)},
  doi = {http://dx.doi.org/10.1190/segam2014-1667.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/peters2014SEGsrh/peters2014SEGsrh.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/peters2014SEGsrh/peters2014SEGsrh_pres.pdf}
}


@CONFERENCE{oghenekohwo2014SEGrsw,
  author = {Felix Oghenekohwo and Rajiv Kumar and Felix J. Herrmann},
  title = {Randomized sampling without repetition in time-lapse surveys},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2014},
  month = {10},
  pages = {4848-4852},
  abstract = {Vouching for higher levels of repeatability in
                  acquisition and processing of time-lapse (4D)
                  seismic data has become the standard with oil and
                  gas contractor companies, with significant
                  investment in the design of acquisition systems and
                  processing algorithms that attempt to address some
                  of the current 4D challenges, in particular, imaging
                  weak 4D signals. Recent developments from the field
                  of compressive sensing have shown the benefits of
                  variants of randomized sampling in marine seismic
                  acquisition and its impact for the future of seismic
                  exploration. Following these developments, we show
                  that the requirement for accurate survey repetition
                  in time-lapse seismic data acquisition can be waived
                  provided we solve a sparsity-promoting convex
                  optimization program that makes use of the shared
                  component between the baseline and monitor data. By
                  setting up a framework for inversion of the stacked
                  sections of a time-lapse data, given the pre-stack
                  data volumes, we are able to extract 4D signals with
                  relatively highfidelity from significant
                  subsamplings. Our formulation is applied to
                  time-lapse data that has been acquired with
                  different source/receiver geometries, paving the way
                  for an efficient approach to dealing with time-lapse
                  data acquired with initially poor repeatability
                  levels, provided the survey geometry details are
                  known afterwards.},
  keywords = {SEG, acquisition, repetition, 4D, time-lapse, random},
  note = {(SEG)},
  doi = {http://dx.doi.org/10.1190/segam2014-1627.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/oghenekohwo2014SEGrsw/oghenekohwo2014SEGrsw.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/oghenekohwo2014SEGrsw/oghenekohwo2014SEGrsw_pres.pdf}
}


@CONFERENCE{ghadermarzy2014SEGsti,
  author = {Navid Ghadermarzy and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Seismic trace interpolation with approximate message passing},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2014},
  month = {10},
  pages = {3621-3626},
  abstract = {Approximate message passing (AMP) is a computationally
                  effective algorithm for recovering high dimensional
                  signals from a few compressed measurements. In this
                  paper we use AMP to solve the seismic trace
                  interpolation problem. We also show that we can
                  exploit the fast AMP algorithm to improve the
                  recovery results of seismic trace interpolation in
                  curvelet domain, both in terms of convergence speed
                  and recovery performance by using AMP in Fourier
                  domain as a preprocessor for the L1 recovery in
                  Curvelet domain.},
  keywords = {SEG, interpolation, AMP},
  note = {(SEG)},
  doi = {http://dx.doi.org/10.1190/segam2014-0577.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/ghadermarzy2014SEGsti/ghadermarzy2014SEGsti.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/ghadermarzy2014SEGsti/ghadermarzy2014SEGsti_pres.pdf}
}


@CONFERENCE{lin2014SEGmdg,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Mitigating data gaps in the estimation of primaries by sparse inversion without data reconstruction},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2014},
  month = {10},
  pages = {4157-4161},
  abstract = {We propose to solve the Estimation of Primaries by
                  Sparse Inversion problem from a sesimic record with
                  missing near-offsets and large holes without any
                  explicit data reconstruction, by instead simulating
                  the missing multiple contributions with terms
                  involving auto-convolutions of the primary
                  wavefield. Exclusion of the unknown data as an
                  inversion variable from the REPSI process is
                  desireable, since it eliminates a significant source
                  of local minima that arises from attempting to
                  invert for the unobserved traces using primary and
                  multiple models that may be far-away from the true
                  solution. In this talk we investigate the necessary
                  modifications to the Robust EPSI algorithm to
                  account for the resulting non-linear modeling
                  operator, and demonstrate that just a few
                  auto-convolution terms are enough to satisfactorily
                  mitigate the effects of data gaps during the
                  inversion process.},
  keywords = {SEG, EPSI, REPSI, multiples, inversion, algorithm},
  note = {(SEG)},
  doi = {http://dx.doi.org/10.1190/segam2014-1680.1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/lin2014SEGmdg/lin2014SEGmdg.html},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2014/lin2014SEGmdg/lin2014SEGmdg_pres.pdf}
}


@CONFERENCE{herrmann2014BIRSlrb,
  author = {Felix J. Herrmann},
  title = {Low-rank based matrix/tensor completions for the "real" (seismic) world},
  year = {2014},
  month = {10},
  date = {5-10},
  booktitle = {Workshop on Sparse Representations, Numerical Linear Algebra, and Optimization; Banff},
  keywords = {workshop, low-rank, matrix completion, hierarchical tucker, source separation},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/BIRS/2014/herrmann2014BIRSlrb_pres.pdf},
  note = {(Workshop at the Banff International Research Station for Mathematical Innovation and Discovery)}
}


@CONFERENCE{esser2014SIAMISsdc,
  author = {Ernie Esser},
  title = {Solving {DC} programs that promote group 1-sparsity},
  year = {2014},
  month = {05},
  booktitle = {SIAM Conference on Imaging Science},
  abstract = {Many interesting applications require solving nonconvex
                  problems that would be convex if not for a group
                  1-sparsity constraint. Splitting methods that are
                  effective for convex problems can still work well in
                  this setting. We propose several nonconvex penalties
                  that can be used to promote group 1-sparsity in the
                  framework of difference of convex or primal dual
                  hybrid gradient (PDHG) methods. Applications to
                  nonlocal inpainting, linear unmixing and phase
                  unwrapping are demonstrated.},
  keywords = {group 1-sparsity, difference of convex, phase unwrapping, nonconvex PDHG, operator splitting},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2014/esser2014SIAMISsdc_pres.pdf},
  note = {(SIAM Conference on Imaging Science)}
}


@CONFERENCE{herrmann2014ROSErpe,
  author = {Felix J. Herrmann},
  title = {Relax the physics and expand the search space – {FWI} via {Wavefield} {Reconstruction} {Inversion}},
  year = {2014},
  month = {05},
  booktitle = {ROSE Consortium; Norway},
  keywords = {ROSE, Consortium, FWI, WRI},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ROSE/2014/herrmann2014ROSErpe.pdf},
  note = {(ROSE Consortium)}
}


@CONFERENCE{herrmann2014EAGEWSrrt,
  author = {Haneet Wason and Felix Oghenekohwo and Felix J. Herrmann},
  title = {Randomization and repeatability in time-lapse marine acquisition},
  year = {2014},
  month = {04},
  booktitle = {EAGE Workshop on Land and Ocean Bottom; Broadband Full Azimuth Seismic Surveys; Spain},
  keywords = {EAGE, workshop, 4D seismic, marine acquisition},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/herrmann2014EAGEWSrrt.pdf},
  note = {(EAGE Workshop, Spain)}
}


@CONFERENCE{lago2014CMCCRMN,
  author = {Rafael Lago and Art Petrenko and Zhilong Fang and Felix J. Herrmann},
  title = {{CRMN} method for solving time-harmonic wave equation},
  year = {2014},
  month = {04},
  booktitle = {Copper Mountain Conference},
  abstract = {We address the solution of PDEs associated with the wave
                  propagation phenomena in heterogeneous media using
                  CGMN method. It consists of a conjugate gradients
                  method using Kaczmarz double sweeps as
                  preconditioner. This preconditioner has the property
                  of ``symmetrizing" the problem allowing short
                  recursion and Lanczos type of Krylov methods to be
                  employed. In this talk we propose the use of CR, a
                  minimal residual Krylov method closely related to
                  CG. We study the proposed method which we call CRMN
                  and discuss crucial aspects as the behaviour of the
                  norm of the residual and the norm of the true
                  error. We also discuss the result of an inversion of
                  a small subsample of the 3D velocity model SEG/EAGE
                  Overthrust using frugal full-waveform inversion
                  method with CRMN and CGMN for solving the associated
                  PDE, showing a strong interest in studying the
                  behaviour of CRMN for larger realistic cases.},
  keywords = {CRMN, CGMN, forward modelling, Helmholtz equation},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CMC/2014/lago2014CMCCRMN.pdf},
  note = {(Copper Mountain)}
}


@CONFERENCE{lago2014EAGEfst,
  author = {Rafael Lago and Art Petrenko and Zhilong Fang and Felix J. Herrmann},
  title = {Fast solution of time-harmonic wave-equation for full-waveform inversion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2014},
  month = {06},
  abstract = {For many full-waveform inversion techniques, the most
                  computationally intensive step is the computation of
                  a numerical solution for the wave equation on every
                  iteration. In the frequency domain approach, this
                  requires the solution of very large, complex,
                  sparse, ill-conditioned linear systems. In this
                  extended abstract we bring out attention
                  specifically to CGMN method for solving PDEs, known
                  for being flexible (i.e. it is able to treat equally
                  acoustic data as well as visco-elastic or more
                  complex scenarios) efficient with respect both to
                  memory and computation time, and controllable
                  accuracy of the final approximation. We propose an
                  improvement for the known CGMN method by imposing a
                  minimal residual condition, which incurs in one
                  extra model vector storage. The resulting algorithm
                  called CRMN enjoys several interesting properties as
                  monotonically nonincreasing behaviour of the norm of
                  the residual and minimal residual, guaranteeing
                  optimal convergence for the relative residual
                  criterion.  We discuss numerical experiments both in
                  an isolated PDE solve and also within the inversion
                  procedure, showing that in a realistic scenario we
                  can expect a speedup around 25\% when using CRMN
                  rather than CGMN.},
  keywords = {CRMN, CGMN, FWI, time-harmonic wave equation, EAGE},
  doi = {10.3997/2214-4609.20140812},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/lago2014EAGEfst/lago2014EAGEfst.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/lago2014EAGEfst/lago2014EAGEfst_pres.pdf}
}


@CONFERENCE{peters2014EAGEweb,
  author = {Bas Peters and Felix J. Herrmann and Tristan van Leeuwen},
  title = {Wave-equation based inversion with the penalty method: adjoint-state versus wavefield-reconstruction inversion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2014},
  month = {06},
  abstract = {In this paper we make a comparison between wave-equation
                  based inversions based on the adjoint-state and
                  penalty methods. While the adjoint-state method
                  involves the minimization of a data-misfit and exact
                  solutions of the wave-equation for the current
                  velocity model, the penalty-method aims to first
                  find a wavefield that jointly fits the data and
                  honours the physics, in a least-squares sense. Given
                  this reconstructed wavefield, which is a proxy for
                  the true wavefield in the true model, we calculate
                  updates for the velocity model. Aside from being
                  less nonlinear–the acoustic wave equation is linear
                  in the wavefield and model parameters but not in
                  both–the inversion is carried out over a solution
                  space that includes both the model and the
                  wavefield. This larger search space allows the
                  algortihm to circumnavigate local minima, very much
                  in the same way as recently proposed model
                  extentions try to acomplish. We include examples for
                  low frequencies, where we compare full-waveform
                  inversion results for both methods, for good and bad
                  starting models, and for high frequencies where we
                  compare reverse-time migration with linearized
                  imaging based on wavefield-reconstruction
                  inversion. The examples confirm the expected
                  benefits of the proposed method.},
  keywords = {full-waveform inversion, optimization, imaging, penalty method, EAGE},
  doi = {10.3997/2214-4609.20140704},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/peters2014EAGEweb/peters2014EAGEweb.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/peters2014EAGEweb/peters2014EAGEweb_pres.pdf}
}


@CONFERENCE{leeuwen2014EAGEntf,
  author = {Tristan van Leeuwen and Felix J. Herrmann and Bas Peters},
  title = {A new take on {FWI}: wavefield reconstruction inversion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2014},
  month = {06},
  abstract = {We discuss a recently proposed novel method for waveform
                  inversion: Wavefield Reconstruction Inversion
                  (WRI). As opposed to conventional FWI -- which
                  attempts to minimize the error between observed and
                  predicted data obtained by solving a wave equation
                  -- WRI reconstructs a wave-field from the data and
                  extracts a model-update from this wavefield by
                  minimizing the wave-equation residual. The method
                  does not require explicit computation of an adjoint
                  wavefield as all the necessary information is
                  contained in the reconstructed wavefield. We show
                  how the corresponding model updates can be
                  interpreted physically analogously to the
                  conventional imaging-condition-based approach.},
  keywords = {wavefield reconstruction inversion, penalty method, optimization, full-waveform inversion, EAGE},
  doi = {10.3997/2214-4609.20140703},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/leeuwen2014EAGEntf/leeuwen2014EAGEntf.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/leeuwen2014EAGEntf/leeuwen2014EAGEntf_pres.pdf}
}


@CONFERENCE{oghenekohwo2014EAGEtls,
  author = {Felix Oghenekohwo and Ernie Esser and Felix J. Herrmann},
  title = {Time-lapse seismic without repetition: reaping the benefits from randomized sampling and joint recovery},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2014},
  month = {06},
  abstract = {In the current paradigm of 4-D seismic, guaranteeing
                  repeatability in acquisition and processing of the
                  baseline and monitor surveys ranks highest amongst
                  the technical challenges one faces in detecting
                  time-lapse signals. By using recent insights from
                  the field of compressive sensing, we show that the
                  condition of survey repeatability can be relaxed as
                  long as we carry out a sparsitypromoting program
                  that exploits shared information between the
                  baseline and monitor surveys. By inverting for the
                  baseline and monitor survey as the common
                  "background", we are able to compute high-fidelity
                  4-D differences from carefully selected synthetic
                  surveys that have different sets of source/receivers
                  missing. This synthetic example is proof of concept
                  of an exciting new approach to randomized 4-D
                  acquisition where time-lapse signal can be computed
                  as long as the survey details, such as
                  source/receiver locations are known afterwards.},
  keywords = {4-D seismic, time-lapse, joint recovery, EAGE},
  doi = {10.3997/2214-4609.20141478},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/oghenekohwo2014EAGEtls/oghenekohwo2014EAGEtls.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/oghenekohwo2014EAGEtls/oghenekohwo2014EAGEtls_pres.pdf}
}


@CONFERENCE{dasilva2014EAGEhtucknoisy,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Low-rank promoting transformations and tensor interpolation - applications to seismic data denoising},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2014},
  month = {06},
  abstract = {In this abstract, we extend our previous work in
                  Hierarchical Tucker (HT) tensor completion, which
                  uses an extremely efficient representation for
                  representing high-dimensional tensors exhibit- ing
                  low-rank structure, to handle subsampled tensors
                  with noisy entries. We consider a "low-noise" case,
                  so that the energies of the noise and the signal are
                  nearly indistinguishable, and a ’high-noise’ case,
                  in which the noise energy is now scaled to the
                  amplitude of the entire data volume. We examine the
                  effect of the noise in terms of the singular values
                  along different matricizations of the data, i.e.,
                  reshaping of the tensor along different modes. By
                  interpreting this effect in the context of tensor
                  completion, we demonstrate the inefficacy of
                  denoising by this method in the source-receiver do-
                  main. In light of this observation, we transform the
                  decimated, noisy data in to the midpoint-offset
                  domain, which promotes low-rank behaviour in the
                  signal and high-rank behaviour in the noise. This
                  distinction between signal and noise allows low-rank
                  interpolation to effectively denoise the signal with
                  only a marginal increase in computational cost. We
                  demonstrate the effectiveness of this approach on a
                  4D frequency slice.},
  keywords = {hierarchical tucker, structured tensor, tensor interpolation, Riemannian optimization, low-rank transform, seismic denoising, EAGE},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/dasilva2014EAGEhtucknoisy/dasilva2014EAGEhtucknoisy.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/dasilva2014EAGEhtucknoisy/dasilva2014EAGEhtucknoisy_pres.pdf}
}


@CONFERENCE{esser2014EAGEacp,
  author = {Ernie Esser and Felix J. Herrmann},
  title = {Application of a convex phase retrieval method to blind seismic deconvolution},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2014},
  month = {06},
  abstract = {A classical strategy for blind seismic deconvolution is
                  to first estimate the autocorrelation of the unknown
                  source wavelet from the data and then recover the
                  wavelet by assuming it has minimum phase. However,
                  computing the minimum phase wavelet directly from
                  the amplitude spectrum can be sensitive to even
                  extremely small errors, especially in the
                  coefficients close to zero. Since the minimum phase
                  requirement follows from an assumption that the
                  wavelet should be as impulsive as possible, we
                  propose to directly estimate an impulsive wavelet by
                  minimizing a weighted l2 penalty subject to a
                  constraint on its amplitude spectrum. This nonconvex
                  model has the form of a phase retrieval problem, in
                  this case recovering a signal given only estimates
                  of the magnitudes of its Fourier
                  coefficients. Following recent work on convex
                  relaxations of phase retrieval problems, we propose
                  a convex semidefinite program for computing an
                  impulsive minimum phase wavelet whose amplitude
                  spectrum is close to a given estimate, and we show
                  that this can be robustly solved by a Douglas
                  Rachford splitting method for convex optimization.},
  keywords = {source wavelet estimation, blind deconvolution, convex phase retrieval, EAGE},
  doi = {10.3997/2214-4609.20141590},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/esser2014EAGEacp/esser2014EAGEacp.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/esser2014EAGEacp/esser2014EAGEacp_poster.pdf}
}


@CONFERENCE{lin2014EAGEmas,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Multilevel acceleration strategy for the robust estimation of primaries by sparse inversion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2014},
  month = {06},
  abstract = {We propose a method to substantially reduce the
                  computational costs of the Robust Estimation of
                  Primaries by Sparse Inversion algorithm, based on a
                  multilevel inversion strategy that shifts early
                  iterations of the method to successively coarser
                  spatial sampling grids. This method requires no
                  change in the core implementation of the original
                  algorithm, and additionally only relies on trace
                  decimation, low-pass filtering, and rudimentary
                  interpolation techniques. We furthermore demonstrate
                  with a synthetic seismic line significant
                  computational speedups using this approach.},
  keywords = {multiples, EPSI, REPSI, multigrid, multilevel, multiscale, EAGE},
  doi = {10.3997/2214-4609.20140672},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/lin2014EAGEmas/lin2014EAGEmas.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/lin2014EAGEmas/lin2014EAGEmas_pres.pdf}
}


@CONFERENCE{zheglova2014EAGEams,
  author = {Polina Zheglova and Felix J. Herrmann},
  title = {Application of matrix square root and its inverse to downward wavefield extrapolation},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2014},
  month = {06},
  abstract = {In this paper we propose a method for computation of the
                  square root of the Helmholtz operator and its
                  inverse that arise in downward extrapolation methods
                  based on one-way wave equation. Our approach
                  involves factorization of the discretized Helmholtz
                  operator at each depth by extracting the matrix
                  square root after performing the spectral projector
                  in order to eliminate the evanescent modes. The
                  computation of the square root of the discrete
                  Helmholtz operator and its inverse is done using
                  polynomial recursions and can be combined with low
                  rank matrix approximations to reduce the
                  computational cost for large problems. The resulting
                  square root operator is able to model the
                  propagating modes kinematically correctly at the
                  angles of up to 90 degree. Preliminary results on
                  convergence of iterations are presented in this
                  abstract. Potential applications include seismic
                  modeling, imaging and inversion.},
  keywords = {square root, modelling, one-way wave equation, extrapolation, EAGE},
  doi = {10.3997/2214-4609.20141184},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/zheglova2014EAGEams/zheglova2014EAGEams.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/zheglova2014EAGEams/zheglova2014EAGEams_pres.pdf}
}


@CONFERENCE{fang2014EAGEfuq,
  author = {Zhilong Fang and Curt Da Silva and Felix J. Herrmann},
  title = {Fast uncertainty quantification for {2D} full-waveform inversion with randomized source subsampling},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2014},
  month = {06},
  abstract = {Uncertainties arise in every area of seismic
                  exploration, especially in full-waveform inversion,
                  which is highly non-linear. In the framework of
                  Bayesian inference, uncertainties can be analyzed by
                  sampling the posterior probability density
                  distribution with a Markov chain Monte-Carlo (McMC)
                  method. We reduce the cost of computing the
                  posterior distribution by working with randomized
                  subsets of sources. These approximations, together
                  with the Gaussian assumption and approximation of
                  the Hessian, leads to a computational tractable
                  uncertainty quantification. Application of this
                  approach to a synthetic leads to standard deviations
                  and confidence intervals that are qualitatively
                  consistent with our expectations.},
  keywords = {Uncertainty quantification, FWI, Markov chain Monte Carlo, randomized sources subsampling, EAGE},
  doi = {10.3997/2214-4609.20140715},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/fang2014EAGEfuq/fang2014EAGEfuq.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/fang2014EAGEfuq/fang2014EAGEfuq_pres.pdf}
}


@CONFERENCE{petrenko2014EAGEaih,
  author = {Art Petrenko and Tristan van Leeuwen and Diego Oriato and Simon Tilbury and Felix J. Herrmann},
  title = {Accelerating an iterative {Helmholtz} solver with {FPGAs}},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2014},
  month = {06},
  abstract = {We implement the Kaczmarz row-projection algorithm
                  (Kaczmarz (1937)) on a CPU host + FPGA accelerator
                  platform using techniques of dataflow
                  programming. This algorithm is then used as the
                  preconditioning step in CGMN, a modified version of
                  the conjugate gradients method (Björck and Elfving
                  (1979)) that we use to solve the time-harmonic
                  acoustic isotropic constant density wave
                  equation. Using one accelerator we achieve a
                  speed-up of over 2× compared with one Intel core.},
  keywords = {CGMN, FPGA, Helmholtz equation, Kaczmarz, reconfigurable computing, EAGE},
  doi = {10.3997/2214-4609.20141141},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/petrenko2014EAGEaih/petrenko2014EAGEaih.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/petrenko2014EAGEaih/petrenko2014EAGEaih_pres.pdf}
}


@CONFERENCE{kumar2014EAGErank,
  author = {Rajiv Kumar and Aleksandr Y. Aravkin and Ernie Esser and Hassan Mansour and Felix J. Herrmann},
  title = {{SVD}-free low-rank matrix factorization : wavefield reconstruction via jittered subsampling and reciprocity},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2014},
  month = {06},
  abstract = {Recently computationally efficient rank optimization
                  techniques have been studied extensively to develop
                  a new mathematical tool for the seismic data
                  interpolation. So far, matrix completion problems
                  have been discussed where sources are subsample
                  according to a discrete uniform distribution. In
                  this paper, we studied the effect of two different
                  subsampling techniques on seismic data interpolation
                  using rank-regularized formulations, namely jittered
                  subsampling over uniform random subsampling. The
                  other objective of this paper is to combine the fact
                  of source-receiver reciprocity with the
                  rank-minimization techniques to enhance the accuracy
                  of missing-trace interpolation. We illustrate the
                  advantages of jittered subsampling and reciprocity
                  using a seismic line from Gulf of Suez to obtain
                  high quality results for interpolation, a key
                  application in exploration geophysics.},
  keywords = {low-rank, interpolation, reciprocity, EAGE},
  doi = {10.3997/2214-4609.20141394},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/kumar2014EAGErank/kumar2014EAGErank.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/kumar2014EAGErank/kumar2014EAGErank_pres.pdf}
}


@CONFERENCE{kumar2014EAGEeia,
  author = {Rajiv Kumar and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Extended images in action: efficient {WEMVA} via randomized probing},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2014},
  month = {06},
  abstract = {Image gathers as a function of subsurface offset are an
                  important tool for velocity analysis in areas of
                  complex geology. In this paper, we offer a new
                  perspective on image gathers by organizing the
                  extended image as a function of all subsurface
                  offsets and all subsurface points into a matrix
                  whose (i,j)^{th} entry captures the interaction
                  between gridpoints i and j. For even small problems,
                  it is infeasible to form and store this
                  matrix. Instead, we propose an efficient algorithm
                  to glean information from the image volume via
                  efficient matrix-vector products. We illustrate how
                  this can be used to construct objective functions
                  for automatic MVA.},
  keywords = {extended imaging, MVA, probing, EAGE},
  doi = {10.3997/2214-4609.20141492},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/kumar2014EAGEeia/kumar2014EAGEeia.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2014/kumar2014EAGEeia/kumar2014EAGEeia_pres.pdf}
}


@CONFERENCE{petrenko2014OGHPCaih,
  author = {Art Petrenko and Felix J. Herrmann and Diego Oriato and Simon Tilbury and Tristan van Leeuwen},
  title = {Accelerating an iterative {Helmholtz} solver with {FPGAs}},
  year = {2014},
  month = {03},
  booktitle = {OGHPC},
  abstract = {We implement the Kaczmarz row-projection algorithm
                  [Kaczmarz, 1937] on a CPU host + FPGA accelerator
                  platform using techniques of dataflow programming.
                  This algorithm is then used as the preconditioning
                  step in CGMN, a modified version of the conjugate
                  gradients method [Bj?rck and Elfving, 1979] that we
                  use to solve the time-harmonic acoustic isotropic
                  constant density wave equation.},
  keywords = {OGHPC, Kaczmarz, CGMN, FPGA, reconfigurable computing, Helmholtz equation},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/OGHPC/2014/petrenko2014OGHPCaih/petrenko2014OGHPCaih.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/OGHPC/2014/petrenko2014OGHPCaih/petrenko2014OGHPCaih_poster.pdf}
}


@CONFERENCE{herrmann2014CSEGbsw,
  author = {Felix J. Herrmann},
  title = {Breaking structure - why randomized sampling matters},
  booktitle = {CSEG Technical Luncheon},
  year = {2014},
  month = {01},
  abstract = {During this talk, I will explain how ideas from
                  compressive sensing and big data can be used to
                  reduce costs of seismic data acquisition and
                  wave-equation based inversion. The key idea is to
                  explore structure within the data by deliberately
                  breaking this structure with randomized sampling,
                  e.g., by randomizing source/receiver positions or by
                  source encoding, followed by an optimization
                  procedure that restores the structure and therefore
                  recovers the fully sampled data. These techniques
                  not only underpin recent advances in missing trace
                  interpolation and simultaneous acquisition but they
                  are also responsible for significant improvements in
                  full-waveform inversion and reverse-time
                  migration. We will illustrate these concepts using a
                  variety of compelling examples on realistic
                  synthetics and field data.},
  keywords = {CSEG, randomized sampling},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2014/herrmann2014CSEGbsw_pres.pdf}
}


%-----2013-----%

@CONFERENCE{herrmann2013EAGEfrtm,
  author = {Felix J. Herrmann and Ning Tu},
  title = {Fast {RTM} with multiples and source estimation},
  booktitle = {EAGE/SEG Forum - Turning noise into geological information: The next big step?},
  year = {2013},
  month = {11},
  abstract = {During this talk, we present a computationally efficient
                  (cost of 1-2 RTM's with all data) iterative
                  sparsity-promoting inversion framework where
                  surface-related multiples are jointly imaged with
                  primaries and where the source signature is
                  estimated on the fly. Our imaging algorithm is
                  computationally efficient because it works during
                  each iteration with small independent randomized
                  subsets of data. The multiples are handled by
                  introducing an areal source term that includes the
                  upgoing wavefield. We update the source signature
                  for each iteration using a variable projection
                  method. The resulting algorithm removes imaging
                  artifacts from surface-related multiples, estimates
                  and removes the imprint of the source, recovers true
                  amplitudes, is fast, and robust to linearization
                  errors by virtue of the statistical independence of
                  the subsets of data we are working with at each
                  iteration.},
  keywords = {EAGE, SEG, RTM, multiples, source estimation},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2013/herrmann2013EAGEfrtm/herrmann2013EAGEfrtm_pres.pdf}
}


@CONFERENCE{herrmann2013SPIErse,
  author = {Felix J. Herrmann},
  title = {Randomized sampling in exploration seismology},
  booktitle = {SPIE Optics and Photonics: Wavelets and Sparsity XV},
  year = {2013},
  month = {08},
  keywords = {SPIE, randomized sampling, exploration seismology},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SPIE/2013/herrmann2013SPIErse/herrmann2013SPIErse_pres.pdf}
}


@CONFERENCE{herrmann2013SEGpmc,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {A penalty method for PDE-constrained optimization with applications to wave-equation based seismic inversion},
  booktitle = {SEG Workshop on Computational Mathematics for Geophysics; Houston},
  year = {2013},
  month = {09},
  keywords = {SEG, workshop, penalty method, optimization},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/herrmann2013SEGpmc/herrmann2013SEGpmc_pres.pdf}
}


@CONFERENCE{dasilva2013SEGhtuck,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Structured tensor missing-trace interpolation in the {Hierarchical} {Tucker} format},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2013},
  month = {09},
  volume = {32},
  pages = {3623-3627},
  publisher = {SEG},
  abstract = {Owing to the large scale and dimensionality of a 3D
                  seismic experiment, acquiring fully-sampled data
                  according to the Nyquist criterion is an exceedingly
                  arduous and cost-prohibitive task. In this paper, we
                  develop tools to interpolate 5D seismic volumes with
                  randomly missing sources or receivers using a
                  relatively novel tensor format known as the
                  Hierarchical Tucker (HT) format. By exploiting the
                  underlying smooth structure of HT tensors,
                  specifically its smooth manifold structure, we
                  develop solvers which are fast, immediately
                  parallelizable, and SVD-free, making these solvers
                  amenable to large-scale problems where SVD-based
                  projection methods are far too costly. We also build
                  on intuition of multidimensional sampling from the
                  perspective of matrix-completion and demonstrate the
                  ability of our algorithms to recover frequency
                  slices even amidst very high levels of source
                  subsampling on a synthetic large-scale 3D North Sea
                  dataset.},
  keywords = {SEG, hierarchical tucker, structured tensor, tensor interpolation, differential geometry, Riemannian optimization, Gauss Newton},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/dasilva2013SEGhtuck/dasilva2013SEGhtuck.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/dasilva2013SEGhtuck/dasilva2013SEGhtuck_pres.pdf},
  doi = {10.1190/segam2013-0709.1}
}


@CONFERENCE{kumar2013SEGHSS,
  author = {Rajiv Kumar and Hassan Mansour and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Reconstruction of seismic wavefields via low-rank matrix factorization in the hierarchical-separable matrix representation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2013},
  volume = {32},
  month = {09},
  pages = {3628-3633},
  abstract = {Recent developments in matrix rank optimization have
                  allowed for new computational approaches in the
                  field of seismic data interpolation. In this paper,
                  we propose an approach for seismic data
                  interpolation which incorporates the Hierarchical
                  Semi-Separable Structure (HSS) inside
                  rank-regularized least-squares formulations for the
                  missing-trace interpolation problem. The proposed
                  approach is suitable for large scale problems, since
                  it avoids SVD computations and uses a low-rank
                  factorized formulation instead. We illustrate the
                  advantages of the new HSS approach by interpolating
                  a seismic line from the Gulf of Suez and compare the
                  reconstruction with conventional rank minimization.},
  keywords = {SEG, interpolation, HSS},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/kumar2013SEGHSS/kumar2013SEGHSS.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/kumar2013SEGHSS/kumar2013SEGHSS_pres.pdf},
  doi = {10.1190/segam2013-1165.1}
}


@CONFERENCE{kumar2013SEGAVA,
  author = {Rajiv Kumar and Tristan van Leeuwen and Felix J. Herrmann},
  title = {{AVA} analysis and geological dip estimation via two-way wave-equation based extended images},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2013},
  volume = {32},
  month = {09},
  pages = {423-427},
  abstract = {In this paper, we present an efficient way to compute
                  extended images for all subsurface offsets without
                  explicitly calculating the source and receiver
                  wavefields for all the sources. Because the extended
                  images contain all possible subsurface offsets, we
                  compute the angle-domain image gathers by selecting
                  the subsurface offset that is aligned with the local
                  dip. We also propose a method to compute the local
                  dip information directly from common-image-point
                  gathers. To assess the quality of the angle-domain
                  common-image-points gathers we compute the
                  angle-dependent reflectivity coefficients and
                  compare them with theoretical reflectivity
                  coefficients yielded by the (linearized) Zoeppritz
                  equations for a few synthetic models.},
  keywords = {SEG, AVA, dip, wave-equation, extended images},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/kumar2013SEGAVA/kumar2013SEGAVA.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/kumar2013SEGAVA/kumar2013SEGAVA_pres.pdf},
  doi = {10.1190/segam2013-1348.1}
}


@CONFERENCE{kumar2013SEGMVA,
  author = {Rajiv Kumar and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Efficient {WEMVA} using extended images},
  year = {2013},
  month = {09},
  booktitle = {SEG Workshop on Advances in Model Building, Imaging, and FWI; Houston},
  abstract = {Image gathers as a function of subsurface offset are an
                  important tool for velocity analysis in areas of
                  complex geology. Here, we offer a new perspective on
                  image gathers by organizing the extended image as a
                  function of all subsurface offsets and all
                  subsurface points in to a matrix whose (i,j)th entry
                  captures the interaction between gridpoints i and
                  j. For even small problems, it is infeasible to form
                  and store this matrix. Instead, we propose an
                  efficient algorithm to glean information from the
                  image volume via efficient matrix-vector
                  products. We illustrate how this can be used to
                  construct objective functions for automated MVA.},
  keywords = {SEG, workshop, MVA, inversion, poster},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/kumar2013SEGMVA/kumar2013SEGMVA_poster.pdf}
}


@CONFERENCE{li2013SEGodmvdaiedwawe,
  author = {Xiang Li and Anais Tamalet and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Optimization driven model-space versus data-space approaches to invert elastic data with the acoustic wave equation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2013},
  month = {09},
  volume = {32},
  pages = {986-990},
  abstract = {Inverting data with elastic phases using an acoustic
                  wave equation can lead to erroneous results,
                  especially when the number of iterations is too
                  high, which may lead to over fitting the
                  data. Several approaches have been proposed to
                  address this issue. Most commonly, people apply
                  "data-independent" filtering operations that are
                  aimed to deemphasize the elastic phases in the data
                  in favor of the acoustic phases. Examples of this
                  approach are nested loops over offset range and
                  Laplace parameters. In this paper, we discuss two
                  complementary optimization-driven methods where the
                  minimization process decides adaptively which of the
                  data or model components are consistent with the
                  objective. Specifically, we compare the Student's t
                  misfit function as the data-space alternative and
                  curvelet-domain sparsity promotion as the
                  model-space alternative. Application of these two
                  methods to a realistic synthetic lead to comparable
                  results that we believe can be improved by combining
                  these two methods.},
  keywords = {SEG, full-waveform inversion, elastic, least-squares},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/li2013SEGodmvdaiedwawe/li2013SEGodmvdaiedwawe.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/li2013SEGodmvdaiedwawe/li2013SEGodmvdaiedwawe_pres.pdf},
  doi = {10.1190/segam2013-1375.1}
}


@CONFERENCE{tu2013SEGldi,
  author = {Ning Tu and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Limitations of the deconvolutional imaging condition for two-way propagators},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2013},
  volume = {32},
  month = {09},
  pages = {3916-3920},
  abstract = {The deconvolutional imaging condition has gained wide
                  attention in recent years, as it is often used to
                  image surface-related multiples. However, we noticed
                  on close inspection that this condition was derived
                  from one-way propagation principles. Now that
                  two-way wave-equation based simulations have become
                  more affordable, we revisit the deconvolutional
                  imaging condition and reveal its limitations for
                  two-way propagators. First, it can distort the image
                  due to receiver-side propagation effects. Second,
                  when used to image surface-related multiples, it is
                  not capable of removing all interfering phantom
                  reflectors.},
  keywords = {SEG, migration, inversion, multiples},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/tu2013SEGldi/tu2013SEGldi.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/tu2013SEGldi/tu2013SEGldi_pres.pdf},
  doi = {10.1190/segam2013-1440.1}
}


@CONFERENCE{tu2013SEGcle,
  author = {Ning Tu and Xiang Li and Felix J. Herrmann},
  title = {Controlling linearization errors in $\ell_1$ regularized inversion by rerandomization},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2013},
  volume = {32},
  month = {09},
  pages = {4640-4644},
  abstract = {Linearized inversion is a data fitting procedure that
                  tries to match the observed seismic data with data
                  predicted by linearized modelling. In practice, the
                  observed data is not necessarily in the column space
                  of the linearized modelling operator. This can be
                  caused by lack of an accurate background velocity
                  model or by coherent noises not explained by
                  linearized modelling. Through carefully designed
                  experiments, we ob- serve that a moderate data
                  mismatch does not pose an issue if we can use all
                  the data in the inversion. However, artifacts do
                  arise from the mismatch when randomized
                  dimensionality reduction techniques are adopted to
                  speed up the inversion. To stabilize the inversion
                  for dimensionality reduction with randomized source
                  aggregates, we propose to rerandomize by drawing
                  independent simultaneous sources occasionally during
                  the inversion. The effect of this rerandomization is
                  remarkable because it results in virtually
                  artifact-free images at a cost comparable to a
                  single reverse-time migration. Implications of our
                  method are profound because we are now able to
                  resolve fine-scale steep subsalt features in a
                  computationally feasible manner.},
  keywords = {SEG, sparsity, inversion, rerandomization, message passing},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/tu2013SEGcle/tu2013SEGcle.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/tu2013SEGcle/tu2013SEGcle_pres.pdf},
  doi = {10.1190/segam2013-1302.1}
}


@CONFERENCE{wason2013SEGtjo,
  author = {Haneet Wason and Felix J. Herrmann},
  title = {Time-jittered ocean bottom seismic acquisition},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2013},
  month = {09},
  volume = {32},
  pages = {1-6},
  abstract = {Leveraging ideas from the field of compressed sensing,
                  we show how simultaneous or blended acquisition can
                  be setup as a -- compressed sensing problem. This
                  helps us to design a pragmatic time-jittered
                  marine acquisition scheme where multiple source
                  vessels sail across an ocean-bottom array firing
                  airguns at -- jittered source locations and
                  instances in time, resulting in better spatial
                  sampling, and speedup acquisition. Furthermore, we
                  can significantly impact the reconstruction quality
                  of conventional seismic data (from jittered data)
                  and demonstrate successful recovery by sparsity
                  promotion. In contrast to random (under)sampling,
                  acquisition via jittered (under)sampling helps in
                  controlling the maximum gap size, which is a
                  practical requirement of wavefield reconstruction
                  with localized sparsifying transforms. Results are
                  illustrated with simulations of time-jittered
                  marine acquisition, which translates to jittered
                  source locations for a given speed of the source
                  vessel, for two source vessels.},
  keywords = {SEG, acquisition, marine, OBC, jittered sampling, blending, deblending, interpolation},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/wason2013SEGtjo/wason2013SEGtjo.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/wason2013SEGtjo/wason2013SEGtjo_pres.pdf},
  doi = {10.1190/segam2013-1391.1}
}


@CONFERENCE{lin2013SEGdss,
  author = {Tim T.Y. Lin and Haneet Wason and Felix J. Herrmann},
  title = {Dense shot-sampling via time-jittered marine sources},
  booktitle = {SEG Workshop on Simultaneous Sources; Houston},
  year = {2013},
  month = {09},
  keywords = {SEG, workshop, acquisition, simultaneous sources},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/lin2013SEGdss/lin2013SEGdss_pres.pdf}
}


@CONFERENCE{dasilva2013SAMPTAhtuck,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Hierarchical {Tucker} tensor optimization - applications to tensor completion},
  year = {2013},
  month = {07},
  abstract = {In this work, we develop an optimization framework for
                  problems whose solutions are well-approximated by
                  Hierarchical Tucker tensors, an efficient structured
                  tensor format based on recursive subspace
                  factorizations. Using the differential geometric
                  tools presented here, we construct standard
                  optimization algorithms such as Steepest Descent and
                  Conjugate Gradient, for interpolating tensors in HT
                  format. We also empirically examine the importance
                  of one's choice of data organization in the success
                  of tensor recovery by drawing upon insights from the
                  Matrix Completion literature. Using these
                  algorithms, we recover various seismic data sets
                  with randomly missing source pairs.},
  keywords = {SAMPTA, hierarchical tucker, structured tensor, tensor interpolation, differential geometry, riemannian optimization},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SAMPTA/2013/dasilva2013SAMPTAhtuck/dasilva2013SAMPTAhtuck.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SAMPTA/2013/dasilva2013SAMPTAhtuck/dasilva2013SAMPTAhtuck_pres.pdf}
}


@CONFERENCE{dasilva2013EAGEhtucktensor,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Hierarchical {Tucker} tensor optimization - applications to {4D} seismic data interpolation},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2013},
  month = {06},
  abstract = {In this work, we develop optimization algorithms on the
                  manifold of Hierarchical Tucker (HT) tensors, an
                  extremely efficient format for representing
                  high-dimensional tensors exhibiting particular
                  low-rank structure. With some minor alterations to
                  existing theoretical developments, we develop an
                  optimization framework based on the geometric
                  understanding of HT tensors as a smooth manifold, a
                  generalization of smooth curves/surfaces. Building
                  on the existing research of solving optimization
                  problems on smooth manifolds, we develop Steepest
                  Descent and Conjugate Gradient methods for HT
                  tensors. The resulting algorithms converge quickly,
                  are immediately parallelizable, and do not require
                  the computation of SVDs. We also extend ideas about
                  favourable sampling conditions for missing-data
                  recovery from the field of Matrix Completion to
                  Tensor Completion and demonstrate how the
                  organization of data can affect the success of
                  recovery. As a result, if one has data with randomly
                  missing source pairs, using these ideas, coupled
                  with an efficient solver, one can interpolate
                  large-scale seismic data volumes with missing
                  sources and/or receivers by exploiting the
                  multidimensional dependencies in the data. We are
                  able to recover data volumes amidst extremely high
                  subsampling ratios (in some cases, > 75\%) using this
                  approach.},
  keywords = {EAGE, structured tensor, 3D data interpolation, riemannian optimization},
  doi = {10.3997/2214-4609.20130390},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2013/dasilva2013EAGEhtucktensor/dasilva2013EAGEhtucktensor.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2013/dasilva2013EAGEhtucktensor/dasilva2013EAGEhtucktensor_pres.pdf}
}


@CONFERENCE{kumar2013EAGEsind,
  author = {Rajiv Kumar and Aleksandr Y. Aravkin and Hassan Mansour and Ben Recht and Felix J. Herrmann},
  title = {Seismic data interpolation and denoising using {SVD}-free low-rank matrix factorization},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2013},
  month = {06},
  abstract = {Recent developments in rank optimization have allowed
                  new approaches for seismic data interpolation and
                  denoising. In this paper, we propose an approach for
                  simultaneous seismic data interpolation and
                  denoising using robust rank-regularized
                  formulations. The proposed approach is suitable for
                  large scale problems, since it avoids SVD
                  computations by using factorized formulations. We
                  illustrate the advantages of the new approach using
                  a seismic line from Gulf of Suez and 5D synthetic
                  seismic data to obtain high quality results for
                  interpolation and denoising, a key application in
                  exploration geophysics.},
  keywords = {EAGE, interpolation, denoising},
  doi = {10.3997/2214-4609.20130388},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2013/kumar2013EAGEsind/kumar2013EAGEsind.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2013/kumar2013EAGEsind/kumar2013EAGEsind_pres.pdf}
}


@CONFERENCE{lin2013EAGEcsd,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Cosparse seismic data interpolation},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2013},
  month = {06},
  abstract = {Many modern seismic data interpolation and redatuming
                  algorithms rely on the promotion of transform-domain
                  sparsity for high-quality results. Amongst the large
                  diversity of methods and different ways of realizing
                  sparse reconstruction lies a central question that
                  often goes unaddressed: is it better for the
                  transform-domain sparsity to be achieved through
                  explicit construction of sparse representations
                  (e.g., by thresholding of small transform-domain
                  coefficients), or by demanding that the algorithm
                  return physical signals which produces sparse
                  coefficients when hit with the forward transform?
                  Recent results show that the two approaches give
                  rise to different solutions when the transform is
                  redundant, and that the latter approach imposes a
                  whole new class of constraints related to where the
                  forward transform produces zero coefficients. From
                  this framework, a new reconstruction algorithm is
                  proposed which may allow better reconstruction from
                  subsampled signaled than what the sparsity
                  assumption alone would predict. In this work we
                  apply the new framework and algorithm to the case of
                  seismic data interpolation under the curvelet
                  domain, and show that it admits better
                  reconstruction than some existing L1 sparsity-based
                  methods derived from compressive sensing for a range
                  of subsampling factors.},
  keywords = {EAGE, cosparsity, interpolation, curvelet, algorithm, optimization},
  doi = {10.3997/2214-4609.20130387},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2013/lin2013EAGEcsd/lin2013EAGEcsd.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2013/lin2013EAGEcsd/lin2013EAGEcsd_pres.pdf}
}


@CONFERENCE{tu2013EAGElsm,
  author = {Ning Tu and Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Fast least-squares migration with multiples and source estimation},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2013},
  month = {06},
  abstract = {The advent of modern computing has made it possible to
                  do seismic imaging using least-squares reverse-time
                  migration. We obtain superior images by solving an
                  optimization problem that recovers the
                  true-amplitude images. However, its success hinges
                  on overcoming several issues, including overwhelming
                  problem size, unknown source wavelet, and
                  interfering coherent events like multiples. In this
                  abstract, we reduce the problem size by using ideas
                  from compressive sensing, and estimate source
                  wavelet by generalized variable projection. We also
                  demonstrate how to invert for subsurface information
                  encoded in surface-related multiples by
                  incorporating the free-surface operator as an areal
                  source in reverse-time migration. Our synthetic
                  examples show that multiples help to improve the
                  resolution of the image, as well as remove the
                  amplitude ambiguity in wavelet estimation.},
  keywords = {EAGE, imaging, sparse, source estimation, multiples},
  doi = {10.3997/2214-4609.20130727},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2013/tu2013EAGElsm/tu2013EAGElsm.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2013/tu2013EAGElsm/tu2013EAGElsm_pres.pdf}
}


@CONFERENCE{vanleeuwen2013EAGErobustFWI,
  author = {Tristan van Leeuwen and Aleksandr Y. Aravkin and Henri Calandra and Felix J. Herrmann},
  title = {In which domain should we measure the misfit for robust full waveform inversion?},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2013},
  month = {06},
  abstract = {Full-waveform inversion relies on minimizing the
                  difference between observed and modeled data, as
                  measured by some penalty function. A popular choice,
                  of course, is the least-squares penalty. However,
                  when outliers are present in the data, the use of
                  robust penalties such as the Huber or Student's t
                  may significantly improve the results since they put
                  relatively less weight on large residuals. In order
                  for robust penalties to be effective, the outliers
                  must be somehow localized and distinguishable from
                  the good data. We propose to first transform the
                  residual into a domain where the outliers are
                  localized before measuring the misfit with a robust
                  penalty. This is exactly how one would normally
                  devise filters to remove the noise before applying
                  conventional FWI. We propose to merge the two steps
                  and let the inversion process implicitly filter out
                  the noise. Results on a synthetic dataset show the
                  effectiveness of the approach.},
  keywords = {EAGE, full waveform inversion},
  doi = {10.3997/2214-4609.20130839},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2013/vanleeuwen2013EAGErobustFWI/vanleeuwen2013EAGErobustFWI.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2013/vanleeuwen2013EAGErobustFWI/vanleeuwen2013EAGErobustFWI_pres.pdf}
}


@CONFERENCE{wason2013EAGEobs,
  author = {Haneet Wason and Felix J. Herrmann},
  title = {Ocean bottom seismic acquisition via jittered sampling},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2013},
  month = {06},
  abstract = {We present a pragmatic marine acquisition scheme where
                  multiple source vessels sail across an ocean-bottom
                  array firing at airgunsjittered source locations and
                  instances in time. Following the principles of
                  compressive sensing, we can significantly impact the
                  reconstruction quality of conventional seismic data
                  (from jittered data) and demonstrate successful
                  recovery by sparsity promotion. In contrast to
                  random (under)sampling, acquisition via jittered
                  (under)sampling helps in controlling the maximum gap
                  size, which is a practical requirement of wavefield
                  reconstruction with localized sparsifying
                  transforms. Results are illustrated with simulations
                  of time-jittered marine acquisition, which
                  translates to jittered source locations for a given
                  speed of the source vessel, for two source vessels.},
  keywords = {EAGE, acquisition, blended, marine, deblending, interpolation},
  doi = {10.3997/2214-4609.20130379},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2013/wason2013EAGEobs/wason2013EAGEobs.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2013/wason2013EAGEobs/wason2013EAGEobs_pres.pdf}
}


@CONFERENCE{aravkin2013ICASSPssi,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and Ning Tu},
  title = {Sparse seismic imaging using variable projection},
  booktitle = {ICASSP},
  year = {2013},
  month = {05},
  abstract = {We consider an important class of signal processing
                  problems where the signal of interest is known to be
                  sparse, and can be recovered from data given
                  auxiliary information about how this data was
                  generated. For example, a sparse green's function
                  may be recovered from seismic experimental data
                  using sparsity optimization when the source
                  signature is known. Unfortunately, in practice this
                  information is often missing, and must be recovered
                  from data along with the signal using deconvolution
                  techniques. In this paper, we present a novel
                  methodology to simulta- neously solve for the sparse
                  signal and auxiliary parameters using a recently
                  proposed variable projection technique. Our main
                  contribution is to combine variable projection with
                  spar- sity promoting optimization, obtaining an
                  efficient algorithm for large-scale sparse
                  deconvolution problems. We demon- strate the
                  algorithm on a seismic imaging example.},
  keywords = {imaging, sparsity, optimization, variable projection},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ICASSP/2013/aravkin2013ICASSPssi/aravkin2013ICASSPssi.pdf}
}


@CONFERENCE{petrenko2013HPCSsaoc,
  author = {Art Petrenko and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Software acceleration of {CARP}, an iterative linear solver and preconditioner},
  organization = {HPCS},
  year = {2013},
  month = {06},
  abstract = {We present the results of software optimization of a
                  row-wise preconditioner (Component Averaged Row
                  Projections) for the method of conjugate gradients,
                  which is used to solve the diagonally banded
                  Helmholtz system representing frequency domain,
                  isotropic acoustic seismic wave simulation. We
                  demonstrate that in our application, a
                  preconditioner bound to one processor core and
                  accessing memory contiguously reduces execution time
                  by 7\% for matrices having on the order of 108
                  non-zeros. For reference we note that our C
                  implementation is over 80 times faster than the
                  corresponding code written for a high-level
                  numerical analysis language.},
  keywords = {HPCS, Helmholtz equation, Kaczmarz, software, wave propagation, frequency-domain},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/HPCS/2013/petrenko2013HPCSsaoc/petrenko2013HPCSsaoc.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/HPCS/2013/petrenko2013HPCSsaoc/petrenko2013HPCSsaoc_poster.pdf}
}


@CONFERENCE{herrmann2013KAUSTrse,
  author = {Felix J. Herrmann},
  title = {Randomized sampling in exploration seismology},
  booktitle = {KAUST},
  organization = {KAUST},
  year = {2013},
  month = {05},
  keywords = {KAUST, randomized sampling, exploration seismology},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/KAUST/2013/herrmann2013KAUSTrse/herrmann2013KAUSTrse_pres.pdf}
}


@CONFERENCE{herrmann2013SEGOMANrdw,
  author = {Felix J. Herrmann},
  title = {Recent developments in wave-equation based inversion technology},
  booktitle = {SEG Workshop on FWI; Oman},
  year = {2013},
  month = {04},
  keywords = {SEG, workshop, randomized sampling, exploration seismology, 3D, FWI},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2013/herrmann2013SEGOMANrdw/herrmann2013SEGOMANrdw_pres.pdf}
}


@CONFERENCE{herrmann2013SIAMdrfwi,
  author = {Felix J. Herrmann},
  title = {Dimensionality reduction in {FWI}},
  booktitle = {SIAM},
  year = {2013},
  month = {02},
  keywords = {SIAM, FWI, dimensionality reduction},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2013/herrmann2013SIAMdrfwi/herrmann2013SIAMdrfwi_pres.pdf}
}


@CONFERENCE{miao2013CSEGaospsa,
  author = {Lina Miao and Felix J. Herrmann},
  title = {Acceleration on sparse promoting seismic applications},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2013},
  month = {05},
  abstract = {Sparse promoting oriented problems are never new in
                  seismic applications. Back in 1970s, geophysicists
                  had well exploited the robustness of sparse
                  solutions. Moreover, with the emerging usage of
                  compressed sensing in recent years, sparse recovery
                  have been favored in dealing with 'curse of
                  dimensionality' in various seismic field
                  acquisition, data processing, and imaging
                  applications. Although sparsity has provided a
                  promising approach, solving for it presents a big
                  challenge. How to work efficiently with the
                  extremely large-scale seismic problem, and how to
                  improve the convergence rate reducing computation
                  time are most frequently asked questions in this
                  content. In this abstract, the author proposed a new
                  algorithm -- PQN$\ell_1$, trying to address those
                  questions. One example on seismic data processing is
                  included.},
  keywords = {CSEG, sparsity-promotion, SPG$\ell_1$, projected Quasi Newton},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2013/miao2013CSEGaospsa/miao2013CSEGaospsa.pdf}
}


@CONFERENCE{oghenekohwo2013CSEGnratld,
  author = {Felix Oghenekohwo and Felix J. Herrmann},
  title = {Assessing the need for repeatability in acquisition of time-lapse data},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2013},
  month = {05},
  abstract = {There are several factors that affect the repeatability
                  of 4D(time-lapse) seismic data. One of the most
                  significant factors is the repeatability of the
                  acquisition, particularly the locations of the
                  sources and receivers. It is important to repeat the
                  source-receiver locations, used during the baseline
                  survey, in the monitor or repeat survey. Also, it is
                  essential that the stacked data volumes used for
                  time-lapse analysis are created using the same
                  offset ranges for each survey. This condition is
                  crucial in order to be able to produce an image of
                  the same location over a period of time and enhances
                  proper reservoir characterization. The cost of
                  repeating the seismic acquisition is very expensive,
                  as often times, the receiver array has to be left at
                  the same location over the period for which the data
                  will be acquired. In other words, it is important to
                  repeat the acquisition geometry as much as
                  possible. In this talk, we investigate the results
                  of changing the acquisition geometry, by a random
                  placement of the receivers for both the baseline
                  surveys and newer (monitor) surveys. Results show
                  that we are still able to observe any time-lapse
                  effects from the proposed acquisition geometry. Our
                  experiments have been performed on a synthetic
                  model.},
  keywords = {acquisition, CSEG, time-lapse, migration},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2013/oghenekohwo2013CSEGnratld/oghenekohwo2013CSEGnratld.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2013/oghenekohwo2013CSEGnratld/oghenekohwo2013CSEGnratld_pres.pdf}
}


@CONFERENCE{Mansour11TRwmmw,
  author = {Hassan Mansour and Ozgur Yilmaz},
  title = {Weighted -$\ell_1$ minimization with multiple weighting sets},
  booktitle = {Proc. SPIE},
  year = {2011},
  month = {09},
  volume = {8138},
  pages = {813809-813809-13},
  abstract = {In this paper, we study the support recovery conditions
                  of weighted -$\ell_1$ minimization for signal
                  reconstruction from compressed sensing measurements
                  when multiple support estimate sets with different
                  accuracy are available. We identify a class of
                  signals for which the recovered vector from
                  -$\ell_1$ minimization provides an accurate support
                  estimate. We then derive stability and robustness
                  guarantees for the weighted -$\ell_1$ minimization
                  problem with more than one support estimate. We show
                  that applying a smaller weight to support estimate
                  that enjoy higher accuracy improves the recovery
                  conditions compared with the case of a single
                  support estimate and the case with standard, i.e.,
                  non-weighted,-$\ell_1$ minimization. Our theoretical
                  results are supported by numerical simulations on
                  synthetic signals and real audio signals.},
  keywords = {compressive sensing, optimization},
  notes = {TR-2011-07},
  doi = {10.1117/12.894165},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SPIE/2011/Mansour11TRwmmw/Mansour11TRwmmw.pdf}
}


@CONFERENCE{aravkin2011EAGEnspf,
  author = {Aleksandr Y. Aravkin and James V. Burke and Felix J. Herrmann and Tristan van Leeuwen},
  title = {A nonlinear sparsity promoting formulation and algorithm for full waveform inversion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2011},
  month = {05},
  abstract = {Full Waveform Inversion (FWI) is a computational
                  procedure to extract medium parameters from seismic
                  data. FWI is typically formulated as a nonlinear
                  least squares optimization problem, and various
                  regularization techniques are used to guide the
                  optimization because the problem is illposed. In
                  this paper, we propose a novel sparse regularization
                  which exploits the ability of curvelets to
                  efficiently represent geophysical images. We then
                  formulate a corresponding sparsity promoting
                  constrained optimization problem, which we call
                  Nonlinear Basis Pursuit Denoise (NBPDN) and present
                  an algorithm to solve this problem to recover medium
                  parameters. The utility of the NBPDN formulation and
                  efficacy of the algorithm are demonstrated on a
                  stylized cross-well experiment, where a sparse
                  velocity perturbation is recovered with higher
                  quality than the standard FWI formulation (solved
                  with LBFGS). The NBPDN formulation and algorithm can
                  recover the sparse perturbation even when the data
                  volume is compressed to 5 \% of the original
                  size using random superposition.},
  keywords = {EAGE, full-waveform inversion, optimization},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2011/aravkin11EAGEnspf/aravkin11EAGEnspf_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2011/aravkin11EAGEnspf/aravkin11EAGEnspf.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=50199}
}


@CONFERENCE{aravkin2012ICASSProbustb,
  author = {Aleksandr Y. Aravkin and Michael P. Friedlander and Tristan van Leeuwen},
  title = {Robust inversion via semistochastic dimensionality reduction},
  booktitle = {ICASSP},
  year = {2012},
  pages = {5245-5248},
  organization = {ICASSP},
  abstract = {We consider a class of inverse problems where it is
                  possible to aggregate the results of multiple
                  experiments. This class includes problems where the
                  forward model is the solution operator to linear
                  ODEs or PDEs. The tremendous size of such problems
                  motivates the use dimensionality reduction (DR)
                  techniques based on randomly mixing
                  experiments. These techniques break down, however,
                  when robust data-fitting formulations are used,
                  which are essential in cases of missing data,
                  unusually large errors, and systematic features in
                  the data unexplained by the forward model. We survey
                  robust methods within a statistical framework, and
                  propose a sampling optimization approach that allows
                  DR. The efficacy of the methods are demonstrated for
                  a large-scale seismic inverse problem using the
                  robust Student's t-distribution, where a useful
                  synthetic velocity model is recovered in the extreme
                  scenario of 60\% corrupted data. The sampling
                  approach achieves this recovery using 20\% of the
                  effort required by a direct robust approach.},
  keywords = {ICASSP},
  doi = {10.1109/ICASSP.2012.6289103},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ICASSP/2012/AravkinFriedlanderLeeuwen/AravkinFriedlanderLeeuwen.pdf }
}


@CONFERENCE{aravkin2011SIAMfwi,
  author = {Aleksandr Y. Aravkin and Felix J. Herrmann and Tristan van Leeuwen and James V. Burke and Xiang Li},
  title = {Full waveform inversion with compressive updates},
  booktitle = {SIAM},
  year = {2011},
  organization = {SIAM CS\&E 2011},
  abstract = {Full-waveform inversion relies on large multi-experiment
                  data volumes. While improvements in acquisition and
                  inversion have been extremely successful, the
                  current push for higher quality models reveals
                  fundamental shortcomings handling increasing problem
                  sizes numerically. To address this fundamental
                  issue, we propose a randomized
                  dimensionality-reduction strategy motivated by
                  recent developments in stochastic optimization and
                  compressive sensing. In this formulation
                  conventional Gauss-Newton iterations are replaced by
                  dimensionality-reduced sparse recovery problems with
                  source encodings.},
  keywords = {SLIM, full-waveform inversion},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2011/aravkin2011SIAMfwi/aravkin2011SIAMfwi.pdf}
}


@CONFERENCE{herrmann2011SEGffw,
  author = {Aleksandr Y. Aravkin and Felix J. Herrmann and Tristan van Leeuwen and Xiang Li},
  title = {Fast full-waveform inversion with compressive sensing},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2011},
  keywords = {SEG, SLIM, full-waveform inversion},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2011/HerrmannSEG2011fws/HerrmannSEG2011fws.pdf}
}


@CONFERENCE{aravkin2011EAGEspfwi,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and James V. Burke and Felix J. Herrmann},
  title = {Sparsity promoting formulations and algorithms for {FWI}},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2011},
  keywords = {EAGE, full-waveform inversion},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2011/Aravkin2011EAGEspfwi/Aravkin2011EAGEspfwi.pdf}
}


@CONFERENCE{aravkin2011ICIAMspf,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and James V. Burke and Felix J. Herrmann},
  title = {Sparsity promoting formulations and algorithms for {FWI}},
  booktitle = {ICIAM},
  year = {2011},
  organization = {ICIAM 2011},
  abstract = {Full Waveform Inversion (FWI) is a computational
                  procedure to extract medium parameters from seismic
                  data. FWI is typically formulated as a nonlinear
                  least squares optimization problem, and various
                  regularization techniques are used to guide the
                  optimization because the problem is ill-posed. We
                  propose a novel sparse regularization which exploits
                  the ability of curvelets to efficiently represent
                  geophysical images.  We then formulate a
                  corresponding sparsity promoting constrained
                  optimization problem, which we solve using an open
                  source algorithm. The techniques are applicable to
                  any inverse problem where sparsity modeling is
                  appropriate. We demonstrate the efficacy of the
                  formulation on a toy example (stylized cross-well
                  experiment) and on a realistic Seismic example
                  (partial Marmoussi model). We also discuss the
                  tradeoff between model fit and sparsity promotion,
                  with a view to extend existing techniques for linear
                  inverse problems to the case where the forward model
                  is nonlinear.},
  date-added = {2011-07-15},
  keywords = {SLIM,Presentation,Full-waveform inversion,Optimization},
  month = {07},
  note = {Presented at AMP Medical and Seismic Imaging, 2011, Vancouver BC.},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ICIAM/2011/aravkin2011ICIAMspf/aravkin2011ICIAMspf_pres.pdf}
}


@CONFERENCE{aravkin2011ICIAMrfwiu,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Robust {FWI} using {Student's} t-distribution},
  booktitle = {ICIAM},
  year = {2011},
  organization = {ICIAM 2011},
  abstract = {Iterative inversion algorithms require repeated
                  simulation of 3D time-dependent acoustic, elastic,
                  or electromagnetic wave fields, extending hundreds
                  of wavelengths and hundreds of periods. Also,
                  seismic data is rich in information at every
                  representable scale. Thus simulation-driven
                  optimization approaches to inversion impose great
                  demands on simulator efficiency and accuracy. While
                  computer hardware advances have been of critical
                  importance in bringing inversion closer to practical
                  application, algorithmic advances in simulator
                  methodology have been equally important. Speakers in
                  this two-part session will address a variety of
                  numerical issues arising in the wave simulation, and
                  in its application to inversion. },
  date-added = {2011-07-20},
  keywords = {SLIM, ICIAM, full-waveform inversion, optimization},
  month = {07},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ICIAM/2011/aravkin2011ICIAMrfwiu/aravkin2011ICIAMrfwiu.pdf}
}


@CONFERENCE{aravkin2011SEGrobust,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Robust full-waveform inversion using the {Student's} t-distribution},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2011},
  month = {09},
  volume = {30},
  pages = {2669-2673},
  organization = {SEG},
  abstract = {Full-waveform inversion (FWI) is a computational
                  procedure to extract medium parameters from seismic
                  data. Robust methods for FWI are needed to overcome
                  sensitivity to noise and in cases where modeling is
                  particularly poor or far from the real data
                  generating process. We survey previous robust
                  methods from a statistical perspective, and use this
                  perspective to derive a new robust method by
                  assuming the random errors in our model arise from
                  the Student's t-distribution. We show that in
                  contrast to previous robust methods, the new method
                  progres- sively down-weighs large outliers,
                  effectively ignoring them once they are large
                  enough. This suggests that the new method is more
                  robust and suitable for situations with very poor
                  data quality or modeling. Experiments show that the
                  new method recovers as well or better than previous
                  robust methods, and can recover models with quality
                  comparable to standard methods on noise-free data
                  when some of the data is completely corrupted, and
                  even when a marine acquisition mask is entirely
                  ignored in the modeling. The ability to ignore a
                  marine acquisition mask via robust FWI methods
                  offers an opportunity for stochastic optimization
                  methods in marine acquisition.},
  keywords = {SEG, full-waveform inversion, optimization},
  doi = {10.1190/1.3627747},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2011/aravkin11SEGrobust/aravkin11SEGrobust.pdf}
}


@CONFERENCE{aravkin2012ICASSPfastseis,
  author = {Aleksandr Y. Aravkin and Xiang Li and Felix J. Herrmann },
  title = {Fast seismic imaging for marine data},
  booktitle = {ICASSP},
  year = {2012},
  organization = {ICASSP},
  abstract = {Seismic imaging can be formulated as a linear inverse
                  problem where a medium perturbation is obtained via
                  minimization of a least-squares misfit
                  functional. The demand for higher resolution images
                  in more geophysically complex areas drives the need
                  to develop techniques that handle problems of
                  tremendous size with limited computational
                  resources. While seismic imaging is amenable to
                  dimensionality reduction techniques that collapse
                  the data volume into a smaller set of "super-shots",
                  these techniques break down for complex acquisition
                  geometries such as marine acquisition, where sources
                  and receivers move during acquisition. To meet these
                  challenges, we propose a novel method that combines
                  sparsity-promoting (SP) solvers with random subset
                  selection of sequential shots, yielding a SP
                  algorithm that only ever sees a small portion of the
                  full data, enabling its application to very
                  large-scale problems. Application of this technique
                  yields excellent results for a complicated
                  synthetic, which underscores the robustness of
                  sparsity promotion and its suitability for seismic
                  imaging.},
  keywords = {ICASSP},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ICASSP/2012/AravkinLiHerrmann/AravkinLiHerrmann.pdf}
}


@CONFERENCE{aravkin2012SEGST,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and Kenneth Bube and Felix J. Herrmann},
  title = {On non-uniqueness of the {Student's} t-formulation for linear inverse problems},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2012},
  month = {11},
  volume = {31},
  pages = {1-5},
  organization = {SEG},
  abstract = {We review the statistical interpretation of inverse
                  problem formulations, and the motivations for
                  selecting non-convex penalties for robust behaviour
                  with respect to measurement outliers or artifacts in
                  the data. An important downside of using non-convex
                  formulations such as the Student's t is the
                  potential for non-uniqueness, and we present a
                  simple example where the Student's t penalty can be
                  made to have many local minima by appropriately
                  selecting the degrees of freedom parameter. On the
                  other hand, the non-convexity of the Student's t is
                  precisely what gives it the ability to ignore
                  artifacts in the data. We explain this idea, and
                  present a stylized imaging experiment, where the
                  Student's t is able to recover a velocity
                  perturbation from data contaminated by a very
                  peculiar artifact --- data from a different velocity
                  perturbation. The performance of Student's t
                  inversion is investigated empirically for different
                  values of the degrees of freedom parameter, and
                  different initial conditions.},
  keywords = {Student's t, robust, non-convex, uniqueness, SEG},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2012/aravkin2012SEGST/aravkin2012SEGST.pdf},
  doi = {10.1190/segam2012-1558.1}
}


@CONFERENCE{aravkin2012EAGErobust,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and Henri Calandra and Felix J. Herrmann},
  title = {Source estimation for frequency-domain {FWI} with robust penalties},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2012},
  month = {06},
  abstract = {Source estimation is an essential component of full
                  waveform inversion. In the standard frequency domain
                  formulation, there is closed form solution for the
                  the optimal source weights, which can thus be
                  cheaply estimated on the fly. A growing body of work
                  underscores the importance of robust modeling for
                  data with large outliers or artifacts that are not
                  captured by the forward model. Effectively, the
                  least-squares penalty on the residual is replaced by
                  a robust penalty, such as Huber, Hybrid `1-`2 or
                  Student’s t. As we will demonstrate, it is essential
                  to use the same robust penalty for source
                  estimation. In this abstract, we present a general
                  approach to robust waveform inversion with robust
                  source estimation. In this general formulation,
                  there is no closed form solution for the optimal
                  source weights so we need to solve a scalar
                  optimization problem to obtain these weights.  We
                  can efficiently solve this optimization problem with
                  a Newton-like method in a few iterations. The
                  computational cost involved is of the same order as
                  the usual least-squares source estimation procedure.
                  We show numerical examples illustrating robust
                  source estimation and robust waveform inversion on
                  synthetic data with outliers.},
  keywords = {EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/aravkin2012EAGErobust/aravkin2012EAGErobust_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/aravkin2012EAGErobust/aravkin2012EAGErobust.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=59196}
}


@CONFERENCE{vandenberg07VONipo,
  author = {Ewout van den Berg and Michael P. Friedlander},
  title = {In pursuit of a root},
  year = {2007},
  month = {07},
  organization = {Von Neumann Symposium},
  quality = {1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/vonNeuman/2007/vandenberg07VONipo/vandenberg07VONipo.pdf}
}


@CONFERENCE{beyreuther2005SEGcot,
  author = {Moritz Beyreuther and Jamin Cristall and Felix J. Herrmann},
  title = {Computation of time-lapse differences with {3-D} directional frames},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2005},
  volume = {24},
  pages = {2488-2491},
  organization = {SEG},
  abstract = {We present an alternative method of extracting
                  production related differences from time-lapse
                  seismic data sets. Our method is not based on the
                  actual subtraction of the two data sets, risking the
                  enhancement of noise and introduction of artifacts
                  due to local phase rotation and slightly misaligned
                  events. Rather, it mutes events of the monitor
                  survey with respect to the baseline survey based on
                  the magnitudes of coefficients in a sparse and local
                  atomic decomposition. Our technique is demonstrated
                  to be an effective tool for enhancing the time-lapse
                  signal from surveys which have been cross-equalized.
                  {\copyright}2005 Society of Exploration
                  Geophysicists},
  keywords = {SLIM, SEG},
  doi = {10.1190/1.2148227},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2005/Beyreuther05SEGcot/Beyreuther05SEGcot.pdf}
}


@CONFERENCE{beyreuther2004EAGEcdo,
  author = {Moritz Beyreuther and Felix J. Herrmann and Jamin Cristall},
  title = {Curvelet denoising of {4-D} seismic},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2004},
  month = {06},
  abstract = {With burgeoning world demand and a limited rate of
                  discovery of new reserves, there is increasing
                  impetus upon the industry to optimize recovery from
                  already existing fields. 4D, or time-lapse, seismic
                  imaging is an emerging technology that holds great
                  promise to better monitor and optimise reservoir
                  production. The basic idea behind 4D seismic is that
                  when multiple 3D surveys are acquired at separate
                  calendar times over a producing field, the reservoir
                  geology will not change from survey to survey but
                  the state of the reservoir fluids will change. Thus,
                  taking the difference between two 3D surveys should
                  remove the static geologic contribution to the data
                  and isolate the time- varying fluid flow
                  component. However, a major challenge in 4D seismic
                  is that acquisition and processing differences
                  between 3D surveys often overshadow the changes
                  caused by fluid flow. This problem is compounded
                  when 4D effects are sought to be derived from
                  vintage 3D data sets that were not originally
                  acquired with 4D in mind. The goal of this study is
                  to remove the acquisition and imaging artefacts from
                  a 4D seismic difference cube using Curveket
                  processing techniques.},
  keywords = {SLIM, EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2004/Beyreuther04EAGEcdo/Beyreuther04EAGEcdo_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2004/Beyreuther04EAGEcdo/beyreuther2004EAGEcdo_paper.pdf},
  url2 = {https://circle.ubc.ca/bitstream/handle/2429/453/EAGE4D2004.pdf?sequence=1},
  url3 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=2323}
}


@CONFERENCE{herrmann2012SEGfwi,
  author = {Andrew J. Calvert and Ian Hanlon and Mostafa Javanmehri and Rajiv Kumar and Tristan van Leeuwen and Xiang Li and Brendan R. Smithyman and Eric Takam Takougang and Haneet Wason and Felix J. Herrmann},
  title = {{FWI} from the {West} {Coasts}: lessons learned from "Gulf of Mexico Imaging
	Challenges: What Can Full Waveform Inversion Achieve?"},
  booktitle = {SEG Workshop on FWI; Las Vegas},
  year = {2012},
  organization = {SEG},
  keywords = {workshop, FWI, SEG},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2012/herrmann2012SEGfwi/herrmann2012SEGfwi_pres.pdf}
}


@CONFERENCE{cristall2004CSEGcpa,
  author = {Jamin Cristall and Moritz Beyreuther and Felix J. Herrmann},
  title = {Curvelet processing and imaging: {4-D} adaptive subtraction},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2004},
  organization = {CSEG},
  abstract = {With burgeoning world demand and a limited rate of
                  discovery of new reserves, there is increasing
                  impetus upon the industry to optimize recovery from
                  already existing fields. 4D, or time-lapse, seismic
                  imaging holds great promise to better monitor and
                  optimise reservoir production. The basic idea behind
                  4D seismic is that when multiple 3D surveys are
                  acquired at separate calendar times over a producing
                  field, the reservoir geology will not change from
                  survey to survey but the state of the reservoir
                  fluids will change. Thus, taking the difference
                  between two 3D surveys should remove the static
                  geologic contribution to the data and isolate the
                  time-varying fluid flow component. However, a major
                  challenge in 4D seismic is that acquisition and
                  processing differences between 3D surveys often
                  overshadow the changes caused by fluid flow. This
                  problem is compounded when 4D effects are sought to
                  be derived from legacy 3D data sets that were not
                  originally acquired with 4D in mind. The goal of
                  this study is to remove the acquisition and imaging
                  artefacts from a 4D seismic difference cube using
                  Curvelet processing techniques.},
  keywords = {SLIM},
  month = {05},
  url = {http://www.cseg.ca/assets/files/resources/abstracts/2004/059S0201-Cristall_J_Curvelet_4D.pdf}
}


@CONFERENCE{dasilva2012EAGEprobingprecond,
  author = {Curt {Da Silva} and Felix J. Herrmann},
  title = {Matrix probing and simultaneous sources: a new approach for preconditioning the {Hessian}},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2012},
  month = {06},
  abstract = {Recent advances based on the mathematical understanding
                  of the Hessian as, under certain conditions, a
                  pseudo-differential operator have resulted in a new
                  preconditioner by L. Demanet et al. Basing their
                  approach on a suitable basis expansion for the
                  Hessian, by suitably 'probing' the Hessian,
                  i.e. applying the Hessian to a small number of
                  randomized model perturbations, one can obtain an
                  approximation to the inverse Hessian in an efficient
                  manner. Building upon this approach, we consider
                  this preconditioner in the context of least-squares
                  migration and Full Waveform Inversion and
                  specifically dimensionality reduction techniques in
                  these domains. By utilizing previous work in
                  simultaneous sources, we are able to develop an
                  efficient least-squares migration scheme which
                  recovers higher quality images and hence higher
                  quality search directions in the context of a
                  Gauss-Newton method for Full Waveform Inversion
                  while simultaneously avoiding inordinate amounts of
                  additional work.},
  keywords = {EAGE, matrix probing, pseudo-differential operator},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/dasilva2012EAGEprobingprecond/dasilva2012EAGEprobingprecond_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/dasilva2012EAGEprobingprecond/dasilva2012EAGEprobingprecond.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=59193}
}


@CONFERENCE{erlangga2009EAGEmwi,
  author = {Yogi A. Erlangga and Felix J. Herrmann},
  title = {Migration with implicit solvers for the time-harmonic {Helmholtz} equation},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2009},
  month = {06},
  abstract = {From the measured seismic data, the location and the
                  amplitude of reflectors can be determined via a
                  migration algorithm. Classically, following
                  Claerbout{\textquoteright}s imaging principle [2], a
                  reflector is located at the position where the
                  source{\textquoteright}s forward-propagated
                  wavefield correlates with the backward-propagated
                  wavefield of the receiver data. Lailly and Tarantola
                  later showed that this imaging principle is an
                  instance of inverse problems, with the associated
                  migration operator formulated via a least-squares
                  functional; see [6, 12, 13]. Furthermore, they
                  showed that the migrated image is associated with
                  the gradient of this functional with respect to the
                  image. If the solution of the least-squares
                  functional is done iteratively, the
                  correlation-based image coincides up to a constant
                  with the first iteration of a gradient method. In
                  practice, this migration is done either in the time
                  domain or in the frequency domain. In the
                  frequency-domain migration, the main bottleneck thus
                  far, which renders its full implementation to large
                  scale problems, is the lack of efficient solvers for
                  computing wavefields. Robust direct methods easily
                  run into excessive memory requirements as the size
                  of the problem increases. On the other hand,
                  iterative methods, which are less demanding in terms
                  of memory, suffered from lack of convergence. During
                  the past years, however, progress has been made in
                  the development of an efficient iterative method [4,
                  3] for the frequency-domain wavefield
                  computations. In this paper, we will show the
                  significance of this method (called MKMG) in the
                  context of the frequency-domain migration, where
                  multi-shot-frequency wavefields (of order of 10,000
                  related wavefields) need to be computed.},
  keywords = {EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2009/Erlangga09EAGEmwi/Erlangga09EAGEmwi_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2009/Erlangga09EAGEmwi/Erlangga09EAGEmwi.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=23955}
}


@CONFERENCE{erlangga2009SEGswi,
  author = {Yogi A. Erlangga and Felix J. Herrmann},
  title = {Seismic waveform inversion with {Gauss-Newton-Krylov} method},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  month = {10},
  volume = {28},
  pages = {2357-2361},
  organization = {SEG},
  abstract = {This abstract discusses an implicit implementation of
                  the Gauss-Newton method, used for the
                  frequency-domain full-waveform inversion, where the
                  inverse of the Hessian for the update is never
                  formed explicitly. Instead, the inverse of the
                  Hessian is computed approximately by a conjugate
                  gradient (CG) method, which only requires the action
                  of the Hessian on the CG search direction. This
                  procedure avoids an excessive computer storage,
                  usually needed for storing the Hessian, at the
                  expense of extra computational work in CG. An
                  effective preconditioner for the Hessian is
                  important to improve the convergence of CG, and
                  hence to reduce the overall computational work.},
  keywords = {SEG},
  doi = {10.1190/1.3255332},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/erlangga09SEGswi/erlangga09SEGswi_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/erlangga09SEGswi/erlangga09SEGswi.pdf}
}


@CONFERENCE{erlangga2008SEGaim,
  author = {Yogi A. Erlangga and Felix J. Herrmann},
  title = {An iterative multilevel method for computing wavefields in frequency-domain seismic inversion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  pages = {1957-1960},
  organization = {SEG},
  abstract = {We describe an iterative multilevel method for solving
                  linear systems representing forward modeling and
                  back propagation of wavefields in frequency-domain
                  seismic inversions. The workhorse of the method is
                  the so-called multilevel Krylov method, applied to a
                  multigrid-preconditioned linear system, and is
                  called multigrid-multilevel Krylov (MKMG) method.
                  Numerical experiments are presented for 2D Marmousi
                  synthetic model for a range of frequencies. The
                  convergence of the method is fast, and depends only
                  mildly on frequency. The method can be considered as
                  the first viable alternative to LU factorization,
                  which is practically prohibitive for 3D seismic
                  inversions.},
  keywords = {SLIM, SEG},
  doi = {10.1190/1.3059279},
  month = {11},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/erlangga08SEGaim/erlangga08SEGaim_pres.pdf},
  url = { https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/erlangga08SEGaim/erlangga08SEGaim.pdf }
}


@CONFERENCE{eso2008SEGira,
  author = {R. A. Eso and S. Napier and Felix J. Herrmann and D. W. Oldenburg},
  title = {Iterative reconstruction algorithm for non-linear operators},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  pages = {579-583},
  organization = {SEG},
  abstract = {Iterative soft thresholding of a models wavelet
                  coefficients can be used to obtain models that are
                  sparse with respect to a known basis function. We
                  generate sparse models for non-linear forward
                  operators by applying the soft thresholding operator
                  to the model obtained through a Gauss-Newton
                  iteration and apply the technique in a synthetic
                  2.5D DC resistivity crosswell tomographic example.},
  keywords = {SLIM, SEG},
  doi = {10.1190/1.3063719},
  month = {11},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/eso08SEGira/eso08SEGira.pdf }
}


@CONFERENCE{fomel2007ICASSPrepro,
  author = {Sergey Fomel and Gilles Hennenfent},
  title = {Reproducible computational experiments using scons},
  booktitle = {ICASSP},
  year = {2007},
  organization = {ICASSP},
  abstract = {SCons (from Software Construction) is a well-known open-
                  source program designed primarily for building
                  software. In this paper, we describe our method of
                  extending SCons for managing data processing flows
                  and reproducible computational experiments. We
                  demonstrate our usage of SCons with a simple
                  example.},
  keywords = {ICASSP},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ICASSP/2007/fomel07ICASSPrepro/fomel07ICASSPrepro.pdf }
}


@CONFERENCE{friedlander2009NUalssr,
  author = {Michael P. Friedlander},
  title = {Algorithms for large-scale sparse reconstruction},
  booktitle = {IEMS},
  year = {2009},
  address = {Northwestern University},
  organization = {IEMS Colloquim Speaker},
  keywords = {minimization, SLIM}
}


@CONFERENCE{friedlander2009VIETcsgpa,
  author = {Michael P. Friedlander},
  title = {Computing sparse and group-sparse approximations},
  booktitle = {VIET},
  year = {2009},
  address = {Hanoi, Vietnam},
  organization = {2009 High Performance Scientific Computing Conference},
  keywords = {minimization, SLIM}
}


@CONFERENCE{friedlander2008SIAMasa,
  author = {Michael P. Friedlander},
  title = {Active-set approaches to basis pursuit denoising},
  booktitle = {SIAM Optimization},
  year = {2008},
  organization = {SIAM Optimization},
  file = {http://www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf:PDF},
  keywords = {SLIM},
  month = {05},
  url = {http://www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf}
}


@CONFERENCE{friedlander2008SINBADafl,
  author = {Michael P. Friedlander},
  title = {Algorithms for large-scale sparse reconstruction},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {Many signal processing applications seek to approximate
                  a signal as a linear combination of only a few
                  elementary atoms drawn from a large collection. This
                  is known as sparse reconstruction, and the theory of
                  compressed sensing allows us to pose it as a
                  structured convex optimization problem. I will
                  discuss the role of duality in revealing some
                  unexpected and useful properties of these problems,
                  and will show how they can lead to practical,
                  large-scale algorithms. I will also describe some
                  applications of these algorithms.},
  keywords = {SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/friedlander2008SINBADafl/friedlander2008SINBADafl.pdf}
}


@CONFERENCE{friedlander2008WCOMasm,
  author = {Michael P. Friedlander and M. A. Saunders},
  title = {Active-set methods for basis pursuit},
  booktitle = {WCOM},
  year = {2008},
  organization = {West Coast Opitmization Meeting (WCOM)},
  abstract = {Many imaging and compressed sensing applications seek
                  sparse solutions to large under-determined
                  least-squares problems. The basis pursuit (BP)
                  approach minimizes the 1-norm of the solution, and
                  the BP denoising (BPDN) approach balances it against
                  the least-squares fit. The duals of these problems
                  are conventional linear and quadratic programs.  We
                  introduce a modified parameterization of the BPDN
                  problem and explore the effectiveness of active-set
                  methods for solving its dual. Our basic algorithm
                  for the BP dual unifies several existing algorithms
                  and is applicable to large-scale examples.},
  file = {http://www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf:PDF},
  month = {07},
  url = {http://www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf}
}


@CONFERENCE{frijlink2010EAGEcos,
  author = {M. O. Frijlink and Reza Shahidi and Felix J. Herrmann and R. G. van Borselen},
  title = {Comparison of standard adaptive subtraction and primary-multiple separation in the curvelet domain},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2010},
  month = {06},
  abstract = {In recent years, data-driven multiple prediction methods
                  and wavefield extrapolation methods have proven to
                  be powerful methods to attenuate multiples from data
                  acquired in complex 3-D geologic environments.
                  These methods make use of a two-stage approach,
                  where first the multiples (surface-related and / or
                  internal) multiples are predicted before they are
                  subtracted from the original input data in an
                  adaptively. The quality of these predicted multiples
                  often raises high expectations for the adaptive
                  subtraction techniques, but for various reasons
                  these expectations are not always met in
                  practice. Standard adaptive subtraction methods use
                  the well-known minimum energy criterion, stating
                  that the total energy after optimal multiple
                  attenuation should be minimal. When primaries and
                  multiples interfere, the minimum energy criterion is
                  no longer appropriate. Also, when multiples of
                  different orders interfere, adaptive energy
                  minimization will lead to a compromise between
                  different amplitudes corrections for the different
                  orders of multiples. This paper investigates the
                  performance of two multiple subtraction schemes for
                  a real data set that exhibits both interference
                  problems. Results from an adaptive subtraction in
                  the real curvelet domain, separating primaries and
                  multiples, are compared to those obtained using a
                  more conventional adaptive subtraction method in the
                  spatial domain.},
  keywords = {EAGE},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2010/frijlink10EAGEcos/frijlink10EAGEcos.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=39875}
}


@CONFERENCE{hennenfent2008SINBADnii2,
  author = {Gilles Hennenfent},
  title = {New insights into one-norm solvers from the {Pareto} curve},
  booktitle = {SINBAD},
  year = {2008},
  abstract = {Several geophysical ill-posed inverse problems are
                  successfully solved by promoting sparsity using
                  one-norm regularization. The practicality of this
                  approach depends on the effectiveness of the
                  one-norm solver used and on its robustness under
                  limited number of iterations. We propose an approach
                  to understand the behavior and evaluate the
                  performance of one-norm solvers. The technique
                  consists of tracking on a graph the data misfit
                  versus the one norm of successive iterates. By
                  comparing the solution paths to the Pareto curve, we
                  are able to assess the performance of the solvers
                  and the quality of the solutions. Such an assessment
                  is particularly relevant given the renewed interest
                  in one-norm regularization.},
  keywords = {SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/hennenfent2008SINBADnii2/hennenfent2008SINBADnii2.pdf}
}


@CONFERENCE{hennenfent2008SINBADsdw2,
  author = {Gilles Hennenfent},
  title = {Simply denoise: wavefield reconstruction via jittered undersampling},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We present a new discrete undersampling scheme designed
                  to favor wavefield reconstruction by
                  sparsity-promoting inversion with transform elements
                  that are localized in the Fourier domain. Our work
                  is motivated by empirical observations in the
                  seismic community, corroborated by recent results
                  from compressive sampling, which indicate favorable
                  (wavefield) reconstructions from random as opposed
                  to regular undersampling. As predicted by theory,
                  random undersampling renders coherent aliases into
                  harmless incoherent random noise, effectively
                  turning the interpolation problem into a much
                  simpler denoising problem. A practical requirement
                  of wavefield reconstruction with localized
                  sparsifying transforms is the control on the maximum
                  gap size. Unfortunately, random undersampling does
                  not provide such a control and the main purpose of
                  this paper is to introduce a sampling scheme, coined
                  jittered undersampling, that shares the benefits of
                  random sampling, while offering control on the
                  maximum gap size. Our contribution of jittered
                  sub-Nyquist sampling proofs to be key in the
                  formulation of a versatile wavefield
                  sparsity-promoting recovery scheme that follows the
                  principles of compressive sampling. After studying
                  the behavior of the jittered-undersampling scheme in
                  the Fourier domain, its performance is studied for
                  curvelet recovery by sparsity-promoting inversion
                  (CRSI). Our findings on synthetic and real seismic
                  data indicate an improvement of several decibels
                  over recovery from regularly-undersampled data for
                  the same amount of data collected.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/hennenfent2008SINBADsdw2/hennenfent2008SINBADsdw2.pdf}
}


@CONFERENCE{hennenfent07gradsem,
  author = {Gilles Hennenfent},
  title = {Reproducible research in computational (geo)sciences},
  booktitle = {Graduate seminar series},
  year = {2007},
  month = {01},
  organization = {Graduate Seminar Series},
  owner = {Shruti},
  quality = {1},
  timestamp = {2013.01.16},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/Misc/hennenfent07gradsem.pdf}
}


@CONFERENCE{hennenfent2007SINBADjdn,
  author = {Gilles Hennenfent},
  title = {Just denoise: nonlinear recovery from randomly sampled data},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {In this talk, we turn the interpolation problem of
                  coarsely-sampled data into a denoising problem. From
                  this point of view, we illustrate the benefit of
                  random sampling at sub-Nyquist rate over regular
                  sampling at the same rate. We show that, using
                  nonlinear sparsity-promoting optimization, coarse
                  random sampling may actually lead to significantly
                  better wavefield reconstruction than equivalent
                  regularly sampled data.},
  keywords = {Presentation, SINBAD, SLIM}
}


@CONFERENCE{hennenfent06SINBADscons,
  author = {Gilles Hennenfent},
  title = {Basic Processing flows with SCons},
  booktitle = {SINBAD},
  year = {2006},
  organization = {SINBAD},
  quality = {1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/hennenfent06SINBADscons/hennenfent06SINBADscons.pdf}
}


@CONFERENCE{hennenfent06SINBADssr,
  author = {Gilles Hennenfent},
  title = {A primer on stable signal recovery},
  booktitle = {SINBAD},
  year = {2006},
  organization = {SINBAD},
  quality = {1},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/hennenfent06SINBADssr/hennenfent06SINBADssr.pdf}
}


@CONFERENCE{hennenfent2006SINBADapo,
  author = {Gilles Hennenfent},
  title = {A primer on sparsity transforms: curvelets and wave atoms},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {During this presentation an introduction will be given
                  on the method of stable recovery from noisy and
                  incomplete data. Strong recovery conditions that
                  guarantee the recovery for arbitrary acquisition
                  geometries will be reviewed and numerical recovery
                  examples will be presented.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/hennenfent2006SINBADapo/hennenfent2006SINBADapo.pdf}
}


@CONFERENCE{hennenfent2006SINBADros,
  author = {Gilles Hennenfent},
  title = {Recovery of seismic data: practical considerations},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {We propose a method for seismic data interpolation based
                  on 1) the reformulation of the problem as a stable
                  signal recovery problem and 2) the fact that seismic
                  data is sparsely represented by curvelets. This
                  method does not require information on the seismic
                  velocities. Most importantly, this formulation
                  potentially leads to an explicit recovery
                  condition. We also propose a large-scale problem
                  solver for the l1-regularization minimization
                  involved in the recovery and successfully illustrate
                  the performance of our algorithm on 2D synthetic and
                  real examples.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/hennenfent2006SINBADros/hennenfent2006SINBADros.pdf}
}


@CONFERENCE{hennenfent2006SINBADtnf,
  author = {Gilles Hennenfent},
  title = {The {Nonuniform} {Fast} {Discrete} {Curvelet} {Transform} ({NFDCT})},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {The authors present an extension of the fast discrete
                  curvelet transform (FDCT) to nonuniformly sampled
                  data. This extension not only restores curvelet
                  compression rates for nonuniformly sampled data but
                  also removes noise and maps the data to a regular
                  grid.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/hennenfent2006SINBADtnf/hennenfent2006SINBADtnf.pdf}
}


@CONFERENCE{hennenfent2008SEGonri,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {One-norm regularized inversion: learning from the {Pareto} curve},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  organization = {SEG},
  abstract = {Geophysical inverse problems typically involve a trade
                  off between data misfit and some prior. Pareto
                  curves trace the optimal trade off between these two
                  competing aims. These curves are commonly used in
                  problems with two-norm priors where they are plotted
                  on a log-log scale and are known as L-curves. For
                  other priors, such as the sparsity-promoting one
                  norm, Pareto curves remain relatively
                  unexplored. First, we show how these curves provide
                  an objective criterion to gauge how robust one-norm
                  solvers are when they are limited by a maximum
                  number of matrix-vector products that they can
                  perform. Second, we use Pareto curves and their
                  properties to define and compute one-norm
                  compressibilities. We argue this notion is key to
                  understand one-norm regularized inversion. Third, we
                  illustrate the correlation between the one-norm
                  compressibility and the perfor- mance of Fourier and
                  curvelet reconstructions with sparsity promoting
                  inversion.},
  keywords = {SEG},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/hennenfent08SEGonri/hennenfent08SEGonri.pdf}
}


@CONFERENCE{hennenfent2007EAGEcrw,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Curvelet reconstruction with sparsity-promoting inversion: successes and challenges},
  booktitle = {EAGE Workshop on Curvelets, contourlets, seislets, … in seismic data processing - where are we and where are we going?},
  year = {2007},
  month = {06},
  abstract = {In this overview of the recent Curvelet Reconstruction
                  with Sparsity-promoting Inversion (CRSI) method, we
                  present our latest 2-D and 3-D interpolation results
                  on both synthetic and real datasets. We compare
                  these results to interpolated data using other
                  existing methods. Finally, we discuss the challenges
                  related to sparsity-promoting solvers for the
                  large-scale problems the industry faces.},
  keywords = {Presentation, SLIM, EAGE, workshop},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEcrw/hennenfent07EAGEcrw_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEcrw/hennenfent07EAGEcrw.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=7553}
}


@CONFERENCE{hennenfent2007EAGEisf,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Irregular sampling: from aliasing to noise},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2007},
  month = {06},
  abstract = {Seismic data is often irregularly and/or sparsely
                  sampled along spatial coordinates. We show that
                  these acquisition geometries are not necessarily a
                  source of adversity in order to accurately
                  reconstruct adequately-sampled data. We use two
                  examples to illustrate that it may actually be
                  better than equivalent regularly subsampled
                  data. This comment was already made in earlier works
                  by other authors. We explain this behavior by two
                  key observations. Firstly, a noise-free
                  underdetermined problem can be seen as a noisy
                  well-determined problem. Secondly, regularly
                  subsampling creates strong coherent acquisition
                  noise (aliasing) difficult to remove unlike the
                  noise created by irregularly subsampling that is
                  typically weaker and Gaussian-like.},
  keywords = {Presentation, SLIM, EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEisf/hennenfent07EAGEisf_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEisf/hennenfent07EAGEisf.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEisf/hennenfent07EAGEisf_WS.pdf},
  url3 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=6487}
}


@CONFERENCE{hennenfent2007SEGrsn,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Random sampling: new insights into the reconstruction of coarsely sampled wavefields},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  pages = {2575-2579},
  organization = {SEG},
  abstract = {In this paper, we turn the interpolation problem of
                  coarsely-sampled data into a denoising problem. From
                  this point of view, we illustrate the benefit of
                  random sampling at sub-Nyquist rate over regular
                  sampling at the same rate. We show that, using
                  nonlinear sparsity-promoting optimization, coarse
                  random sampling may actually lead to significantly
                  better wavefield reconstruction than equivalent
                  regularly sampled data. {\copyright}2007 Society of
                  Exploration Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.2793002},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/hennenfent07SEGrsn/hennenfent07SEGrsn_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/hennenfent07SEGrsn/hennenfent07SEGrsn.pdf }
}


@CONFERENCE{hennenfent2007SINBADrii,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Recent insights in $\ell_1$ solvers},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {During this talk, an overview is given on our work on
                  norm-one solvers as part of the DNOISE
                  project. Gilles will explain the ins and outs of our
                  iterative thresholding solver based on log cooling
                  while Felix will present the work of Michael
                  Friedlander "A Newton root-finding algorithms for
                  large-scale basis pursuit denoise". Both approaches
                  involve the solution of the basis pursuit problem
                  that seeks a minimum one-norm solution of an
                  underdetermined least-squares problem. Basis pursuit
                  denoise (BPDN) fits the least-squares problem only
                  approximately, and a single parameter determines a
                  curve that traces the trade-off between the
                  least-squares fit and the one-norm of the
                  solution. In the work of Friedlander, it is shown
                  show that the function that describes this curve is
                  convex and continuously differentiable over all
                  points of interest. They describe an efficient
                  procedure for evaluating this function and its
                  derivatives. As a result, they can compute arbitrary
                  points on this curve. Their method is suitable for
                  large-scale problems. Only matrix-vector operations
                  are required. This is joint work with Ewout van der
                  Berg and Michael P. Friedlander},
  keywords = {Presentation, SINBAD, SLIM}
}


@CONFERENCE{hennenfent2006SEGaos,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Application of stable signal recovery to seismic data interpolation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2006},
  pages = {2797-2801},
  organization = {SEG},
  abstract = {We propose a method for seismic data interpolation based
                  on 1) the reformulation of the problem as a stable
                  signal recovery problem and 2) the fact that seismic
                  data is sparsely represented by curvelets. This
                  method does not require information on the seismic
                  velocities. Most importantly, this formulation
                  potentially leads to an explicit recovery
                  condition. We also propose a large-scale problem
                  solver for the 1-regularization minimization
                  involved in the recovery and successfully illustrate
                  the performance of our algorithm on 2D synthetic and
                  real examples. {\copyright}2006 Society of
                  Exploration Geophysicists},
  keywords = {Presentation, SLIM, curvelets, interpolation, seismic data, regularization
	minimization, iterative thresholding, amplitude, SEG, continuity, fast transform},
  doi = {10.1190/1.2370105},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2006/hennenfent06SEGaos/hennenfent06SEGaos_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2006/hennenfent06SEGaos/hennenfent06SEGaos.pdf}
}


@CONFERENCE{hennenfent2005SEGscd,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Sparseness-constrained data continuation with frames: applications to missing traces and aliased signals in {2/3-D}},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2005},
  pages = {2162-2165},
  organization = {SEG},
  abstract = {We present a robust iterative sparseness-constrained
                  interpolation algorithm using 2-/3-D curvelet frames
                  and Fourier-like transforms that exploits continuity
                  along reflectors in seismic data. By choosing
                  generic transforms, we circumvent the necessity to
                  make parametric assumptions (e.g. through
                  linear/parabolic Radon or demigration) regarding the
                  shape of events in seismic data. Simulation and real
                  data examples for data with moderately sized gaps
                  demonstrate that our algorithm provides interpolated
                  traces that accurately reproduce the wavelet shape
                  as well as the AVO behavior. Our method also shows
                  good results for de-aliasing judged by the behavior
                  of the ($f-k$)-spectrum before and after
                  regularization. {\copyright}2005 Society of
                  Exploration Geophysicists},
  keywords = {Presentation, SEG, SLIM},
  doi = {10.1190/1.2148142},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2005/Hennenfent05SEGscd/Hennenfent05SEGscd_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2005/Hennenfent05SEGscd/Hennenfent05SEGscd.pdf }
}


@CONFERENCE{hennenfent2004SEGtta,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Three-term amplitude-versus-offset ({AVO}) inversion revisited by curvelet and wavelet transforms},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2004},
  pages = {211-214},
  organization = {SEG},
  abstract = {We present a new method to stabilize the three-term AVO
                  inversion using Curvelet and Wavelet
                  transforms. Curvelets are basis functions that
                  effectively represent otherwise smooth objects
                  having discontinuities along smooth curves. The
                  applied formalism explores them to make the most of
                  the continuity along reflectors in seismic
                  images. Combined with Wavelets, Curvelets are used
                  to denoise the data by penalizing high frequencies
                  and small contributions in the AVO-cube. This
                  approach is based on the idea that rapid amplitude
                  changes along the ray-parameter axis are most likely
                  due to noise. The AVO-inverse problem is linearized,
                  formulated and solved for all (x, z) at once. Using
                  densities and velocities of the Marmousi model to
                  define the fluctuations in the elastic properties,
                  the performance of the proposed method is studied
                  and compared with the smoothing along the
                  ray-parameter direction only. We show that our
                  method better approximates the true data after the
                  denoising step, especially when noise level
                  increases. {\copyright}2004 Society of Exploration
                  Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.1851201},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2004/Hennenfent04SEGtta/Hennenfent04SEGtta_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2004/Hennenfent04SEGtta/Hennenfent04SEGtta.pdf }
}


@CONFERENCE{hennenfent2005CSEGscs,
  author = {Gilles Hennenfent and Felix J. Herrmann and R. Neelamani},
  title = {Sparseness-constrained seismic deconvolution with curvelets},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2005},
  organization = {CSEG},
  abstract = {Continuity along reflectors in seismic images is used
                  via Curvelet representation to stabilize the
                  convolution operator inversion. The Curvelet
                  transform is a new multiscale transform that
                  provides sparse representations for images that
                  comprise smooth objects separated by piece-wise
                  smooth discontinuities (e.g. seismic images). Our
                  iterative Curvelet-regularized deconvolution
                  algorithm combines conjugate gradient-based
                  inversion with noise regularization performed using
                  non-linear Curvelet coefficient thresholding. The
                  thresholding operation enhances the sparsity of
                  Curvelet representations. We show on a synthetic
                  example that our algorithm provides improved
                  resolution and continuity along reflectors as well
                  as reduced ringing effect compared to the iterative
                  Wiener-based deconvolution approach.},
  keywords = {Presentation, SLIM},
  month = {05},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2005/Hennenfent05CSEGscs/Hennenfent05CSEGscs_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2005/Hennenfent05CSEGscs/Hennenfent05CSEGscs.pdf}
}


@CONFERENCE{hennenfent2005EAGEsdr,
  author = {Gilles Hennenfent and R. Neelamani and Felix J. Herrmann},
  title = {Seismic deconvolution revisited with curvelet frames},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2005},
  month = {06},
  abstract = {We propose an efficient iterative curvelet-regularized
                  deconvolution algorithm that exploits continuity
                  along reflectors in seismic images.  Curvelets are a
                  new multiscale transform that provides sparse
                  representations for images (such as seismic images)
                  that comprise smooth objects separated by piece-wise
                  smooth discontinuities. Our technique combines
                  conjugate gradient-based convolution operator
                  inversion with noise regularization that is
                  performed using non-linear curvelet coefficient
                  shrinkage (thresholding). The shrinkage operation
                  leverages the sparsity of curvelets
                  representations. Simulations demonstrate that our
                  algorithm provides improved resolution compared to
                  the traditional Wiener-based deconvolution
                  approach.},
  keywords = {Presentation, SLIM, EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2005/Hennenfent05EAGEsdr/Hennenfent05EAGEsdr_poster.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2005/Hennenfent05EAGEsdr/Hennenfent05EAGEsdr.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=1383}
}


@CONFERENCE{herrmann2003SPIEmsa,
  author = {Felix J. Herrmann},
  title = {Multifractional splines: application to seismic imaging},
  booktitle = {Proceedings of SPIE Technical Conference on Wavelets: Applications
	in Signal and Image Processing X},
  year = {2003},
  editor = {Michael A. Unser and Akram Aldroubi and Andrew F. Laine},
  volume = {5207},
  pages = {240-258},
  organization = {SPIE},
  abstract = {Seismic imaging commits itself to locating singularities
                  in the elastic properties of the
                  Earth{\textquoteright}s subsurface. Using the
                  high-frequency ray-Born approximation for scattering
                  from non-intersecting smooth interfaces, seismic
                  data can be represented by a generalized Radon
                  transform mapping the singularities in the medium to
                  seismic data. Even though seismic data are bandwidth
                  limited, signatures of the singularities in the
                  medium carry through this transform and its inverse
                  and this mapping property presents us with the
                  possibility to develop new imaging techniques that
                  preserve and characterize the singularities from
                  incomplete, bandwidth-limited and noisy data. In
                  this paper we propose a non-adaptive
                  Curvelet/Contourlet technique to image and preserve
                  the singularities and a data-adaptive Matching
                  Pursuit method to characterize these imaged
                  singularities by Multi-fractional Splines. This
                  first technique borrows from the ideas within the
                  Wavelet-Vaguelette/Quasi-SVD approach. We use the
                  almost diagonalization of the scattering operator to
                  approximately compensate for (i) the coloring of the
                  noise and hence facilitate estimation; (ii) the
                  normal operator itself. Results of applying these
                  techniques to seismic imaging are encouraging
                  although many open fundamental questions remain.},
  keywords = {Presentation, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SPIE/2003/herrmann2003SPIEmsa/herrmann2003SPIEmsa.pdf}
}


@CONFERENCE{Herrmann13NIPSrse,
  author = {Felix J. Herrmann},
  title = {Randomized sampling in exploration seismology},
  booktitle = {NIPS},
  year = {2013},
  timestamp = {2013.01.09},
  url = {http://techtalks.tv/talks/randomized-sampling-in-exploration-seismology/57871/},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/NIPS/2013/Herrmann13NIPSrse/Herrmann13NIPSrse.pdf}
}


@CONFERENCE{herrmann2012EAGEpmr,
  author = {Felix J. Herrmann},
  title = {Pass on the message: recent insights in large-scale sparse recovery},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2012},
  month = {06},
  abstract = {Data collection, data processing, and imaging in
                  exploration seismology increasingly hinge on
                  large-scale sparsity promoting solvers to remove
                  artifacts caused by efforts to reduce costs. We show
                  how the inclusion of a "message term" in the
                  calculation of the residuals improves the
                  convergence of these iterative solvers by breaking
                  correlations that develop between the model iterate
                  and the linear system that needs to be inverted. We
                  compare this message-passing scheme to
                  state-of-the-art solvers for problems in
                  missing-trace interpolation and in
                  dimensionality-reduced imaging with phase encoding.},
  keywords = {EAGE, message passing, sparse inversion},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/herrmann2012EAGEpmr/herrmann2012EAGEpmr_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/herrmann2012EAGEpmr/herrmann2012EAGEpmr.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=58935}
}


@CONFERENCE{herrmann2012SEGals,
  author = {Felix J. Herrmann},
  title = {Accelerated large-scale inversion with message passing},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2012},
  month = {11},
  volume = {31},
  pages = {1-6},
  organization = {SEG},
  abstract = {To meet current-day challenges, exploration seismology
                  increasingly relies on more and more sophisticated
                  algorithms that require multiple paths through all
                  data. This requirement leads to problems because the
                  size of seismic data volumes is increasing
                  exponentially, exposing bottlenecks in IO and
                  computational capability. To overcome these
                  bottlenecks, we follow recent trends in machine
                  learning and compressive sensing by proposing a
                  sparsity-promoting inversion technique that works on
                  small randomized subsets of data only. We boost the
                  performance of this algorithm significantly by
                  modifying a state-of-the-art l1-norm solver to
                  benefit from message passing, which breaks the build
                  up of correlations between model iterates and the
                  randomized linear forward model. We demonstrate the
                  performance of this algorithm on a toy
                  sparse-recovery problem and on a realistic
                  reverse-time-migration example with random source
                  encoding. The improvements in speed, memory use, and
                  output quality are truly remarkable.},
  keywords = {imaging, optimization, compressive sensing, SEG},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2012/herrmann2012SEGals/herrmann2012SEGals_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2012/herrmann2012SEGals/herrmann2012SEGals.pdf},
  doi = {10.1190/segam2012-0847.1}
}


@CONFERENCE{herrmann2012SSPamp,
  author = {Felix J. Herrmann},
  title = {Approximate message passing meets exploration seismology},
  booktitle = {2012 IEEE Statistical Signal Processing Workshop (SSP) (SSP'12)},
  year = {2012},
  address = {Ann Arbor, Michigan, USA},
  organization = {IEEE},
  abstract = {Data collection, data processing, and imaging in
                  exploration seismology increasingly hinge on
                  large-scale sparsity promoting solvers to remove
                  artifacts caused by efforts to reduce costs. We show
                  how the inclusion of a 'message term' in the
                  calculation of the residuals improves the
                  convergence of these iterative solvers by breaking
                  correlations that develop between the model iterate
                  and the linear system that needs to be inverted. We
                  compare this message-passing scheme to
                  state-of-the-art solvers for problems in
                  missing-trace interpolation and in
                  dimensionality-reduced imaging with phase en-
                  coding.},
  keywords = {exploration seismology, compressive sensing, transform-domain sparsity
	promotion, seismic imaging},
  month = {03},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SSP/2012/herrmann2012SSPamp/herrmann2012SSPamp_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SSP/2012/herrmann2012SSPamp/herrmann2012SSPamp.pdf}
}


@CONFERENCE{herrmann2012UW,
  author = {Felix J. Herrmann},
  title = {Compressive sensing and sparse recovery in exploration seismology},
  booktitle = {Talk at University of Wisconsin},
  year = {2012},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/Wisconsin/2012/herrmann2012UW/herrmann2012UW.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/Wisconsin/2012/herrmann2012UW/herrmann2012UW.pdf}
}


@CONFERENCE{herrmann11SLIMsummer2,
  author = {Felix J. Herrmann},
  title = {Lecture 2. {Gene} {Golub} {SIAM} {Summer} {School} {July} 4 - 15, 2011},
  booktitle = {SLIM},
  year = {2011},
  keywords = {Presentation},
  month = {08},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2011/herrmann11SLIMsummer2/herrmann11SLIMsummer2.pdf},
  timestamp = {2011.08.05}
}


@CONFERENCE{herrmann2011SLIMsummer1,
  author = {Felix J. Herrmann},
  title = {{Gene} {Golub} {SIAM} {Summer} {School} {July} 4 - 15, 2011},
  booktitle = {SLIM},
  year = {2011},
  keywords = {Presentation},
  month = {08},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2011/herrmann11SLIMsummer1/herrmann11SLIMsummer1.pdf},
  timestamp = {2011.08.05}
}


@CONFERENCE{herrmann2010EAGErss,
  author = {Felix J. Herrmann},
  title = {Randomized sampling strategies},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2010},
  month = {06},
  abstract = {Seismic exploration relies on the collection of massive
                  data volumes that are subsequently mined for
                  information during seismic processing. While this
                  approach has been extremely successful in the past,
                  the current trend towards higher quality images in
                  increasingly complicated regions continues to reveal
                  fundamental shortcomings in our workflows for
                  high-dimensional data volumes. Two causes can be
                  identified. First, there is the so-called
                  {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which puts disproportionate strain on
                  current acquisition and processing systems as the
                  size and desired resolution of our survey areas
                  continues to increase. Secondly, there is the recent
                  {\textquoteleft}{\textquoteleft}departure from
                  Moore{\textquoteright}s
                  law{\textquoteright}{\textquoteright} that forces us
                  to lower our expectations to compute ourselves out
                  of this curse of dimensionality. In this paper, we
                  offer a way out of this situation by a deliberate
                  randomized subsampling combined with
                  structure-exploiting transform-domain sparsity
                  promotion. Our approach is successful because it
                  reduces the size of seismic data volumes without
                  loss of information. As such we end up with a new
                  technology where the costs of acquisition and
                  processing are no longer dictated by the size of the
                  acquisition but by the transform-domain sparsity of
                  the end-product.},
  keywords = {Presentation, EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2010/herrmann10EAGErss/herrmann10EAGErss_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2010/herrmann10EAGErss/herrmann10EAGErss.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=39131}
}


@CONFERENCE{herrmann2010IRISsns,
  author = {Felix J. Herrmann},
  title = {Sub-{Nyquist} sampling and sparsity: getting more information from fewer samples},
  booktitle = {IRIS},
  year = {2010},
  abstract = {Many seismic exploration techniques rely on the
                  collection of massive data volumes. While this
                  approach has been extremely successful in the past,
                  current efforts toward higher resolution images in
                  increasingly complicated regions of the Earth
                  continue to reveal fundamental shortcomings in our
                  workflows. Chiefly amongst these is the so-called
                  {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which disproportionately strains current
                  acquisition and processing systems as the size and
                  desired resolution of our survey areas continues to
                  increase. In this presentation, we offer an
                  alternative sampling method leveraging recent
                  insights from compressive sensing towards seismic
                  acquisition and processing of severely under-sampled
                  data. The main outcome of this approach is a new
                  technology where acquisition and processing related
                  costs are no longer determined by overly stringent
                  sampling criteria, such as Nyquist. At the heart of
                  our approach lies randomized incoherent sampling
                  that breaks subsampling related interferences by
                  turning them into harmless noise, which we
                  subsequently remove by promoting transform-domain
                  sparsity. Now, costs no longer grow significantly
                  with resolution and dimensionality of the survey
                  area, but instead depend on transform-domain
                  sparsity only. Our contribution is twofold. First,
                  we demonstrate by means of carefully designed
                  numerical experiments that compressive sensing can
                  successfully be adapted to seismic
                  exploration. Second, we show that accurate recovery
                  can be accomplished for compressively sampled data
                  volumes sizes that exceed the size of conventional
                  transform-domain data volumes by only a small
                  factor. Because compressive sensing combines
                  transformation and encoding by a single linear
                  encoding step, this technology is directly
                  applicable to acquisition and to dimensionality
                  reduction during processing. In either case,
                  sampling, storage, and processing costs scale with
                  transform-domain sparsity. Many seismic exploration
                  techniques rely on the collection of massive data
                  volumes. While this approach has been extremely
                  successful in the past, current efforts toward
                  higher resolution images in increasingly complicated
                  regions of the Earth continue to reveal fundamental
                  shortcomings in our workflows. Chiefly amongst these
                  is the so-called
                  {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which disproportionately strains current
                  acquisition and processing systems as the size and
                  desired resolution of our survey areas continues to
                  increase. In this presentation, we offer an
                  alternative sampling method leveraging recent
                  insights from compressive sensing towards seismic
                  acquisition and processing of severely under-sampled
                  data. The main outcome of this approach is a new
                  technology where acquisition and processing related
                  costs are no longer determined by overly stringent
                  sampling criteria, such as Nyquist. At the heart of
                  our approach lies randomized incoherent sampling
                  that breaks subsampling related interferences by
                  turning them into harmless noise, which we
                  subsequently remove by promoting transform-domain
                  sparsity. Now, costs no longer grow significantly
                  with resolution and dimensionality of the survey
                  area, but instead depend on transform-domain
                  sparsity only. Our contribution is twofold. First,
                  we demonstrate by means of carefully designed
                  numerical experiments that compressive sensing can
                  successfully be adapted to seismic
                  exploration. Second, we show that accurate recovery
                  can be accomplished for compressively sampled data
                  volumes sizes that exceed the size of conventional
                  transform-domain data volumes by only a small
                  factor. Because compressive sensing combines
                  transformation and encoding by a single linear
                  encoding step, this technology is directly
                  applicable to acquisition and to dimensionality
                  reduction during processing. In either case,
                  sampling, storage, and processing costs scale with
                  transform-domain sparsity.},
  keywords = {Presentation},
  note = {Presented at the IRIS Workshop},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/IRIS/2010/herrmann2010IRISsns/herrmann2010IRISsns.pdf}
}


@CONFERENCE{herrmann2010MATHIAScssr,
  author = {Felix J. Herrmann},
  title = {Compressive sensing and sparse recovery in exploration seismology},
  booktitle = {MATHIAS},
  year = {2010},
  abstract = {During this presentation, I will talk about how recent
                  results from compressive sensing and sparse recovery
                  can be used to solve problems in exploration
                  seismology where incomplete sampling is ubiquitous.
                  I will also talk about how these ideas apply to
                  dimensionality reduction of full-waveform inversion
                  by randomly phase encoded sources.},
  keywords = {Presentation},
  note = {Presented at MATHIAS 2010 organized by Total SA. Paris},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/MATHIAS/2010/herrmann2010MATHIAScssr/herrmann2010MATHIAScssr.pdf}
}


@CONFERENCE{herrmann2009PIMScssr1,
  author = {Felix J. Herrmann},
  title = {Compressed sensing and sparse recovery in exploration seismology},
  booktitle = {PIMS},
  year = {2009},
  abstract = {In this course, I will present how recent results from
                  compressed sensing and sparse recovery apply to
                  exploration seismology. During the first lecture, I
                  will present the basic principles of compressive
                  sensing; the importance of random jitter sampling
                  and sparsifying transforms; and large-scale one-norm
                  solvers. I will discuss the application of these
                  techniques to missing trace interpolation. The
                  second lecture will be devoted to coherent signal
                  separation based on curveletdomain matched filtering
                  and Bayesian separation with sparsity
                  promotion. Applications of these techniques to the
                  primary-multiple wavefield-separation problem on
                  real data will be discussed as well. The third
                  lecture will be devoted towards sparse recovery in
                  seismic modeling and imaging and includes the
                  problem of preconditioning the imaging operators,
                  and the recovery from simultaneous source-acquired
                  data},
  keywords = {Presentation},
  note = {Lecture I presented at the PIMS Summer School on Seismic Imaging, Seattle},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/PIMS/2009/herrmann2009PIMScssr1/herrmann2009PIMScssr1.pdf}
}


@CONFERENCE{herrmann2009PIMScssr2,
  author = {Felix J. Herrmann},
  title = {Compressed sensing and sparse recovery in exploration seismology},
  booktitle = {PIMS},
  year = {2009},
  abstract = {In this course, I will present how recent results from
                  compressed sensing and sparse recovery apply to
                  exploration seismology. During the first lecture, I
                  will present the basic principles of compressive
                  sensing; the importance of random jitter sampling
                  and sparsifying transforms; and large-scale one-norm
                  solvers. I will discuss the application of these
                  techniques to missing trace interpolation. The
                  second lecture will be devoted to coherent signal
                  separation based on curveletdomain matched filtering
                  and Bayesian separation with sparsity
                  promotion. Applications of these techniques to the
                  primary-multiple wavefield-separation problem on
                  real data will be discussed as well. The third
                  lecture will be devoted towards sparse recovery in
                  seismic modeling and imaging and includes the
                  problem of preconditioning the imaging operators,
                  and the recovery from simultaneous source-acquired
                  data.},
  keywords = {Presentation},
  note = {Lecture II presented at the PIMS Summer School on Seismic Imaging, Seattle},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/PIMS/2009/herrmann2009PIMScssr2/herrmann2009PIMScssr2.pdf}
}


@CONFERENCE{herrmann2009PIMScssr3,
  author = {Felix J. Herrmann},
  title = {Compressed sensing and sparse recovery in exploration seismology},
  booktitle = {PIMS},
  year = {2009},
  abstract = {In this course, I will present how recent results from
                  compressed sensing and sparse recovery apply to
                  exploration seismology. During the first lecture, I
                  will present the basic principles of compressive
                  sensing; the importance of random jitter sampling
                  and sparsifying transforms; and large-scale one-norm
                  solvers. I will discuss the application of these
                  techniques to missing trace interpolation. The
                  second lecture will be devoted to coherent signal
                  separation based on curveletdomain matched filtering
                  and Bayesian separation with sparsity
                  promotion. Applications of these techniques to the
                  primary-multiple wavefield-separation problem on
                  real data will be discussed as well. The third
                  lecture will be devoted towards sparse recovery in
                  seismic modeling and imaging and includes the
                  problem of preconditioning the imaging operators,
                  and the recovery from simultaneous source-acquired
                  data.},
  keywords = {Presentation},
  note = {Lecture III presented at the PIMS Summer School on Seismic Imaging, Seattle},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/PIMS/2009/herrmann2009PIMScssr3/herrmann2009PIMScssr3.pdf}
}


@CONFERENCE{herrmann2009SEGcib,
  author = {Felix J. Herrmann},
  title = {Compressive imaging by wavefield inversion with group sparsity},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  month = {10},
  volume = {28},
  pages = {2337-2341},
  organization = {SEG},
  abstract = {Migration relies on multi-dimensional correlations
                  between source- and residual wavefields. These
                  multi-dimensional correlations are computationally
                  expensive because they involve operations with
                  explicit and full matrices that contain both
                  wavefields. By leveraging recent insights from
                  compressive sampling, we present an alternative
                  method where linear correlation-based imaging is
                  replaced by imaging via multidimensional
                  deconvolutions of compressibly sampled wavefields.
                  Even though this approach goes at the expense of
                  having to solve a sparsity-promotion recovery
                  program for the image, our wavefield inversion
                  approach has the advantage of reducing the system
                  size in accordance to transform-domain sparsity of
                  the image. Because seismic images also exhibit a
                  focusing of the energy towards zero offset, the
                  compressive-wavefield inversion itself is carried
                  out using a recent extension of one-norm solver
                  technology towards matrix-valued problems. These
                  so-called hybrid $(1,\,2)$-norm solvers allow us to
                  penalize pre-stack energy away from zero offset
                  while exploiting joint sparsity amongst near-offset
                  images. Contrary to earlier work to reduce modeling
                  and imaging costs through random phase-encoded
                  sources, our method compressively samples wavefields
                  in model space. This approach has several advantages
                  amongst which improved system-size reduction, and
                  more flexibility during subsequent inversions for
                  subsurface properties.},
  keywords = {Presentation, SEG},
  doi = {10.1190/1.3255328},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/herrmann09SEGcib/herrmann09SEGcib_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/herrmann09SEGcib/herrmann09SEGcib.pdf}
}


@CONFERENCE{herrmann2009SEGrpl,
  author = {Felix J. Herrmann},
  title = {Reflector-preserved lithological upscaling},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  month = {10},
  volume = {28},
  pages = {3466-3470},
  organization = {SEG},
  abstract = {By combining Percolation models with lithological
                  smoothing, we arrive at method for upscaling rock
                  elastic constants that preserves reflections. In
                  this approach, the Percolation model predicts sharp
                  onsets in the elastic moduli of sand-shale mixtures
                  when the shales reach a critical volume fraction. At
                  that point, the shale inclusions form a connected
                  cluster, and the macroscopic rock properties change
                  with the power-law growth of the cluster. This
                  switch-like nonlinearity preserves singularities,
                  and hence reflections, even if no sharp transition
                  exists in the lithology or if they are smoothed out
                  using standard upscaling procedures.},
  keywords = {Presentation, SEG},
  doi = {10.1190/1.3255582},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/herrmann09SEGrpl/herrmann09SEGrpl_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/herrmann09SEGrpl/herrmann09SEGrpl.pdf}
}


@CONFERENCE{herrmann2009SEGsns,
  author = {Felix J. Herrmann},
  title = {Sub-{Nyquist} sampling and sparsity: how to get more information from fewer samples},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  month = {10},
  volume = {28},
  pages = {3410-3415},
  organization = {SEG},
  abstract = {Seismic exploration relies on the collection of massive
                  data volumes that are subsequently mined for
                  information during seismic processing. While this
                  approach has been extremely successful in the past,
                  the current trend of incessantly pushing for higher
                  quality images in increasingly complicated regions
                  of the Earth continues to reveal fundamental
                  shortcomings in our workflows to handle massive
                  high-dimensional data volumes. Two causes can be
                  identified as the main culprits responsible for this
                  barrier. First, there is the so-called
                  {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which puts disproportionate strain on
                  current acquisition and processing systems as the
                  size and desired resolution of our survey areas
                  continues to increase. Secondly, there is the recent
                  {\textquoteleft}{\textquoteleft}departure from
                  Moore{\textquoteright}s
                  law{\textquoteright}{\textquoteright} that forces us
                  to lower our expectations to compute ourselves out
                  of this curse of dimensionality. In this paper, we
                  offer a way out of this situation by a deliberate
                  \emph{randomized} subsampling combined with
                  structure-exploiting transform-domain sparsity
                  promotion. Our approach is successful because it
                  reduces the size of seismic data volumes without
                  loss of information. Because of this size reduction
                  both impediments are removed and we end up with a
                  new technology where the costs of acquisition and
                  processing are no longer dictated by the \emph{size
                  of the acquisition} but by the transform-domain
                  \emph{sparsity} of the end-product after
                  processing.},
  keywords = {Presentation, SEG},
  doi = {10.1190/1.3255570},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/herrmann09SEGsns/herrmann09SEGsns_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/herrmann09SEGsns/herrmann09SEGsns.pdf}
}


@CONFERENCE{herrmann2008IONcsa,
  author = {Felix J. Herrmann},
  title = {Compressive sampling: a new paradigm for seismic data acquistion and processing?},
  booktitle = {ION},
  year = {2008},
  abstract = {Seismic data processing and imaging are firmly rooted in
                  the well-established paradigm of regular Nyquist
                  sampling. Faced with a typical uncooperative
                  environment, practitioners of seismic data
                  acquisition make all efforts to comply to this
                  theory by creating regularly-sampled seismic-data
                  volumes that are suitable for Fourier-based
                  processing flows. The current advent of new
                  alternative transform domains{\textendash}- such as
                  the sparsifying curvelet domain, where seismic data
                  is decomposed into localized, multiscale and
                  multidirectional plane waves{\textendash}- opens the
                  possibility to change this paradigm by no longer
                  combating sampling irregularity but by embracing
                  it. During this talk, we show that as long as
                  seismic data volumes permit a compressible
                  representation{\textendash}-i.e., data can be
                  represented as a superposition of relatively few
                  number of elementary waveforms{\textendash}- Nyquist
                  sampling is unnecessary pessimistic. So far, nothing
                  new, we all know from the work on Fourier- or other
                  transform-based seismic-data regularization
                  methodologies that wavefields can be recovered
                  accurately from sub-Nyquist samplings through some
                  sort of optimization procedure. What is new,
                  however, are recent insights from the field of
                  "compressive sampling", which dictate the conditions
                  that guarantee or, at least, in practice provide
                  conditions that favor sparsity-promoting recovery
                  from sub-Nyquist sampling. Random sub-sampling, or
                  to be more precise, jitter sub-sampling creates
                  favorable conditions for curvelet-based recovery. We
                  explain this phenomenon by arguing that this type of
                  sampling leads to noisy data, hence our slogan
                  "Simply denoise: wavefield reconstruction via
                  jittered undersampling", where we bank on separating
                  incoherent sub-sampling noise with curvelet-domain
                  sparsity promotion. During our presentation, we
                  introduce you to what curvelets are, why random
                  jitter sampling is important and why this opens a
                  pathway towards a new paradigm of curvelet-domain
                  seismic data processing. Our claims will be
                  supported by examples on synthetic and field
                  data. This is joint work with Gilles Hennenfent,
                  PhD. student at SLIM.},
  keywords = {ION, Presentation, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/ION/herrmann2008IONcsa/herrmann08ion_pres.pdf}
}


@CONFERENCE{herrmann2008SEGcdm3,
  author = {Felix J. Herrmann},
  title = {Curvelet-domain matched filtering},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  pages = {3643-3649},
  organization = {SEG},
  abstract = {Matching seismic wavefields lies at the heart of seismic
                  processing whether one is adaptively subtracting
                  multiples predictions or groundroll. In both cases,
                  the predictions are matched to the actual
                  to-be-separated wavefield components in the observed
                  data. The success of these wavefield matching
                  procedures depends on our ability to (i) control
                  possible overfitting, which may lead to accidental
                  removal of primary energy, (ii) handle data with
                  nonunique dips, and (iii) apply wavefield separation
                  after matching stably. In this paper, we show that
                  the curvelet transform allows us to address these
                  issues by imposing smoothness in phase space, by
                  using their capability to handle conflicting dips,
                  and by leveraging their ability to represent seismic
                  data sparsely.},
  keywords = {SEG, SLIM},
  doi = {10.1190/1.3064089},
  month = {11},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/Herrmann08SEGcdm3/Herrmann08SEGcdm3.pdf }
}


@CONFERENCE{herrmann2008SEGgbu,
  author = {Felix J. Herrmann},
  title = {Seismic noise: the good, the bad, \& the ugly},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  keywords = {Presentation, SEG, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/herrmann08SEGgbu/herrmann08SEGgbu.pdf}
}


@CONFERENCE{herrmann2008SINBADacd2,
  author = {Felix J. Herrmann},
  title = {Adaptive curvelet-domain primary-multiple separation},
  booktitle = {SINBAD},
  year = {2008},
  organization = {SINBAD},
  note = {SINBAD 2008},
  abstract = {In many exploration areas, successful separation of
                  primaries and multiples greatly determines the
                  quality of seismic imaging. Despite major advances
                  made by Surface-Related Multiple Elimination (SRME),
                  amplitude errors in the predicted multiples remain a
                  problem. When these errors vary for each type of
                  multiple differently (as a function of offset, time
                  and dip), these amplitude errors pose a serious
                  challenge for conventional least-squares matching
                  and for the recently introduced separation by
                  curvelet-domain thresholding. We propose a
                  data-adaptive method that corrects amplitude errors,
                  which vary smoothly as a function of location, scale
                  (frequency band) and angle. In that case, the
                  amplitudes can be corrected by an element-wise
                  curvelet-domain scaling of the predicted
                  multiples. We show that this scaling leads to a
                  successful estimation of the primaries, despite
                  amplitude, sign, timing and phase errors in the
                  predicted multiples. Our results on synthetic and
                  real data show distinct improvements over
                  conventional least-squares matching, in terms of
                  better suppression of multiple energy and
                  high-frequency clutter and better recovery of the
                  estimated primaries.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/herrmann2008SINBADacd2/herrmann2008SINBADacd2.pdf}
}


@CONFERENCE{herrmann2008SINBADfwr,
  author = {Felix J. Herrmann},
  title = {(De)-{Focused} wavefield reconstructions},
  booktitle = {SINBAD 2008},
  year = {2008},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/herrmann2008SINBADfwr/herrmann2008SINBADfwr.pdf}
}


@CONFERENCE{herrmann2008SINBADpsm,
  author = {Felix J. Herrmann},
  title = {Phase-space matched filtering and migration preconditioning},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {During this talk, I will report on new phase-space
                  regularization functionals defined in terms of
                  splines. This spline representation reduces the
                  dimensionality of estimating our phase-space matched
                  filter. We will discuss how this filter can be used
                  in migration preconditioning. This is joint work
                  with Christiaan Stolk.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/herrmann2008SINBADpsm/herrmann2008SINBADpsm.pdf}
}


@CONFERENCE{herrmann2008SINBADs2c,
  author = {Felix J. Herrmann},
  title = {{SINBAD} 2008 Consortium meeting},
  booktitle = {SINBAD 2008},
  year = {2008},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/herrmann2008SINBADs2c/herrmann2008SINBADs2c.pdf}
}


@CONFERENCE{herrmann2007AIPsit,
  author = {Felix J. Herrmann},
  title = {Seismic inversion through operator overloading},
  booktitle = {AIP},
  year = {2007},
  abstract = {Inverse problems in (exploration) seismology are known
                  for their large to very large scale. For instance,
                  certain sparsity-promoting inversion techniques
                  involve vectors that easily exceed 230 unknowns
                  while seismic imaging involves the construction and
                  application of matrix-free discretized operators
                  where single matrix-vector evaluations may require
                  hours, days or even weeks on large compute
                  clusters. For these reasons, software development in
                  this field has remained the domain of highly
                  technical codes programmed in low-level languages
                  with little eye for easy development, code reuse and
                  integration with (nonlinear) programs that solve
                  inverse problems. Following ideas from the
                  Symes{\textquoteright} Rice Vector Library and
                  Bartlett{\textquoteright}s C++ object-oriented
                  interface, Thyra, and Reduction/Transformation
                  operators (both part of the Trilinos software
                  package), we developed a software-development
                  environment based on overloading. This environment
                  provides a pathway from in-core prototype
                  development to out-of-core and MPI
                  {\textquoteright}production{\textquoteright} code
                  with a high level of code reuse. This code reuse is
                  accomplished by integrating the out-of-core and MPI
                  functionality into the dynamic object-oriented
                  programming language Python. This integration is
                  implemented through operator overloading and allows
                  for the development of a coordinate-free solver
                  framework that (i) promotes code reuse; (ii)
                  analyses the statements in an abstract syntax tree
                  and (iii) generates executable statements. In the
                  current implementation, we developed an interface to
                  generate executable statements for the out-of-core
                  unix-pipe based (seismic) processing package
                  RSF-Madagascar (rsf.sf.net). The modular design
                  allows for interfaces to other seismic processing
                  packages and to in-core Python packages such as
                  numpy. So far, the implementation overloads linear
                  operators and elementwise reduction/transformation
                  operators. We are planning extensions towards
                  nonlinear operators and integration with existing
                  (parallel) solver frameworks such as Trilinos.},
  keywords = {AIP, Presentation, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/AIP/2007/herrmann07AIPsit/herrmann07AIPsit.pdf}
}


@CONFERENCE{herrmann2007AIPssd,
  author = {Felix J. Herrmann},
  title = {Stable seismic data recovery},
  booktitle = {AIP},
  year = {2007},
  abstract = {In this talk, directional frames, known as curvelets,
                  are used to recover seismic data and images from
                  noisy and incomplete data. Sparsity and invariance
                  properties of curvelets are exploited to formulate
                  the recovery by a {\textquoteleft}1-norm promoting
                  program. It is shown that our data recovery approach
                  is closely linked to the recent theory of
                  {\textquoteleft}{\textquoteleft}compressive
                  sensing{\textquoteright}{\textquoteright} and can be
                  seen as a first step towards a nonlinear sampling
                  theory for wavefields. The second problem that will
                  be discussed concerns the recovery of the amplitudes
                  of seismic images in clutter. There, the invariance
                  of curvelets is used to approximately invert the
                  Gramm operator of seismic imaging. In the
                  high-frequency limit, this Gramm matrix corresponds
                  to a pseudo-differential operator, which is near
                  diagonal in the curvelet domain.In this talk,
                  directional frames, known as curvelets, are used to
                  recover seismic data and images from noisy and
                  incomplete data. Sparsity and invariance properties
                  of curvelets are exploited to formulate the recovery
                  by a l1-norm promoting program. It is shown that our
                  data recovery approach is closely linked to the
                  recent theory of
                  {\textquoteleft}{\textquoteleft}compressive
                  sensing{\textquoteright}{\textquoteright} and can be
                  seen as a first step towards a nonlinear sampling
                  theory for wavefields. The second problem that will
                  be discussed concerns the recovery of the amplitudes
                  of seismic images in clutter. There, the invariance
                  of curvelets is used to approximately invert the
                  Gramm operator of seismic imaging.  In the
                  high-frequency limit, this Gramm matrix corresponds
                  to a pseudo-differential operator, which is near
                  diagonal in the curvelet domain.},
  keywords = {AIP, Presentation, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/AIP/2007/herrmann07AIPssd/herrmann07AIPssd.pdf}
}


@CONFERENCE{herrmann2007AMScsi,
  author = {Felix J. Herrmann},
  title = {Compressive seismic imaging},
  booktitle = {AMS Von Neumann},
  year = {2007},
  abstract = {Seismic imaging involves the solution of an
                  inverse-scattering problem during which the energy
                  of (extremely) large data volumes is collapsed onto
                  the Earth's reflectors. We show how the ideas from
                  "compressive sampling" can alleviate this task by
                  exploiting the curvelet transform's "wavefront-set
                  detection" capability and "invariance" property
                  under wave propagation. First, a wavelet-vaguellete
                  technique is reviewed, where seismic amplitudes are
                  recovered from complete data by diagonalizing the
                  Gramm matrix of the linearized scattering
                  problem. Next, we show how the recovery of seismic
                  wavefields from incomplete data can be cast into a
                  compressive sampling problem, followed by a proposal
                  to compress wavefield extrapolation operators via
                  compressive sampling in the modal domain. During the
                  latter approach, we explicitly exploit the mutual
                  incoherence between the eigenfunctions of the
                  Helmholtz operator and the curvelet frame elements
                  that compress the extrapolated wavefield. This is
                  joint work with Gilles Hennenfent, Peyman Moghaddam,
                  Tim Lin, Chris Stolk and Deli Wang.},
  keywords = {AMS Von Neumann, Presentation, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/vonNeuman/2007/herrmann07AMScsi/herrmann07AMScsi_pres.pdf}
}


@CONFERENCE{herrmann2007COIPpti,
  author = {Felix J. Herrmann},
  title = {Phase transitions in explorations seismology: statistical mechanics meets information theory},
  booktitle = {COIP},
  year = {2007},
  abstract = {In this paper, two different applications of phase
                  transitions to exploration seismology will be
                  discussed. The first application concerns a phase
                  diagram ruling the recovery conditions for seismic
                  data volumes from incomplete and noisy data while
                  the second phase transition describes the behavior
                  of bi-compositional mixtures as a function of the
                  volume fraction. In both cases, the phase
                  transitions are the result of randomness in large
                  system of equations in combination with
                  nonlinearity. The seismic recovery problem from
                  incomplete data involves the inversion of a
                  rectangular matrix. Recent results from the field of
                  "compressive sensing" provide the conditions for a
                  successful recovery of functions that are sparse in
                  some basis (wavelet) or frame (curvelet)
                  representation, by means of a sparsity
                  ($\ell_1$-norm) promoting nonlinear program. The
                  conditions for a successful recovery depend on a
                  certain randomness of the matrix and on two
                  parameters that express the matrix{\textquoteright}
                  aspect ratio and the ratio of the number of nonzero
                  entries in the coefficient vector for the sparse
                  signal representation over the number of
                  measurements. It appears that the ensemble average
                  for the success rate for the recovery of the sparse
                  transformed data vector by a nonlinear sparsity
                  promoting program, can be described by a phase
                  transition, demarcating the regions for the two
                  ratios for which recovery of the sparse entries is
                  likely to be successful or likely to
                  fail. Consistent with other phase transition
                  phenomena, the larger the system the sharper the
                  transition. The randomness in this example is
                  related to the construction of the matrix, which for
                  the recovery of spike trains corresponds to the
                  randomly restricted Fourier matrix. It is shown,
                  that these ideas can be extended to the curvelet
                  recovery by sparsity-promoting inversion (CRSI). The
                  second application of phase transitions in
                  exploration seismology concerns the upscaling
                  problem. To counter the intrinsic smoothing of
                  singularities by conventional equivalent medium
                  upscaling theory, a percolation-based nonlinear
                  switch model is proposed. In this model, the
                  transport properties of bi-compositional mixture
                  models for rocks undergo a sudden change in the
                  macroscopic transport properties as soon as the
                  volume fraction of the stronger material reaches a
                  critical point. At this critical point, the stronger
                  material forms a connected cluster, which leads to
                  the creation of a cusp-like singularity in the
                  elastic moduli, which in turn give rise to specular
                  reflections. In this model, the reflectivity is no
                  longer explicitly due to singularities in the rocks
                  composition. Instead, singularities are created
                  whenever the volume fraction exceeds the critical
                  point. We will show that this concept can be used
                  for a singularity-preserved lithological upscaling.},
  keywords = {Presentation, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/COIP/2007/herrmann07COIPpti/herrmann07COIPpti_pres.pdf}
}


@CONFERENCE{herrmann2007CYBERsmc,
  author = {Felix J. Herrmann},
  title = {Seismology meets compressive sampling},
  booktitle = {Cyber},
  year = {2007},
  abstract = {Presented at Cyber-Enabled Discovery and Innovation:
                  Knowledge Extraction as a success story lecture. See
                  for more detail
                  https://www.ipam.ucla.edu/programs/cdi2007/},
  keywords = {Cyber, Presentation, SLIM},
  note = {Presented at the joint NSF-IPAM meeting. Los Angeles. October, 2007},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/Cyber/2007/herrmann07CYBERsmc/herrmann07CYBERsmc.pdf}
}


@CONFERENCE{herrmann2007EAGErdi,
  author = {Felix J. Herrmann},
  title = {Recent developments in curvelet-based seismic processing},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2007},
  month = {06},
  abstract = {Combinations of parsimonious signal representations with
                  nonlinear sparsity promoting programs hold the key
                  to the next-generation of seismic data processing
                  algorithms, since they allow for a formulation that
                  is stable w.r.t. noise \& incomplete data do not
                  require prior information on the velocity or
                  locations and dips of the events},
  keywords = {Presentation, SLIM, EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/herrmann07EAGErdi/herrmann07EAGErdi_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/herrmann07EAGErdi/herrmann07EAGErdi.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=7548}
}


@CONFERENCE{herrmann2007EAGEsrm,
  author = {Felix J. Herrmann},
  title = {Surface related multiple prediction from incomplete data},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2007},
  month = {06},
  abstract = {Incomplete data, unknown source-receiver signatures and
                  free-surface reflectivity represent challenges for a
                  successful prediction and subsequent removal of
                  multiples. In this paper, a new method will be
                  represented that tackles these challenges by
                  combining what we know about wavefield
                  (de-)focussing, by weighted
                  convolutions/correlations, and recently developed
                  curvelet-based recovery by sparsity-promoting
                  inversion (CRSI). With this combination, we are able
                  to leverage recent insights from wave physics
                  towards a nonlinear formulation for the
                  multiple-prediction problem that works for
                  incomplete data and without detailed knowledge on
                  the surface effects.},
  keywords = {Presentation, SLIM, EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/herrmann07EAGEsrm/herrmann07EAGEsrm_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/herrmann07EAGEsrm/herrmann07EAGEsrm.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=6496}
}


@CONFERENCE{herrmann2007PIMScsm,
  author = {Felix J. Herrmann},
  title = {Compressive sampling meets seismic imaging},
  booktitle = {PIMS},
  year = {2007},
  keywords = {PIMS, Presentation, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/PIMS/2007/herrmann07PIMScsm/herrmann07PIMScsm_pres.pdf}
}


@CONFERENCE{herrmann2007SEGmpf,
  author = {Felix J. Herrmann},
  title = {Multiple prediction from incomplete data with the focused curvelet transform},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  pages = {2505-2600},
  abstract = {Incomplete data represents a major challenge for a
                  successful prediction and subsequent removal of
                  multiples. In this paper, a new method will be
                  represented that tackles this challenge in a
                  two-step approach. During the first step, the
                  recenly developed curvelet-based recovery by
                  sparsity-promoting inversion (CRSI) is applied to
                  the data, followed by a prediction of the
                  primaries. During the second high-resolution step,
                  the estimated primaries are used to improve the
                  frequency content of the recovered data by combining
                  the focal transform, defined in terms of the
                  estimated primaries, with the curvelet
                  transform. This focused curvelet transform leads to
                  an improved recovery, which can subsequently be used
                  as input for a second stage of multiple prediction
                  and primary-multiple separation.},
  keywords = {SEG, Presentation, SLIM},
  doi = {10.1190/1.2792987},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/herrmann07SEGmpf/herrmann07SEGmpf_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/herrmann07SEGmpf/herrmann07SEGmpf.pdf}
}


@CONFERENCE{herrmann2007SINBADcwe,
  author = {Felix J. Herrmann},
  title = {Compressed wavefield extrapolation},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {An explicit algorithm for the extrapolation of one-way
                  wavefields is proposed which combines recent
                  developments in information theory and theoretical
                  signal processing with the physics of wave
                  propagation. Because of excessive memory
                  requirements, explicit formulations for wave
                  propagation have proven to be a challenge in 3-D. By
                  using ideas from
                  {\textquoteleft}{\textquoteleft}compressed
                  sensing{\textquoteright}{\textquoteright}, we are
                  able to formulate the (inverse) wavefield
                  extrapolation problem on small subsets of the data
                  volume, thereby reducing the size of the
                  operators. According to compressed sensing theory,
                  signals can successfully be recovered from an
                  incomplete set of measurements when the measurement
                  basis is incoherent with the representation in which
                  the wavefield is sparse. In this new approach, the
                  eigenfunctions of the Helmholtz operator are
                  recognized as a basis that is incoherent with
                  curvelets that are known to compress seismic
                  wavefields. By casting the wavefield extrapolation
                  problem in this framework, wavefields can
                  successfully be extrapolated in the modal domain via
                  a computationally cheaper operation. A proof of
                  principle for the
                  {\textquoteleft}{\textquoteleft}compressed
                  sensing{\textquoteright}{\textquoteright} method is
                  given for wavefield extrapolation in 2-D. The
                  results show that our method is stable and produces
                  identical results compared to the direct application
                  of the full extrapolation operator. This is joint
                  work with Tim Lin.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2007/herrmann2007SINBADcwe/herrmann2007SINBADcwe.pdf}
}


@CONFERENCE{herrmann2007SINBADfrw,
  author = {Felix J. Herrmann},
  title = {Focused recovery with the curvelet transform},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {Incomplete data represents a major challenge for a
                  successful prediction and subsequent removal of
                  multiples. In this paper, a new method will be
                  represented that tackles this challenge in a
                  two-step approach. During the first step, the
                  recently developed curvelet-based recovery by
                  sparsity-promoting inversion (CRSI) is applied to
                  the data, followed by a prediction of the
                  primaries. During the second high-resolution step,
                  the estimated primaries are used to improve the
                  frequency content of the recovered data by combining
                  the focal transform, defined in terms of the
                  estimated primaries, with the curvelet
                  transform. This focused curvelet transform leads to
                  an improved recovery, which can subsequently be used
                  as input for a second stage of multiple prediction
                  and primary-multiple separation. This is joint work
                  with Deli Wang and Gilles Hennenfent.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/sites/data/Papers/Herrmann2007SINBADfoc.pdf}
}


@CONFERENCE{herrmann2007SINBADrdi2,
  author = {Felix J. Herrmann},
  title = {Recent developments in primary-multiple separation},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {In this talk, we present a novel primary-multiple
                  separation scheme which makes use of the sparsity of
                  both primaries and multiples in a transform domain,
                  such as the curvelet transform, to provide estimates
                  of each. The proposed algorithm utilizes seismic
                  data as well as the output of a preliminary step
                  that provides (possibly) erroneous predictions of
                  the multiples. The algorithm separates the signal
                  components, i.e., the primaries and multiples, by
                  solving an optimization problem that assumes noisy
                  input data and can be derived from a Bayesian
                  perspective. More precisely, the optimization
                  problem can be arrived at via an assumption of a
                  weighted Laplacian distribution for the primary and
                  multiple coefficients in the transform domain and of
                  white Gaussian noise contaminating both the seismic
                  data and the preliminary prediction of the
                  multiples, which both serve as input to the
                  algorithm. Time permitted, we will also briefly
                  discuss a propasal for adaptive curvelet-domain
                  matched filtering. This is joint work with Deli
                  Wang, Rayan Saaba, {\o}zgur Yilmaz and Eric
                  Verschuur.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2007/herrmann07SINBADrdi2/herrmann07SINBADrdi2_pres.pdf}
}


@CONFERENCE{herrmann2007SINBADsia2,
  author = {Felix J. Herrmann},
  title = {Seismic image amplitude recovery},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {In this talk, we recover the amplitude of a seismic
                  image by approximating the normal
                  (demigration-migration) operator. In this
                  approximation, we make use of the property that
                  curvelets remain invariant under the action of the
                  normal operator. We propose a seismic amplitude
                  recovery method that employs an eigenvalue like
                  decomposition for the normal operator using
                  curvelets as eigen-vectors. Subsequently, we propose
                  an approximate nonlinear singularity-preserving
                  solution to the least-squares seismic imaging
                  problem with sparseness in the curvelet domain and
                  spatial continuity constraints. Our method is tested
                  with a reverse-time
                  {\textquoteright}wave-equation{\textquoteright}
                  migration code simulating the acoustic wave equation
                  on the SEG-AA salt model. This is joint work with
                  Peyman Moghaddam and Chris Stolk (University of
                  Twente)},
  keywords = {Presentation, SINBAD, SLIM}
}


@CONFERENCE{herrmann2007SLIMfsd,
  author = {Felix J. Herrmann},
  title = {From seismic data to the composition of rocks: an interdisciplinary and multiscale approach to exploration seismology},
  booktitle = {Berkhout{\textquoteright}s valedictory address: the conceptual approach of understanding},
  year = {2007},
  abstract = {In this essay, a nonlinear and multidisciplinary
                  approach is presented that takes seismic data to the
                  composition of rocks. The presented work has deep
                  roots in the
                  {\textquoteleft}gedachtengoed{\textquoteright}
                  (philosophy) of Delphi spearheaded by Guus
                  Berkhout. Central themes are multiscale,
                  object-orientation and a multidisciplinary
                  approach.},
  keywords = {SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/Misc/herrmann07SLIMfsd/herrmann07SLIMfsd.pdf }
}


@CONFERENCE{herrmann2006SINBADapo1,
  author = {Felix J. Herrmann},
  title = {A primer on sparsity transforms: curvelets and wave atoms},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {During this presentation an overview will be given on
                  the different sparsity transforms that are used at
                  SLIM. Emphasis will be on two directional and
                  multiscale wavelet transforms, namely the curvelet
                  and the recently introduced wave-atom
                  transforms. The main properties of these transforms
                  will be listed and their performance on seismic data
                  will be discussed.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/hennenfent2006SINBADapo/hennenfent2006SINBADapo.pdf}
}


@CONFERENCE{herrmann2006SINBADapow,
  author = {Felix J. Herrmann},
  title = {A primer on weak conditions for stable recovery},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {During this presentation an introduction will be given
                  on the method of stable recovery from noisy and
                  incomplete data. Weak recovery conditions that
                  guarantee the recovery for typical acquisition
                  geometries will be reviewed and numerical recovery
                  examples will be presented. The advantage of these
                  weak conditions is that they are less pessimistic
                  and {\textquoteleft}verifiable{\textquoteright} or
                  very large-scale acquisition geometries.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/herrmann2006SINBADapow/herrmann2006SINBADapow.pdf}
}


@CONFERENCE{herrmann2006SINBADmpf,
  author = {Felix J. Herrmann},
  title = {Multiple prediction from incomplete data},
  booktitle = {SINBAD 2006},
  year = {2006},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/herrmann2006SINBADmpf/herrmann2006SINBADmpf.pdf}
}


@CONFERENCE{herrmann2006SINBADom,
  author = {Felix J. Herrmann},
  title = {Opening meeting},
  booktitle = {SINBAD 2006},
  year = {2006},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/herrmann2006SINBADom/herrmann2006SINBADom.pdf}
}


@CONFERENCE{herrmann2006SINBADsac,
  author = {Felix J. Herrmann},
  title = {Sparsity- and continuity-promoting seismic image recovery with curvelets},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {A nonlinear singularity-preserving solution to seismic
                  image recovery with sparseness and continuity
                  constraints is proposed. The method explicitly
                  explores the curvelet transform as a directional
                  frame expansion that, by virtue of its sparsity on
                  seismic images and its invariance under the Hessian
                  of the linearized imaging problem, allows for a
                  stable recovery of the migration amplitudes from
                  noisy data. The method corresponds to a
                  preconditioning that corrects the amplitudes during
                  a post-processing step. The solution is formulated
                  as a nonlinear optimization problem where sparsity
                  in the curvelet domain as well as continuity along
                  the imaged reflectors are jointly promoted. To
                  enhance sparsity, the l1-norm on the curvelet
                  coefficients is minimized while continuity is
                  promoted by minimizing an anisotropic diffusion norm
                  on the image. The performance of the recovery scheme
                  is evaluated with
                  {\textquoteright}wave-equation{\textquoteright}
                  migration code on a synthetic dataset. This is joint
                  work with Peyman Moghaddam.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/herrmann2006SINBADsac/herrmann2006SINBADsac.pdf}
}


@CONFERENCE{herrmann2006SINBADsra,
  author = {Felix J. Herrmann},
  title = {Stable recovery and separation of seismic data},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {During this presentation an overview will be given on
                  how seismic data regularization and separation
                  problems can be cast into the framework of stable
                  signal recovery. It is shown that the successful
                  solution of these two problems depends on the
                  existence of signal expansions that are
                  compressible. Preliminary examples will be shown.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/herrmann2006SINBADsra/herrmann2006SINBADsra.pdf}
}


@CONFERENCE{herrmann2004CSEGcia,
  author = {Felix J. Herrmann},
  title = {Curvelet imaging and processing: an overview},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2004},
  organization = {CSEG},
  abstract = {In this paper an overview is given on the application of
                  directional basis functions, known under the name
                  Curvelets/Contourlets, to various aspects of seismic
                  processing and imaging. Key conceps in the approach
                  are the use of (i) directional basis functions that
                  localize in both domains (e.g. space and angle);
                  (ii) non-linear estimation, which corresponds to
                  localized muting on the coefficients, possibly
                  supplemented by constrained optimization (iii)
                  invariance of the basis functions under the imaging
                  operators. We will discuss applications that include
                  multiple and ground roll removal;
                  sparseness-constrained least-squares migration and
                  the computation of 4-D difference cubes.},
  keywords = {Presentation, SLIM},
  month = {05},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2004/Herrmann04CSEGcia/Herrmann04CSEGcia.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2004/Herrmann04CSEGcia/Herrmann04CSEGcia_paper.pdf}
}


@CONFERENCE{herrmann2003SEGoiw,
  author = {Felix J. Herrmann},
  title = {"Optimal" imaging with curvelets},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2003},
  volume = {22},
  pages = {997-1000},
  abstract = {In this paper we present a non-linear edge-preserving
                  solution to linear inverse scattering problems based
                  on optimal basis-function decompositions. Optimality
                  of the basis functions allow us to (i) reduce the
                  dimensionality of the inverse problem; (ii) devise
                  non-linear thresholding operators that approximate
                  minimax (minimize the maximal mean square error
                  given the worst possible prior) and that
                  significantly improve the signal-to-noise ratio on
                  the image. We present a reformulation of the
                  standard generalized least-squares formulation of
                  the seismic inversion problem into a formulation
                  based on thresholding, where the singular values,
                  vectors and linear estimators are replaced by
                  quasi-singular values, basis-functions and
                  thresholding. To limit the computational burden we
                  use a Monte-Carlo sampling method to compute the
                  quasi-singular values. With the proposed method, we
                  aim to significantly improve the signal-to-noise
                  ratio (SNR) on the model space and hence the
                  resolution of the seismic image. While classical
                  Tikhonov-regularized methods only gain the
                  square-root of the SNR on the data for the SNR on
                  the model our method scales almost linearly.  This
                  significant improvement of the SNR allows us to
                  discern events at high frequencies which would
                  normally be in the noise.},
  keywords = {Presentation, SEG, SLIM},
  doi = {10.1190/1.1818117},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2003/Herrmann03SEGoiw/Herrmann03SEGoiw_pres.pdf}
}


@CONFERENCE{herrmann2001EAGEsas,
  author = {Felix J. Herrmann},
  title = {Scaling and seismic reflectivity: implications of scaling on {AVO}},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2001},
  abstract = {AVO analysis of seismic data is based on the assumption
                  that transitions in the earth consist of jump
                  discontinuities only. Generalization of these
                  transitions to more realistic transitions shows a
                  drastic change in observed AVO behavior, especially
                  for the large angles currently attained by
                  increasing cable lengths. We propose a simple
                  ities. After renormalization, the inverted
                  fluctuations regain their relative magnitudes which,
                  due to the scaling, may have been significantly
                  distorted.},
  keywords = {SLIM},
  month = {06},
}


@CONFERENCE{herrmann2011ICIAMconvexcompfwi,
  author = {Felix J. Herrmann and Aleksandr Y. Aravkin and Tristan van Leeuwen and Xiang Li},
  title = {{FWI} with sparse recovery: a convex-composite approach},
  booktitle = {ICIAM},
  year = {2011},
  organization = {ICIAM 2011},
  abstract = {Iterative inversion algorithms require repeated
                  simulation of 3D time-dependent acoustic, elastic,
                  or electromagnetic wave fields, extending hundreds
                  of wavelengths and hundreds of periods. Also,
                  seismic data is rich in information at every
                  representable scale. Thus simulation-driven
                  optimization approaches to inversion impose great
                  demands on simulator efficiency and accuracy. While
                  computer hardware advances have been of critical
                  importance in bringing inversion closer to practical
                  application, algorithmic advances in simulator
                  methodology have been equally important. Speakers in
                  this two-part session will address a variety of
                  numerical issues arising in the wave simulation, and
                  in its application to inversion.},
  date-added = {2011-07-20},
  keywords = {ICIAM, full-waveform inversion, optimization},
  month = {07},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ICIAM/2011/herrmann2011ICIAMconvexcompfwi/herrmann2011ICIAMconvexcompfwi.pdf}
}


@CONFERENCE{herrmann2011SLRAfwi,
  author = {Felix J. Herrmann and Aleksandr Y. Aravkin and Xiang Li and Tristan van Leeuwen},
  title = {Full waveform inversion with compressive updates},
  booktitle = {SLRA},
  year = {2011},
  organization = {Sparse and Low Rank Approximation 2011},
  abstract = {Full-waveform inversion relies on large multi-experiment
                  data volumes. While improvements in acquisition and
                  inversion have been extremely successful, the
                  current push for higher quality models reveals
                  fundamental shortcomings handling increasing problem
                  sizes numerically. To address this fundamental
                  issue, we propose a randomized
                  dimensionality-reduction strategy motivated by
                  recent developments in stochastic optimization and
                  compressive sensing. In this formulation
                  conventional Gauss-Newton iterations are replaced by
                  dimensionality-reduced sparse recovery problems with
                  source encodings.},
  keywords = {full-waveform inversion},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SLRA/2011/herrmann2011SLRAfwi/herrmann2011SLRAfwi.pdf}
}


@CONFERENCE{herrmann2006SINBADpms,
  author = {Felix J. Herrmann and Urs Boeniger and D. J. Verschuur},
  title = {Primary-multiple separation by curvelet frames},
  booktitle = {SINBAD 2006},
  year = {2006},
  volume = {170},
  pages = {781-799},
  organization = {Geophysical Journal International},
  abstract = {Predictive multiple suppression methods consist of two
                  main steps: a prediction step, during which
                  multiples are predicted from seismic data, and a
                  primary-multiple separation step, during which the
                  predicted multiples are
                  {\textquoteright}matched{\textquoteright} with the
                  true multiples in the data and subsequently
                  removed. The last step is crucial in practice: an
                  incorrect separation will cause residual multiple
                  energy in the result or may lead to a distortion of
                  the primaries, or both. To reduce these adverse
                  effects, a new transformed-domain method is proposed
                  where primaries and multiples are separated rather
                  than matched. This separation is carried out on the
                  basis of differences in the multiscale and
                  multidirectional characteristics of these two signal
                  components. Our method uses the curvelet transform,
                  which maps multidimensional data volumes into almost
                  orthogonal localized multidimensional prototype
                  waveforms that vary in directional and
                  spatio-temporal content. Primaries-only and
                  multiples-only signal components are recovered from
                  the total data volume by a nonlinear optimization
                  scheme that is stable under noisy input data. During
                  the optimization, the two signal components are
                  separated by enhancing sparseness (through weighted
                  l1-norms) in the transformed domain subject to
                  fitting the observed data as the sum of the
                  separated components to within a user-defined
                  tolerance level. Whenever the prediction for the two
                  signal components in the transformed domain
                  correlate, the recovery is suppressed while for
                  regions where the correlation is small the method
                  seeks the sparsest set of coefficients that
                  represent each signal component. Our algorithm does
                  not seek a matched filter and as such it differs
                  fundamentally from traditional adaptive subtraction
                  methods. The method derives its stability from the
                  sparseness obtained by a non-parametric multiscale
                  and multidirectional overcomplete signal
                  representation. This sparsity serves as prior
                  information and allows for a Bayesian interpretation
                  of our method during which the log-likelihood
                  function is minimized while the two signal
                  components are assumed to be given by a
                  superposition of prototype waveforms, drawn
                  independently from a probability function that is
                  weighted by the predicted primaries and
                  multiples. In this paper, the predictions are based
                  on the data-driven surface-related multiple
                  elimination (SRME) method. Synthetic and field data
                  examples show a clean separation leading to a
                  considerable improvement in multiple suppression
                  compared to the conventional method of adaptive
                  matched filtering. This improved separation
                  translates into an improved stack.},
  keywords = {Presentation, SINBAD, SLIM},
  doi = {10.1111/j.1365-246X.2007.03360.x},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/herrmann2006SINBADpms/herrmann2006SINBADpms.pdf}
}


@CONFERENCE{herrmann2009EAGEcsa,
  author = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title = {Compressive sensing applied to full-waveform inversion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2009},
  month = {06},
  abstract = {With the recent resurgence of full-waveform inversion,
                  the computational cost of solving forward modeling
                  problems has become{\textendash}-aside from issues
                  with non-uniqueness{\textendash}-one of the major
                  impediments withstanding successful application of
                  this technology to industry-size data volumes. To
                  overcome this impediment, we argue that further
                  improvements in this area will depend on a problem
                  formulation with a computational complexity that is
                  no longer strictly determined by the size of the
                  discretization but by transform-domain sparsity of
                  its solution. In this new paradigm, we bring
                  computational costs in par with our ability to
                  compress seismic data and images. This premise is
                  related to two recent developments. First, there is
                  the new field of compressive sensing (CS in short
                  throughout the paper, Cand{\textquoteleft}es et al.,
                  2006; Donoho, 2006){\textendash}-where the argument
                  is made, and rigorously proven, that compressible
                  signals can be recovered from severely sub-Nyquist
                  sampling by solving a sparsity promoting
                  program. Second, there is in the seismic community
                  the recent resurgence of simultaneous-source
                  acquisition (Beasley, 2008; Krohn and Neelamani,
                  2008; Herrmann et al., 2009; Berkhout, 2008;
                  Neelamani et al., 2008), and continuing efforts to
                  reduce the cost of seismic modeling, imaging, and
                  inversion through phase encoding of simultaneous
                  sources (Morton and Ober, 1998; Romero et al., 2000;
                  Krohn and Neelamani, 2008; Herrmann et al., 2009),
                  removal of subsets of angular frequencies (Sirgue
                  and Pratt, 2004; Mulder and Plessix, 2004; Lin et
                  al., 2008) or plane waves (Vigh and Starr, 2008). By
                  using CS principles, we remove sub-sampling
                  interferences asocciated with these approaches
                  through a combination of exploiting transform-domain
                  sparsity, properties of certain sub-sampling
                  schemes, and the existence of sparsity promoting
                  solvers.},
  keywords = {EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2009/Herrmann09EAGEcsa/Herrmann09EAGEcsa_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2009/Herrmann09EAGEcsa/Herrmann09EAGEcsa.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=23961}
}


@CONFERENCE{herrmann2009IAPcsisa,
  author = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title = {Compressive seismic imaging with simultaneous acquisition},
  booktitle = {IAP},
  year = {2009},
  abstract = {The shear size of seismic data volumes forms one of the
                  major impediments for the inversion of seismic
                  data. Turning forward modeling and inversion into a
                  compressive sensing (CS) problem - where simulated
                  data are recovered from a relatively small number of
                  independent sources - can effectively mitigate this
                  high-cost impediment. Our key contribution lies in
                  the design of a sub-sampling operator that commutes
                  with the time-harmonic Helmholtz system. As in
                  compressive sensing, this leads to a reduction of
                  simulation cost. This reduction is commensurate with
                  the transform-domain sparsity of the solution.,
                  implying that computational costs are no longer
                  determined by the size of the discretization but by
                  transform-domain sparsity of the solution of the CS
                  problem that recovers the data. The combination of
                  this sub-sampling strategy with our recent work on
                  preconditioned implicit solvers for the
                  time-harmonic Helmholtz equation provides a viable
                  alternative to full-waveform inversion schemes based
                  on explicit time-domain finite-difference methods.},
  keywords = {Presentation},
  note = {Presented at the IAP meeting, Vienna},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/AIP/2009/herrmann2009IAPcsisa/Herrmann09AIP1.pdf}
}


@CONFERENCE{herrmann2009SAMPTAcws,
  author = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title = {Compressive-wavefield simulations},
  booktitle = {SAMPTA},
  year = {2009},
  organization = {SAMPTA},
  abstract = {Full-waveform inversion{\textquoteright}s high demand on
                  computational resources forms, along with the
                  non-uniqueness problem, the major impediment
                  withstanding its widespread use on industrial-size
                  datasets. Turning modeling and inversion into a
                  compressive sensing problem{\textendash}-where
                  simulated data are recovered from a relatively small
                  number of independent simultaneous
                  sources{\textendash}-can effectively mitigate this
                  high-cost impediment. The key is in showing that we
                  can design a sub-sampling operator that commutes
                  with the time-harmonic Helmholtz system. As in
                  compressive sensing, this leads to a reduction in
                  simulation cost. Moreover, this reduction is
                  commensurate with the transform-domain sparsity of
                  the solution, implying that computational costs are
                  no longer determined by the size of the
                  discretization but by transform-domain sparsity of
                  the solution of the CS problem which forms our data.
                  The combination of this sub-sampling strategy with
                  our recent work on implicit solvers for the
                  Helmholtz equation provides a viable alternative to
                  full-waveform inversion schemes based on explicit
                  finite-difference methods.},
  keywords = {SAMPTA},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SAMPTA/2009/Herrmann09SAMPTAcws/Herrmann09SAMPTAcws.pdf}
}


@CONFERENCE{herrmann2008SIAMcsm,
  author = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title = {Compressive sampling meets seismic imaging},
  booktitle = {SIAM},
  year = {2008},
  abstract = {Compressive sensing has led to fundamental new insights
                  in the recovery of compressible signals from
                  sub-Nyquist samplings. It is shown how jittered
                  subsampling can be used to create favorable recovery
                  conditions. Applications include mitigation of
                  incomplete acquisitions and wavefield
                  computations. While the former is a direct
                  adaptation of compressive sampling, the latter
                  application represents a new way of compressing
                  wavefield extrapolation operators. Operators are not
                  diagonalized but are compressively sampled reducing
                  the computational costs.},
  keywords = {Presentation, SIAM, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2008/herrmann2008SIAMcsm/herrmann2008SIAMcsm.pdf}
}


@CONFERENCE{herrmann2008SINBADitc,
  author = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin and Cody R. Brown},
  title = {Introduction to compressive (wavefield) computation},
  booktitle = {SINBAD 2008},
  year = {2008},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/herrmann2008SINBADitc/herrmann2008SINBADitc.pdf}
}


@CONFERENCE{herrmann2005CSEGnld,
  author = {Felix J. Herrmann and Gilles Hennenfent},
  title = {Non-linear data continuation with redundant frames},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2005},
  organization = {CSEG},
  abstract = {We propose an efficient iterative data interpolation
                  method using continuity along reflectors in seismic
                  images via curvelet and discrete cosine
                  transforms. The curvelet transform is a new
                  multiscale transform that provides sparse
                  representations for images that comprise smooth
                  objects separated by piece-wise smooth
                  discontinuities (e.g. seismic images). The advantage
                  of using curvelets is that these frames are sparse
                  for high-frequency caustic-free solutions of the
                  wave-equation. Since we are dealing with less than
                  ideal data (e.g. bandwidth-limited), we compliment
                  the curvelet frames with the discrete cosine
                  transform.  The latter is motivated by the
                  successful data continuation with the discrete
                  Fourier transform. By choosing generic basis
                  functions we circumvent the necessity to make
                  parametric assumptions (e.g., through
                  linear/parabolic Radon or demigration) regarding the
                  shape of events in seismic data. Synthetic and real
                  data examples demonstrate that our algorithm
                  provides interpolated traces that accurately
                  reproduce the wavelet shape as well as the AVO
                  behavior along events in shot gathers.},
  keywords = {Presentation, SLIM},
  month = {05},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2005/Herrmann05CSEGnld/Herrmann05CSEGnld_pres.pdf},
  url = {http://www.cseg.ca/assets/files/resources/abstracts/2005/101S0201-Herrmann_F_Non_Linear_Data_Continuation.pdf}
}


@CONFERENCE{herrmann2005EAGErcd,
  author = {Felix J. Herrmann and Gilles Hennenfent},
  title = {Robust curvelet-domain data continuation with sparseness constraints},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2005},
  month = {06},
  abstract = {A robust data interpolation method using curvelets
                  frames is presented. The advantage of this method is
                  that curvelets arguably provide an optimal sparse
                  representation for solutions of wave equations with
                  smooth coefficients. As such curvelets frames
                  circum- vent {\textendash} besides the assumption of
                  caustic-free data {\textendash} the necessity to
                  make parametric assumptions (e.g. through
                  linear/parabolic Radon or demigration) regarding the
                  shape of events in seismic data. A brief sketch of
                  the theory is provided as well as a number of
                  examples on synthetic and real data.},
  keywords = {Presentation, SLIM, EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2005/Herrmann05EAGErcd/Herrmann05EAGErcd.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2005/Herrmann05EAGErcd/Herrmann05EAGErcd.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=1112}
}


@CONFERENCE{herrmann2007EAGEsia,
  author = {Felix J. Herrmann and Gilles Hennenfent and Peyman P. Moghaddam},
  title = {Seismic imaging and processing with curvelets},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2007},
  month = {06},
  abstract = {In this paper, we present a nonlinear curvelet-based
                  sparsity-promoting formulation for three problems in
                  seismic processing and imaging namely, seismic data
                  regularization from data with large percentages of
                  traces missing; seismic amplitude recovery for
                  sub-salt images obtained by reverse-time migration
                  and primary-multiple separation, given an inaccurate
                  multiple prediction. We argue why these nonlinear
                  formulations are beneficial.},
  keywords = {Presentation, SLIM, EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/herrmann07EAGEsia/herrmann07EAGEsia_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/herrmann07EAGEsia/herrmann07EAGEsia.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=7075}
}


@CONFERENCE{Herrmann2011BG,
  author = {Felix J. Herrmann and Tristan van Leeuwen},
  title = {{SINBAD's} research program},
  year = {2011},
  month = {11},
  owner = {Shruti},
  quality = {1},
  timestamp = {2013.01.16},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/Misc/Herrmann2011BG.pdf}
}


@CONFERENCE{herrmann2011EAGEefmsp,
  author = {Felix J. Herrmann and Xiang Li},
  title = {Efficient least-squares migration with sparsity promotion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2011},
  month = {05},
  abstract = {Seismic imaging relies on the collection of
                  multi-experimental data volumes in combination with
                  a sophisticated back-end to create high-fidelity
                  inversion results. While significant improvements
                  have been made in linearized inversion, the current
                  trend of incessantly pushing for higher quality
                  models in increasingly complicated regions reveals
                  fundamental shortcomings in handling increasing
                  problem sizes numerically. The so-called ``curse of
                  dimensionality" is the main culprit because it leads
                  to an exponential growth in the number of sources
                  and the corresponding number of wavefield
                  simulations required by ``wave-equation"
                  migration. We address this issue by reducing the
                  number of sources by a randomized dimensionality
                  reduction technique that combines recent
                  developments in stochastic optimization and
                  compressive sensing. As a result, we replace the
                  cur- rent formulations of imaging that rely on all
                  data by a sequence of smaller imaging problems that
                  use the output of the previous inversion as input
                  for the next. Empirically, we find speedups of at
                  least one order-of-magnitude when each reduced
                  experiment is considered theoretically as a separate
                  compressive-sensing experiment.},
  keywords = {Presentation, EAGE, imaging},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2011/herrmann11EAGEefmsp/herrmann11EAGEefmsp_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2011/herrmann11EAGEefmsp/herrmann11EAGEefmsp.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=50333}
}


@CONFERENCE{herrmann2010EAGErds,
  author = {Felix J. Herrmann and Xiang Li},
  title = {Randomized dimensionality reduction for full-waveform inversion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2010},
  month = {06},
  abstract = {Full-waveform inversion relies on the collection of
                  large multi-experiment data volumes in combination
                  with a sophisticated back-end to create
                  high-fidelity inversion results. While improvements
                  in acquisition and inversion have been extremely
                  successful, the current trend of incessantly pushing
                  for higher quality models in increasingly
                  complicated regions of the Earth continues to reveal
                  fundamental shortcomings in our ability to handle
                  the ever increasing problem size numerically. Two
                  causes can be identified as the main culprits
                  responsible for this barrier. First, there is the
                  so-called {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which puts disproportionate strain on
                  current acquisition and processing systems as the
                  size and desired resolution of our survey areas
                  continues to increase. Secondly, there is the recent
                  {\textquoteleft}{\textquoteleft}departure from
                  Moore{\textquoteright}s
                  law{\textquoteright}{\textquoteright} that forces us
                  to lower our expectations to compute ourselves out
                  of this. In this paper, we address this situation by
                  randomized dimensionality reduction, which we adapt
                  from the field of compressive sensing. In this
                  approach, we combine deliberate randomized
                  subsampling with structure-exploiting
                  transform-domain sparsity promotion. Our approach is
                  successful because it reduces the size of seismic
                  data volumes without loss of information. With this
                  reduction, we compute Newton-like updates at the
                  cost of roughly one gradient update for the
                  fully-sampled wavefield.},
  keywords = {Presentation, EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2010/herrmann10EAGErds/herrmann10EAGErds_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2010/herrmann10EAGErds/herrmann10EAGErds.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=39352}
}


@CONFERENCE{herrmann2011SPIEmsp,
  author = {Felix J. Herrmann and Xiang Li and Aleksandr Y. Aravkin and Tristan van Leeuwen},
  title = {A modified, sparsity promoting, {Gauss-Newton} algorithm for seismic waveform inversion},
  booktitle = {Proc. SPIE},
  year = {2011},
  number = {81380V},
  abstract = {Images obtained from seismic data are used by the oil
                  and gas industry for geophysical
                  exploration. Cutting-edge methods for transforming
                  the data into interpretable images are moving away
                  from linear approximations and high-frequency
                  asymptotics towards Full Waveform Inversion (FWI), a
                  nonlinear data-fitting procedure based on full data
                  modeling using the wave-equation. The size of the
                  problem, the nonlinearity of the forward model, and
                  ill-posedness of the formulation all contribute to a
                  pressing need for fast algorithms and novel
                  regularization techniques to speed up and improve
                  inversion results. In this paper, we design a
                  modified Gauss-Newton algorithm to solve the PDE-
                  constrained optimization problem using ideas from
                  stochastic optimization and compressive
                  sensing. More specifically, we replace the
                  Gauss-Newton subproblems by randomly subsampled,
                  -$\ell_1$ regularized subproblems. This allows us us
                  significantly reduce the computational cost of
                  calculating the updates and exploit the
                  compressibility of wavefields in Curvelets. We
                  explain the relationships and connections between
                  the new method and stochastic optimization and
                  compressive sensing (CS), and demonstrate the
                  efficacy of the new method on a large-scale
                  synthetic seismic example.},
  issn = {1},
  keywords = {SLIM, compressive sensing, optimization, full-waveform inversion},
  notes = {TR-2011-05},
  doi = {10.1117/12.893861},
  month = {08},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SPIE/2011/herrmann2011SPIEmsp/herrmann2011SPIEmsp.pdf}
}


@CONFERENCE{herrmann2007EAGEjda,
  author = {Felix J. Herrmann and Peyman P. Moghaddam},
  title = {Just diagonalize: a curvelet-based approach to seismic amplitude recovery},
  booktitle = {EAGE Workshop on Curvelets, contourlets, seislets, … in seismic data processing - where are we and where are we going?},
  year = {2007},
  month = {06},
  abstract = {In his presentation we present a nonlinear
                  curvelet-based sparsity-promoting formulation for
                  the recovery of seismic amplitudes. We show that the
                  curvelet's wavefront detection capability and
                  invariance under wave propagation lead to a
                  formulation of this recovery problem that is stable
                  under noise and missing data. {\copyright}2007
                  Society of Exploration Geophysicists},
  keywords = {Presentation, SLIM, EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2007/herrmann2007EAGEjda/herrmann2007EAGEjda_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2007/herrmann2007EAGEjda/herrmann2007EAGEjda_paper.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=7555}
}


@CONFERENCE{herrmann2005CSEGnlr,
  author = {Felix J. Herrmann and Peyman P. Moghaddam},
  title = {Non-linear regularization in seismic imaging},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2005},
  organization = {CSEG},
  abstract = {Two complementary solution strategies to the
                  least-squares imaging problem with sparseness \&
                  continuity continuity constraints are proposed. The
                  applied formalism explores the sparseness of
                  curvelets coefficients of the reflectivity and their
                  invariance under the demigration-migration
                  operator. We achieve the solution by jointly
                  minimizing a weighted l1-norm on the curvelet
                  coefficients and an anisotropic difussion or total
                  variation norm on the imaged reflectivity model. The
                  l1-norm exploits the sparsenss of the reflectivity
                  in the curvelet domain whereas the anisotropic norm
                  enhances the continuity along the reflections while
                  removing artifacts residing in between
                  reflectors. While the two optimization methods
                  (convex versus non-convex) share the same type of
                  regularization, they differ in flexibility how to
                  handle additional constraints on the coefficients of
                  the imaged reflectivity and in computational
                  expense. A brief sketch of the theory is provided
                  along with a number of synthetic examples.},
  keywords = {Presentation, SLIM},
  month = {05},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2005/Herrmann05CSEGnlr/Herrmann05CSEGnlr_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2005/Herrmann05CSEGnlr/Herrmann05CSEGnlr.pdf}
}


@CONFERENCE{herrmann2004CSEGcia2,
  author = {Felix J. Herrmann and Peyman P. Moghaddam},
  title = {Curvelet imaging and processing: sparseness-constrained least-squares migration},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2004},
  organization = {CSEG},
  abstract = {A non-linear edge-preserving solution to the
                  least-squares migration problem with sparseness
                  constraints is introduced. The applied formalism
                  explores Curvelets as basis functions that, by
                  virtue of their sparseness and locality, not only
                  allow for a reduction of the dimensionality of the
                  imaging problem but which also naturally lead to a
                  non-linear solution with significantly improved
                  signal-to-noise ratio. Additional conditions on the
                  image are imposed by solving a constrained
                  optimization problem on the estimated Curvelet
                  coefficients initialized by thresholding. This
                  optimization is designed to also restore the
                  amplitudes by (approximately) inverting the normal
                  operator, which is, like-wise to the (de)-migration
                  operators, almost diagonalized by the Curvelet
                  transform.},
  keywords = {Presentation, SLIM},
  month = {05},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2004/Herrmann04CSEGcia2/Herrmann04CSEGcia2.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2004/Herrmann04CSEGcia2/Herrmann04CSEGcia2_paper.pdf}
}


@CONFERENCE{herrmann2004EAGEcdl,
  author = {Felix J. Herrmann and Peyman P. Moghaddam},
  title = {Curvelet-domain least-squares migration with sparseness constraints},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2004},
  month = {06},
  abstract = {A non-linear edge-preserving solution to the
                  least-squares migration problem with sparseness
                  constraints is introduced. The applied formalism
                  explores Curvelets as basis functions that, by
                  virtue of their sparseness and locality, not only
                  allow for a reduction of the dimensionality of the
                  imaging problem but which also naturally lead to a
                  non-linear solution with significantly improved
                  signal-to-noise ratio. Additional conditions on the
                  image are imposed by solving a constrained
                  optimization problem on the estimated Curvelet
                  coefficients initialized by thresholding.  This
                  optimization is designed to also restore the
                  amplitudes by (approximately) inverting the normal
                  operator, which is like-wise the (de)-migration
                  operators, almost diagonalized by the Curvelet
                  transform.},
  keywords = {Presentation, SLIM, EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2004/Herrmann04EAGEcdl/Herrmann04EAGEcdl_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2004/Herrmann04EAGEcdl/Herrmann04EAGEcdl.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=2073}
}


@CONFERENCE{herrmann2004EAGEcdp,
  author = {Felix J. Herrmann and Peyman P. Moghaddam},
  title = {Curvelet-domain preconditioned 'wave-equation' depth-migration with sparseness and illumination constraints},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2004},
  abstract = {A non-linear edge-preserving solution to the
                  least-squares migration problem with sparseness
                  constraints is introduced. The applied formalism
                  explores Curvelets as basis functions that, by
                  virtue of their sparseness and locality, not only
                  allow for a reduction of the dimensionality of the
                  imaging problem but which also naturally lead to a
                  non-linear solution with significantly improved
                  signal- to-noise ratio. Additional conditions on the
                  image are imposed by solving a constrained
                  optimization problem on the estimated Curvelet
                  coefficients initialized by thresholding.  This
                  optimization is designed to also restore the
                  amplitudes by (approximately) inverting the normal
                  operator, which is like-wise the (de)-migration
                  operators, almost diagonalized by the Curvelet
                  transform.},
  keywords = {SLIM},
  month = {06},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2004/herrmann2004EAGEcdp/herrmann2004EAGEcdp.pdf}
}


@CONFERENCE{herrmann2004SEGcbn,
  author = {Felix J. Herrmann and Peyman P. Moghaddam},
  title = {Curvelet-based non-linear adaptive subtraction with sparseness constraints},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2004},
  volume = {23},
  pages = {1977-1980},
  organization = {SEG},
  abstract = {In this paper an overview is given on the application of
                  directional basis functions, known under the name
                  Curvelets/Contourlets, to various aspects of seismic
                  processing and imaging, which involve adaptive
                  subtraction. Key concepts in the approach are the
                  use of directional basis functions that localize in
                  both domains (e.g. space and angle); non-linear
                  estimation, which corresponds to localized muting on
                  the coefficients, possibly supplemented by
                  constrained optimization. We will discuss
                  applications that include multiple, ground-roll
                  removal and migration denoising. {\copyright}2004
                  Society of Exploration Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.1851181},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2004/Herrmann04SEGcbn/Herrmann04SEGcbn_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2004/Herrmann04SEGcbn/Herrmann04SEGcbn.pdf }
}


@CONFERENCE{herrmann2005EAGEosf,
  author = {Felix J. Herrmann and Peyman P. Moghaddam and R. Kirlin},
  title = {Optimization strategies for sparseness- and continuity-enhanced imaging: theory},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2005},
  month = {06},
  abstract = {Two complementary solution strategies to the
                  least-squares migration problem with sparseness- and
                  continuity constraints are proposed. The applied
                  formalism explores the sparseness of curvelets on
                  the reflectivity and their invariance under the
                  demigration-migration operator. Sparseness is
                  enhanced by (approximately) minimizing a (weighted)
                  l1-norm on the curvelet coefficients. Continuity
                  along imaged reflectors is brought out by minimizing
                  the anisotropic diffusion or total variation norm
                  which penalizes variations along and in between
                  reflectors. A brief sketch of the theory is provided
                  as well as a number of synthetic examples. Technical
                  details on the implementation of the optimization
                  strategies are deferred to an accompanying paper:
                  implementation.},
  keywords = {SLIM, EAGE},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2005/Herrmann05EAGEosf/Herrmann05EAGEosf.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=1343}
}


@CONFERENCE{herrmann2008SEGcdm,
  author = {Felix J. Herrmann and Peyman P. Moghaddam and Deli Wang},
  title = {Curvelet-domain matched filtering},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  number = {3643-3649},
  organization = {SEG},
  abstract = {Matching seismic wavefields and images lies at the heart
                  of many pre-/post-processing steps part of seismic
                  imaging{\textendash}- whether one is matching
                  predicted wavefield components, such as multiples,
                  to the actual to-be-separated wavefield components
                  present in the data or whether one is aiming to
                  restore migration amplitudes by scaling, using an
                  image-to-remigrated-image matching procedure to
                  calculate the scaling coefficients. The success of
                  these wavefield matching procedures depends on our
                  ability to (i) control possible overfitting, which
                  may lead to accidental removal of energy or to
                  inaccurate image-amplitude corrections, (ii) handle
                  data or images with nonunique dips, and (iii) apply
                  subsequent wavefield separations or migraton
                  amplitude corrections stably. In this paper, we show
                  that the curvelet transform allows us to address all
                  these issues by imposing smoothness in phase space,
                  by using their capability to handle conflicting
                  dips, and by leveraging their ability to represent
                  seismic data and images sparsely. This latter
                  property renders curvelet-domain sparsity promotion
                  an effective prior.},
  keywords = {SLIM,Presentation, SEG},
  month = {08},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/herrmann08SEGcdm/herrmann08SEGcdm_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/herrmann08SEGcdm/herrmann08SEGcdm.pdf }
}


@CONFERENCE{herrmann09EAGEbnrs,
  author = {Felix J. Herrmann and Gang Tang and Reza Shahidi and Gilles Hennenfent
	and Tim T.Y. Lin},
  title = {Beating Nyquist by randomized sampling},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2009},
  note = {Presented at the EAGE (workshop), Amsterdam},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2009/herrmann09EAGEbnrs/herrmann09EAGEbnrs.pdf}
}


@CONFERENCE{herrmann2005EAGErcd1,
  author = {Felix J. Herrmann and D. J. Verschuur},
  title = {Robust curvelet-domain primary-multiple separation with sparseness constraints},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2005},
  month = {06},
  abstract = {A non-linear primary-multiple separation method using
                  curvelets frames is presented. The advantage of this
                  method is that curvelets arguably provide an optimal
                  sparse representation for both primaries and
                  multiples. As such curvelets frames are ideal
                  candidates to separate primaries from multiples
                  given inaccurate predictions for these two data
                  components. The method derives its robustness
                  regarding the presence of noise; errors in the
                  prediction and missing data from the curvelet
                  frame{\textquoteright}s ability (i) to represent
                  both signal components with a limited number of
                  multi-scale and directional basis functions; (ii) to
                  separate the components on the basis of differences
                  in location, orientation and scales and (iii) to
                  minimize correlations between the coefficients of
                  the two components. A brief sketch of the theory is
                  provided as well as a number of examples on
                  synthetic and real data.},
  keywords = {SLIM, EAGE},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2005/Herrmann05EAGErcd1/Herrmann05EAGErcd1.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=1384}
}


@CONFERENCE{herrmann2004CSEGcia1,
  author = {Felix J. Herrmann and D. J. Verschuur},
  title = {Curvelet imaging and processing: adaptive multiple elimination},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2004},
  organization = {CSEG},
  abstract = {Predictive multiple suppression methods consist of two
                  main steps: a prediction step, in which multiples
                  are predicted from the seismic data, and a
                  subtraction step, in which the predicted multiples
                  are matched with the true multiples in the data. The
                  last step appears crucial in practice: an incorrect
                  adaptive subtraction method will cause multiples to
                  be sub-optimally subtracted or primaries being
                  distorted, or both. Therefore, we propose a new
                  domain for separation of primaries and multiples via
                  the Curvelet transform. This transform maps the data
                  into almost orthogonal localized events with a
                  directional and spatial-temporal component. The
                  multiples are suppressed by thresholding the input
                  data at those Curvelet components where the
                  predicted multiples have large amplitudes. In this
                  way the more traditional filtering of predicted
                  multiples to fit the input data is avoided. An
                  initial field data example shows a considerable
                  improvement in multiple suppression.},
  keywords = {Presentation, SLIM},
  month = {05},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2004/Herrmann04CSEGcia1/Herrmann04CSEGcia1.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2004/Herrmann04CSEGcia1/Herrmann04CSEGcia1_paper.pdf}
}


@CONFERENCE{herrmann2004EAGEsop,
  author = {Felix J. Herrmann and D. J. Verschuur},
  title = {Separation of primaries and multiples by non-linear estimation in the curvelet domain},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2004},
  abstract = {Predictive multiple suppression methods consist of two
                  main steps: a prediction step, in which multiples
                  are predicted from the seismic data, and a
                  subtraction step, in which the predicted multiples
                  are matched with the true multiples in the data. The
                  last step appears crucial in practice: an incorrect
                  adaptive subtraction method will cause multiples to
                  be sub-optimally subtracted or primaries being
                  distorted, or both. Therefore, we propose a new
                  domain for separation of primaries and multiples via
                  the Curvelet transform. This transform maps the data
                  into almost orthogonal localized events with a
                  directional and spatial-temporal component. The
                  multiples are suppressed by thresholding the input
                  data at those Curvelet components where the
                  predicted multiples have large amplitudes. In this
                  way the more traditional filtering of predicted
                  multiples to fit the input data is avoided. An
                  initial field data example shows a considerable
                  improvement in multiple suppression.},
  keywords = {SLIM},
  month = {06},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2004/herrmann2004EAGEsop/herrmann2004EAGEsop.pdf}
}


@CONFERENCE{herrmann2004SEGcdm,
  author = {Felix J. Herrmann and D. J. Verschuur},
  title = {Curvelet-domain multiple elimination with sparseness constraints},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2004},
  volume = {23},
  pages = {1333-1336},
  organization = {SEG},
  abstract = {Predictive multiple suppression methods consist of two
                  main steps: a prediction step, in which multiples
                  are predicted from the seismic data, and a
                  subtraction step, in which the predicted multiples
                  are matched with the true multiples in the data. The
                  last step appears crucial in practice: an incorrect
                  adaptive subtraction method will cause multiples to
                  be sub-optimally subtracted or primaries being
                  distorted, or both. Therefore, we propose a new
                  domain for separation of primaries and multiples via
                  the Curvelet transform. This transform maps the data
                  into almost orthogonal localized events with a
                  directional and spatial-temporal component. The
                  multiples are suppressed by thresholding the input
                  data at those Curvelet components where the
                  predicted multiples have large amplitudes. In this
                  way the more traditional filtering of predicted
                  multiples to fit the input data is avoided. An
                  initial field data example shows a considerable
                  improvement in multiple suppression.
                  {\copyright}2004 Society of Exploration
                  Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.1851110},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2004/Herrmann04SEGcdm/Herrmann04SEGcdm_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2004/Herrmann04SEGcdm/Herrmann04SEGcdm.pdf}
}


@CONFERENCE{herrmann2008SEGswi,
  author = {Felix J. Herrmann and Deli Wang},
  title = {Seismic wavefield inversion with curvelet-domain sparsity promotion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  pages = {2497-2501},
  organization = {SEG},
  abstract = {Inverting seismic wavefields lies at the heart of
                  seismic data processing and imaging{\textendash}-
                  whether one is applying
                  {\textquoteleft}{\textquoteleft}a poor
                  man{\textquoteright}s
                  inverse{\textquoteright}{\textquoteright} by
                  correlating wavefields during imaging or whether one
                  inverts wavefields as part of a focal transform
                  interferrometric deconvolution or as part of
                  computing the {\textquoteright}data
                  verse{\textquoteright}. The success of these
                  wavefield inversions depends on the stability of the
                  inverse with respect to data imperfections such as
                  finite aperture, bandwidth limitation, and missing
                  data. In this paper, we show how curvelet domain
                  sparsity promotion can be used as a suitable prior
                  to invert seismic wavefields. Examples include,
                  seismic data regularization with the focused
                  curvelet-based recovery by sparsity-promoting
                  inversion (fCRSI), which involves the inversion of
                  the primary-wavefield operator, the prediction of
                  multiples by inverting the adjoint of the primary
                  operator, and finally the inversion of the data
                  itself {\textendash}- the so-called
                  {\textquoteright}data inverse{\textquoteright}. In
                  all cases, curvelet-domain sparsity leads to a
                  stable inversion.},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.3063862},
  month = {11},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/herrmann08SEGswi/herrmann08SEGswi_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/herrmann08SEGswi/herrmann08SEGswi.pdf }
}


@CONFERENCE{herrmann2007SEGsdp,
  author = {Felix J. Herrmann and Deli Wang and Gilles Hennenfent and Peyman P. Moghaddam},
  title = {Seismic data processing with curvelets: a multiscale and nonlinear approach},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  pages = {2220-2224},
  organization = {SEG},
  abstract = {In this abstract, we present a nonlinear curvelet-based
                  sparsity-promoting formulation of a seismic
                  processing flow, consisting of the following steps:
                  seismic data regularization and the restoration of
                  migration amplitudes. We show that the
                  curvelet{\textquoteright}s wavefront detection
                  capability and invariance under the
                  migration-demigration operator lead to a formulation
                  that is stable under noise and missing
                  data. {\copyright}2007 Society of Exploration
                  Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.2792927},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/herrmann07SEGsdp/herrmann07SEGsdp_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/herrmann07SEGsdp/herrmann07SEGsdp.pdf }
}


@CONFERENCE{herrmann2012EAGEcsm,
  author = {Felix J. Herrmann and Haneet Wason},
  title = {Compressive sensing in marine acquisition and beyond},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2012},
  month = {06},
  abstract = {Simultaneous-source marine acquisition is an example of
                  compressive sensing where acquisition with a single
                  vessel is replaced by simultaneous acquisition by
                  multiple vessels with sources that fire at randomly
                  dithered times. By identifying simultaneous
                  acquisition as compressive sensing, we are able to
                  design acquisitions that favour recovery by sparsity
                  promotion. Compared to conventional processing that
                  yields estimates for sequential data, sparse
                  recovery leads to significantly improved results for
                  simultaneous data volumes that are collected in
                  shorter times. These improvements are the result of
                  proper design of the acquisition, selection of the
                  appropriate transform domain, and solution of the
                  recovery problem by sparsity promotion. During this
                  talk, we will show how these design principles can
                  be applied to marine acquisition and to other
                  problems in exploration seismology that can benefit
                  from compressive sensing.},
  keywords = {EAGE, workshop, acquisition, marine},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/herrmann2012EAGEcsm/herrmann2012EAGEcsm_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/herrmann2012EAGEcsm/herrmann2012EAGEcsm.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=59854}
}


@CONFERENCE{herrmann2007SEGsnt,
  author = {Felix J. Herrmann and D. Wilkinson},
  title = {Seismic noise: the good, the bad and the ugly},
  booktitle = {SEG Summer Research Workshop: Seismic Noise: Origins Preventions, Mitigation, Utilization},
  year = {2007},
  note = {Presented at SEG Summer Research Workshop: Seismic Noise: Origins,
	Prevention, Mitigation, Utilization},
  abstract = {In this paper, we present a nonlinear curvelet-based
                  sparsity-promoting formulation for three problems
                  related to seismic noise, namely the
                  {\textquoteright}good{\textquoteright},
                  corresponding to noise generated by random sampling;
                  the {\textquoteright}bad{\textquoteright},
                  corresponding to coherent noise for which
                  (inaccurate) predictions exist and the
                  {\textquoteright}ugly{\textquoteright} for which no
                  predictions exist. We will show that the compressive
                  capabilities of curvelets on seismic data and images
                  can be used to tackle these three categories of
                  noise-related problems.},
  keywords = {SLIM, SEG},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/herrmann07SEGsnt/herrmann07SEGsnt.pdf }
}


@CONFERENCE{johnson2008SINBADsdi,
  author = {James Johnson and Gilles Hennenfent},
  title = {Seismic data interpolation with symmetry},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {Due to the physics of reciprocity seismic data sets are
                  symmetric in the source and receiver
                  coordinates. Often seismic data sets are incomplete
                  and the missing data must be
                  interpolated. Typically, missing traces do not occur
                  symmetrically. The purpose of this project is to
                  extend the current formulation for solving the
                  seismic interpolation problems in such a way that
                  they enforce reciprocity. The method decomposes the
                  seismic data volume into symmetric and antisymmetric
                  parts. This decomposition leads to an augmented
                  system of equations for the L1-solver that promotes
                  sparsity in the curvelet domain. Interpolation is
                  carried out on the entire system during which the
                  asymmetric component of the volume is forced to
                  zero, while the symmetric part of the data volume is
                  matched to the measured data.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/johnson2008SINBADsdi/johnson2008SINBADsdi.pdf}
}


@CONFERENCE{johnson2010EAGEeop,
  author = {James Johnson and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Estimation of primaries via sparse inversion with reciprocity},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2010},
  abstract = {Accurate removal of surface related multiples is a key
                  step in seismic data processing. The industry
                  standard for removing multiples is SRME, which
                  involves convolving the data with itself to predict
                  the multiples, followed by an adaptive subtraction
                  procedure to recover the primaries (Verschuur and
                  Berkhout, 1997). Other methods involve
                  multidimensional division of the up-going and
                  down-going wavefields (Amundsen, 2001). However,
                  this approach may suffer from stability
                  problems. With the introduction of the
                  {\textquoteleft}{\textquoteleft}estimation of
                  primaries by sparse
                  inversion{\textquoteright}{\textquoteright}(EPSI),
                  van Groenestijn and Verschuur (2009) recentely
                  reformulated SRME to jointly estimate the
                  surface-free impulse response and the source
                  signature directly from the data. The advantage of
                  EPSI is that it recovers the primary response
                  directly, and does not require a second processing
                  step for the subtraction of estimated multiples from
                  the original data. However, because it estimates
                  both the primary impulse response and source
                  signature from the data EPSI must be regularized.
                  Motivated by recent successful application of the
                  curvelet transform in seismic data processing
                  (Herrmann et al., 2007), we formulate EPSI as a
                  bi-convex optimization problem that seeks sparsity
                  on the surface-free Green{\textquoteright}s function
                  and Fourier-domain smoothness on the source
                  wavelet. Our main contribution compared to previous
                  work (Lin and Herrmann, 2009), and the contribution
                  of that author to the proceedings of this
                  meeting(Lin and Herrmann, 2010), is that we employ
                  the physical principle of as source-receiver
                  reciprocity to improve the inversion.},
  keywords = {EAGE},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2010/johnson10EAGEeop/johnson10EAGEeop.pdf}
}


@CONFERENCE{jumah2011SEGdrepsi,
  author = {Bander Jumah and Felix J. Herrmann},
  title = {Dimensionality-reduced estimation of primaries by sparse inversion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2011},
  month = {09},
  volume = {30},
  pages = {3520-3525},
  organization = {SEG},
  abstract = {Data-driven methods---such as the estimation of
                  primaries by sparse inversion---suffer from the
                  "curse of dimensionality", which leads to
                  disproportional growth in computational and storage
                  demands when moving to realistic 3-D field data. To
                  remove this fundamental impediment, we propose a
                  dimensionality reduction technique where the "data
                  matrix" is approximated adaptively by a randomized
                  low-rank approximation. Compared to conventional
                  methods, our approach has the advantage that the
                  cost of the low-rank approximation is reduced
                  significantly, which may lead to considerable
                  reductions in storage and computational costs of the
                  sparse inversion. Application of the proposed
                  formalism to synthetic data shows that significant
                  improvements are achievable at low computational
                  overhead required to compute the low-rank
                  approximations.},
  keywords = {Presentation, SEG, processing},
  doi = {10.1190/1.3627931},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2011/Jumah11SEGdrepsi/Jumah11SEGdrepsi_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2011/Jumah11SEGdrepsi/Jumah11SEGdrepsi.pdf}
}


@CONFERENCE{kumar2008SINBADcd,
  author = {Vishal Kumar},
  title = {Curvelet denoising},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {The separation of signal and noise is an important issue
                  in seismic data processing. By noise we refer to the
                  incoherent noise which is present in the data. In
                  our case, we showed curvelets concentrate seismic
                  signal energy in few significant coefficients unlike
                  noise energy that is spread all over the
                  coefficients. The sparsity of seismic data in the
                  curvelet domain makes curvelets an ideal choice for
                  separating the noise from the seismic data. In our
                  approach the denoising problem is framed as
                  curvelet-regularized inversion problem. After
                  initial processing, we applied the algorithm to the
                  poststack data and compared our results with
                  conventional wavelet denoising.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/kumar2008SINBADcd/kumar2008SINBADcd.pdf}
}


@CONFERENCE{kumar2008SINBADcrd,
  author = {Vishal Kumar},
  title = {Curvelet-regularized deconvolution},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {The removal of source signature from seismic data is an
                  important step in seismic data processing. The
                  Curvelet transform provides sparse representations
                  for images that comprise smooth objects separated by
                  piece-wise smooth discontinuities (e.g. seismic
                  reflectivity). In this approach the sparseness of
                  reflectivity in Curvelet domain is used as a prior
                  to stabilize the inversion process. Our
                  Curvelet-regularized deconvolution algorithm uses
                  recently developed SPGL1 solver which does adaptive
                  sampling of the trade-off curve. We applied the
                  algorithm on a synthetic example and compared our
                  results with that of Spiky deconvolution approach.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/kumar08SINBADcrd/kumar08SINBADcrd_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/kumar2008SINBADcrd/kumar2008SINBADcrd.pdf}
}


@CONFERENCE{kumar2009SEGins,
  author = {Vishal Kumar and Felix J. Herrmann},
  title = {Incoherent noise suppression with curvelet-domain sparsity},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  month = {10},
  volume = {28},
  pages = {3356-3360},
  organization = {SEG},
  abstract = {The separation of signal and noise is a key issue in
                  seismic data processing. By noise we refer to the
                  incoherent noise that is present in the data. We use
                  the recently introduced multiscale and
                  multidirectional curvelet transform for suppression
                  of random noise. The curvelet transform decomposes
                  data into directional plane waves that are local in
                  nature. The coherent features of the data occupy the
                  large coefficients in the curvelet domain, whereas
                  the incoherent noise lives in the small
                  coefficients. In other words, signal and noise have
                  minimal overlap in the curvelet domain. This gives
                  us a chance to use curvelets to suppress noise
                  present in data.},
  keywords = {Presentation, SEG},
  doi = {10.1190/1.3255557},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/kumar09SEGins/kumar09SEGins_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/kumar09SEGins/kumar09SEGins.pdf}
}


@CONFERENCE{kumar2008CSEGcrs,
  author = {Vishal Kumar and Felix J. Herrmann},
  title = {Curvelet-regularized seismic deconvolution},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2008},
  organization = {CSEG},
  abstract = {There is an inherent continuity along reflectors of a
                  seismic image. We use the recently introduced
                  multiscale and multidirectional curvelet transform
                  to exploit this continuity along reflectors for
                  cases in which the assumption of spiky reflectivity
                  may not hold. We show that such type of seismic
                  reflectivity can be represented in the
                  curvelet-domain by a vector whose entries decay
                  rapidly. This curvelet-domain compression of
                  reflectivity opens new perspectives towards solving
                  classical problems in seismic processing including
                  the deconvolution problem. In this paper, we present
                  a formulation that seeks curvelet-domain sparsity
                  for non-spiky reflectivity and we compare our
                  results with those of spiky deconvolution.},
  keywords = {Presentation, SLIM},
  month = {05},
  presentation = {http://slim.gatech.edu/Publications/Public/Conferences/CSEG/2008/kumar2008CSEGcrs/kumar2008CSEGcrs_pres.pdf},
  url = {http://slim.gatech.edu/Publications/Public/Conferences/CSEG/2008/kumar2008CSEGcrs/kumar2008CSEGcrs.pdf}
}


@CONFERENCE{kumar2008SEGdwc,
  author = {Vishal Kumar and Felix J. Herrmann},
  title = {Deconvolution with curvelet-domain sparsity},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  pages = {1996-2000},
  organization = {SEG},
  abstract = {There is an inherent continuity along reflectors of a
                  seismic image. We use the recently introduced
                  multiscale and multidirectional curvelet transform
                  to exploit this continuity along reflectors for
                  cases in which the assumption of spiky reflectivity
                  may not hold. We show that such type of seismic
                  reflectivity can be represented in the
                  curvelet-domain by a vector whose entries decay
                  rapidly. This curvelet-domain compression of
                  reflectivity opens new perspectives towards solving
                  classical problems in seismic processing including
                  the deconvolution problem. In this paper, we present
                  a formulation that seeks curvelet-domain sparsity
                  for non-spiky reflectivity and we compare our
                  results with those of spiky deconvolution.},
  keywords = {SLIM, Presentation, SEG},
  doi = {10.1190/1.3059287},
  month = {11},
  presentation = { https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/kumar08SEGdwc/kumar08SEGdwc_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/kumar08SEGdwc/kumar08SEGdwc.pdf }
}


@CONFERENCE{lebed2008SINBADaoc,
  author = {Evgeniy Lebed},
  title = {Curvelet / {Surfacelet} comparison},
  booktitle = {SINBAD},
  year = {2008},
  abstract = {Curvelets and Surfacelets are two transforms that aim to
                  achieve a multiscale and a multidirectional
                  decomposition of arbitrary N-dimensional ($N>=2$)
                  signals. While both transforms are Fourier-based,
                  their construction is intrinsically different. In
                  this talk we will give and overview of the
                  construction of the two transforms, and explore
                  their properties such as frequency domain / spatial
                  domain coherence, sparsity, redundancy and
                  computational complexity.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/lebed2008SINBADaoc/lebed2008SINBADaoc.pdf}
}


@CONFERENCE{lebed2008SINBADaoc1,
  author = {Evgeniy Lebed},
  title = {Applications of curvelets/surfacelets to seismic data processing},
  booktitle = {SINBAD},
  year = {2008},
  abstract = {In this talk we explore several applications of the
                  curvelet and surfacelet transforms to seismic data
                  processing. The first application is stable signal
                  recovery in the physical domain - seismic data
                  acquisition is often limited by physical and
                  economic constraints, and the goal is to interpolate
                  the data from a given subset of seismic traces. The
                  second application is signal recovery in a transform
                  domain - we assume that our data comes in a form of
                  a random subset of temporal frequencies and the goal
                  is to recover the missing frequencies from this
                  data. Since seismic signals are generally not
                  bandwidth limited, this in fact becomes an
                  anti-aliasing problem. In both these problems the
                  recovery is resolved via a robust $\ell$_1 solver
                  that exploits the sparsity of the signals in
                  curvelet/surfacelet domains. In the last application
                  we explore the problem of primary-multiple
                  separation by simple thresholding.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/lebed2008SINBADaoc1/lebed2008SINBADaoc1.pdf}
}


@CONFERENCE{lebed2008SEGhggt,
  author = {Evgeniy Lebed and Felix J. Herrmann},
  title = {A hitchhiker{\textquoteright}s guide to the galaxy of transform-domain sparsification},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  organization = {SEG},
  abstract = {The ability to efficiently and sparsely represent
                  seismic data is becoming an increasingly important
                  problem in geophysics. Over the last decade many
                  transforms such as wavelets, curvelets, contourlets,
                  surfacelets, shearlets, and many other types of
                  {\textquoteright}x-lets{\textquoteright} have been
                  developed to try to resolve this issue. In this
                  abstract we compare the properties of four of these
                  commonly used transforms, namely the shift-invariant
                  wavelets, complex wavelets, curvelets and
                  surfacelets. We also briefly explore the performance
                  of these transforms for the problem of recovering
                  seismic wavefields from incomplete measurements.},
  keywords = {SEG},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/lebed08SEGhggt/lebed08SEGhggt.pdf}
}


@CONFERENCE{vanleeuwen2011ICIAMcbmcwe,
  author = {Tristan van Leeuwen},
  title = {A correlation-based misfit criterion for wave-equation traveltime tomography},
  booktitle = {ICIAM},
  year = {2011},
  organization = {ICIAM 2011},
  abstract = {The inference of subsurface medium parameters from
                  seismic data can be posed as a PDE-constrained
                  data-fitting procedure. This approach is successful
                  in reconstructing medium perturbations that are in
                  the order of the wavelength. In practice, the data
                  lack low frequency content and this means that one
                  needs a good initial guess of the slowly varying
                  component of the medium. For a wrong starting model
                  an iterative reconstruction procedure is likely to
                  end up in a local minimum. We propose to use a
                  different measure of the misfit that makes the
                  optimization problem well-posed in terms of the
                  slowly varying velocity structures. This procedure
                  can be seen as a generalization of ray-based
                  traveltime tomography. We discuss the theoretical
                  underpinnings of the method and give some numerical
                  examples.},
  date-added = {2011-07-19},
  keywords = {Presentation,ICIAM,Imaging},
  month = {07},
  note = {Presented at ICIAM 2011, Vancouver BC},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ICIAM/2011/vanleeuwen2011ICIAMcbmcwe/vanleeuwen2011ICIAMcbmcwe.pdf}
}


@CONFERENCE{vanleeuwen2011SEGext,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {Probing the extended image volume},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2011},
  month = {09},
  volume = {30},
  pages = {4045-4050},
  organization = {SEG},
  abstract = {The prestack image volume can be defined as a cross-
                  correlation of the source and receivers wavefields
                  for non-zero space and time lags. If the background
                  velocity is kinemati- cally acceptable, this image
                  volume will have its main contributions at zero lag,
                  even for complex models. Thus, it is an ideal tool
                  for wave-equation migration velocity analysis in the
                  presence of strong lateral heterogeneity. In
                  particular, it allows us to pose migration velocity
                  analysis as a PDE-constrained optimization problem,
                  where the goal is to minimize the energy in the
                  image volume at non-zero lag subject to fitting the
                  data approximately. However, it is computationally
                  infeasi- ble to explicitly form the whole image
                  volume. In this paper, we discuss several ways to
                  reduce the computational costs involved in computing
                  the image volume and evaluating the focusing
                  criterion. We reduce the costs for calculating the
                  data by randomized source synthesis. We also present
                  an efficient way to subsample the image
                  volume. Finally, we propose an alternative
                  optimization criterion and suggest a multiscale
                  inversion strategy for wave-equation MVA.},
  keywords = {SEG, imaging},
  doi = {10.1190/1.3628051},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2011/vanleeuwen11SEGext/vanleeuwen11SEGext.pdf}
}


@CONFERENCE{vanleeuwen2011WAVESpeiv,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {Probing the extended image volume for seismic velocity inversion},
  booktitle = {WAVES},
  year = {2011},
  organization = {Waves 2011},
  abstract = {In seismic velocity inversion one aims to reconstruct a
                  kinematically correct subsurface velocity model that
                  can be used as input for further processing and
                  inversion of the data. An important tool in velocity
                  inversion is the prestack image volume. This image
                  volume can be defined as a cross-correlation of the
                  source and receivers wavefields for non-zero space
                  and time lags. If the background velocity is
                  kinematically acceptable, this image volume will
                  have its main contributions at zero lag, even for
                  complex models. Thus, it is an ideal tool for
                  wave-equation migration velocity analysis in the
                  presence of strong lateral heterogeneity. In
                  particular, it allows us to pose migration velocity
                  analysis as a PDE-constrained optimization problem,
                  where the goal is to minimize the energy in the
                  image volume at non-zero lag subject to fitting the
                  data approximately. However, it is computationally
                  infeasible to explicitly form the whole image
                  volume. In this paper, we discuss several ways to
                  reduce the computational costs involved in computing
                  the image volume and evaluating the focusing
                  criterion. We reduce the costs for calculating the
                  data by randomized source synthesis. We also present
                  an efficient way to subsample the image
                  volume. Finally, we propose an alternative
                  optimization criterion and suggest a multiscale
                  inversion strategy for wave-equation MVA.},
  date-added = {2011-07-29},
  keywords = {Presentation},
  month = {07},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ICIAM/2011/vanleeuwen2011WAVESpeiv/vanleeuwen2011WAVESpeiv.pdf}
}


@CONFERENCE{vanleeuwen2011EAGEhsdomwi,
  author = {Tristan van Leeuwen and Felix J. Herrmann and Mark Schmidt and Michael P. Friedlander},
  title = {A hybrid stochastic-deterministic optimization method for waveform inversion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2011},
  month = {05},
  abstract = {Present-day high quality 3D acquisition can give us
                  lower frequencies and longer offsets with which to
                  invert. However, the computational costs involved in
                  handling this data explosion are
                  tremendous. Therefore, recent developments in
                  full-waveform inversion have been geared towards
                  reducing the computational costs involved. A key
                  aspect of several approaches that have been proposed
                  is a dramatic reduction in the number of sources
                  used in each iteration. A reduction in the number of
                  sources directly translates to less PDE-solves and
                  hence a lower computational cost. Recent attention
                  has been drawn towards reducing the sources by
                  randomly combining the sources in to a few
                  supershots, but other strategies are also
                  possible. In all cases, the full data misfit, which
                  involves all the sequential sources, is replaced by
                  a reduced misfit that is much cheaper to evaluate
                  because it involves only a small number of sources
                  (batchsize). The batchsize controls the accuracy
                  with which the reduced misfit approximates the full
                  misfit. The optimization of such an inaccurate, or
                  noisy, misfit is the topic of stochastic
                  optimization. In this paper, we propose an
                  optimization strategy that borrows ideas from the
                  field of stochastic optimization. The main idea is
                  that in the early stage of the optimization, far
                  from the true model, we do not need a very accurate
                  misfit. The strategy consists of gradually
                  increasing the batchsize as the iterations
                  proceed. We test the proposed strategy on a
                  synthetic dataset. We achieve a very reasonable
                  inversion result at the cost of roughly 13
                  evaluations of the full misfit. We observe a
                  speed-up of roughly a factor 20.},
  keywords = {Presentation, EAGE, full-waveform inversion, optimization},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2011/vanleeuwen11EAGEhsdomwi/vanleeuwen11EAGEhsdomwi_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2011/vanleeuwen11EAGEhsdomwi/vanleeuwen11EAGEhsdomwi.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=50341}
}


@CONFERENCE{vanleeuwen2011SIAMGEOmawt,
  author = {Tristan van Leeuwen and Wim A. Mulder},
  title = {Multiscale aspects of waveform tomography},
  booktitle = {SIAMGEO},
  year = {2011},
  organization = {SIAM GeoSciences 2011},
  abstract = {We consider the inference of medium velocity from
                  transmitted acoustic waves. Typically, the
                  measurements are done in a narrow frequency band. As
                  a result the sensitivity of the data with respect to
                  velocity perturbations varies dramatically with the
                  scale of the perturbation.
                  {\textquoteleft}Smooth{\textquoteright}
                  perturbations will cause a phase shift, whereas
                  perturbations that vary on the wavelength-scale
                  cause amplitude variations. We investigate how to
                  incorporate this scale dependent behavior in the
                  formulation of the inverse problem.},
  keywords = {Presentation},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SIAM/2011/vanleeuwen2011SIAMGEOmawt/vanleeuwen2011SIAMGEOmawt.pdf}
}


@CONFERENCE{vanleeuwen2011AMPhsdmwi,
  author = {Tristan van Leeuwen and Mark Schmidt and Michael P. Friedlander and Felix J. Herrmann},
  title = {A hybrid stocahstic-deterministic method for waveform inversion},
  booktitle = {AMP},
  year = {2011},
  organization = {WAVES 2011},
  abstract = {A lot of seismic and medical imaging problems can be
                  written as a least-squares data- fitting problem. In
                  particular, we consider the case of multi-experiment
                  data, where the data consists of a large number of
                  "independent" measurements. Solving the inverse
                  problem then involves repeatedly forward modeling
                  the data for each of these experiments. In case the
                  number of experiments is large and the modeling
                  kernel expensive to apply, such an approach may be
                  prohibitively expensive. We review techniques from
                  stochastic optimization which aim at dramatically
                  reducing the number of experiments that need to be
                  modeled at each iteration. This reduction is
                  typically achieved by randomly subsampling the
                  data. Special care needs to be taken in the
                  optimization to deal with the stochasticity that is
                  introduced in this way.},
  date-added = {2011-07-15},
  keywords = {Presentation},
  month = {07},
  note = {Presented at AMP Medical and Seismic Imaging, 2011, Vancouver BC},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ICIAM/2011/vanleeuwen2011AMPhsdmwi/vanleeuwen2011AMPhsdmwi.pdf}
}


@CONFERENCE{li2011EAGEfwirr,
  author = {Xiang Li and Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Full-waveform inversion with randomized {L1} recovery for the model updates},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2011},
  month = {05},
  abstract = {Full-waveform inversion (FWI) is a data fitting
                  procedure that relies on the collection of seismic
                  data volumes and sophisticated computing to create
                  high-resolution results. With the advent of FWI, the
                  improvements in acquisition and inversion have been
                  substantial, but these improvements come at a high
                  cost because FWI involves extremely large
                  multi-experiment data volumes. The main obstacle is
                  the "curse of dimensionality" exemplified by
                  NyquistÕs sampling criterion, which puts a
                  disproportionate strain on current acquisition and
                  processing systems as the size and desired
                  resolution increases. In this paper, we address the
                  "curse of dimensionality" by randomized
                  dimensionality reduction of the FWI problem adapted
                  from the field of CS. We invert for model updates by
                  replacing the Gauss-Newton linearized subproblem for
                  subsampled FWI with a sparsity promoting
                  formulation, and solve this formulation using the
                  SPGl1 algorithm. We speed up the algorithm and avoid
                  overfitting the data by solving for the linearized
                  updates only approximately. Our approach is
                  successful because it reduces the size of seismic
                  data volumes without loss of information. With this
                  reduction, we can compute a Newton-like update with
                  the reduced data volume at the cost of roughly one
                  gradient update for the fully sampled wavefield.},
  keywords = {Presentation, EAGE, full-waveform inversion},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2011/li11EAGEfwirr/li11EAGEfwirr_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2011/li11EAGEfwirr/li11EAGEfwirr.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=50345}
}


@CONFERENCE{li2011CSEGefimag,
  author = {Xiang Li and Felix J. Herrmann},
  title = {Efficient full-waveform inversion with marine acquisition geometry},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2012},
  abstract = {Full-waveform inversion (FWI) is a nonlinear data
                  fitting procedure based on seismic data to derive a
                  accurate velocity model. With the increasing demand
                  for high resolution images in complex geological
                  settings, the importance of improvements in
                  acquisition and inversion become more and more
                  critical. However, these improvements will be
                  obtained at high computational cost, as a typical
                  marine survey contains thousands of shot and
                  receiver positions, and FWI needs several passes
                  through massive seismic data. Computational cost of
                  FWI will grow exponentially as the size of seismic
                  data and desired resolution increase. In this paper
                  we present a modified Gauss-Newton (GN) method that
                  borrows ideas from compressive sensing, where we
                  compute the GN updates from a few randomly selected
                  sequential shots. Each subproblem is solved by using
                  a sparsity promoting algorithm. With this approach,
                  we dramatically reduce the size and hence the
                  computational costs of the problem, whilst we
                  control information loss by redrawing a different
                  set of sequential shots for each subproblem.},
  keywords = {CSEG},
  month = {02},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2012/li2011CSEGefimag/li2011CSEGefimag.pdf}
}


@CONFERENCE{li2012SEGspmamp,
  author = {Xiang Li and Felix J. Herrmann},
  title = {Sparsity-promoting migration accelerated by message passing},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2012},
  month = {11},
  volume = {31},
  pages = {1-6},
  organization = {SEG},
  abstract = {Seismic imaging via linearized inversion requires
                  multiple iterations to minimize the least-squares
                  misfit as a function of the medium
                  perturbation. Unfortunately, the cost for these
                  iterations are prohibitive because each iteration
                  requires many wave-equation simulations, which
                  without direct solvers require an expensive separate
                  solve for each source. To overcome this problem, we
                  use dimensionality-reduction to decrease the size of
                  seismic imaging problem by turning the large number
                  of sequential shots into a much small number of
                  simultaneous shots. In our approach, we take
                  advantage of sparsifying transforms to remove source
                  crosstalk resulting from randomly weighting and
                  stacking sequential shots into a few super shots. We
                  also take advantage of the fact that the convergence
                  of large-scale sparsity-promoting solvers can be
                  improved significantly by borrowing ideas from
                  message passing, which are designed to break
                  correlation built up between the linear system and
                  the model iterate. In this way, we arrive at a
                  formulation where we run the sparsity-promoting
                  solver for a relatively large number of very
                  iterations. Aside from leading to a significant
                  speed up, our approach had the advantage of greatly
                  reducing the memory imprint and IO requirements. We
                  demonstrate this feature by solving a
                  sparsity-promoting imaging problem with operators of
                  reverse-time migration, which is computationally
                  infeasible without the dimensionality reduction.},
  keywords = {SEG, imaging, inversion},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2012/li2012SEGspmamp/li2012SEGspmamp_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2012/li2012SEGspmamp/li2012SEGspmamp.pdf},
  doi = {10.1190/segam2012-1500.1}
}


@CONFERENCE{li2010SEGfwi,
  author = {Xiang Li and Felix J. Herrmann},
  title = {Full-waveform inversion from compressively recovered model updates},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2010},
  month = {10},
  volume = {29},
  pages = {1029-1033},
  organization = {SEG},
  abstract = {Full-waveform inversion relies on the collection of
                  large multi-experiment data volumes in combination
                  with a sophisticated back-end to create
                  high-fidelity inversion results. While improvements
                  in acquisition and inversion have been extremely
                  successful, the current trend of incessantly pushing
                  for higher quality models in increasingly
                  complicated regions of the Earth reveals fundamental
                  shortcomings in our ability to handle increasing
                  problem size numerically. Two main culprits can be
                  identified. First, there is the so-called
                  {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which puts disproportionate strain on
                  current acquisition and processing systems as the
                  size and desired resolution increases. Secondly,
                  there is the recent
                  {\textquoteleft}{\textquoteleft}departure from
                  Moore{\textquoteright}s
                  law{\textquoteright}{\textquoteright} that forces us
                  to lower our expectations to compute ourselves out
                  of this. In this paper, we address this situation by
                  randomized dimensionality reduction, which we adapt
                  from the field of compressive sensing. In this
                  approach, we combine deliberate randomized
                  subsampling with structure-exploiting
                  transform-domain sparsity promotion. Our approach is
                  successful because it reduces the size of seismic
                  data volumes without loss of information. With this
                  reduction, we compute Newton-like updates at the
                  cost of roughly one gradient update for the
                  fully-sampled wavefield.},
  keywords = {Presentation, SEG, full-waveform inversion},
  doi = {10.1190/1.3513022},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2010/li10SEGfwi/li10SEGfwi_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2010/li10SEGfwi/li10SEGfwi.pdf}
}


@CONFERENCE{li2011SBGFmgnsu,
  author = {Xiang Li and Felix J. Herrmann and Tristan van Leeuwen and Aleksandr Y. Aravkin},
  title = {Modified {Gauss-Newton} with sparse updates},
  booktitle = {SBGF},
  year = {2011},
  organization = {SBGF},
  abstract = {Full-waveform inversion (FWI) is a data fitting
                  procedure that relies on the collection of seismic
                  data volumes and sophisticated computing to create
                  high-resolution models.With the advent of FWI, the
                  improvements in acquisition and inversion have been
                  substantial, but these improvements come at a high
                  cost because FWI involves extremely large
                  multi-experiment data volumes. The main obstacle is
                  the {\textquoteleft}curse of
                  dimensionality{\textquoteright} exemplified by
                  Nyquist{\textquoteright}s sampling criterion, which
                  puts a disproportionate strain on current
                  acquisition and processing systems as the size and
                  desired resolution increases. In this paper, we
                  address the {\textquoteleft}curse of
                  dimensionality{\textquoteright} by using randomized
                  dimensionality reduction of the FWI problem, coupled
                  with a modified Gauss-Newton (GN) method designed to
                  promote curvelet-domain sparsity of model
                  updates. We solve for these updates using the
                  spectral projected gradient method, implemented in
                  the SPG￿1 software package. Our approach is
                  successful because it reduces the size of seismic
                  data volumes without loss of information. With this
                  reduction, we can compute Gauss-Newton updates with
                  the reduced data volume at the cost of roughly one
                  gradient update for the fully sampled wavefield},
  keywords = {SBGF, full-waveform inversion},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SBGF/2011/li11SBGFmgnsu/li11SBGFmgnsu.pdf}
}


@CONFERENCE{lin2006SINBADci,
  author = {Tim T.Y. Lin},
  title = {Compressed imaging},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {In 1998 Grimbergen et. al. introduced a new method for
                  computing wavefield propagation which improved on
                  the previously employed local explicit operator
                  method in that it exhibited no dip limitation,
                  accurately handled laterally varying background
                  ground velocity models, and is unconditionally
                  stable. These desirable properties are mainly
                  attributed to bringing the propagation problem into
                  an eigenvector basis that diagonalizes the
                  propagation operators. This modal-transform method,
                  however, requires at each depth-level the solution
                  of a large-scale sparse eigenvalue problem to
                  compute the square-root of the Helmholtz
                  operator. By using recent results from compressed
                  sensing, we hope to reduce these computational costs
                  that typically involve the synthesizes of the
                  imaging operators and the cost of matrix-vector
                  products.  To reduce these costs, we compress the
                  extrapolation operators by using only a fraction of
                  the positive eigenvalues and temporal frequencies.
                  This reduction not only leads to smaller matrices
                  but also to reduced synthesis costs. These
                  reductions go at the expense of solving a recovery
                  problem from incomplete data. During the
                  presentation, we show that wavefields can accurately
                  be extrapolated with a compressed operators and
                  competitive costs.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/lin2006SINBADci/lin2006SINBADci.pdf}
}


@CONFERENCE{lin2009SEGcsf,
  author = {Tim T.Y. Lin and Yogi A. Erlangga and Felix J. Herrmann},
  title = {Compressive simultaneous full-waveform simulation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  month = {10},
  volume = {28},
  pages = {2577-2581},
  organization = {SEG},
  abstract = {The fact that the computational complexity of wavefield
                  simulation is proportional to the size of the
                  discretized model and acquisition geometry, and not
                  to the complexity of the simulated wavefield, is a
                  major impediment within seismic imaging. By turning
                  simulation into a compressive sensing
                  problem{\textendash}where simulated data is
                  recovered from a relatively small number of
                  independent simultaneous sources{\textendash}we
                  remove this impediment by showing that compressively
                  sampling a simulation is equivalent to compressively
                  sampling the sources, followed by solving a reduced
                  system. As in compressive sensing, this allows for a
                  reduction in sampling rate and hence in simulation
                  costs. We demonstrate this principle for the
                  time-harmonic Helmholtz solver. The solution is
                  computed by inverting the reduced system, followed
                  by a recovery of the full wavefield with a sparsity
                  promoting program. Depending on the
                  wavefield{\textquoteright}s sparsity, this approach
                  can lead to a significant cost reduction, in
                  particular when combined with the implicit
                  preconditioned Helmholtz solver, which is known to
                  converge even for decreasing mesh sizes and
                  increasing angular frequencies. These properties
                  make our scheme a viable alternative to explicit
                  time-domain finite-difference.},
  keywords = {Presentation, SEG},
  doi = {10.1190/1.3255381},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/Lin09SEGcsf/Lin09SEGcsf_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/Lin09SEGcsf/Lin09SEGcsf.pdf}
}


@CONFERENCE{lin2011EAGEepsic,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Estimating primaries by sparse inversion in a curvelet-like representation domain},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2011},
  month = {05},
  abstract = {We present an uplift in the fidelity and wavefront
                  continuity of results obtained from the Estimation
                  of Primaries by Sparse Inversion (EPSI) program by
                  reconstructing the primary events in a hybrid
                  wavelet-curvelet representation domain. EPSI is a
                  multiple removal technique that belongs to the class
                  of wavefield inversion methods, as an alternative to
                  the traditional adaptive-subtraction process. The
                  main assumption is that the correct primary events
                  should be as sparsely-populated in time as
                  possible. A convex reformulation of the original
                  EPSI algorithm allows its convergence property to be
                  preserved even when the solution wavefield is not
                  formed in the physical domain. Since wavefronts and
                  edge-type singularities are sparsely represented in
                  the curvelet domain, sparse solutions formed in this
                  domain will exhibit vastly improved continuity when
                  compared to those formed in the physical domain,
                  especially for the low-energy events at later
                  arrival times. Further- more, a wavelet-type
                  representation domain will preserve sparsity in the
                  reflected events even if they originate from
                  non-zero-order discontinuities in the subsurface,
                  providing an additional level of robustness. This
                  method does not require any changes in the
                  underlying computational algorithm and does not
                  explicitly impose continuity constraints on each
                  update.},
  keywords = {Presentation, EAGE, processing},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2011/lin11EAGEepsic/lin11EAGEepsic_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2011/lin11EAGEepsic/lin11EAGEepsic.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=50427}
}


@CONFERENCE{lin2011SEGrssde,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Robust source signature deconvolution and the estimation of primaries by sparse inversion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2011},
  month = {09},
  volume = {30},
  pages = {4354-4359},
  organization = {Dept. of Earth and Ocean Sciences, University of British Columbia},
  abstract = {The past few years had seen some concentrated interest
                  on a particular wavefield-inversion approach to the
                  popular SRME multiple removal technique called
                  Estimation of Primaries by Sparse Inversion (EPSI).
                  EPSI promises greatly improved tolerance to noise,
                  missing data, edge effect, and other physi- cal
                  phenomenon generally not described by the SRME
                  relation (van Groenestijn and Verschuur,
                  2009a,b). It is based on the premise that it is
                  possible to stably invert for both the primary
                  impulse response and the source signature despite
                  beforehand having no (or very limited) explicit
                  knowledge of latter. The key to successful
                  applications of EPSI, as shown in very recent works
                  (Savels et al., 2010), is a robust way to
                  reconstruct very sparse primary impulse response
                  events as part of the inversion process. Based on
                  the various successful demonstrations in literature,
                  there is a very strong sense that EPSI will also
                  play an important role in future developments of
                  source sig- nature deconvolution and the general
                  recovering of wavefield spectrum.},
  keywords = {Presentation, deconvolution, SEG, sparse inversion, processing},
  doi = {10.1190/1.3628116},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2011/lin11SEGrssde/lin11SEGrssde_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2011/lin11SEGrssde/lin11SEGrssde.pdf}
}


@CONFERENCE{lin2010EAGEseo,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Stabilized estimation of primaries via sparse inversion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2010},
  month = {06},
  abstract = {Estimation of Primaries by Sparse Inversion (EPSI) is a
                  recent method for surface-related multiple removal
                  using a direct estimation method closely related to
                  Amundsen inversion, where under a sparsity
                  assumption the primary impulse response is
                  determined directly from a data-driven wavefield
                  inversion process. One of the major difficulties in
                  its practical adoption is that one must have precise
                  knowledge of a time-window that contains
                  multiple-free primaries during each
                  update. Moreover, due to the nuances involved in
                  regularizing the model impulse response in the
                  inverse problem, the EPSI approach has an additional
                  number of inversion parameters where it may be
                  difficult to choose a reasonable value. We show that
                  the specific sparsity constraint on the EPSI updates
                  lead to an inherently intractable problem, and that
                  the time-window and other inversion variables arise
                  in the context of additional regularizations that
                  attempts to drive towards a meaningful solution. We
                  furthermore suggest a way to remove almost all of
                  these parameters via convexification, which
                  stabilizes the inversion while preserving the
                  crucial sparsity assumption in the primary impulse
                  response model.},
  keywords = {Presentation, EAGE, processing},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2010/lin10EAGEseo/lin10EAGEseo_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2010/lin10EAGEseo/lin10EAGEseo.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=39122}
}


@CONFERENCE{lin2009EAGEdsa,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Designing simultaneous acquisitions with compressive sensing},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2009},
  month = {06},
  abstract = {The goal of this paper is in designing a functional
                  simultaneous acquisition scheme by applying the
                  principles of compressive sensing. By framing the
                  acquisition in a compressive sensing setting we
                  immediately gain insight into not only how to choose
                  the source signature and shot patterns, but also in
                  how well we can hope to demultiplex the data when
                  given a set amount of reduction in the number of
                  sweeps. The principles of compressive sensing
                  dictates that the quality of the demultiplexed data
                  is closely related to the transform-domain sparsity
                  of the solution. This means that, given an estimate
                  in the complexity of the expectant data wavefield,
                  it is possible to controllably reduce the number of
                  shots that needs to be recorded in the field. We
                  show a proof of concept by introducing an
                  acquisition compatible with compressive sensing
                  based on randomly phase-encoded vibroseis sweeps.},
  keywords = {Presentation, EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2009/lin09EAGEdsa/lin2009EAGEdsa_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2009/lin09EAGEdsa/lin09EAGEdsa.pdf},
  yrl2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=23951}
}


@CONFERENCE{lin2009SEGucs,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Unified compressive sensing framework for simultaneous acquisition with primary estimation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  month = {10},
  volume = {28},
  pages = {3113-3117},
  abstract = {The central promise of simultaneous acquisition is a
                  vastly improved crew efficiency during acquisition
                  at the cost of additional post-processing to obtain
                  conventional source-separated data volumes. Using
                  recent theories from the field of compressive
                  sensing, we present a way to systematically model
                  the effects of simultaneous acquisition. Our
                  formulation form a new framework in the study of
                  acquisition design and naturally leads to an
                  inversion-based approach for the separation of shot
                  records. Furthermore, we show how other
                  inversion-based methods, such as a recently proposed
                  method from van Groenestijn and Verschuur (2009) for
                  primary estimation, can be processed together with
                  the demultiplexing problem to achieve a better
                  result compared to a separate treatment of these
                  problems.},
  keywords = {Presentation, SEG},
  doi = {10.1190/1.3255502},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/lin09SEGucs/lin09SEGucs_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/lin09SEGucs/lin09SEGucs.pdf}
}


@CONFERENCE{lin2008SINBADcwe,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Compressed wavefield extrapolation},
  booktitle = {SINBAD},
  year = {2008},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/sites/data/Papers/lin08cwe.pdf}
}


@CONFERENCE{lin2007SEGcwe1,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Compressed wavefield extrapolation with curvelets},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  pages = {1997-2001},
  abstract = {An explicit algorithm for the extrapolation of one-way
                  wavefields is proposed which combines recent
                  developments in information theory and theoretical
                  signal processing with the physics of wave
                  propagation. Because of excessive memory
                  requirements, explicit formulations for wave
                  propagation have proven to be a challenge in 3-D. By
                  using ideas from
                  {\textquoteleft}{\textquoteleft}compressed
                  sensing{\textquoteright}{\textquoteright}, we are
                  able to formulate the (inverse) wavefield
                  extrapolation problem on small subsets of the data
                  volume, thereby reducing the size of the
                  operators. According to compressed sensing theory,
                  signals can successfully be recovered from an
                  imcomplete set of measurements when the measurement
                  basis is incoherent with the representation in which
                  the wavefield is sparse. In this new approach, the
                  eigenfunctions of the Helmholtz operator are
                  recognized as a basis that is incoherent with
                  curvelets that are known to compress seismic
                  wavefields. By casting the wavefield extrapolation
                  problem in this framework, wavefields can
                  successfully be extrapolated in the modal domain via
                  a computationally cheaper operation. A proof of
                  principle for the
                  {\textquoteleft}{\textquoteleft}compressed
                  sensing{\textquoteright}{\textquoteright} method is
                  given for wavefield extrapolation in 2-D. The
                  results show that our method is stable and produces
                  identical results compared to the direct application
                  of the full extrapolation operator. {\copyright}2007
                  Society of Exploration Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.2792882},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/lin07SEGcwe1/lin07SEGcwe1_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/lin07SEGcwe1/lin07SEGcwe1.pdf }
}


@CONFERENCE{lin2009DELPHIrwi,
  author = {Tim T.Y. Lin and Felix J. Herrmann and Yogi A. Erlangga},
  title = {Randomized wavefield inversion},
  booktitle = {DELPHI},
  year = {2009},
  keywords = {Presentation},
  note = {Presented at the DELPHI meeting. The Hague},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/Delphi/2009/lin2009DELPHIrwi/lin2009DELPHIrwi.pdf}
}


@CONFERENCE{lin2008SEGiso,
  author = {Tim T.Y. Lin and Evgeniy Lebed and Yogi A. Erlangga and Felix J. Herrmann},
  title = {Interpolating solutions of the {Helmholtz} equation with compressed sensing},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  pages = {2122-2126},
  organization = {SEG},
  abstract = {We present an algorithm which allows us to model
                  wavefields with frequency-domain methods using a
                  much smaller number of frequencies than that
                  typically required by the classical sampling theory
                  in order to obtain an alias-free result. The
                  foundation of the algorithm is the recent results on
                  the compressed sensing, which state that data can be
                  successfully recovered from an incomplete
                  measurement if the data is sufficiently
                  sparse. Results from numerical experiment show that
                  only 30\% of the total frequency spectrum is need to
                  capture the full wavefield information when working
                  in the hard 2D synthetic Marmousi model.},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.3059307},
  month = {01},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/lin08SEGiso/lin08SEGiso.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/lin08SEGiso/lin08SEGiso.pdf }
}


@CONFERENCE{lin2010SEGspm,
  author = {Tim T.Y. Lin and Ning Tu and Felix J. Herrmann},
  title = {Sparsity-promoting migration from surface-related multiples},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2010},
  month = {10},
  volume = {29},
  pages = {3333-3337},
  organization = {SEG},
  abstract = {Seismic imaging typically begins with the removal of
                  multiple energy in the data, out of fear that it may
                  introduce erroneous structure. However, seismic
                  multiples have effectively seen more of the
                  earth{\textquoteright}s structure, and if treated
                  correctly can potential supply more information to a
                  seismic image compared to primaries. Past approaches
                  to accomplish this leave ample room for improvement;
                  they either require extensive modification to
                  standard migration techniques, rely too much on
                  prior information, require extensive pre-processing,
                  or resort to full-waveform inversion. We take some
                  valuable lessons from these efforts and present a
                  new approach balanced in terms of ease of
                  implementation, robustness, efficiency and
                  well-posedness, involving a sparsity-promoting
                  inversion procedure using standard Born migration
                  and a data-driven multiple modeling approach based
                  on the focal transform.},
  keywords = {Presentation, SEG, processing},
  doi = {10.1190/1.3513540},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2010/lin10SEGspm/lin10SEGspm_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2010/lin10SEGspm/lin10SEGspm.pdf}
}


@CONFERENCE{mansour2012SSPwspgl1,
  author = {Hassan Mansour},
  title = {Beyond $\ell_1$ norm minimization for sparse signal recovery},
  booktitle = {2012 IEEE Statistical Signal Processing Workshop (SSP) (SSP'12)},
  year = {2012},
  address = {Ann Arbor, Michigan, USA},
  organization = {IEEE},
  abstract = {Sparse signal recovery has been dominated by the basis
                  pursuit denoise (BPDN) problem formulation for over
                  a decade. In this paper, we propose an algorithm
                  that outperforms BPDN in finding sparse solutions to
                  underdetermined linear systems of equations at no
                  additional computational cost. Our algorithm, called
                  WSPGL1, is a modification of the spectral projected
                  gradient for $\ell_1$ minimization (SPGL1) algorithm
                  in which the sequence of LASSO subproblems are
                  replaced by a sequence of weighted LASSO subproblems
                  with constant weights applied to a support
                  estimate. The support estimate is derived from the
                  data and is updated at every iteration. The
                  algorithm also modifies the Pareto curve at every
                  iteration to reflect the new weighted $\ell_1$
                  minimization problem that is being solved. We
                  demonstrate through extensive simulations that the
                  sparse recovery performance of our algorithm is
                  superior to that of $\ell_1$ minimization and
                  approaches the recovery performance of iterative
                  re-weighted $\ell_1$ (IRWL1) minimization of
                  Cand{\`e}s, Wakin, and Boyd. Moreover, our algorithm
                  has the computational cost of a single BPDN
                  problem.},
  keywords = {sparse recovery, compressed sensing, iterative algorithms, weighted $\ell_1$ minimization, partial support recovery},
  month = {03},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SSP/2012/mansour2012SSPwspgl1/mansour2012SSPwspgl1_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SSP/2012/mansour2012SSPwspgl1/mansour2012SSPwspgl1.pdf}
}


@CONFERENCE{mansour2011SBGFcspsma,
  author = {Hassan Mansour and Haneet Wason and Tim T.Y. Lin and Felix J. Herrmann},
  title = {A compressive sensing perspective on simultaneous marine acquisition},
  booktitle = {SBGF},
  year = {2011},
  organization = {SBGF},
  abstract = {The high cost of acquiring seismic data in marine
                  environments compels the adoption of simultaneous-
                  source acquisition - an emerging technology that is
                  stimulating both geophysical research and commercial
                  efforts. In this paper, we discuss the properties of
                  randomized simultaneous acquisition matrices and
                  demonstrate that sparsity-promoting recovery
                  improves the quality of the reconstructed seismic
                  data volumes. Simultaneous marine acquisition calls
                  for the development of a new set of design
                  principles and post-processing tools. Leveraging
                  established findings from the field of compressed
                  sensing, the recovery from simultaneous sources
                  depends on a sparsifying transform that compresses
                  seismic data, is fast, and reasonably incoherent
                  with the compressive sampling matrix. To achieve
                  this incoherence, we use random time dithering where
                  sequential acquisition with a single airgun is
                  replaced by continuous acquisition with multiple
                  airguns firing at random times and at random
                  locations. We demonstrate our results with
                  simulations of simultaneous Marine acquisition using
                  periodic and randomized time dithering.},
  keywords = {Presentation, SBGF, acquisition, compressive sensing},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SBGF/2011/Mansour11SBGFcspsma/Mansour11SBGFcspsma.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SBGF/2011/Mansour11SBGFcspsma/Mansour11SBGFcspsma.pdf}
}


@CONFERENCE{mansour2012ICASSadapt,
  author = {Hassan Mansour and Ozgur Yilmaz},
  title = {Adaptive compressed sensing for video acquisition},
  booktitle = {ICASSP},
  year = {2012},
  optorganization = {ICASSP},
  abstract = {In this paper, we propose an adaptive compressed sensing
                  scheme that utilizes a support estimate to focus the
                  measurements on the large valued coefficients of a
                  compressible signal. We embed a "sparse-filtering"
                  stage into the measurement matrix by weighting down
                  the contribution of signal coefficients that are
                  outside the support estimate. We present an
                  application which can benefit from the proposed
                  sampling scheme, namely, video compressive
                  acquisition. We demonstrate that our proposed
                  adaptive CS scheme results in a significant
                  improvement in reconstruction quality compared with
                  standard CS as well as adaptive recovery using
                  weighted $\ell_1$ minimization.},
  keywords = {ICASSP},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ICASSP/2012/MansourYilmazICASSPaCS/MansourYilmazICASSPaCS.pdf}
}


@CONFERENCE{mansour2012ICASSsupport,
  author = {Hassan Mansour and Ozgur Yilmaz},
  title = {Support driven reweighted $\ell_1$ minimization},
  booktitle = {ICASSP},
  year = {2012},
  optorganization = {ICASSP},
  abstract = {In this paper, we propose a support driven reweighted
                  $\ell_1$ minimization algorithm (SDRL1) that solves
                  a sequence of weighted $\ell_1$ problems and relies
                  on the support estimate accuracy. Our SDRL1
                  algorithm is related to the IRL1 algorithm proposed
                  by Candes, Wakin, and Boyd. We demonstrate that it
                  is sufficient to find support estimates with good
                  accuracy and apply constant weights instead of using
                  the inverse coefficient magnitudes to achieve gains
                  similar to those of IRL1. We then prove that given a
                  support estimate with sufficient accuracy, if the
                  signal decays according to a specific rate, the
                  solution to the weighted $\ell_1$ minimization
                  problem results in a support estimate with higher
                  accuracy than the initial estimate. We also show
                  that under certain conditions, it is possible to
                  achieve higher estimate accuracy when the
                  intersection of support estimates is considered. We
                  demonstrate the performance of SDRL1 through
                  numerical simulations and compare it with that of
                  IRL1 and standard $\ell_1$ minimization.},
  keywords = {ICASSP},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ICASSP/2012/MansourYilmazICASSPwL1/MansourYilmazICASSPwL1.pdf}
}


@CONFERENCE{maysami2006SINBADrro,
  author = {Mohammad Maysami},
  title = {Recent results on seismic deconvolution},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {One of the important steps in seismic imaging is to
                  provide suitable information about boundaries. Sharp
                  variation of physical properties at a layer boundary
                  cause reflection the wavefield. In previous work
                  done by C. M. Dupuis, seismic signal
                  characterization is divided into two steps:
                  detection and estimation. In the detection phase,
                  the goal is to find all singularities in a seismic
                  section regardless of their order and then to
                  categorize the data to different events by windowing
                  each singularity. In the estimation step, we
                  determine the order of singularity more precisely by
                  using a rough estimate based on the detection
                  phase. Traditionally, a redundant dictionary method
                  is employed for the detection part. However, we
                  attempt to instead use a new L1-solver developed by
                  D.L. Donoho: the Stagewise Orthogonal Matching
                  Pursuit (StOMP). It approximates the solution to
                  inverse problems while promoting the sparsity in the
                  solution vector. This algorithm will allow us to
                  experimentally confirm the recent analysis by
                  S. Mallat on spiky deconvolution limits, which
                  imposes a required minimum distance between
                  spikes. This required minimum distance between
                  different spikes is dependent on the number of
                  spikes as well as the width of the chosen source
                  wavelet used in convolution with the train. These
                  results allow for the design of more robust and
                  accurate detection schemes for seismic signal
                  characterization.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/maysami06SINBADrro/maysami06SINBADrro_pres.pdf}
}


@CONFERENCE{maysami2008SEGlcf,
  author = {Mohammad Maysami and Felix J. Herrmann},
  title = {Lithological constraints from seismic waveforms: application to opal-{A} to opal-{CT} transition},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  pages = {2011-2015},
  organization = {SEG},
  abstract = {In this paper, we present a new method for seismic
                  waveform characterization whose aim is threefold,
                  namely (i) extraction of detailed information on the
                  sharpness of transitions in the subsurface from
                  seismic waveforms, (ii) reflector modeling, based on
                  binary-mixture and percolation theory, and (iii)
                  establishment of well-seismic ties, through
                  parameterizations of our waveform and critical
                  reflector model. We test this methodology on the
                  opal-A (Amorphous) to opal-CT
                  (Cristobalite/Tridymite) transition imaged in a
                  migrated section of North Sea field data West of the
                  Shetlands.},
  keywords = {SEG, SLIM},
  doi = {10.1190/1.3059400},
  month = {11},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/maysami08SEGlcf/maysami08SEGlcf.pdf}
}


@CONFERENCE{maysami2007EAGEsrc,
  author = {Mohammad Maysami and Felix J. Herrmann},
  title = {Seismic reflector characterization by a multiscale detection-estimation method},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2007},
  month = {06},
  abstract = {Seismic transitions of the subsurface are typically
                  considered as zero-order singularities (step
                  functions). According to this model, the
                  conventional deconvolution problem aims at
                  recovering the seismic reflectivity as a sparse
                  spike train. However, recent multiscale analysis on
                  sedimentary records revealed the existence of
                  accumulations of varying order singularities in the
                  subsurface, which give rise to fractional-order
                  discontinuities. This observation not only calls for
                  a richer class of seismic reflection waveforms, but
                  it also requires a different methodology to detect
                  and characterize these reflection events. For
                  instance, the assumptions underlying conventional
                  deconvolution no longer hold. Because of the
                  bandwidth limitation of seismic data, multiscale
                  analysis methods based on the decay rate of wavelet
                  coefficients may yield ambiguous results. We avoid
                  this problem by formulating the estimation of the
                  singularity orders by a parametric nonlinear
                  inversion method.},
  keywords = {Presentation, SLIM, EAGE},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/maysami07EAGEsrc/maysami07EAGEsrc_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/maysami07EAGEsrc/maysami07EAGEsrc.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=7081}
}


@CONFERENCE{modzelewski2008SINBADdas,
  author = {Henryk Modzelewski},
  title = {Design and specifications for {SLIM{\textquoteright}s} software framework},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {The SLIM group is actively developing software for
                  seismic imaging. This talk will give a general
                  overview of the software development during SINBAD
                  project with focus on the final release in February
                  2008. The covered topics will include: 1) adopting
                  Python for object-oriented programming, 2) including
                  parallelism into the algorithms used in seismic
                  imaging/modeling, 3) in-house algorithms for seismic
                  imaging, and 4) contributions to Madagascar
                  (RSF). The talk will serve as an introduction to the
                  other presentations in the session "SINBAD Software
                  releases".},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/modzelewski2008SINBADdas/modzelewski2008SINBADdas.pdf}
}


@CONFERENCE{modzelewski2006SINBADdas,
  author = {Henryk Modzelewski},
  title = {Design and specifications for {SLIM{\textquoteright}s} software framework},
  booktitle = {SINBAD 2006},
  year = {2006},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/modzelewski2006SINBADdas/modzelewski2006SINBADdas.pdf}
}


@CONFERENCE{moghaddam2008SINBADrtm,
  author = {Peyman P. Moghaddam},
  title = {Reverse-time migration amplitude recovery with curvelets},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We recover the amplitude of a seismic image by
                  approximating the normal (demigration-migration)
                  operator. In this approximation, we make use of the
                  property that curvelets remain invariant under the
                  action of the normal operator. We propose a seismic
                  amplitude recovery method that employs an eigenvalue
                  like decomposition for the normal operator using
                  curvelets as eigenvectors. Subsequently, we propose
                  an approximate nonlinear singularity-preserving
                  solution to the least-squares seismic imaging
                  problem with sparseness in the curvelet domain and
                  spatial continuity constraints. Our method is tested
                  with a reverse-time
                  {\textquoteright}wave-equation{\textquoteright}
                  migration code simulating the acoustic wave
                  equation.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/moghaddam2008SINBADrtm/moghaddam2008SINBADrtm.pdf}
}


@CONFERENCE{moghaddam2006SINBADioa,
  author = {Peyman P. Moghaddam},
  title = {Imaging operator approximation using curvelets},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {In this presentation, the normal (demigation-migration)
                  operator is studied in terms of a
                  pseudo-differential operator. The invariance of
                  curvelets under this operator and their sparsity on
                  the seismic images is used to precondition the
                  migration operator. A brief overview will be given
                  on some of the theory from micro-local analysis
                  which proofs that curvelets remain approximately
                  invariant under the operator. The proper setting for
                  which a diagonal approximation in the curvelet
                  domain is accurate is discussed together with
                  different methods that estimate this diagonal from
                  of-the-shelf migration operators. This is joint work
                  with Chris Stolk.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/moghaddam2006SINBADioa/moghaddam2006SINBADioa.pdf}
}


@CONFERENCE{moghaddam2006SINBADsac,
  author = {Peyman P. Moghaddam},
  title = {Sparsity- and continuity-promoting norms for seismic images},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {During this presentation, the importance of sparsity and
                  continuity enhancing energy norms is emphasized for
                  seismic imaging and inversion. The continuity
                  promoting energy norm is justified by the apparent
                  smoothness of reflectors in the direction along and
                  the oscillatory behavior across the interfaces. This
                  energy norm is called anisotropic diffusion and will
                  be defined mathematically. Denoising examples will
                  be given during which seismic images are recovered
                  from the noise by a joint norm-one and continuity
                  promoting minimization.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/moghaddam2006SINBADsac/moghaddam2006SINBADsac.pdf}
}


@CONFERENCE{moghaddam2008SEGcbm,
  author = {Peyman P. Moghaddam and Cody R. Brown and Felix J. Herrmann},
  title = {Curvelet-based migration preconditioning},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  pages = {2211-2215},
  organization = {SEG},
  abstract = {In this paper, we introduce a preconditioner for seismic
                  imaging{\textendash}-i.e., the inversion of the
                  linearized Born scattering operator. This
                  preconditioner approximately corrects for the
                  {\textquoteleft}{\textquoteleft}square
                  root{\textquoteright}{\textquoteright} of the
                  normal{\textendash}-i.e., the demigration-migration
                  operator. This approach consists of three parts,
                  namely (i) a left preconditoner, defined by a
                  fractional time integration designed to make the
                  migration operator zero order, and two right
                  preconditioners that apply (ii) a scaling in the
                  physical domain accounting for a spherical
                  spreading, and (iii) a curvelet-domain scaling that
                  corrects for spatial and reflector-dip dependent
                  amplitude errors. We show that a combination of
                  these preconditioners lead to a significant
                  improvement of the convergence for iterative
                  least-squares solutions to the seismic imaging
                  problem based on reverse-time migration operators.},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.3059325},
  month = {11},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/moghaddam08SEGcbm/moghaddam08SEGcbm_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/moghaddam08SEGcbm/moghaddam08SEGcbm.pdf }
}


@CONFERENCE{moghaddam2010SEGrfw,
  author = {Peyman P. Moghaddam and Felix J. Herrmann},
  title = {Randomized full-waveform inversion: a dimenstionality-reduction approach},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2010},
  month = {10},
  volume = {29},
  pages = {977-982},
  organization = {SEG},
  abstract = {Full-waveform inversion relies on the collection of
                  large multi-experiment data volumes in combination
                  with a sophisticated back-end to create
                  high-fidelity inversion results. While improvements
                  in acquisition and inversion have been extremely
                  successful, the current trend of incessantly pushing
                  for higher quality models in increasingly
                  complicated regions of the Earth reveals fundamental
                  shortcomings in our ability to handle increasing
                  problem sizes numerically. Two main culprits can be
                  identified. First, there is the so-called
                  {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which puts disproportionate strain on
                  current acquisition and processing systems as the
                  size and desired resolution increases. Secondly,
                  there is the recent
                  {\textquoteleft}{\textquoteleft}departure from
                  Moore{\textquoteright}s
                  law{\textquoteright}{\textquoteright} that forces us
                  to develop algorithms that are amenable to
                  parallelization. In this paper, we discuss different
                  strategies that address these issues via randomized
                  dimensionality reduction.},
  keywords = {Presentation, SEG, full-waveform inversion, optimization},
  doi = {10.1190/1.3513940},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2010/moghaddam10SEGrfw/moghaddam10SEGrfw_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2010/moghaddam10SEGrfw/moghaddam10SEGrfw.pdf}
}


@CONFERENCE{moghaddam2004SEGmpw,
  author = {Peyman P. Moghaddam and Felix J. Herrmann},
  title = {Migration preconditioning with curvelets},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2004},
  volume = {23},
  pages = {2204-2207},
  organization = {SEG},
  abstract = {In this paper, the property of Curvelet transforms for
                  preconditioning the migration and normal operators
                  is investigated. These operators belong to the class
                  of Fourier integral operators and
                  pseudo-differential operator, respectively. The
                  effect of this pre-conditioner is shown in term of
                  improvement of sparsity, convergence rate, number of
                  iteration for the Krylov-subspace solver and
                  clustering of singular(eigen) values. The migration
                  operator, which we employed in this work is the
                  common-offset Kirchoff-Born
                  migration. {\copyright}2004 Society of Exploration
                  Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.1845213},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2004/Moghaddam04SEGmpw/Moghaddam04SEGmpw_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2004/Moghaddam04SEGmpw/Moghaddam04SEGmpw_paper.pdf}
}


@CONFERENCE{moghaddam2007CSEGmar,
  author = {Peyman P. Moghaddam and Felix J. Herrmann and Christiaan C. Stolk},
  title = {Migration amplitude recovery using curvelets},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2007},
  organization = {CSEG},
  abstract = {In this paper, we recover the amplitude of a seismic
                  image by approximating the normal operator and
                  subsequently inverting it. Normal operator
                  (migration followed by modeling) is an example of
                  pseudo-differential.  curvelets are proven to be
                  invariant under the action of pseudo-differential
                  operators under certain conditions. Subsequently,
                  curvelets are forming as eigen-vectors for such an
                  operator. We propose a seismic amplitude recovery
                  method that employs an eigen-value decomposition for
                  normal operator using curvelets as eigen-vectors and
                  to be estimated eigenvalues. A post-stack
                  reverse-time, wave-equation migration is used for
                  evaluation of the proposed method.},
  file = {http://cseg.ca/assets/files/resources/abstracts/2007/168S0131.pdf},
  keywords = {SLIM},
  month = {05},
  url = {http://cseg.ca/assets/files/resources/abstracts/2007/168S0131.pdf}
}


@CONFERENCE{moghaddam2007CSEGsac,
  author = {Peyman P. Moghaddam and Felix J. Herrmann and Christiaan C. Stolk},
  title = {Sparsity and continuity enhancing seismic imaging},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2007},
  organization = {CSEG},
  abstract = {A non-linear singularity-preserving solution to the
                  least-squares seismic imaging problem with
                  sparseness and continuity constraints is
                  proposed. The applied formalism explores curvelets
                  as a directional frame that, by their sparsity on
                  the image, and their invariance under the imaging
                  operators, allows for a stable recovery of the
                  amplitudes. Our method is based on the estimation of
                  the normal operator in the form of an
                  {\textquoteright}eigenvalue{\textquoteright}
                  decompsoition with curvelets as the
                  eigenvectors{\textquoteright}. Subsequently, we
                  propose an inversion method that derives from
                  estimation of the normal operator and is formulated
                  as a convex optimization problem. Sparsity in the
                  curvelet domain as well as continuity along the
                  reflectors in the image domain are promoted as part
                  of this optimization. Our method is tested with a
                  reverse-time
                  {\textquoteright}wave-equation{\textquoteright}
                  migration code simulating the acoustic wave
                  equation.},
  file = {http://cseg.ca/assets/files/resources/abstracts/2007/091S0130.pdf},
  keywords = {SLIM},
  month = {05},
  url = {http://cseg.ca/assets/files/resources/abstracts/2007/091S0130.pdf}
}


@CONFERENCE{moghaddam2007EAGEsar,
  author = {Peyman P. Moghaddam and Felix J. Herrmann and Christiaan C. Stolk},
  title = {Seismic amplitude recovery with curvelets},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2007},
  month = {06},
  abstract = {A non-linear singularity-preserving solution to the
                  least-squares seismic imaging problem with
                  sparseness and continuity constraints is
                  proposed. The applied formalism explores curvelets
                  as a directional frame that, by their sparsity on
                  the image, and their invariance under the imaging
                  operators, allows for a stable recovery of the
                  amplitudes. Our method is based on the estimation of
                  the normal operator in the form of an
                  {\textquoteright}eigenvalue{\textquoteright}
                  decompsoition with curvelets as the
                  {\textquoteright}eigenvectors{\textquoteright}.
                  Subsequently, we propose an inversion method that
                  derives from estimation of the normal operator and
                  is formulated as a convex optimization
                  problem. Sparsity in the curvelet domain as well as
                  continuity along the reflectors in the image domain
                  are promoted as part of this optimization. Our
                  method is tested with a reverse-time
                  {\textquoteright}wave-equation{\textquoteright}
                  migration code simulating the acoustic wave
                  equation.},
  keywords = {SLIM, EAGE},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/moghaddam07EAGEsar/moghaddam07EAGEsar.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=6935}
}


@CONFERENCE{moghaddam2007SEGrsi,
  author = {Peyman P. Moghaddam and Felix J. Herrmann and Christiaan C. Stolk},
  title = {Robust seismic-images amplitude recovery using curvelets},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  pages = {2225-2229},
  organization = {SEG},
  abstract = {In this paper, we recover the amplitude of a seismic
                  image by approximating the normal
                  (demigration-migration) operator. In this
                  approximation, we make use of the property that
                  curvelets remain invariant under the action of the
                  normal operator. We propose a seismic amplitude
                  recovery method that employs an eigenvalue like
                  decomposition for the normal operator using
                  curvelets as eigen-vectors. Subsequently, we propose
                  an approximate non-linear singularity-preserving
                  solution to the least-squares seismic imaging
                  problem with sparseness in the curvelet domain and
                  spatial continuity constraints. Our method is tested
                  with a reverse-time
                  {\textquoteleft}wave-equation{\textquoteright}
                  migration code simulating the acoustic wave equation
                  on the SEG-AA salt model. {\copyright}2007 Society
                  of Exploration Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.2792928},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/moghaddam07SEGrsi/moghaddam07SEGrsi_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/moghaddam07SEGrsi/moghaddam07SEGrsi.pdf }
}


@CONFERENCE{min2012CSEGrgfe,
  author = {Ju-Won Oh and Dong-Joo Min and Felix J. Herrmann},
  title = {Re-establishment of gradient in frequency-domain elastic waveform inversion},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2012},
  abstract = {To obtain solutions close to global minimum in waveform
                  inversion, the gradients computed at each frequency
                  need to be weighted to appropriately describe the
                  residuals between modeled and field data. While the
                  low-frequency components of the gradients should be
                  weighted to recover the long-wavelength structures,
                  the high-frequency components of the gradients need
                  to be weighted when the short-wavelength structures
                  are restored. However, the conventional elastic
                  waveform inversion algorithms cannot properly weight
                  the gradients computed at each frequency. When
                  gradients are scaled using the pseudo-Hessian matrix
                  inside the frequency loop, gradients obtained at
                  high frequencies are over-emphasized. When the
                  gradients are scaled outside the frequency loop,
                  gradients are weighted by the source spectra. In
                  this study, we propose applying weighting factors to
                  the gradients obtained at each frequency so that
                  gradients can properly reflect the differences
                  between the true and assumed models satisfying the
                  general inverse theory. The weighting factors are
                  composed by the backpropagated residuals. Numerical
                  examples for the simple rectangular-shaped model and
                  the modified version of the Marmousi-2 model show
                  that the weighting method enhances gradient images
                  and inversion results compared to the conventional
                  inversion algorithms.},
  keywords = {elastic, waveform inversion, frequency-domain, weighting factors},
  month = {02},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2012/min2012CSEGrgfe/min2012CSEGrgfe.pdf}
}


@CONFERENCE{min2012EAGEefwi,
  author = {Ju-Won Oh and Dong-Joo Min and Felix J. Herrmann},
  title = {Frequency-domain elastic waveform inversion using weighting factors related to source-deconvolved residuals},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2012},
  month = {06},
  abstract = {One of the limitations in seismic waveform inversion is
                  that inversion results are very sensitive to initial
                  guesses, which may be because the gradients computed
                  at each frequency are not properly weighted
                  depending on given models.Analyzingthe conventional
                  waveform inversion algorithms using the
                  pseudo-Hessian matrix as a pre-conditioner shows
                  that the gradientsdo not properly describe the
                  feature of given models or high- and low-end
                  frequencies do not contribute the model parameter
                  updates due to banded spectra of source wavelet. For
                  a better waveform inversion algorithm, we propose
                  applying weighting factors to gradients computed at
                  each frequency. The weighting factors are designed
                  using the source-deconvolved back-propagated
                  wavefields. Numerical results for the SEG/EAGE salt
                  model show that the weighting method improves
                  gradient images and its inversion results are
                  compatible with true velocities even with poorly
                  estimated initial guesses.},
  keywords = {EAGE, elastic, waveform inversion, frequency-domain, weighting factors},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/min2012EAGEefwi/min2012EAGEefwi.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=59623}
}


@CONFERENCE{ross2008SINBADsit,
  author = {Sean Ross-Ross},
  title = {Seismic inversion through operator overloading},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {Geophysical processing is dominated by many different
                  out of core memory software environments
                  (OOCE). Such environments include Madagascar and SU
                  and are designed to handle data that can not be
                  operated on in memory. Each base operation is
                  created as a main program that reads data from disk
                  and writes the result to disk. The main programs can
                  also be chained together on stdin/out pipes using a
                  shell only writing data to disk at the end. To be
                  efficient, the algorithm using an OOCE must chain
                  together the longest pipe to avoid disk I/O, as a
                  result it is very difficult to use iterative
                  techniques. The algorithms are written in shell
                  scripts can be difficult to read and understand.
                  SLIMpy is a software library that contains
                  definitions of coordinate free vectors and linear
                  operators. It allows the user to design and run
                  algorithms with any out of core package, in a Matlab
                  style interface while maintaining optimal efficiency
                  and speed. SLIMpy looks at each main program of each
                  OOCE as a Matrix vector operation or vector
                  reduction/transformation operation. It uses operator
                  overloading to generate an abstract syntax tree
                  (AST) which can be optimized in many ways before
                  executing its commands. The AST also provides a
                  pathway for embarrassingly parallel applications by
                  splitting the tree over different nodes and
                  processors. SLIMpy provides an interface to these
                  OOCE that allows for optimal construction of
                  commands and allows for iterative techniques. It
                  smoothes the transition from other languages such as
                  Matlab and allows the algorithm designer to write
                  readable and reusable code. SLIMpy also adds to OOCE
                  by allowing for easy parallelization.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/ross2008SINBADsit/ross2008SINBADsit.pdf}
}


@CONFERENCE{saab2008SINBADcps,
  author = {Rayan Saab},
  title = {Curvelet-based primary-multiple separation from a {Bayesian} perspective},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We present a novel primary-multiple separation scheme
                  which makes use of the sparsity of both primaries
                  and multiples in a transform domain, such as the
                  curvelet transform, to provide estimates of each.
                  The proposed algorithm utilizes seismic data as well
                  as the output of a preliminary step that provides
                  (possibly) erroneous predictions of the
                  multiples. The algorithm separates the signal
                  components, i.e., the primaries and multiples, by
                  solving an optimization problem that assumes noisy
                  input data and can be derived from a Bayesian
                  perspective. More precisely, the optimization
                  problem can be arrived at via an assumption of a
                  weighted Laplacian distribution for the primary and
                  multiple coefficients in the transform domain and of
                  white Gaussian noise contaminating both the seismic
                  data and the preliminary prediction of the
                  multiples, which both serve as input to the
                  algorithm.},
  date-modified = {2008-08-22 12:45:53 -0700},
  keywords = {SLIM, SINBAD, Presentation},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/saab2008SINBADcps/saab2008SINBADcp.pdf}
}


@CONFERENCE{saab2008ICASSPssa,
  author = {Rayan Saab and Rick Chartrand and Ozgur Yilmaz},
  title = {Stable sparse approximations via nonconvex optimization},
  booktitle = {ICASSP},
  year = {2008},
  organization = {ICASSP},
  abstract = {We present theoretical results pertaining to the ability
                  of lp minimization to recover sparse and
                  compressible signals from incomplete and noisy
                  measurements. In particular, we extend the results
                  of Cande`s, Romberg and Tao [1] to the p < 1
                  case. Our results indicate that depending on the
                  restricted isometry constants (see, e.g.,[2] and
                  [3]) and the noise level, lp minimization with
                  certain values of p < 1 provides better theoretical
                  guarantees in terms of stability and robustness than
                  l1 minimization does. This is especially true when
                  the restricted isometry constants are relatively
                  large.},
  keywords = {ICASSP},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/ICASSP/2008/saab08ICASSPssa/saab08ICASSPssa.pdf }
}


@CONFERENCE{saab2007SEGcbp,
  author = {Rayan Saab and Deli Wang and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Curvelet-based primary-multiple separation from a {Bayesian} perspective},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  pages = {2510-2514},
  organization = {SEG},
  abstract = {In this abstract, we present a novel primary-multiple
                  separation scheme which makes use of the sparsity of
                  both primaries and multiples in a transform domain,
                  such as the curvelet transform, to provide estimates
                  of each. The proposed algorithm utilizes seismic
                  data as well as the output of a preliminary step
                  that provides (possibly) erroneous predictions of
                  the multiples. The algorithm separates the signal
                  components, i.e., the primaries and multiples, by
                  solving an optimization problem that assumes noisy
                  input data and can be derived from a Bayesian
                  perspective. More precisely, the optimization
                  problem can be arrived at via an assumption of a
                  weighted Laplacian distribution for the primary and
                  multiple coefficients in the transform domain and of
                  white Gaussian noise contaminating both the seismic
                  data and the preliminary prediction of the
                  multiples, which both serve as input to the
                  algorithm. {\copyright}2007 Society of Exploration
                  Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.2792988},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/saab07SEGcbp/saab07SEGcbp_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/saab07SEGcbp/saab07SEGcbp.pdf }
}


@CONFERENCE{saab2009SAMPTAnccs,
  author = {Rayan Saab and Ozgur Yilmaz},
  title = {A short note on non-convex compressed sensing},
  booktitle = {SAMPTA technical program},
  year = {2009},
  organization = {SAMPTA},
  abstract = {In this note, we summarize the results we recently
                  proved in\cite{SY08} on the theoretical performance
                  guarantees of the decoders $\Delta_p$.  These
                  decoders rely on $\ell^p$ minimization with $p {\i}n
                  (0,1)$ to recover estimates of sparse and
                  compressible signals from incomplete and inaccurate
                  measurements. Our guarantees generalize the results
                  of \cite{CRT05} and \cite{Wojtaszczyk08} about
                  decoding by $\ell_p$ minimization with $p = 1$, to
                  the setting where $p {\i}n (0,1)$ and are obtained
                  under weaker sufficient conditions. We also present
                  novel extensions of our results in \cite{SY08} that
                  follow from the recent work of DeVore et al. in
                  \cite{DPW08}. Finally, we show some insightful
                  numerical experiments displaying the trade-off in
                  the choice of $p \in (0,1]$ depending on certain
                  properties of the input signal.},
  keywords = {Presentation},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SAMPTA/2009/saab09SAMPTAnccs/saab09SAMPTAnccs.pdf}
}


@CONFERENCE{sastry2007SINBADnor,
  author = {Challa S. Sastry},
  title = {Norm-one recovery from irregular sampled data},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {Seismic traces are sampled irregularly and
                  insufficiently due to practical and economical
                  limitations. The use of such data in seismic imaging
                  results in image artifacts and poor spatial
                  resolution. Therefore, before being used, the
                  measurements are to be interpolated onto a regular
                  grid. One of the methods achieving this objective is
                  based on the Fourier reconstruction, which deals
                  with the under-determined system of equations. The
                  recent pursuit techniques (namely, basis pursuit,
                  matching pursuit etc) admit certain promising
                  features such as faster and simpler implementation
                  even in large scale settings.  The presentation
                  discusses the application of the pursuit algorithms
                  to the Fourier-based interpolation problem for the
                  signals that have sparse Fourier spectra. In
                  particular, the objective of the presentation
                  includes: 1). studying the performance of the
                  algorithm if, and how far, the measurement
                  coordinates can be shifted from uniform distribution
                  on the continuous interval. 2). studying what could
                  be the allowable misplacement in the measurement
                  coordinates that does not alter the quality of the
                  reconstruction process},
  keywords = {SLIM, SINBAD, Presentation}
}


@CONFERENCE{challa2007EAGEsrf,
  author = {Challa S. Sastry and Gilles Hennenfent and Felix J. Herrmann},
  title = {Signal reconstruction from incomplete and misplaced measurements},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2007},
  month = {06},
  abstract = {Constrained by practical and economical considerations,
                  one often uses seismic data with missing traces. The
                  use of such data results in image artifacts and poor
                  spatial resolution. Sometimes due to practical
                  limitations, measurements may be available on a
                  perturbed grid, instead of on the designated
                  grid. Due to algorithmic requirements, when such
                  measurements are viewed as those on the designated
                  grid, the recovery procedures may result in
                  additional artifacts. This paper interpolates
                  incomplete data onto regular grid via the Fourier
                  domain, using a recently developed greedy
                  algorithm. The basic objective is to study
                  experimentally as to what could be the size of the
                  perturbation in measurement coordinates that allows
                  for the measurements on the perturbed grid to be
                  considered as on the designated grid for faithful
                  recovery. Our experimental work shows that for
                  compressible signals, a uniformly distributed
                  perturbation can be offset with slightly more number
                  of measurements.},
  keywords = {SLIM, EAGE},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/challa07EAGEsrf/challa07EAGEsrf.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=6917}
}


@CONFERENCE{sastry2007SINBADrfu,
  author = {Challa S. Sastry and Gilles Hennenfent and Felix J. Herrmann},
  title = {Recovery from unstructured data},
  booktitle = {SINBAD 2006},
  year = {2006},
  keywords = {SLIM, SINBAD, Presentation},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/sastry2007SINBADrfu/sastry2007SINBADrfu.pdf}
}


@CONFERENCE{shahidi2009SEGcmf,
  author = {Reza Shahidi and Felix J. Herrmann},
  title = {Curvelet-domain matched filtering with frequency-domain regularization},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  month = {10},
  volume = {28},
  pages = {3645-3649},
  organization = {SEG},
  abstract = {In Herrmann et al. (2008), it is shown that zero-order
                  pseudodifferential operators, which model the
                  migration-demigration operator and the operator
                  mapping the predicted multiples to the true
                  multiples, can be represented by a diagonal
                  weighting in the curvelet domain. In that paper, a
                  smoothness constraint was introduced in the phase
                  space of the operator in order to regularize the
                  solution to make it unique.  In this paper, we use
                  recent results in Demanet and Ying (2008) on the
                  discrete symbol calculus to impose a further
                  smoothness constraint, this time in the frequency
                  domain. It is found that with this additional
                  constraint, faster convergence is realized. Results
                  on a synthetic pseudodifferential operator as well
                  as on an example of primary-multiple separation in
                  seismic data are included, comparing the model with
                  and without the new smoothness constraint, from
                  which it is found that results of improved quality
                  are also obtained.},
  keywords = {Presentation,SEG},
  doi = {10.1190/1.3255624},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/shahidi09SEGcmf/shahidi09segcmf.pdf}
}


@CONFERENCE{tang2009SEGhdb,
  author = {Gang Tang and Reza Shahidi and Felix J. Herrmann and Jianwei Ma},
  title = {Higher dimensional blue-noise sampling schemes for curvelet-based seismic data recovery},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  month = {10},
  volume = {28},
  pages = {191-195},
  organization = {SEG},
  abstract = {In combination with compressive sensing, a successful
                  reconstruction scheme called Curvelet-based Recovery
                  by Sparsity-promoting Inversion (CRSI) has been
                  developed, and has proven to be useful for seismic
                  data processing. One of the most important issues
                  for CRSI is the sampling scheme, which can greatly
                  affect the quality of reconstruction. Unlike usual
                  regular undersampling, stochastic sampling can
                  convert aliases to easy-to-eliminate noise. Some
                  stochastic sampling methods have been developed for
                  CRSI, e.g. jittered sampling, however most have only
                  been applied to 1D sampling along a line. Seismic
                  datasets are usually higher dimensional and very
                  large, thus it is desirable and often necessary to
                  develop higher dimensional sampling methods to deal
                  with these data. For dimensions higher than one, few
                  results have been reported, except uniform random
                  sampling, which does not perform well. In the
                  present paper, we explore 2D sampling methodologies
                  for curvelet-based reconstruction, possessing
                  sampling spectra with blue noise characteristics,
                  such as Poisson Disk sampling, Farthest Point
                  Sampling, and the 2D extension of jittered
                  sampling. These sampling methods are shown to lead
                  to better recovery and results are compared to the
                  other more traditional sampling protocols.},
  keywords = {Presentation, SEG},
  doi = {10.1190/1.3255230},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/tang09SEGhdb/tang09SEGhdb.pdf}
}


@CONFERENCE{thomson2006SINBADlss,
  author = {Darren Thomson},
  title = {Large-scale seismic data recovery by the parallel windowed curvelet transform},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {We propose using overlapping, tapered windows to process
                  seismic data in parallel. This method consists of
                  numerically tight linear operators and adjoints that
                  are suitable for use in iterative algorithms. This
                  method is also highly scalable and makes
                  parallelprocessing of large seismic data sets
                  feasible. We use this scheme to define the Parallel
                  Windowed Fast Discrete Curvelet Transform (PWFDCT),
                  which we have applied to a seismic data
                  interpolation algorithm. Some preliminary results
                  will be shown. Henryk Modzeleweski: Design and
                  specifications for SLIMPy's software framework The
                  SLIM group is actively developing software for
                  seismic imaging. This talk will give a general
                  overview of the software development philosophy
                  adopted by SLIM. The covered topics will include: 1)
                  adopting Python for object-oriented programming, 2)
                  including parallelism into the algorithms used in
                  seismic imaging/modeling, 3) in-house algorithms for
                  seismic imaging, and 4) contributions to Madagascar
                  (RSF). The talk will serve as an introduction to the
                  other presentations in the session "SINBAD Software
                  releases".},
  keywords = {SLIM, SINBAD, Presentation},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/thomson2006SINBADlss/thomson2006SINBADlss.pdf}
}


@CONFERENCE{thomson2006SINBADppe,
  author = {Darren Thomson},
  title = {{(P)SLIMPy}: parallel extension},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {The parallel extensions to the SLIMpy environment enable
                  pipe-based processing of large data sets in an
                  MPI-based parallel environment. Parallel processing
                  can be done by straightforward slicing of data, or
                  by using an overlapping domain decomposition that
                  requires communication between different
                  processors. The principal aim of the parallel
                  extensions is to leave abstract numerical algorithms
                  (ANA's) and applications programmed for use in
                  SLIMpy untouched when moving to parallel processing.
                  The object-oriented functionality of Python makes
                  this possible.},
  keywords = {SLIM, SINBAD, Presentation},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2006/thomson2006SINBADppe/thomson2006SINBADppe.pdf}
}


@CONFERENCE{thomson2006SEGpwfd,
  author = {Darren Thomson and Gilles Hennenfent and Henryk Modzelewski and Felix J. Herrmann},
  title = {A parallel windowed fast discrete curvelet transform applied to seismic processing},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2006},
  volume = {25},
  pages = {2767-2771},
  organization = {SEG},
  abstract = {We propose using overlapping, tapered windows to process
                  seismic data in parallel. This method consists of
                  numerically tight linear oper ators and adjoints
                  that are suitable for use in iterative algorithms.
                  This method is also highly scalable and makes
                  parallel processing of large seismic data sets
                  feasible. We use this scheme to define the Parallel
                  Windowed Fast Discrete Curvelet Transform (PWFDCT),
                  which we apply to a seismic data interpolation
                  algorithm. The successful performance of our
                  parallel processing scheme and algorithm on a
                  two-dimensional synthetic data is shown.},
  keywords = {SEG},
  doi = {10.1190/1.2370099},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2006/thomson06SEGpwfd/thomson06SEGpwfd.pdf }
}


@CONFERENCE{tu2012EAGElsm,
  author = {Ning Tu and Felix J. Herrmann},
  title = {Least-squares migration of full wavefield with source encoding},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2012},
  month = {06},
  abstract = {Multiples can provide valuable information that is
                  missing in primaries, and there is a growing
                  interest in using them for seismic imaging. In our
                  earlier work, we proposed to combine primary
                  estimation and migration to image from the total
                  up-going wavefield. The method proves to be
                  effective but computationally expensive. In this
                  abstract, we propose to reduce the computational
                  cost by removing the multi-dimensional convolution
                  required by primary estimation, and reducing the
                  number of PDE solves in migration by introducing
                  simultaneous sources with source renewal. We gain
                  great performance boost without compromising the
                  quality of the image.},
  keywords = {EAGE, depth migration, surface-related multiples},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/tu2012EAGElsm/tu2012EAGElsm_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/tu2012EAGElsm/tu2012EAGElsm.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=59688}
}


@CONFERENCE{tu2012SEGima,
  author = {Ning Tu and Felix J. Herrmann},
  title = {Imaging with multiples accelerated by message passing},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2012},
  month = {11},
  volume = {31},
  pages = {1-6},
  organization = {SEG},
  abstract = {With the growing realization that multiples can provide
                  valuable information, there is a paradigm shift from
                  removing them to using them. For instance, primary
                  estimation by sparse inversion has demonstrated its
                  superiority over surface-related multiple removal in
                  many aspects. Inspired by this shift, we propose a
                  method to image directly from the total up-going
                  wavefield, including surface-related multiples, by
                  sparse inversion. To address the high computational
                  cost associated with this method, we propose to
                  speed up the inversion by having the wave-equation
                  solver carry out the multi-dimensional convolutions
                  implicitly and cheaply by randomized subsampling. We
                  improve the overall performance of this algorithm by
                  selecting new independent copies of the randomized
                  modeling operator, which leads to a cancellation of
                  correlations that hamper the speed of convergence of
                  the solver. We show the merits of our approach on a
                  number of examples.},
  keywords = {SEG, imaging, multiples},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2012/tu2012SEGima/tu2012SEGima_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2012/tu2012SEGima/tu2012SEGima.pdf},
  doi = {10.1190/segam2012-1552.1}
}


@CONFERENCE{tu2011EAGEspmsrm,
  author = {Ning Tu and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Sparsity-promoting migration with surface-related multiples},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2011},
  month = {05},
  abstract = {Multiples, especially the surface-related multiples,
                  form a significant part of the total up-going wave-
                  field. If not properly dealt with, they can lead to
                  false reflectors in the final image. So
                  conventionally practitioners remove them prior to
                  migration. Recently research has revealed that
                  multiples can actually provide extra illumination so
                  different methods are proposed to address the issue
                  that how to use multiples in seismic imaging, but
                  with various kinds of limitations. In this abstract,
                  we combine primary estimation and sparsity-promoting
                  migration into one convex-optimization process to
                  include information from multiples. Synthetic
                  examples show that multiples do make active
                  contributions to seismic migration. Also by this
                  combination, we can benefit from better recoveries
                  of the Greens function by using sparsity-promoting
                  algorithms since reflectivity is sparser than the
                  Greens function.},
  keywords = {Presentation, EAGE, imaging, processing},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2011/tu11EAGEspmsrm/tu11EAGEspmsrm_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2011/tu11EAGEspmsrm/tu11EAGEspmsrm.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=50369}
}


@CONFERENCE{tu2011SEGmult,
  author = {Ning Tu and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Migration with surface-related multiples from incomplete seismic data},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2011},
  month = {09},
  volume = {30},
  pages = {3222-3227},
  organization = {SEG},
  abstract = {Seismic acquisition is confined by limited aperture that
                  leads to finite illumination, which, together with
                  other factors, hinders imaging of subsurface objects
                  in complex geological settings such as salt
                  structures. Conventional processing, including
                  surface-related multiple elimination, further
                  reduces the amount of information we can get from
                  seismic data. With the growing consensus that
                  multiples carry valuable information that is missing
                  from primaries, we are motivated to exploit the
                  extra illumination provided by multiples to image
                  the sub- surface. In earlier research, we proposed
                  such a method by combining primary estimation and
                  sparsity-promoting migration to invert for model
                  perturbations directly from the total up-going
                  wavefield. In this abstract, we focus on a
                  particular case. By exploiting the extra
                  illumination from surface-related multiples, we
                  mitigate the effects caused by migrating from
                  incomplete data with missing sources and missing
                  near-offsets.},
  keywords = {Presentation, SEG, imaging, processing},
  doi = {10.1190/1.3627865},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2011/tu11SEGmult/tu11SEGmult_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2011/tu11SEGmult/tu11SEGmult.pdf}
}


@CONFERENCE{vandenberg2008IAMesr,
  author = {Ewout {van den Berg}},
  title = {Exact sparse reconstruction and neighbourly polytopes},
  booktitle = {IAM},
  year = {2008},
  date-added = {2008-08-26 15:44:44 -0700},
  date-modified = {2008-08-26 15:45:58 -0700},
  keywords = {SLIM, IAM, Presentation, private},
  presentation = {https://slim.gatech.edu/Publications/Private/Conferences/IAM/2008/vandenberg2008IAMesr/vandenberg2008IAMesr.pdf}
}


@CONFERENCE{vandenberg2008SINBADsat,
  author = {Ewout {van den Berg}},
  title = {Sparco: A testing framework for sparse reconstruction},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {Sparco is a framework for testing and benchmarking
                  algorithms for sparse reconstruction. It includes a
                  large collection of sparse reconstruction problems
                  drawn from the imaging, compressed sensing, and
                  geophysics literature. Sparco is also a framework
                  for implementing new test problems and can be used
                  as a tool for reproducible research. We describe the
                  software environment, and demonstrate its usefulness
                  for testing and comparing solvers for sparse
                  reconstruction.},
  date-modified = {2008-08-22 12:54:25 -0700},
  keywords = {SLIM, SINBAD, Presentation},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/vandenberg2008SINBADsat/vandenberg2008SINBADsat.pdf}
}


@CONFERENCE{friedlander2009SCAIMspot,
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  title = {Spot: A linear-operator toolbox for Matlab},
  booktitle = {SCAIM},
  year = {2009},
  address = {University of British Columbia},
  organization = {SCAIM Seminar},
  keywords = {minimization, Presentation, SLIM},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2009/VandenBerg-Mon-1130.pdf}
}


@CONFERENCE{vandenberg2007SINBADipo1,
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  title = {In pursuit of a root},
  booktitle = {2007 Von Neumann Symposium},
  year = {2007},
  keywords = {minimization, Presentation, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2007/vandenberg2007SINBADipo1/vandenberg2007SINBADipo1.pdf}
}


@CONFERENCE{vandenberg2009SLIMocf,
  author = {Ewout {van den Berg} and Mark Schmidt and Michael P. Friedlander and K. Murphy},
  title = {Optimizing costly functions with simple constraints: a limited-memory projected {Quasi-Newton} algorithm},
  booktitle = {SLIM},
  year = {2009},
  volume = {12},
  series = {Twelfth International Conference on Artificial Intelligence and Statistics},
  abstract = {An optimization algorithm for minimizing a smooth
                  function over a convex set is described. Each
                  iteration of the method computes a descent direction
                  by minimizing, over the original constraints, a
                  diagonal-plus low-rank quadratic approximation to
                  the function. The quadratic approximation is
                  constructed using a limited-memory quasi-Newton
                  update. The method is suitable for large-scale
                  problems where evaluation of the function is
                  substan- tially more expensive than projection onto
                  the constraint set. Numerical experiments on
                  one-norm regularized test problems indicate that the
                  proposed method is competitve with state-of-the-art
                  methods such as bound-constrained L-BFGS and
                  orthant-wise descent. We further show that the
                  method generalizes to a wide class of problems, and
                  substantially improves on state-of-the-art methods
                  for problems such as learning the structure of
                  Gaussian graphi- cal models (involving
                  positive-definite matrix constraints) and Markov
                  random fields (involving second-order cone
                  constraints).},
  date-added = {2009-01-29 17:16:34 -0800},
  date-modified = {2009-01-29 17:16:34 -0800},
  keywords = {SLIM},
  month = {04},
  url = {http://www.cs.ubc.ca/~mpf/papers/SchmidtBergFriedMurph09.pdf}
}


@CONFERENCE{vanderneut2012EAGEdecomp,
  author = {Joost {van der Neut} and Felix J. Herrmann},
  title = {Up / down wavefield decomposition by sparse inversion},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2012},
  month = {06},
  abstract = {Expressions have been derived for the decomposition of
                  multi-component seismic recordings into up- and
                  down-going constituents. However, these expressions
                  contain singularities at critical angles and can be
                  sensitive for noise. By interpreting wavefield
                  decomposition as an inverse problem and imposing
                  constraints on the sparseness of the solution, we
                  arrive at a robust formalism that can be applied to
                  noisy data. The method is demonstrated on synthetic
                  data with multi-component receivers in a horizontal
                  borehole, but can also be applied for different
                  configurations, including OBC and dual-sensor
                  streamers.},
  keywords = {EAGE, wavefield decomposition, sparse inversion},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/vanderneut2012EAGEdecomp/vanderneut2012EAGEdecomp_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/vanderneut2012EAGEdecomp/vanderneut2012EAGEdecomp.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=58907}
}


@CONFERENCE{vanderneut2012SEGirs,
  author = {Joost {van der Neut} and Felix J. Herrmann and Kees Wapenaar},
  title = {Interferometric redatuming with simultaneous and missing sources using sparsity promotion in the curvelet domain},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2012},
  month = {11},
  volume = {31},
  pages = {1-7},
  organization = {SEG},
  abstract = {Interferometric redatuming is a velocity-independent
                  method to turn downhole receivers into virtual
                  sources. Accurate redatuming involves solving an
                  inverse problem, which can be highly ill-posed,
                  especially in the presence of noise, incomplete data
                  and limited aperture. We address these issues by
                  combining interferometric redatuming with
                  transform-domain sparsity promotion, leading to a
                  formulation that deals with data imperfections. We
                  show that sparsity promotion improves the retrieval
                  of virtual shot records under a salt flank. To
                  reduce acquisition costs, it can be beneficial to
                  reduce the number of sources or shoot them
                  simultaneously. It is shown that sparse inversion
                  can still provide a stable solution in such cases.},
  keywords = {processing, imaging, optimization, interferometry, SEG},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2012/vanderneut2012SEGirs/vanderneut2012SEGirs.pdf},
  doi = {10.1190/segam2012-0566.1}
}


@CONFERENCE{vanleeuwen2012EAGEcarpcg,
  author = {Tristan van Leeuwen and Dan Gordon and Rachel Gordon and Felix J. Herrmann},
  title = {Preconditioning the {Helmholtz} equation via row-projections},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2012},
  month = {06},
  abstract = {3D frequency-domain full waveform inversion relies on
                  being able to efficiently solve the 3D Helmholtz
                  equation. Iterative methods require sophisticated
                  preconditioners because the Helmholtz matrix is
                  typically indefinite. We review a preconditioning
                  technique that is based on row-projections. Notable
                  advantages of this preconditioner over existing ones
                  are that it has low algorithmic complexity, is
                  easily parallelizable and extendable to
                  time-harmonic vector equations.},
  keywords = {EAGE, Helmholtz equation, precondition},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/vanleeuwen2012EAGEcarpcg/vanleeuwen2012EAGEcarpcg_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/vanleeuwen2012EAGEcarpcg/vanleeuwen2012EAGEcarpcg.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=58891}
}


@CONFERENCE{vanleeuwen2012EAGEext,
  author = {Tristan {van Leeuwen} and Felix J. Herrmann},
  title = {Wave-equation extended images: computation and velocity continuation},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2012},
  month = {06},
  abstract = {An extended image is a multi-dimensional correlation of
                  source and receiver wavefields. For a kinematically
                  correct velocity, most of the energy will be
                  concentrated at zero offset. Because of the
                  computational cost involved in correlating the
                  wavefields for all offsets, such exteded images are
                  computed for a subsurface offset that is aligned
                  with the local dip. In this paper, we present an
                  efficient way to compute extended images for all
                  subsurface offsets without explicitly calculating
                  the receiver wavefields, thus making it
                  computationally feasible to compute such extended
                  images. We show how more conventional image gathers,
                  where the offset is aligned with the dip, can be
                  extracted from this extended image. We also present
                  a velocity continuation procedure that allows us to
                  compute the extended image for a given velocity
                  without recomputing all the source wavefields.},
  keywords = {EAGE, extended image, velocity continuation},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/vanleeuwen2012EAGEext/vanleeuwen2012EAGEext_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/vanleeuwen2012EAGEext/vanleeuwen2012EAGEext.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=59616}
}


@CONFERENCE{verschuur2007SEGmmp,
  author = {D. J. Verschuur and Deli Wang and Felix J. Herrmann},
  title = {Multiterm multiple prediction using separated reflections and diffractions combined with curvelet-based subtraction},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  pages = {2535-2539},
  organization = {SEG},
  abstract = {The surface-related multiple elimination (SRME) method
                  has proven to be successful on a large number of
                  data cases. Most of the applications are still 2D,
                  as the full 3D implementation is still expensive and
                  under development. However, the earth is a 3D
                  medium, such that 3D effects are difficult to
                  avoid. Most of the 3D effects come from diffractive
                  structures, whereas the specular reflections
                  normally have less of a 3D behavior. By separating
                  the seismic data in a specular reflecting and a
                  diffractive part, multiple prediction can be carried
                  out with these different subsets of the input data,
                  resulting in several categories of predicted
                  multiples. Because each category of predicted
                  multiples can be subtracted from the input data with
                  different adaptation filters, a more flexible SRME
                  procedure is obtained. Based on some initial results
                  from a Gulf of Mexico dataset, the potential of this
                  approach is investigated. {\copyright}2007 Society
                  of Exploration Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.2792993},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/verschuur07SEGmmp/verschuur07SEGmmp_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/verschuur07SEGmmp/verschuur07SEGmmp.pdf }
}


@CONFERENCE{wang2008SINBADrri,
  author = {Deli Wang and Rayan Saab and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Recent results in curvelet-based primary-multiple separation},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We present a nonlinear curvelet-based sparsity-promoting
                  formulation for the primary-multiple separation
                  problem. We show that these coherent signal
                  components can be separated robustly by explicitly
                  exploiting the locality of curvelets in phase space
                  (space-spatial frequency plane) and their ability to
                  compress data volumes that contain wavefronts. This
                  work is an extension of earlier results and the
                  presented algorithms are shown to be stable under
                  noise and moderately erroneous multiple
                  predictions.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/wang2008SINBADrri/wang2008SINBADrri.pdf}
}


@CONFERENCE{wang2007SEGrri,
  author = {Deli Wang and Rayan Saab and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Recent results in curvelet-based primary-multiple separation: application to real data},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  pages = {2500-2504},
  organization = {SEG},
  abstract = {In this abstract, we present a nonlinear curvelet-based
                  sparsity-promoting formulation for the
                  primary-multiple separation problem. We show that
                  these coherent signal components can be separated
                  robustly by explicitly exploting the locality of
                  curvelets in phase space (space-spatial frequency
                  plane) and their ability to compress data volumes
                  that contain wavefronts. This work is an extension
                  of earlier results and the presented algorithms are
                  shown to be stable under noise and moderately
                  erroneous multiple predictions. {\copyright}2007
                  Society of Exploration Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.2792986},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/wang07SEGrri/wang07SEGrri_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2007/wang07SEGrri/wang07SEGrri.pdf }
}


@CONFERENCE{wason2012CSEGode,
  author = {Haneet Wason and Felix J. Herrmann},
  title = {Only dither: efficient simultaneous marine acquisition},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2012},
  abstract = {Simultaneous-source acquisition is an emerging
                  technology that is stimulating both geophysical
                  research and commercial efforts. Simultaneous marine
                  acquisition calls for the development of a new set
                  of design principles and post-processing tools. The
                  focus here is on simultaneous-source marine
                  acquisition design and sparsity-promoting
                  sequential-source data recovery. We propose a
                  pragmatic simultaneous-source, randomized marine
                  acquisition scheme where multiple vessels sail
                  across an ocean-bottom array firing airguns at ---
                  sequential locations and randomly time-dithered
                  instances. By leveraging established findings from
                  the field of compressive sensing, where the choice
                  of the sparsifying transform needs to be incoherent
                  with the compressive sampling matrix, we can
                  significantly impact the reconstruction quality, and
                  demonstrate that the compressive sampling matrix
                  resulting from the proposed sampling scheme is
                  sufficiently incoherent with the curvelet transform
                  to yield successful recovery by sparsity
                  promotion. Results are illustrated with simulations
                  of “purely” random marine acquisition, which
                  requires an airgun to be located at each source
                  location, and random time-dithering marine
                  acquisition with one and two source vessels. Size of
                  the collected data volumes in all cases is the
                  same. Compared to the recovery from the former
                  acquisition scheme (SNR = 10.5dB), we get good
                  results by dithering with only one source vessel
                  (SNR = 8.06dB) in the latter scheme, which improve
                  at the cost of having an additional source vessel
                  (SNR = 9.85dB).},
  keywords = {CSEG, acquisition, marine, simultaneous},
  month = {02},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2012/wason2012CSEGode/wason2012CSEGode.pdf}
}


@CONFERENCE{wason2012EAGEode,
  author = {Haneet Wason and Felix J. Herrmann},
  title = {Only dither: efficient simultaneous marine acquisition},
  booktitle = {EAGE Annual Conference Proceedings},
  year = {2012},
  month = {06},
  abstract = {Simultaneous-source acquisition is an emerging
                  technology that is stimulating both geophysical
                  research and commercial efforts. The focus here is
                  on simultaneous-source marine acquisition design and
                  sparsity-promoting sequential-source data
                  recovery. We propose a pragmatic
                  simultaneous-source, randomized marine acquisition
                  scheme where multiple vessels sail across an
                  ocean-bottom array firing airguns at --- sequential
                  locations and randomly time-dithered
                  instances. Within the context of compressive
                  sensing, where the choice of the sparsifying
                  transform needs to be incoherent with the
                  compressive sampling matrix, we can significantly
                  impact the reconstruction quality, and demonstrate
                  that the compressive sampling matrix resulting from
                  the proposed sampling scheme is sufficiently
                  incoherent with the curvelet transform to yield
                  successful recovery by sparsity promotion. Results
                  are illustrated with simulations of ``purely" random
                  marine acquisition, which requires an airgun to be
                  located at each source location, and random
                  time-dithering marine acquisition with one and two
                  source vessels. Size of the collected data volumes
                  in all cases is the same. Compared to the recovery
                  from the former acquisition scheme (SNR = 10.5dB),
                  we get good results by dithering with only one
                  source vessel (SNR = 8.06dB) in the latter scheme,
                  which improve at the cost of having an additional
                  source vessel (SNR = 9.44dB).},
  keywords = {EAGE, acquisition, marine, simultaneous},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/wason2012EAGEode/wason2012EAGEode_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2012/wason2012EAGEode/wason2012EAGEode.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=58915}
}


@CONFERENCE{wason2011SEGsprsd,
  author = {Haneet Wason and Felix J. Herrmann and Tim T.Y. Lin},
  title = {Sparsity-promoting recovery from simultaneous data: a compressive sensing approach},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2011},
  month = {09},
  volume = {30},
  pages = {6-10},
  organization = {SEG},
  abstract = {Seismic data acquisition forms one of the main
                  bottlenecks in seismic imaging and inversion. The
                  high cost of acquisition work and collection of
                  massive data volumes compel the adoption of
                  simultaneous-source seismic data acquisition - an
                  emerging technology that is developing rapidly,
                  stimulating both geophysical research and commercial
                  efforts. Aimed at improving the performance of
                  marine- and land-acquisition crews, simultaneous
                  acquisition calls for development of a new set of
                  design principles and post-processing
                  tools. Leveraging developments from the field of
                  compressive sensing the focus here is on
                  simultaneous-acquisition design and
                  sequential-source data recovery. Apart from proper
                  compressive sensing sampling schemes, the recovery
                  from simultaneous simulations depends on a
                  sparsifying transform that compresses seismic data,
                  is fast, and reasonably incoherent with the
                  compressive-sampling matrix. Using the curvelet
                  transform, in which seismic data can be represented
                  parsimoniously, the recovery of the
                  sequential-source data volumes is achieved using the
                  sparsity-promoting program {\textemdash} SPGL1, a
                  solver based on projected spectral gradients. The
                  main outcome of this approach is a new technology
                  where acquisition related costs are no longer
                  determined by the stringent Nyquist sampling
                  criteria.},
  keywords = {Presentation, SEG, acquisition, compressive sensing},
  doi = {10.1190/1.3628174},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2011/wason11SEGsprsd/wason11SEGsprsd_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2011/wason11SEGsprsd/wason11SEGsprsd.pdf}
}


@CONFERENCE{yan2008SINBADwru,
  author = {Jiupeng Yan},
  title = {Wavefield reconstruction using simultaneous denoising interpolation vs. denoising after interpolation},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {This report represents and compares two methods of
                  wavefield reconstruction from noisy seismic data
                  with missing traces. The two methods are (i) First
                  interpolate incomplete noisy data to get complete
                  noisy data and then denoise, and (ii) Interpolate
                  and denoise the incomplete noisy data
                  simultaneously. A sample test of synthetic data will
                  be presented. The results of tests show that
                  denoising after interpolation is better than
                  simultaneous denoising and interpolation if the
                  parameter of the denoising problem is chosen
                  appropriately.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/yan2008SINBADwru/yan2008SINBADwru.pdf}
}


@CONFERENCE{yan2009SEGgpb,
  author = {Jiupeng Yan and Felix J. Herrmann},
  title = {Groundroll prediction by interferometry and separation by curvelet-domain matched filtering},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  month = {10},
  volume = {28},
  pages = {3297-3301},
  organization = {SEG},
  abstract = {The removal of groundroll in land based seismic data is
                  a critical step for seismic imaging. In this paper,
                  we introduce a work flow to predict the groundroll
                  by interferometry and then separate the groundroll
                  in the curvelet domain. Thus workflow is similar to
                  the workflow of surface-related multiple elimination
                  (SRME). By exploiting the adaptability and sparsity
                  of curvelets, we are able to significantly improve
                  the separation of groundroll in comparison to
                  results yielded by frequency-domain adaptive
                  subtraction methods. We provide synthetic data
                  example to illustrate our claim.},
  keywords = {Presentation, SEG},
  doi = {10.1190/1.3255544},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/yan09SEGgpb/yan09SEGgpb_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/yan09SEGgpb/yan09SEGgpb.pdf}
}


@CONFERENCE{yan2009SEGgpb2,
  author = {Jiupeng Yan and Felix J. Herrmann},
  title = {Groundroll prediction by interferometry and separation by curvelet-domain filtering},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  abstract = {The removal of groundroll in land based seismic data is
                  a critical step for seismic imaging. In this paper,
                  we introduce a work flow to predict the groundroll
                  by interferometry and then separate the groundroll
                  in the curvelet domain. Thus workflow is similar to
                  the workflow of surface-related multiple elimination
                  (SRME). By exploiting the adaptability and sparsity
                  of curvelets, we are able to significantly improve
                  the separation of groundroll in comparison to
                  results yielded by frequency-domain adaptive
                  subtraction methods. We provide synthetic data
                  example to illustrate our claim.},
  keywords = {Presentation},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2009/yan2009SEGgpb2/yan2009SEGgpb2.pdf}
}


@CONFERENCE{yarham2008SINBADbss,
  author = {Carson Yarham},
  title = {Bayesian signal separation applied to ground-roll removal},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {Accurate and adaptive noise removal is a critical part
                  in seismic processing. Recent developments in signal
                  separation methods have allowed a more flexible and
                  accurate framework in which to perform ground roll
                  and reflector separation. The use of a new Bayesian
                  separation scheme developed at the SLIM group that
                  contains control parameters to adjust for the
                  uniqueness of specific problems is used. The
                  sensitivity and variation of the control parameters
                  is examined and this method is applied to synthetic
                  and real data and the results are compared to
                  previous methods.},
  date-modified = {2008-08-22 12:42:58 -0700},
  keywords = {Presentation, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/yarham2008SINBADbss/yarham2008SINBADbss.pdf}
}


@CONFERENCE{yarham2007EAGEnsw,
  author = {Carson Yarham},
  title = {Nonlinear surface wave prediction and separation},
  booktitle = {EAGE 2007},
  year = {2007},
  abstract = {Removal of surface waves is an integral step in seismic
                  processing. There are many standard techniques for
                  removal of this type of coherent noise, such as f-k
                  filtering, but these methods are not always
                  effective. One of the common problems with removal
                  of surface waves is that they tend to be aliased in
                  the frequency domain. This can make removal
                  difficult and affect the frequency content of the
                  reflector signals, as this signals will not be
                  completely separated. As seen in (Hennenfent, G. and
                  F. Herrmann, 2006, Application of stable signal
                  recovery to seismic interpolation) interpolation can
                  be used effectively to resample the seismic record
                  thus dealiasing the surface waves. This separates
                  the signals in the frequency domain allowing for a
                  more precise and complete removal. The use of this
                  technique with curvelet based surface wave
                  predictions and an iterative L1 separation scheme
                  can be used to remove surface waves from shot
                  records more completely that with standard
                  techniques.},
  keywords = {Presentation, EAGE, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/yarham2007EAGEnsw/yarham2007EAGEnsw.pdf}
}


@CONFERENCE{yarham2006SEGcgrr,
  author = {Carson Yarham and Urs Boeniger and Felix J. Herrmann},
  title = {Curvelet-based ground roll removal},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2006},
  volume = {25},
  pages = {2777-2782},
  organization = {SEG},
  abstract = {We have effectively identified and removed ground roll
                  through a two-step process. The first step is to
                  identify the major components of the ground roll
                  through various methods including multiscale
                  separation, directional or frequency filtering or by
                  any other method that identifies the ground
                  roll. Given this estimate for ground roll, the
                  recorded signal is separated during the second step
                  through a block-coordinate relaxation method that
                  seeks the sparsest set for weighted curvelet
                  coefficients of the ground roll and the sought-after
                  reflectivity. The combination of these two methods
                  allows us to separate out the ground roll signal
                  while preserving the reflector information. Since
                  our method is iterative, we have control of the
                  separation process. We successfully tested our
                  algorithm on a real data set with a complex ground
                  roll and reflector structure.},
  keywords = {SEG},
  doi = {10.1190/1.2370101},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2006/yarham06SEGcgrr/yarham06SEGcgrr.pdf }
}


@CONFERENCE{yarham2007EAGEcai,
  author = {Carson Yarham and Gilles Hennenfent and Felix J. Herrmann},
  title = {Curvelet applications in surface wave removal},
  booktitle = {EAGE Workshop on Curvelets, contourlets, seislets, … in seismic data processing - where are we and where are we going?},
  year = {2007},
  month = {06},
  abstract = {Ground roll removal of seismic signals can be a
                  challenging prospect. Dealing with undersampleing
                  causing aliased waves amplitudes orders of magnitude
                  higher than reflector signals and low frequency loss
                  of information due to band ...},
  keywords = {SLIM, EAGE},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/EAGE/2007/yarham07EAGEcai/yarham07EAGEcai.pdf},
  url2 = {http://earthdoc.eage.org/publication/publicationdetails/?publication=7590}
}


@CONFERENCE{yarham2008SEGbgr,
  author = {Carson Yarham and Felix J. Herrmann},
  title = {Bayesian ground-roll seperation by curvelet-domain sparsity promotion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  pages = {2576-2580},
  organization = {SEG},
  abstract = {The removal of coherent noise generated by surface waves
                  in land based seismic is a prerequisite to imaging
                  the subsurface. These surface waves, termed as
                  ground roll, overlay important reflector information
                  in both the t-x and f-k domains. Standard techniques
                  of ground-roll removal commonly alter reflector
                  information. We propose the use of the curvelet
                  domain as a sparsifying transform in which to
                  preform signal-separation techniques that preserves
                  reflector information while increasing ground-roll
                  removal. We look at how this method preforms on
                  synthetic data for which we can build quantitative
                  results and a real field data set.},
  keywords = {Presentation, SLIM, SEG},
  doi = {10.1190/1.3063878},
  month = {11},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/yarham08SEGbgr/yarham08SEGbgr_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2008/yarham08SEGbgr/yarham08SEGbgr.pdf }
}


@CONFERENCE{yarham2004CSEGgrr,
  author = {Carson Yarham and Felix J. Herrmann and Daniel Trad},
  title = {Ground roll removal using non-separable wavelet transforms},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2004},
  keywords = {Presentation},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2004/Yarham04CSEGgrr/Yarham04CSEGgrr.pdf}
}


@CONFERENCE{yarham2004CSEGcpa,
  author = {Carson Yarham and Daniel Trad and Felix J. Herrmann},
  title = {Curvelet processing and imaging: adaptive ground roll removal},
  booktitle = {CSEG Annual Conference Proceedings},
  year = {2004},
  organization = {CSEG},
  abstract = {In this paper we present examples of ground roll
                  attenuation for synthetic and real data gathers by
                  using Contourlet and Curvelet transforms. These
                  non-separable wavelet transforms are locoalized both
                  (x,t)- and (k,f)-domains and allow for adaptive
                  seperation of signal and ground roll. Both linear
                  and non-linear filtering are discussed using the
                  unique properties of these basis that allow for
                  simultaneous localization in the both
                  domains. Eventhough, the linear filtering techniques
                  are encouraging the true added value of these
                  basis-function techniques becomes apparent when we
                  use these decompositions to adaptively substract
                  modeled ground roll from data using a non-linear
                  thesholding procedure. We show real and synthetic
                  examples and the results suggest that these
                  directional-selective basis functions provide a
                  usefull tool for the removal of coherent noise such
                  as ground roll.},
  keywords = {Presentation, SLIM, CSEG},
  month = {05},
  presentation = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2004/Yarham04CSEGcpa/Yarham04CSEGcpa_pres.pdf},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/CSEG/2004/Yarham04CSEGcpa/yarham04csegcpa.pdf}
}


@CONFERENCE{yilmaz2008SINBADsse,
  author = {Ozgur Yilmaz},
  title = {Stable sparse expansions via non-convex optimization},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We present theoretical results pertaining to the ability
                  of p-(quasi)norm minimization to recover sparse and
                  compressible signals from incomplete and noisy
                  measurements. In particular, we extend the results
                  of Candes, Romberg and Tao for 1-norm to the p $\ll$
                  1 case. Our results indicate that depending on the
                  restricted isometry constants and the noise level,
                  p-norm minimization with certain values of p $\ll$ 1
                  provides better theoretical guarantees in terms of
                  stability and robustness compared to 1-norm
                  minimization. This is especially true when the
                  restricted isometry constants are relatively large,
                  or equivalently, when the data is significantly
                  undersampled.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2008/yilmaz2008SINBADsse/yilmaz2008SINBADsse.pdf}
}
% This file was created with JabRef 2.9.
% Encoding: ISO8859_1

@MASTERSTHESIS{lin08THccl,
  author = {Tim T.Y. Lin},
  title = {Compressed computation of large-scale wavefield extrapolation in inhomogeneous medium},
  school = {University of British Columbia},
  year = {2008},
  type = {masters},
  abstract = {In this work an explicit algorithm for the extrapolation
                  of one-way wavefields is proposed which combines
                  recent developments in information theory and
                  theoretical signal processing with the physics of
                  wave propagation. Because of excessive memory
                  requirements, explicit formulations for wave
                  propagation have proven to be a challenge in 3-D. By
                  using ideas from
                  {\textquoteleft}{\textquoteleft}compressed
                  sensing{\textquoteright}{\textquoteright}, we are
                  able to formulate the (inverse) wavefield
                  extrapolation problem on small subsets of the data
                  volume, thereby reducing the size of the
                  operators. Compressed sensing entails a new paradigm
                  for signal recovery that provides conditions under
                  which signals can be recovered from incomplete
                  samplings by \emph{nonlinear} recovery methods that
                  promote sparsity of the to-be-recovered
                  signal. According to this theory, signals can
                  successfully be recovered when the measurement basis
                  is \emph{incoherent} with the representation in
                  which the wavefield is sparse. In this new approach,
                  the eigenfunctions of the Helmholtz operator are
                  recognized as a basis that is incoherent with
                  sparsity transforms that are known to compress
                  seismic wavefields. By casting the wavefield
                  extrapolation problem in this framework, wavefields
                  can successfully be extrapolated in the modal
                  domain, despite evanescent wave modes. The degree to
                  which the wavefield can be recovered depends on the
                  number of missing (evanescent) wave modes and on the
                  complexity of the wavefield. A proof of principle
                  for the {\textquoteleft}{\textquoteleft}compressed
                  sensing{\textquoteright}{\textquoteright} method is
                  given for inverse wavefield extrapolation in
                  2-D. The results show that our method is stable, has
                  reduced dip limitations and handles evanescent waves
                  in inverse extrapolation.},
  keywords = {BSc, SLIM},
  month = {04},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2008/lin08THccl.pdf}
}


%----- 2017 (FALL) -----%

@PRESENTATION{alfaraj2017SINBADFros,
  title = {Reconstruction of S-waves from low-cost randomized acquisition},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2017},
  abstract = {Due to the lower shear wave velocity compared with compressional waves, finer spatial sampling is required to properly record the earlier according to the Nyquist sampling criterion. To avoid higher acquisition costs and to utilize the multicomponent data to its available full extent, we propose acquiring randomly undersampled ocean bottom seismic data. We present two up- and down-going shear wave reconstruction methods: (i) rank minimization reconstruction followed by elastic wavefield decomposition, and (ii) sparsity promoting joint interpolation decomposition using all the multicomponent data in one optimization problem.},
  keywords = {presentation, SINBAD, SINBADFALL2017, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/alfaraj2017SINBADFros/alfaraj2017SINBADFros.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/alfaraj2017SINBADFros/alfaraj2017SINBADFros.mov},
  author = {Ali M. Alfaraj and Rajiv Kumar and Felix J. Herrmann}
}

@PRESENTATION{daskalakis2017SINBADFsof,
  title = {Stochastic Optimization from the perspective of dynamical systems},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2017},
  abstract = {We present improvements to a family of methods (Linearized Bregman, Kaczmarz and Stochastic Gradient Descent) that are often use in optimization problems. We explain the link of those methods with dynamical systems and we draw ideas for improving their performance. We use a simple idea to improve the stability and the performance of our family of optimization methods especially for ill-posed, inconsistent large-scale problems. Finally we present an application at a least squares migration problem, which highlight the importance of the suggested improvements on large scale Geophysical problems.},
  keywords = {presentation, SINBAD, SINBADFALL2017, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/daskalakis2017SINBADFsof/daskalakis2017SINBADFsof.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/daskalakis2017SINBADFsof/daskalakis2017SINBADFsof.m4v},
  author = {Emmanouil Daskalakis and Rachel Kuske and Mengmeng Yang and Felix J. Herrmann}
}

@PRESENTATION{fang2017SINBADFpfg,
  title = {PDE-free Gauss-Newton Hessian for Wavefield Reconstruction Inversion},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2017},
  abstract = {In this work, we present a PDE-free Gauss-Newton Hessian for Wavefield Reconstruction Inversion. With this PDE-free Gauss-Newton Hessian, we can compute matrix-vector products without additional PDE solves. Thus, we are able to use the second order optimization method Gauss-Newton method with a roughly equal computational cost of first-order methods such as the gradient-descent method.},
  keywords = {presentation, SINBAD, SINBADFALL2017, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/fang2017SINBADFpfg/fang2017SINBADFpfg.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/fang2017SINBADFpfg/fang2017SINBADFpfg.mov},
  author = {Zhilong Fang and Felix J. Herrmann}
}

@PRESENTATION{graff2017SINBADFlrp,
  title = {Low-rank representation of omnidirectional subsurface extended image volumes},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2017},
  abstract = {Extended image volumes are an important migration tool in seismic exploration. However the computation and the storage of omnidirectional subsurface extended image volumes are usually prohibitive. That is why some solutions have been already proposed for instance by focusing on horizontal offsets only. In our work, we will consider a linear algebra approach to deal with the low-rank representation of extended image volumes with full offsets. We will never build entirely the resulting matrix but get only actions of it on well-chosen probing vectors, based on Low-Rank decomposition or randomized SVD. This representation allows us to have access to all the energy of the extended image volume matrix and still limits the storage of the information and the computational cost.},
  keywords = {presentation, SINBAD, SINBADFALL2017, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/graff2017SINBADFlrp/graff2017SINBADFlrp.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/graff2017SINBADFlrp/graff2017SINBADFlrp.mov},
  author = {Marie Graff-Kray and Rajiv Kumar and Felix J. Herrmann}
}

@PRESENTATION{herrmann2017SINBADFhrc,
  title = {Highly repeatable 3D compressive full-azimuth towed-streamer time-lapse acquisition –- a numerical feasibility study at scale},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2017},
  abstract = {Most conventional 3D time-lapse (or 4D) acquisitions are ocean-bottom cable (OBC) or ocean-bottom node (OBN) surveys since these surveys are relatively easy to replicate compared to towed-streamer surveys. To attain high degrees of repeatability, survey replicability and dense periodic sampling has become the norm for 4D surveys that renders this technology expensive. Conventional towed-streamer acquisitions suffer from limited illumination of subsurface due to narrow azimuth. Although, acquisition techniques such as multi-azimuth, wide-azimuth, rich-azimuth acquisition, etc., have been developed to illuminate the subsurface from all possible angles, these techniques can be prohibitively expensive for densely sampled surveys. This leads to uneven sampling, i.e., dense receiver and coarse source sampling or vice-versa, in order to make these acquisitions more affordable. Motivated by the design principles of Compressive Sensing (CS), we acquire economic, randomly subsampled (or compressive) and simultaneous towed-streamer time-lapse data without the need of replicating the surveys. We recover densely sampled time-lapse data on one and the same periodic grid by using a joint-recovery model (JRM) that exploits shared information among different time-lapse recordings, coupled with a computationally cheap and scalable rank-minimization technique. The acquisition is low cost since we have subsampled measurements (about 70% subsampled), simulated with a simultaneous long-offset acquisition configuration of two source vessels travelling across a survey area at random azimuths. We analyze the performance of our proposed compressive acquisition and subsequent recovery strategy by conducting a synthetic, at scale, seismic experiment on a 3D time-lapse model containing geological features such as channel systems, dipping and faulted beds, unconformities and a gas cloud. Our findings indicate that the insistence on replicability between surveys and the need for OBC/OBN 4D surveys can, perhaps, be relaxed. Moreover, this is a natural next step beyond the successful CS acquisition examples discussed during this session.},
  keywords = {presentation, SINBAD, SINBADFALL2017, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/herrmann2017SINBADFhrc/herrmann2017SINBADFhrc.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/herrmann2017SINBADFhrc/herrmann2017SINBADFhrc.mov},
  author = {Felix J. Herrmann and Rajiv Kumar and Haneet Wason and Shashin Sharan and Felix Oghenekohwo}
}

@PRESENTATION{herrmann2017SINBADFofp,
  title = {Overview & {Future} {Plans} {SINBAD} {Consortium}},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2017},
  keywords = {presentation, SINBAD, SINBADFALL2017, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/herrmann2017SINBADFofp/herrmann2017SINBADFofp.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/herrmann2017SINBADFofp/herrmann2017SINBADFofp.mov},
  author = {Felix J. Herrmann}
}

@PRESENTATION{kumar2017SINBADFfas,
  title = {Full-azimuth seismic data processing w/ coil acquisition},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2017},
  abstract = {In this work, we will demonstrate the performance of our in-house 5D low-rank based interpolation method on a seismic data acquired using coil shooting full-azimuth acquisition. We will show that we can recover full-azimuthal interpolated data from highly subsampled data, where the subsampling ratio is 4%. This is the first time, we are testing our interpolation ideas on real 3D marine seismic data acquisition. Our findings show that we can avoid the general practice of windowing the data while performing the interpolation, specially using rank-minimization based framework.},
  keywords = {presentation, SINBAD, SINBADFALL2017, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/kumar2017SINBADFfas/kumar2017SINBADFfas.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/kumar2017SINBADFfas/kumar2017SINBADFfas.mov},
  author = {Rajiv Kumar and Nick Moldoveanu and Keegan Lensink and Felix J. Herrmann}
}

@PRESENTATION{kumar2017SINBADFmdt,
  title = {Multi-domain target-oriented imaging using extreme-scale matrix factorization},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2017},
  abstract = {In this work, we present an alternative approach to redatum both source and receivers at depth, under the framework of reflectivity-based extended images with two-way wave propagation in the background medium. We propose a randomized svd based probing scheme that takes advantage of the algebraic structure of the extended imaging system to overcome the computational cost and memory usage associated with the number of wave-equation solutions and explicit storage employed by conventional migration methods. Experimental results on complex geological models demonstrate the efficacy of proposed methodology in performing multi-domain target imaging.},
  keywords = {presentation, SINBAD, SINBADFALL2017, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/kumar2017SINBADFmdt/kumar2017SINBADFmdt.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/kumar2017SINBADFmdt/kumar2017SINBADFmdt.mov},
  author = {Rajiv Kumar and Marie Graff-Kray and Ivan Vasconcelos and Felix J. Herrmann}
}

@PRESENTATION{lopez2017SINBADFagf,
  title = {A Guide for Successful Low-Rank Matrix Recovery in Seismic Applications},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2017},
  abstract = {This talk presents recent results in the theory of low-rank matrix recovery as heuristics for seismic practitioners. In the theory of matrix completion, we discuss the spectral gap as a means to quantify how successful a given sub sampling scheme will be for trace interpolation. Additionally, we consider previously proposed random sampling techniques and develop conditions on the sampling distribution that guarantees successful low-rank matrix recovery. The results apply to time-jittered acquisition, off-the-grid trace interpolation and source separation for simultaneous towed-streamer marine acquisition. Put together, the talk provides practical instruments that help design acquisition schemes in favor of rank penalization techniques.},
  keywords = {presentation, SINBAD, SINBADFALL2017, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/lopez2017SINBADFagf/lopez2017SINBADFagf.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/lopez2017SINBADFagf/lopez2017SINBADFagf.mov},
  author = {Oscar Lopez and Rajiv Kumar}
}

@PRESENTATION{lopez2017SINBADFmci,
  title = {Matrix Completion in Parallel Architectures: Julia Implementation},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2017},
  abstract = {Matrix completion techniques offer potential tools for frugal seismic data acquisition, where dense acquisition is replaced by optimization. This shift of focus means that efficient numerical methods are critical for the implementation of these techniques in large-scale seismic applications. To this end, this talk modifies rank-penalization methodologies to suit parallel architectures. By adopting factorization-based alternating minimization schemes, each program can be decoupled into independent sub-problems handled in parallel. We showcase a distributed parallel execution in Julia and explore the scalability of the approach.},
  keywords = {presentation, SINBAD, SINBADFALL2017, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/lopez2017SINBADFmci/lopez2017SINBADFmci.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/lopez2017SINBADFmci/lopez2017SINBADFmci.mov},
  author = {Oscar Lopez and Keegan Lensink and Rajiv Kumar and Henryk Modzelewski}
}

@PRESENTATION{louboutin2017SINBADFddg,
  title = {Data driven Gradient Sampling for seismic inversion},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2017},
  abstract = {We present in this work an extension of the Gradient Sampling algorithm presented at the last EAGE in Paris. We previously showed the potential of this algorithm playing with implicit time-shifts to represent the wavefield of a slightly perturbed velocity model. We introduce an extension where the weights of the Gradient Sampling algorithm are obtained with the solve of data-based quadratic subproblem instead of at random. The update direction is the a more accurate representation of the true Gradient Sampling update direction.},
  keywords = {presentation, SINBAD, SINBADFALL2017, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/louboutin2017SINBADFddg/louboutin2017SINBADFddg.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/louboutin2017SINBADFddg/louboutin2017SINBADFddg.mov},
  author = {Mathias Louboutin and Felix J. Herrmann}
}

@PRESENTATION{louboutin2017SINBADFldi,
  title = {Latest developments in Devito},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2017},
  abstract = {We present an overview of the latest developments in Devito. We introduced Devito in the previous meeting as a prototype finite-difference DSL for seismic modelling and inversion. We are presenting here the latest improvements and functionalities of Devito. We will also discuss the current future plans as well as non-supported features that the audience may be interested in. This presentation will be followed by/mixed with a hands-in tutorial if the time and resources allows it.},
  keywords = {presentation, SINBAD, SINBADFALL2017, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/louboutin2017SINBADFldi/louboutin2017SINBADFldi.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/louboutin2017SINBADFldi/louboutin2017SINBADFldi.mov},
  author = {Mathias Louboutin and Michael Lange and Fabio Luporini and Navjot Kurjeka and Jan Hueckelheim and Gerard Gorman and Philipp A. Witte and Felix J. Herrmann}
}

@PRESENTATION{peters2017SINBADFaaj,
  title = {Algorithms and Julia software for FWI with multiple constraints},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2017},
  abstract = {We present a framework to add multiple convex and non-convex constraints to nonlinear inverse problems, specifically FWI. The constraints mitigate problems related to noisy data, artifacts arising from working with very few simultaneous sources, inaccurate starting models and using approximate physical forward models. Compared to earlier work at SLIM, the current framework is algorithmically simpler and computationally more efficient. We show examples where the model estimation is improved when we use very limited prior knowledge directly as constraints. We also present the software implementation in Julia and how it is used together with other software that compute data-misfit values and gradients w.r.t. the model parameters.},
  keywords = {presentation, SINBAD, SINBADFALL2017, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/peters2017SINBADFaaj/peters2017SINBADFaaj.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/peters2017SINBADFaaj/peters2017SINBADFaaj.m4v},
  author = {Bas Peters and Felix J. Herrmann}
}

@PRESENTATION{sharan2017SINBADFtts,
  title = {Tracking the spatial-temporal evolution of fractures by microseismic source collocation},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2017},
  abstract = {Unlike conventional reservoirs, unconventional plays are not naturally viable for economical production of oil and gas. They require stimulation by injecting high-pressure fluid causing fractures in the rocks. These fractures make the medium more permeable, hence, the extraction of oil and gas becomes feasible. For drilling purposes and to prevent potentially hazardous situations, we need to have good knowledge of the location of these fractures. Also, we need to have good knowledge about how these fractures originated in time. Hydraulic fracturing changes stress in rocks, which results in the emission of microseismic waves. The opening of cracks due to high pressure fluid injection during hydraulic fracturing mainly causes this change in stress in the rocks. Therefore, microseismic events are mostly localized along these fractures and have finite energy along time. To accurately track the evolution of fractures in both space and time, we need to locate closely spaced microseismic events along these fractures activating at very small time intervals. A naive approach can be the back propagation of the observed data to find out a point in space and time where maximum focusing of back propagating energy occurs. This point corresponds to the location and origin time of a microseismic source. This approach, although simpler, suffers from low resolution and requires scanning of complete 4D volume (3D in space and 1D in time). Hence, this method can be challenging when there are multiple closely spaced microseismic sources originating at different times. We in this work propose a sparsity promotion based method that can locate closely spaced microseismic events, with spatial separation as low as within half a wavelength, activating at small time intervals. We simultaneously estimate the origin time of microseismic events by estimating their source time functions. Our method exploits the fact that microseismic events are localized in space and have finite energy. We use accelerated Linearized Bregman algorithm with a preconditioning operator to arrive at a computationally feasible scheme.},
  keywords = {presentation, SINBAD, SINBADFALL2017, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/sharan2017SINBADFtts/sharan2017SINBADFtts.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/sharan2017SINBADFtts/sharan2017SINBADFtts.mov},
  author = {Shashin Sharan and Rongrong Wang and Felix J. Herrmann}
}

@PRESENTATION{siahkoohi2017SINBADFsdi,
  title = {Seismic data interpolation with Generative Adversarial Networks},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2017},
  abstract = {In this project we implement an algorithm to predict the missing traces in the seismic shot gathers. The missing traces can be either regular or irregular. Any interpolation scheme assumes a prior knowledge on the data. Here the prior information used to interpolate the data is obtained from interaction of two trained deep neural networks, namely Generator and Discriminator. The combination of these two neural networks is called Generative Adversarial Network (GAN). GAN is trained on finely sampled seismic shot gathers. By employing the trained GAN we can project shot gathers with missing traces into the domain of the generator network. Then by computing the output of generator given the found projection, we can fill in the initial gather.},
  keywords = {presentation, SINBAD, SINBADFALL2017, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/siahkoohi2017SINBADFsdi/siahkoohi2017SINBADFsdi.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/siahkoohi2017SINBADFsdi/siahkoohi2017SINBADFsdi.mov},
  author = {Ali Siahkoohi and Felix J. Herrmann}
}

@PRESENTATION{wang2017SINBADFnra,
  title = {Noise robust and time-domain formulations of Wavefield Reconstruction Inversion},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2017},
  abstract = {We propose a wave-equation-based subsurface inversion method that in many cases is more robust than conventional Full-Waveform Inversion. The new formulation is written in a denoising form that allows the synthetic data to match the observed ones up to a small error. Compared to regular Full-Waveform Inversion, our method treats the noise arising from the data meassuring/recording process and that from the synthetic modelling process separately. Compared to Wavefields Reconstruction Inversion, the new formulation mitigates the difficulty of choosing the penalty parameter λ. To solve the proposed optimization problem, we develop an efficient frequency domain algorithm that alternatively updates the model and the data. Numerical experiments confirm strong stability of the proposed method by comparisons between the results of our algorithm with that from both plain FWI and a weighted formulation of the FWI. We also discuss a new memory efficient time-domain formulation for Wavefield Reconstruction Inversion based on duality.},
  keywords = {presentation, SINBAD, SINBADFALL2017, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/wang2017SINBADFnra/wang2017SINBADFnra.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/wang2017SINBADFnra/wang2017SINBADFnra.mov},
  author = {Rongrong Wang and Mathias Louboutin and Bas Peters and Emmanouil Daskalakis and Felix J. Herrmann}
}

@PRESENTATION{witte2017SINBADFals,
  title = {A large-scale framework in Julia for fast prototyping of seismic inversion algorithms},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2017},
  abstract = {We present our progress on a large-scale seismic modeling workflow in Julia for wave-equation based inversion. The software offers a range of high-level abstractions to easily express PDE constrained optimization problems in terms of linear algebra expressions, while utilizing the DSL Devito to symbolically express the underlying PDEs and to generate fast and parallel code for solving them. Data containers and linear operators can be set up without much effort from input SEG-Y data and scale to large-scale 3D applications. This talk provides an overview of the basic functionalities of our software and applications to least squares imaging and 3D FWI.},
  keywords = {presentation, SINBAD, SINBADFALL2017, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/witte2017SINBADFals/witte2017SINBADFals.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/witte2017SINBADFals/witte2017SINBADFals.mov},
  author = {Philipp A. Witte and Mathias Louboutin and Felix J. Herrmann}
}

@PRESENTATION{yang2017SINBADFiwm,
  title = {Imaging with multiples in shallow water},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2017},
  abstract = {Based on the latest developments of research in inversion technology with optimization, researchers have made significant progress in the implementation of least-squares reverse-time migration (LS-RTM) of primaries. In Marine data however, these applications rely on the success of a pre-imaging separation of primaries and multiples, which can be modeled as a multi-dimensional convolution between the vertical derivative of the surface-free Green’s function and the down-going receiver wavefield. Instead of imaging the primaries and multiples separately, we implement the LS-RTM of the total down-going wavefield by combining areal source injection and linearized Born modelling, where strong surface related multiples are generated from a strong density variation at the ocean bottom. The advantage including surface related multiples in LS-RTM is the extra illumination we obtain from these multiples without incurring additional computational costs related to carrying out multi-dimensional convolutions part of conventional multiple prediction procedures. Even though we are able to avert these computational costs, our approach shares the large costs of LS-RTM. We reduce these costs by combining randomized source subsampling with our sparsity-promoting imaging technology, which produces artifact-free, high-resolution images, with the surface-related multiples migrated properly.},
  keywords = {presentation, SINBAD, SINBADFALL2017, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/yang2017SINBADFiwm/yang2017SINBADFiwm.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/yang2017SINBADFiwm/yang2017SINBADFiwm.mov},
  author = {Mengmeng Yang and Emmanouil Daskalakis and Felix J. Herrmann}
}

@PRESENTATION{zhang2017SINBADFmsd,
  title = {Massive seismic data compression & recovery w/ on-the-fly data extraction},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2017},
  abstract = {Industrial seismic exploration has moved towards complex geological areas, which requires typically long-offset and dense sampling data in order to avoid aliasing and inaccuracy in wave-equation based inversion algorithms. These strict requirements lead to massive data volume size and prohibitive demands on computational resources. In this work, we propose to compress our dense data in hierarchical Tucker tensor format by exploiting the low-rank structure of the data in a transformed domain. Then, we devise on-the-fly common shot or receiver gather extraction directly via the highly compressed factors. In subsampling scenarios, by interpolating this novel tensor format, we can also reconstruct the shot or receiver gather on a per-query basis rather than expanding the data to its fully-sampled form. We demonstrate the effective performance of our proposed technique on 3D stochastic full-waveform inversion, which allows the stochastic algorithm to extract shot gathers as it requires them throughout the inversion process. Moreover, we finally show how to computational effectively generate the CIGs from this compressed low-rank tensor representation of the data with the help of fast simultaneous shot or receiver gather generation.},
  keywords = {presentation, SINBAD, SINBADFALL2017, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/zhang2017SINBADFmsd/zhang2017SINBADFmsd.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/zhang2017SINBADFmsd/zhang2017SINBADFmsd.mov},
  author = {Yiming Zhang and Curt Da Silva and Rajiv Kumar and Felix J. Herrmann}
}

%----- 2016 (FALL) -----%

@PRESENTATION{bougher2016SINBADFaaa,
  title = {Amplitude vs. angle analysis as an unsupervised learning problem},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2016},
  abstract = {Amplitude vs. angle analysis (AVA) of pre-stack seismic
                  data is a commonly used method for inferring
                  petrophysical information from seismic
                  data. Conventionally, a two-term linearized rock
                  physics model (Shuey equation) is used to invert
                  angle-domain common-image gathers. Multivariate
                  analysis of the inverted terms leads to a background
                  of siliciclastic interfaces, where outlying points
                  are associated with hydrocarbon saturated sands.
                  The acquisition and processing of seismic data does
                  not result in highly-calibrated measurements that
                  adhere to the rock physics model, which often
                  inhibits the success of AVA analysis. We offer an
                  alternative approach that uses PCA-based methods to
                  learn projections directly from the data without the
                  need of a physical model. Results on synthetic and
                  field data show that PCA-based projections can
                  improve segmentation of potential reservoirs in
                  seismic data.},
  keywords = {presentation, SINBAD, SINBADFALL2016, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/bougher2016SINBADFaaa/bougher2016SINBADFaaa.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/bougher2016SINBADFaaa/bougher2016SINBADFaaa.mov},
  author = {Ben B. Bougher and Felix J. Herrmann}
}


@PRESENTATION{dasilva2016SINBADFccs,
  title = {Composite convex smooth optimization with seismic data processing applications},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2016},
  abstract = {In this work, we show a general technique for solving
                  optimization problems that are comprised of
                  minimizing a composition of a convex, non-smooth
                  function with a smooth function. We demonstrate this
                  technique in the seismic data processing context
                  applied to robust missing trace interpolation in the
                  low-rank Hierarchical Tucker tensor format as well
                  as cosparsity-based missing trace interpolation.},
  keywords = {presentation, SINBAD, SINBADFALL2016, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/dasilva2016SINBADFccs/dasilva2016SINBADFccs.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/dasilva2016SINBADFccs/dasilva2016SINBADFccs.mov},
  author = {Curt Da Silva and Felix J. Herrmann}
}


@PRESENTATION{dasilva2016SINBADFuse,
  title = {A unified {2D}/{3D} software environment for large scale time-harmonic full waveform inversion},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2016},
  abstract = {Full Waveform Inversion is a costly and complex
                  procedure, resulting in industrial codebases that
                  are often written entirely in low-level languages,
                  making them hard to understand, maintain, improve,
                  and extend. In this work, we propose a software
                  framework for organizing a 3D FWI environment that
                  helps mitigate design complexities inherent in the
                  problem in a straightforward way while delegating
                  the performance-critical portions to low level
                  languages to maintain efficiency. The result is a
                  software framework that has a unified interface for
                  2D and 3D and is flexible, efficient, scalable, and
                  demonstrably correct. We demonstrate the
                  effectiveness of this approach on a large scale 3D
                  FWI example.},
  keywords = {presentation, SINBAD, SINBADFALL2016, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/dasilva2016SINBADFuse/dasilva2016SINBADFuse.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/dasilva2016SINBADFuse/dasilva2016SINBADFuse.mov},
  author = {Curt Da Silva and Felix J. Herrmann}
}


@PRESENTATION{fang2016SINBADFeaq,
  title = {Efficient approach for quantifying uncertainty of wavefield reconstruction inversion},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2016},
  abstract = {Due to the noisy measurements, sparse observations,
                  uncertain forward models, and uncertain prior
                  parameter information, significant uncertainty
                  arises in the inverted velocity model of
                  full-waveform inversion. The uncertain velocity
                  model leads to the uncertainty in the correct
                  positions of events in the migrated images, which
                  subsequently influences the important
                  decision-makings in the oil and gas exploration and
                  production (E&P) business such as the drilling
                  decisions and economic evaluations. Therefore,
                  understanding the uncertainty in the inverted
                  velocity model is essential for the mitigation of
                  the E&P risks. A common approach to accounting for
                  uncertainty in seismic inverse problems like
                  full-waveform inversion is to describe the unknown
                  parameters in probabilistic by the Bayesian
                  inference. The Bayesian inference aims at
                  incorporating the information from the data,
                  physics, and one’s prior knowledge to formulate a
                  posterior distribution function to characterize the
                  statistical properties of the unknown
                  parameters. However, characterizing uncertainties
                  for the large-scale full-waveform inversion in
                  seismic exploration is prohibitively expensive since
                  it requires thousands of function evaluations to
                  sample the parameter space (e.g. via Markov chain
                  Monte Carlo sampling). Apart from this, the existing
                  methods of uncertainty quantification for
                  full-waveform inversion is through applying the
                  Bayesian inference to the standard method such as
                  the adjoint-state method with the assumption that no
                  uncertainty arises from the wave-equation. However,
                  the wave-equation may not be able to reflect all the
                  physics, hence, incorporating uncertainty from
                  wave-equation might benefit the quantification of
                  uncertainty. In this work, we propose to perform
                  uncertainty quantification using the recently
                  proposed wavefield reconstruction inversion
                  technique, which gives an appropriate framework to
                  tackle the uncertainty in wave-equation solve by
                  adding the misfit of the wave-equation to the
                  likelihood distribution and relaxing the
                  wave-equation constraint. Moreover, we propose a
                  computationally feasible approach to analyzing the
                  posterior distribution of the wavefield
                  reconstruction inversion method, which incorporates
                  a Gaussian distribution that approximates the
                  posterior distribution with an optimization-driven
                  Gaussian simulator to sample the Gaussian
                  distribution efficiently. Numerical examples show
                  that comparing to the McMC method, our approach is
                  able to produce comparable results with less
                  computational cost. This is joint work with Curt da
                  Silva and Rachel Kuske.},
  keywords = {presentation, SINBAD, SINBADFALL2016, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/fang2016SINBADFeaq/fang2016SINBADFeaq.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/fang2016SINBADFeaq/fang2016SINBADFeaq.mov},
  author = {Zhilong Fang and Chia Ying Lee and Curt Da Silva and Tristan van Leeuwen and Felix J. Herrmann and Rachel Kuske}
}


@PRESENTATION{gorman2016SINBADFopp,
  title = {Open {Performance} {portablE} {SeismiC} {Imaging}---{OPESCI}},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2016},
  abstract = {In this project, we introduce OPESCI-FD, a Python
                  package built on symbolic mathematics to
                  automatically generate Finite Difference models from
                  a high-level description of the model equations. We
                  investigate applying this framework to generate the
                  propagator program used in seismic imaging. We
                  implement the 3D acoustic and anisotropic FD scheme
                  as an example and demonstrate the advantages of
                  usability, flexibility and accuracy of the
                  framework. The design of OPESCI-FD aims to allow
                  rapid development, analysis and optimisation of
                  Finite Difference programs. OPESCI-FD is the
                  foundation for continuing development by the OPESCI
                  project team, building on the research presented in
                  this report. This talk concludes by reviewing the
                  further developments that are already under way, as
                  well as the scope for extension to cater for other
                  equations and numerical schemes. This joint work
                  with SINBAD and SENAI CIMATEC and has received
                  additional funding from Intel.},
  keywords = {presentation, SINBAD, SINBADFALL2016, SLIM, private},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/gorman2016SINBADFopp/gorman2016SINBADFopp.pdf},
  url2 = {https://slim.gatech.edu/Publications/Private/Conferences/SINBAD/2016/Fall/gorman2016SINBADFopp/gorman2016SINBADFopp.mov},
  author = {Gerard Gorman and Marcos de Aguiar and David Ham and Felix J. Herrmann and Paul H. J. Kelly and Navjot Kukreja and Michael Lange and Mathias Louboutin and Fabio Luporini and Paulius Velesko and Vincenzo Pandolfo and Felippe Vieira Zacarias}
}


@PRESENTATION{herrmann2016SINBADFcvp,
  title = {Constraints versus penalties for edge-preserving full-waveform inversion},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2016},
  abstract = {Full-waveform inversion is challenging in complex
                  geological areas. Even when provided with an
                  accurate starting model, the inversion algorithms
                  often struggle to update the velocity
                  model. Contrary to other areas in applied
                  geophysics, including prior information in
                  full-waveform inversion is still relatively in its
                  infancy. In part this is due to the fact that
                  incorporating prior information that relates to
                  geological settings where strong discontinuities in
                  the velocity model dominate is difficult, because
                  these settings call for non-smooth
                  regularizations. We tackle this problem by including
                  constraints on the spatial variations and value
                  ranges of the inverted velocities, as opposed to
                  adding penalties to the objective as is more
                  customary in main stream geophysical inversion. By
                  demonstrating the lack of predictability of
                  edge-preserving inversion when the regularization is
                  in the form of an added penalty term, we advocate
                  the inclusion of constraints instead. Our examples
                  show that the latter lead to more predictable
                  results and to significant improvements in the
                  delineation of Salt bodies. This is joint work with
                  Bas Peters.},
  keywords = {presentation, SINBAD, SINBADFALL2016, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/herrmann2016SINBADFcvp/herrmann2016SINBADFcvp.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/herrmann2016SINBADFcvp/herrmann2016SINBADFcvp.mov},
  author = {Felix J. Herrmann and Bas Peters}
}


@PRESENTATION{herrmann2016SINBADFwom,
  title = {Welcome & {Overview} of the {Meeting}},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2016},
  keywords = {presentation, SINBAD, SINBADFALL2016, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/herrmann2016SINBADFwom/herrmann2016SINBADFwom.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/herrmann2016SINBADFwom/herrmann2016SINBADFwom.mov},
  author = {Felix J. Herrmann}
}


@PRESENTATION{kumar2016SINBADFels,
  title = {Efficient large-scale {5D} seismic data acquisition and processing using rank-minimization},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2016},
  abstract = {Seismic data collection is becoming challenging because
                  of increased demands for high-quality, long-offset
                  and wide-azimuth data. Leveraging ideas from CS, in
                  this work we establish a cost effective acquisition
                  and processing techniques, which are no longer
                  dominated by survey area size but by the sparsity of
                  seismic data volumes. In the first part of abstract,
                  we establish connections between random time
                  dithering and jittered sampling in
                  space. Specifically, we recover high-quality 5D
                  seismic data volumes from time-jittered marine
                  acquisition where the average inter-shot time is
                  significantly reduced, leading to cheaper surveys
                  due to fewer overlapping shots. The time-jittered
                  acquisition, in conjunction with the shot separation
                  by Singular-Value Decomposition (SVD)-free
                  factorization based rank-minimization approach,
                  allows us to recover high quality 5D seismic data
                  volumes. Results are illustrated for simulations of
                  simultaneous time-jittered continuous recording for
                  a 3D ocean-bottom cable survey, where we outperforms
                  existing techniques, by an order of magnitude
                  computational speedup and using 1/20th of the
                  memory, that use sparsity in transforms domains. The
                  second part of abstract focussed on leveraging
                  low-rank structure in seismic data to solve
                  extremely large data recovery (interpolation)
                  problems. We introduced a large–scale SVD-free
                  optimization framework that is robust with respect
                  to outliers and that uses information on the
                  support. We test the efficacy of the proposed
                  interpolation framework on a large-scale 5D seismic
                  data, generated from the geologically complex
                  synthetic 3D Compass velocity model, where 80\% of
                  the data has been removed. Our findings show that
                  major computational and memory gains are possible
                  compared to curvelet-based reconstruction.},
  keywords = {presentation, SINBAD, SINBADFALL2016, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/kumar2016SINBADFels/kumar2016SINBADFels.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/kumar2016SINBADFels/kumar2016SINBADFels.mov},
  author = {Rajiv Kumar and Shashin Sharan and Haneet Wason and Felix J. Herrmann}
}


@PRESENTATION{kumar2016SINBADFlrm,
  title = {Low-rank methods for on-the-fly slicing & dicing of seismic data & image volumes},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2016},
  abstract = {Conventional oil and gas fields are increasingly
                  difficult to explore and produce, which calls for
                  more complex wave-equation based inversion (WEI)
                  algorithms that require dense long-offset
                  samplings. These requirements result in an
                  exponential growth in data volumes and prohibitive
                  demands on computational resources. In this work, we
                  propose a fast, resilient, and scalable
                  wave-equation based inversion methodology for both
                  the data and image-domain, which can handle
                  complicated wave physics. First we show that both
                  the data and image domains exhibit low-rank
                  structure in a transform-domain, which can be
                  exploited to compress the dense data or image
                  volumes. Then, by accessing information from the
                  compressed volumes on-the fly, we devise a scalable
                  computational inversion framework driven by gradient
                  calculations, which works with small subsets of
                  source experiments. In the full-waveform inversion
                  context, we demonstrate the efficacy of low-rank
                  interpolation to improve downstream inversion
                  results compared to merely inverting the velocity
                  model using the subsampled data volume directly. We
                  demonstrate the effectiveness of the proposed
                  framework on two different waveform-inversion
                  formulations, specifically performing full-waveform
                  inversion on 5D data set generated using the
                  overthrust model, and wave-equation based migration
                  velocity analysis on the Marmousi model. This is
                  joint work with Curt Da Silva and Yiming Zhang.},
  keywords = {presentation, SINBAD, SINBADFALL2016, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/kumar2016SINBADFlrm/kumar2016SINBADFlrm.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/kumar2016SINBADFlrm/kumar2016SINBADFlrm.mov},
  author = {Rajiv Kumar and Curt Da Silva and Yiming Zhang and Felix J. Herrmann}
}


@PRESENTATION{lopez2016SINBADFlrm,
  title = {Low-rank matrix recovery for parallel architectures},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2016},
  abstract = {Low-rank matrix recovery (LRMR) techniques offer
                  potential tools for frugal seismic data acquisition,
                  where dense acquisition is replaced by large-scale
                  optimization. This shift of focus means that
                  efficient numerical methods are critical for
                  successful LRMR implementation in seismic
                  applications. In this talk, we extend the
                  rank-penalization methodology to a parallelizable
                  framework. We adopt a factorization-based
                  alternating minimization scheme and decouple it into
                  an independent system of simpler sub-problems that
                  can be handled in parallel. The methodology is
                  flexible, where the approach can be adapted to the
                  number of workers available. Numerical experiments
                  are conducted to demonstrate that the quality of
                  reconstruction is comparable to existing methods at
                  a fraction of the computational time.},
  keywords = {presentation, SINBAD, SINBADFALL2016, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/lopez2016SINBADFlrm/lopez2016SINBADFlrm.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/lopez2016SINBADFlrm/lopez2016SINBADFlrm.mov},
  author = {Oscar Lopez and Rajiv Kumar and Felix J. Herrmann}
}


@PRESENTATION{louboutin2016SINBADFhps,
  title = {High-performance seismic applications of {OPESCI}},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2016},
  abstract = {We present our latest geophysical applications built on
                  OPESCI. By using a high-level symbolic API, we allow
                  for fast development and easy implementation of
                  various (acoustic, VT, TTI) wave propagators
                  relevant to exploration geophysics. We start by
                  highlighting possibilities in an acoustic setting
                  including classical operators such as forward
                  modelling and linearised forward (Born) modelling as
                  well as more advanced operators deriving from wave
                  equations with double dipoles and the application of
                  the PDE to a wavefield instead of applying its
                  inverse. We will also show that the performance
                  (time to solution) of this code is on par with
                  industrial software libraries (10\% faster on the
                  full SEAM model). We finally present our
                  implementation of 3D TTI modelling and its adjoint
                  including out comprehensive testing framework. This
                  is joint work with Gerard Gorman},
  keywords = {presentation, SINBAD, SINBADFALL2016, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/louboutin2016SINBADFhps/louboutin2016SINBADFhps.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/louboutin2016SINBADFhps/louboutin2016SINBADFhps.mov},
  author = {Mathias Louboutin and Felix J. Herrmann}
}


@PRESENTATION{oghenekohwo2016SINBADFast,
  title = {Asymmetric sampling for time-lapse surveys},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2016},
  abstract = {In this talk, we use the joint recovery model (JRM),
                  which derives from distributed compressed sensing,
                  to reconstruct time-lapse data acquired via
                  time-jittered marine sources that are significantly
                  subsampled in the monitor survey(s) in comparison to
                  the baseline survey, hence, saving acquisition
                  cost. We analyze the effects of two or more
                  asymmetrically subsampled monitor surveys on
                  wavefield reconstruction via JRM. We leverage the
                  common information shared amongst the vintages
                  during our sparsity-promoting recovery.},
  keywords = {presentation, SINBAD, SINBADFALL2016, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/oghenekohwo2016SINBADFast/oghenekohwo2016SINBADFast.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/oghenekohwo2016SINBADFast/oghenekohwo2016SINBADFast.mov},
  author = {Felix Oghenekohwo and Haneet Wason and Felix J. Herrmann}
}


@PRESENTATION{peters2016SINBADFcnc,
  title = {Convex & non-convex constraint sets for full-waveform inversion},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2016},
  abstract = {We extend some of our previous work on constrained
                  formulations of full-waveform inversion. Some
                  motivating examples illustrate why solving
                  constrained problems directly is a simpler approach
                  for solving non-linear inverse problems than
                  penalized problem formulations. So far we used
                  constraints which can be represented as convex sets,
                  for which most optimization theory is
                  developed. Some non-convex counterparts of convex
                  sets are introduced and examples show that the
                  algorithms developed for convex sets also work well
                  with non-convex sets. The motivation to also explore
                  non-convex sets, is that they sometimes translate
                  prior geological knowledge into mathematical
                  constraints in a more direct way. All presented
                  material is available as a user-friendly
                  toolbox. The toolbox works with any code which can
                  provide a data-misfit function value and a gradient
                  direction.},
  keywords = {presentation, SINBAD, SINBADFALL2016, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/peters2016SINBADFcnc/peters2016SINBADFcnc.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/peters2016SINBADFcnc/peters2016SINBADFcnc.mov},
  author = {Bas Peters and Felix J. Herrmann}
}


@PRESENTATION{sharan2016SINBADFhrm,
  title = {High resolution microseismic source collocation},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2016},
  abstract = {Microseismic waves generated by hydraulic fracturing are
                  used to locate the fractures created in the
                  reservoir. Most of the existing source collocation
                  methods have their limitations in terms of
                  resolution of very closely spaced microseismic
                  events. In this work we propose a method to
                  collocate microseismic events with high resolution
                  along with simultaneous estimation of source time
                  function. In the context of waves, the
                  finite-difference modelling operator acts as an
                  analysis operator for wavefields. This implies that
                  wavefields under the action of a finite-difference
                  modelling kernel can be focused to sources
                  generating these wavefields. We exploit this
                  property of wavefields to locate microseismic events
                  and estimate the source time function jointly. To
                  arrive at a computational feasible scheme, we use
                  the accelerated version of Linearized Bregman (LBR)
                  algorithm to solve the above mentioned problem. This
                  is joint work with Rongrong Wang.},
  keywords = {presentation, SINBAD, SINBADFALL2016, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/sharan2016SINBADFhrm/sharan2016SINBADFhrm.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/sharan2016SINBADFhrm/sharan2016SINBADFhrm.mov},
  author = {Shashin Sharan and Rongrong Wang and Felix J. Herrmann}
}


@PRESENTATION{wang2016SINBADFtmf,
  title = {Two methods for frequency down-extrapolation},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2016},
  abstract = {It is well known that low frequency data are essential
                  to the success of the Full-Waveform Inversion. Field
                  data from a usual marine acquisition, however,
                  typically lack reliable low frequencies due to the
                  existing physical limitations. One research
                  direction focuses on studying low-frequency
                  extrapolation, the process of extending reliable
                  frequency bands of the raw data towards the lower
                  end of the spectrum. We propose two optimization
                  problems, by solving which low frequencies can be
                  made out of the high SNR high frequency
                  component. We present both theoretical explanation
                  and numerical experiments to demonstrate the
                  efficacy as well as limitation of these methods.},
  keywords = {presentation, SINBAD, SINBADFALL2016, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/wang2016SINBADFtmf/wang2016SINBADFtmf.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/wang2016SINBADFtmf/wang2016SINBADFtmf.mov},
  author = {Rongrong Wang and Felix J. Herrmann}
}


@PRESENTATION{wason2016SINBADFlcr,
  title = {Low-cost, randomized {3D} towed-marine time-lapse seismic acquisition},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2016},
  abstract = {Most often conventional 3D towed-marine seismic data
                  acquisition suffers from limited illumination of
                  subsurface due to narrow azimuth. Although,
                  acquisition techniques such as Ocean Bottom Cable
                  (OBC), Ocean Bottom Node (OBN), Rich-Azimuth (RAZ)
                  acquisition, Multi-Azimuth (MAZ) acquisition have
                  been developed to illuminate the subsurface from all
                  possible angles, these acquisition techniques are
                  expensive. Motivated by the design principles of
                  compressed sensing, we acquire randomly subsampled
                  and simultaneous (e.g., simultaneous long offset) 3D
                  towed-marine time-lapse seismic data for few
                  (random) azimuths, and recover densely sampled
                  multiple azimuth baseline and monitor data via
                  sparsity promotion and rank minimization. Our
                  acquisition is low cost since we have subsampled
                  measurements.},
  keywords = {presentation, SINBAD, SINBADFALL2016, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/wason2016SINBADFlcr/wason2016SINBADFlcr.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/wason2016SINBADFlcr/marine_acq_mask.pdf},
  url3 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/wason2016SINBADFlcr/wason2016SINBADFlcr.mov},
  author = {Rajiv Kumar and Felix Oghenekohwo and Shashin Sharan and Haneet Wason and Felix J. Herrmann}
}


@PRESENTATION{witte2016SINBADFlst,
  title = {A large-scale time-domain modeling and inversion workflow in {Julia}},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2016},
  abstract = {We present our initial steps towards the development of
                  a large-scale seismic modeling workflow in Julia
                  which provides a framework for wave-equation based
                  inversion methods like full waveform inversion or
                  least squares migration. Our framework is based on
                  the Devito tool for optimized finite difference
                  computations that generates highly optimized code
                  from symbolic PDEs. We aim at developing a flexible
                  workflow which is based on abstract operators and
                  allows coding that is “close to the math”, while at
                  the same time relying on extremely fast and
                  efficient, state of the art wave equation solvers.},
  keywords = {presentation, SINBAD, SINBADFALL2016, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/witte2016SINBADFlst/witte2016SINBADFlst.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/witte2016SINBADFlst/witte2016SINBADFlst.mov},
  author = {Philipp A. Witte and Felix J. Herrmann}
}


@PRESENTATION{witte2016SINBADFpve,
  title = {Phase velocity error minimizing scheme for the anisotropic pure {P-wave} equation},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2016},
  abstract = {Pure P-wave equations for acoustic modeling in
                  transverse isotropic media are derived by
                  approximating the exact pure P-wave dispersion
                  relation. In this work, we present an alternative to
                  the common Taylor expansion based equations, in
                  which we approximate the exact dispersion relation
                  through a polynomial expansion and determine its
                  coefficients by solving a least squares problem that
                  minimizes the phase velocity error over the complete
                  range of phase angles. We show that this approach is
                  up to an order of magnitude more accurate than other
                  pure P-wave equations at a comparable computational
                  cost.},
  keywords = {presentation, SINBAD, SINBADFALL2016, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/witte2016SINBADFpve/witte2016SINBADFpve.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/witte2016SINBADFpve/witte2016SINBADFpve.mov},
  author = {Philipp A. Witte and Christiaan C. Stolk and Felix J. Herrmann}
}


@PRESENTATION{witte2016SINBADFtdl,
  title = {Time-domain least-squares {RTM} with sparsity promotion on field data},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2016},
  abstract = {Least squares reverse time migration (LSRTM) is an
                  inversion based imaging algorithm which can surpress
                  acquisition related artifacts and provides images
                  with increased frequency content, decreased noise
                  level and balanced amplitudes. In this work
                  demonstrate our time-domain LSRTM workflow using
                  various pre-conditioners, source-subsampling and
                  sparsity promotion and show its application to the
                  2D Machar field data set.},
  keywords = {presentation, SINBAD, SINBADFALL2016, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/witte2016SINBADFtdl/witte2016SINBADFtdl.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/witte2016SINBADFtdl/witte2016SINBADFtdl.mov},
  author = {Philipp A. Witte and Felix J. Herrmann}
}


@PRESENTATION{yang2016SINBADFtds,
  title = {Time-domain sparsity-promoting least-squares migration with source estimation},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2016},
  abstract = {Compared to traditional reverse-time migration (RTM),
                  least-squares RTM (LS_RTM) is able to obtain true
                  amplitude images as solutions of $\ell_2$-norm
                  minimization problems by fitting the synthetic and
                  observed reflection data. The shortcoming is that
                  solutions of these $\ell_2$ problems tend to be
                  overfitted and computationally too expensive. By
                  working with randomized subsets of data only, the
                  computational costs of LS-RTM can be brought down to
                  an acceptable level, producing artifact-free
                  high-resolution images. By including on-the-fly
                  source-time function estimation into the method of
                  Linearized Bregman (LB), we tackle the open issues
                  of these "compressive imaging" in the aspects of
                  algorithmic complexity of solver, guaranteed
                  convergence and source estimation. We also
                  inbvestigate whether the algorithm can be
                  accelerated when we combine LB with the Nesterov
                  method. Application of our algorithm on a 2D
                  synthetic shows that we are able to get
                  high-resolution images, with accurate estimates of
                  the wavelet, for one single data pass.},
  keywords = {presentation, SINBAD, SINBADFALL2016, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/yang2016SINBADFtds/yang2016SINBADFtds.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2016/Fall/yang2016SINBADFtds/yang2016SINBADFtds.m4v},
  author = {Mengmeng Yang and Philipp A. Witte and Zhilong Fang and Felix J. Herrmann}
}


%----- 2015 (FALL) -----%

@PRESENTATION{dasilva2015SINBADFnso,
  title = {A new software organization for {3D} {FWI}},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {In this work, I have implemented a scalable and
                  extendable framework for 3D Full Waveform Inversion
                  in Matlab. This approach uses modern software design
                  principles to create a codebase that is easy to
                  understand, maintain, and extend and that also
                  allows for rapid prototyping of new algorithmic
                  ideas that can be easily transferred to large scale
                  problems. The code itself is modularized in a proper
                  way, which allows straightforward testing of each
                  component (e.g., Taylor error test, adjoint tests,
                  etc.). Improvements to the computational kernel
                  (i.e., Helmholtz solves with more efficient
                  matrix-vector products, new preconditioners, etc.)
                  propagate to the entire framework.},
  keywords = {presentation, SINBAD, SINBADFALL2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/dasilva2015SINBADFnso/dasilva2015SINBADFans.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/dasilva2015SINBADFnso/dasilva2015SINBADFsdd.pdf},
  url3 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/dasilva2015SINBADFnso/dasilva2015SINBADFsdd.mov},
  author = {Curt Da Silva and Felix J. Herrmann}
}


@PRESENTATION{dasilva2015SINBADFsss,
  title = {Scaling {SINBAD} software to {3-D} on {Yemoja}},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {We present early results on the scalability of SINBAD’s
                  wavefield reconstruction and wave-equation based
                  inversion technologies on Yemoja, a 17k core cluster
                  made available to us by BG Group at SENAI CIMATEC
                  Supercomputing Centre in Brazil.},
  keywords = {presentation, SINBAD, SINBADFALL2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/dasilva2015SINBADFsss/dasilva2015SINBADFsss.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/dasilva2015SINBADFsss/dasilva2015SINBADFsss.mov},
  author = {Curt Da Silva and Haneet Wason and Mathias Louboutin and Bas Peters and Shashin Sharan and Zhilong Fang and Felix J. Herrmann}
}


@PRESENTATION{esser2015SINBADFasd,
  title = {Automatic salt delineation — {Wavefield} {Reconstruction} {Inversion} with convex constraints},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {We extend full-waveform inversion by Wavefield
                  Reconstruction Inversion by including convex
                  constraints on the model. Contrary to the
                  conventional adjoint-state formulations, Wavefield
                  Reconstruction Inversion has the advantage that the
                  Gauss-Newton Hessian is well approximated by a
                  diagonal scaling, which allows us to add convex
                  constraints, such as the box- and the
                  edge-preserving total-variation constraint, on the
                  square slowness without incurring significant
                  increases in computational costs. As the examples
                  demonstrate, including these constraints yields far
                  superior results in complex geological areas that
                  contain high-velocity high-contrast bodies
                  (e.g. salt or basalt). Without these convex
                  constraints, adjoint-state and Wavefield
                  Reconstruction Inversion get trapped in local minima
                  for poor starting models.},
  keywords = {presentation, SINBAD, SINBADFALL2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/esser2015SINBADFasd/esser2015SINBADFasd.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/esser2015SINBADFasd/esser2015SINBADFasd.mov},
  author = {Ernie Esser and Llu\'{i}s Guasch and Felix J. Herrmann}
}


@PRESENTATION{esser2015SINBADFrsa,
  title = {Resolving scaling ambiguities with the {L1}/{L2} norm in a blind deconvolution problem with feedback},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {Compared to more mundane blind deconvolution problems,
                  blind deconvolution in seismic applications involves
                  a feedback mechanism related to the free surface.
                  The presence of this feedback mechanism gives us an
                  unique opportunity to remove ambiguities that have
                  plagued blind deconvolution for a long time. While
                  beneficial, this feedback by itself is insufficient
                  to remove the ambiguities even with L1
                  constraints. However, when paired with an L1/L2
                  constraint the feedback allows us to resolve the
                  scaling ambiguity under relatively mild
                  assumptions. Inspired by lifting approaches, we
                  propose to split the sparse signal into positive and
                  negative components and apply an \ell_1/\ell_2
                  constraint to the difference, thereby obtaining a
                  constraint that is easy to implement.  Numerical
                  experiments demonstrate robustness to the
                  initialization as well as to noise in the data.},
  keywords = {presentation, SINBAD, SINBADFALL2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/esser2015SINBADFrsa/esser2015SINBADFrsa.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/esser2015SINBADFrsa/esser2015SINBADFrsa.mov},
  author = {Ernie Esser and Rongrong Wang and Tim T.Y. Lin and Felix J. Herrmann}
}


@PRESENTATION{fang2015SINBADFuqw,
  title = {Uncertainty quantification for {Wavefield}-{Reconstruction} {Inversion} using a positive-definite approximated {Hessian}},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {We analyze the Hessian of wavefield reconstruction
                  inversion (WRI) and propose a new approximated
                  Hessian. Instead of requiring PDE solves, the
                  matrix-vector multiplication action of the
                  approximate Hessian can be achieved with several
                  matrix-vector multiplications. The diagonal part of
                  the approximated Hessian can be also calculated
                  without additional PDE solves. We apply this
                  approximated Hessian to uncertainty quantification
                  and obtain statistical parameters such as standard
                  deviation and confidence interval. Numerical example
                  illustrate the accuracy of the estimated Hessian and
                  the feasibility of the method to quantify
                  uncertainties.},
  keywords = {presentation, SINBAD, SINBADFALL2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/fang2015SINBADFuqw/fang2015SINBADFuqw.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/fang2015SINBADFuqw/fang2015SINBADFuqw.mov},
  author = {Zhilong Fang and Chia Ying Lee and Curt Da Silva and Felix J. Herrmann and Rachel Kuske}
}


@PRESENTATION{fang2015SINBADFwri,
  title = {Wavefield-reconstruction inversion with source estimation and minimum smoothness constraint – application to the {Chevron} 2014 dataset},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {We present a robust wavefield reconstruction inversion
                  with source estimation. The source wavelet is
                  estimated with the reconstruction of wavefield
                  simultaneously by solving an extended data-augmented
                  problem. We apply this method to the 2014 Chevron
                  synthetic blind test dataset and show the robustness
                  of our method.},
  keywords = {presentation, SINBAD, SINBADFALL2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/fang2015SINBADFwri/fang2015SINBADFwri.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/fang2015SINBADFwri/fang2015SINBADFwri.mov},
  author = {Zhilong Fang and Bas Peters and Felix J. Herrmann}
}


@PRESENTATION{herrmann2015SINBADFwom,
  title = {Welcome & {Overview} of the {Meeting}},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2015},
  keywords = {presentation, SINBAD, SINBADFALL2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/herrmann2015SINBADFwom/herrmann2015SINBADFwom.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/herrmann2015SINBADFwom/herrmann2015SINBADFwom.mov},
  author = {Felix J. Herrmann}
}


@PRESENTATION{kumar2015SINBADFaoi,
  title = {Affordable omnidirectional image volumes extension to {3D}},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {Image gathers are an important tool for velocity
                  analysis, AVA analysis and targeted imaging,
                  however, the computational cost involving in forming
                  the image volumes makes them prohibitively expensive
                  in case of large scale 3D seismic data
                  acquisition. In this work, we propose the extension
                  of our probing techniques from 2D to 3D to form the
                  extended image volumes efficiently. We show the
                  efficacy of proposed formulation on synthetic data
                  set.},
  keywords = {presentation, SINBAD, SINBADFALL2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/kumar2015SINBADFaoi/kumar2015SINBADFaoi.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/kumar2015SINBADFaoi/kumar2015SINBADFaoi.mov},
  author = {Rajiv Kumar and Tristan van Leeuwen and Felix J. Herrmann}
}


@PRESENTATION{lin2015SINBADFosd,
  title = {Our student-driven {HPC} environment},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {A major role of academic environments is to provide
                  learning experiences for students to critically
                  analyze and develop methods, both at a high-level of
                  mathematical rigour, and at a low-enough level of
                  implementation in order to yield experimental
                  results on real datasets. Often these two goals are
                  in conflict with each other, in terms of both
                  learning time and attention. At SLIM, we strive to
                  strike a balance between the two by abstracting away
                  many of the low-level aspects of distributed HPC
                  under a framework that matches syntactically with
                  the mathematics of our field, while exposing enough
                  control parameters for tuning performance
                  characteristics. This talk will touch on the history
                  of our efforts at SLIM, and culminate in an overview
                  of our current method of interacting with the
                  in-house compute cluster.},
  keywords = {presentation, SINBAD, SINBADFALL2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/lin2015SINBADFosd/lin2015SINBADFosd.pdf},
  author = {Tim T.Y. Lin and Felix J. Herrmann}
}


@PRESENTATION{lopez2015SINBADFumc,
  title = {Universal matrix completion: applications to seismic data acquisition},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {This talk discusses the potential applications of
                  universal matrix completion for infill management
                  and acquisition design. We consider recent
                  developments in the theory of matrix completion that
                  produce practical tools to quantify how successful a
                  given sub sampling mask will be for seismic trace
                  interpolation via nuclear norm minimization. This
                  approach provides an on the fly infill management
                  system as well as instruments to design acquisition
                  schemes that favor interpolation via rank
                  penalization techniques.},
  keywords = {presentation, SINBAD, SINBADFALL2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/lopez2015SINBADFumc/lopez2015SINBADFumc.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/lopez2015SINBADFumc/lopez2015SINBADFumc.mov},
  author = {Oscar Lopez and Haneet Wason and Rajiv Kumar and Felix J. Herrmann}
}


@PRESENTATION{louboutin2015SINBADFess,
  title = {Extending the search space of time-domain adjoint-state {FWI} w/ randomized implicit time shifts},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {We introduce a modified adjoint-state method for time
                  domain FWI that allows us to extend the research
                  space. As a result, we arrive at a formulation where
                  the sensitivity to cycle skipping is reduced. Our
                  method obtains results with the same computational
                  costs as FWI (The PDE solved is the same) but with
                  significantly reduced memory costs. We use new
                  results in non-convex optimization to justify the
                  method as well as new regularization techniques and
                  stochastic optimization to improve the behavior of
                  the algorithm.},
  keywords = {presentation, SINBAD, SINBADFALL2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/louboutin2015SINBADFess/louboutin2015SINBADFess.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/louboutin2015SINBADFess/louboutin2015SINBADFess.mov},
  author = {Mathias Louboutin and Felix J. Herrmann}
}


@PRESENTATION{oghenekohwo2015SINBADFcst,
  title = {Comparative study of time-lapse {FWI} approaches},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {I will illustrate the performance of our joint recovery
                  model for time-lapse FWI. Specifically, I will
                  compare our method to other conventional methods,
                  namely, parallel difference and sequential
                  difference FWI. I will illustrate the robustness of
                  our method compared to others especially in the
                  presence of missing data caused by large acquisition
                  gaps. In addition, I will assess the performance of
                  these methods in the detectability of the time-lapse
                  change in synthetic data. Analysis of these methods
                  under different starting FWI model will also be
                  highlighted.},
  keywords = {presentation, SINBAD, SINBADFALL2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/oghenekohwo2015SINBADFcst/oghenekohwo2015SINBADFcst.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/oghenekohwo2015SINBADFcst/oghenekohwo2015SINBADFcst.mov},
  author = {Felix Oghenekohwo and Felix J. Herrmann}
}


@PRESENTATION{oghenekohwo2015SINBADFtli,
  title = {Time-lapse imaging with multiples and distributed compressed sensing},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {In this talk, we leverage our joint-recovery model,
                  which uses the fact that time-lapse images generally
                  have a lot in common, to improve imaging problems
                  with large gaps. We demonstrate that the adverse
                  effects of large acquisition gaps can be mitigated
                  by using this joint-recovery model especially in
                  combination with imaging with surface-related
                  multiples.},
  keywords = {presentation, SINBAD, SINBADFALL2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/oghenekohwo2015SINBADFtli/oghenekohwo2015SINBADFtli.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/oghenekohwo2015SINBADFtli/oghenekohwo2015SINBADFtli.mov},
  author = {Felix Oghenekohwo and Felix J. Herrmann}
}


@PRESENTATION{peters2015SINBADF3dw,
  title = {3D {WRI}},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {The Wavefield Reconstruction Inversion (WRI) approach to
                  seismic inversion is a topic of active research in
                  the SLIM group. Several variants have been proposed,
                  all of which rely on the solution of a linear
                  least-squares problem with a Helmholtz
                  discretization. While algorithms based on LU or QR
                  factorization are very efficient for 2D problems, 3D
                  problems required the development of a
                  factorization-free iterative method. This talk
                  introduces the current version of the algorithm,
                  which requires significantly less memory and
                  computation and uses existing Helmholtz solvers as
                  an important building block.  We plan to discuss
                  early results of 3-D WRI on a small 3-D synthetic
                  example.},
  keywords = {presentation, SINBAD, SINBADFALL2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/peters2015SINBADF3dw/peters2015SINBADF3dw.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/peters2015SINBADF3dw/peters2015SINBADF3dw.mov},
  author = {Bas Peters and Chen Greif and Felix J. Herrmann}
}


@PRESENTATION{peters2015SINBADFqpf,
  title = {A quadratic-penalty full-space method for waveform inversion},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {In PDE-constrained optimization problems other than
                  geophysical full-waveform inversion, full-space
                  optimization methods are commonly used. This type of
                  optimization method updates both the medium
                  parameters and the wavefields, instead of solving
                  wave equations explicitly. In the FWI context, this
                  means the objective function value and gradient can
                  be obtained at very little computational cost. A
                  major obstacle is, however, the requirement to have
                  all wavefields in memory.},
  keywords = {presentation, SINBAD, SINBADFALL2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/peters2015SINBADFqpf/peters2015SINBADFqpf.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/peters2015SINBADFqpf/peters2015SINBADFqpf.mov},
  author = {Bas Peters and Felix J. Herrmann}
}


@PRESENTATION{peters2015SINBADFrwi,
  title = {Regularizing waveform inversion by projections onto intersections of convex sets},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {Common strategies to regularize waveform inversion (and
                  other geophysical inverse problems) are adding
                  quadratic penalty terms to the objective function or
                  filtering the gradients used to update the model
                  estimate. An example are penalties or filters to
                  prevent/filter spurious high spatial frequency
                  oscillations in the model while working with low
                  frequency data. We present an alternative way of
                  regularization, which works by projecting the model
                  onto an intersection of convex sets, where each sets
                  encodes certain desired model properties. This
                  approach has certain theoretical and practical
                  advantages over quadratic penalties or gradient
                  filters. Some examples of useful convex sets in
                  various challenging waveform inversion settings are
                  shown on both real and synthetic data.},
  keywords = {presentation, SINBAD, SINBADFALL2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/peters2015SINBADFrwi/peters2015SINBADFrwi.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/peters2015SINBADFrwi/peters2015SINBADFrwi.mov},
  author = {Bas Peters and Brendan R. Smithyman and Mathias Louboutin and Felix J. Herrmann}
}


@PRESENTATION{sharan2015SINBADFscu,
  title = {Source collocation using the method of {Linearized} {Bregman}},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {In this work, we present a method to collocate sources
                  from seismic reflections recorded at receiver
                  locations. We use the method of linearized
                  Bregman. This algorithm focuses unknown sources by
                  promoting sparsity in the “Helmholtz domain”—i.e.,
                  the wavefield under the action of the the Helmholtz
                  system—under the constraint of a data misfit within
                  a particular tolerance. We extend this method to
                  noisy measurements. We are in particular interested
                  in situations where the noise is coherent because it
                  is in the range of the Helmholtz equation.},
  keywords = {presentation, SINBAD, SINBADFALL2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/sharan2015SINBADFscu/sharan2015SINBADFscu.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/sharan2015SINBADFscu/sharan2015SINBADFscu.mov},
  author = {Shashin Sharan and Rongrong Wang and Felix J. Herrmann}
}


@PRESENTATION{wang2015SINBADFifw,
  title = {Improving full waveform inversion with spectral extrapolation},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {We develop a sparsity based frequency extrapolation
                  method to obtain low frequency data from the high
                  frequency ones. As other frequency extrapolation
                  methods, the extrapolation introduces noise due to
                  dispersion and possibly densely distributed
                  reflectors. To mitigate such noise, we incorporate
                  Curvlet coefficient regularization in the
                  extrapolation algorithm. Numerical results
                  demonstrate the effectiveness of the extrapolation
                  in frequency domain FWI in models of moderate
                  complexity.},
  keywords = {presentation, SINBAD, SINBADFALL2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/wang2015SINBADFifw/wang2015SINBADFifw.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/wang2015SINBADFifw/wang2015SINBADFifw.mov},
  author = {Rongrong Wang and Felix J. Herrmann}
}


@PRESENTATION{wason2015SINBADFrtl,
  title = {Randomization of time-lapse marine surveys},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {We present an extension of our time-jittered
                  (simultaneous) marine acquisition for time-lapse
                  surveys, where we work on more realistic field
                  acquisition scenarios by incorporating irregular
                  spatial grids without insisting on repeatability
                  between the surveys. Since we are always subsampled
                  in both the baseline and monitor surveys, we recover
                  the densely sampled baseline and monitor and then
                  the (complete) 4-D difference from subsampled
                  baseline and monitor data.},
  keywords = {presentation, SINBAD, SINBADFALL2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/wason2015SINBADFrtl/wason2015SINBADFrtl.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/wason2015SINBADFrtl/wason2015SINBADFrtl.mov},
  author = {Haneet Wason and Felix Oghenekohwo and Felix J. Herrmann}
}


@PRESENTATION{wason2015SINBADFsss,
  title = {Source separation for simultaneous towed-streamer acquisition via compressed sensing},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {We show a comparison between two compressed-sensing
                  based source-separation algorithms, i.e., sparsity
                  promotion and rank minimization, for simultaneous
                  data.},
  keywords = {presentation, SINBAD, SINBADFALL2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/wason2015SINBADFsss/wason2015SINBADFsss.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/wason2015SINBADFsss/wason2015SINBADFsss.mov},
  author = {Haneet Wason and Rajiv Kumar and Felix J. Herrmann}
}


@PRESENTATION{witte2015SINBADFara,
  title = {Anisotropic {RTM} applied to field data},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {In this talk we present our latest reverse
                  time-migration results of the BP Machar data set, a
                  2D seismic line from the North sea featuring a large
                  chalk dome. In contrast to our prior imaging
                  attempts, we now account for anisotropic effects in
                  the data using our new TTI pure quasi-P wave
                  modeling code in the time domain.},
  keywords = {presentation, SINBAD, SINBADFALL2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/witte2015SINBADFara/witte2015SINBADFara.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/witte2015SINBADFara/witte2015SINBADFara.mov},
  author = {Philipp A. Witte and Felix J. Herrmann}
}


@PRESENTATION{witte2015SINBADFtdf,
  title = {Time-domain {FWI} in {TTI} media},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {We develop an inversion workflow for tilted transverse
                  isotropic (TTI) media using a purely acoustic
                  formulation of the wave equation. The anisotropic
                  modeling kernel is used for the forward modeling
                  operator, as well as for the adjoint Jacobian to
                  back propagate the data residual, thus providing the
                  true gradient of the FWI objective function. We
                  apply this workflow on a synthetic FWI example and
                  perform RTM on a field data set. Furthermore, we
                  discuss alternatives for the pseudo-spectral
                  Laplacian operator of the current implementation.},
  keywords = {presentation, SINBAD, SINBADFALL2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/witte2015SINBADFtdf/witte2015SINBADFtdf.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/witte2015SINBADFtdf/witte2015SINBADFtdf.mov},
  author = {Philipp A. Witte and Felix J. Herrmann}
}


@PRESENTATION{yang2015SINBADFfis,
  title = {Fast imaging with surface-related multiples: – shallow water multiples},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {In this talk, we will show surface-related multiples
                  imaging with source estimation. Here the algorithms
                  are based on linearized bregman, and we give the
                  source estimation formulation in this case. With the
                  featured blocked structure of linearized bregman, we
                  can do efficient imaging in straightforward way by
                  randomly choosing sources and frequencies. Another
                  benefit is the simplicity to delivery the
                  algorithm.},
  keywords = {presentation, SINBAD, SINBADFALL2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/yang2015SINBADFfis/yang2015SINBADFfis.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Fall/yang2015SINBADFfis/yang2015SINBADFfis.mov},
  author = {Mengmeng Yang and Ning Tu and Felix J. Herrmann}
}


%----- 2015 (SPRING) -----%

@PRESENTATION{dasilva2015SINBADrii,
  title = {Recent improvements to the {2/3—D} imaging & inversion algorithms in the {SLIM} software release},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {The SLIM software release contains algorithms for
                  frequency domain 2D imaging and inversion based on
                  direct LU and QR factorizations. Recent improvments
                  include significant speedups, by utilizing the
                  latest parallel direct factorization algorithms and
                  inversion strategies which solve for each frequency
                  on its corresponding grid. The iterative Helmholtz
                  solvers available in the software release are now
                  also applicable to 2D problems, extending the 2D
                  algorithms to very large & high frequency 2D imaging
                  and inversion.},
  keywords = {presentation, SINBAD, SINBADSPRING2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Spring/dasilva2015SINBADrii/dasilva2015SINBADrii.pdf},
  author = {Curt Da Silva and Bas Peters and Felix J. Herrmann}
}


@PRESENTATION{dasilva2015SINBADogt,
  title = {Off the grid tensor completion for seismic data interpolation},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {In this work, we extend our previous low-rank tensor
                  techniques for seismic data interpolation to the
                  “off-the-grid” case, wherein the data is not
                  acquired on a regularly-spaced grid. This irregular
                  sampling scheme causes the underlying tensor to
                  become high-rank, thus preventing our previous
                  Hierarchical Tucker tensor algorithms from
                  successfully recovering the volume. Instead of
                  applying these techniques naively, we introduce a
                  fictitious, regular grid on which the volume
                  exhibits low-rank behaviour and a resampling
                  operator that interpolates the data from the
                  regular, low-rank domain to the irregular sampling
                  domain. By incorporating these changes in to our
                  previous framework, we can interpolate irregularly
                  sampled data volumes with costs determined primarily
                  by the choice of resampling transform. In our case,
                  we use a non-uniform Fast Fourier Transform, but
                  other choices are possible.},
  keywords = {presentation, SINBAD, SINBADSPRING2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Spring/dasilva2015SINBADogt/dasilva2015SINBADogt.pdf},
  author = {Curt Da Silva and Felix J. Herrmann}
}


@PRESENTATION{esser2015SINBADasd,
  title = {Automatic salt delineation — {Wavefield} {Reconstruction} {Inversion} with convex constraints},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {We extend full-waveform inversion by Wavefield
                  Reconstruction Inversion by including convex
                  constraints on the model. Contrary to the
                  conventional adjoint-state formulations, Wavefield
                  Reconstruction Inversion has the advantage that the
                  Gauss-Newton Hessian is well approximated by a
                  diagonal scaling, which allows us to add convex
                  constraints, such as the box- and the
                  edge-preserving total-variation constraint, on the
                  square slowness without incurring significant
                  increases in computational costs. As the examples
                  demonstrate, including these constraints yields far
                  superior results in complex geological areas that
                  contain high-velocity high-contrast bodies
                  (e.g. salt or basalt). Without these convex
                  constraints, adjoint-state and Wavefield
                  Reconstruction Inversion get trapped in local minima
                  for poor starting models.},
  keywords = {presentation, SINBAD, SINBADSPRING2015, SLIM},
  note = {posthumously presented by Felix J. Herrmann},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Spring/esser2015SINBADasd/esser2015SINBADasd.pdf},
  author = {Ernie Esser and Llu\'{i}s Guasch and Felix J. Herrmann}
}


@PRESENTATION{fang2015SINBADwri,
  title = {Wavefield reconstruction inversion with source estimation and its application to 2014 {Chevron} synthetic blind test dataset},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {We present a robust wavefield reconstruction inversion
                  with source estimation. The source wavelet is
                  estimated with the reconstruction of wavefield
                  simultaneously by solving an extended data
                  augmentation problem. We apply this method to the
                  2014 Chevron synthetic blind test dataset and show
                  the robustness of our method.},
  keywords = {presentation, SINBAD, SINBADSPRING2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Spring/fang2015SINBADwri/fang2015SINBADwri.pdf},
  author = {Zhilong Fang and Felix J. Herrmann}
}


@PRESENTATION{herrmann2015SINBADfom,
  title = {Fast “online” migration with Compressive Sensing},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {We present a novel adaptation of a recently developed
                  relatively simple iterative algorithm to solve
                  large-scale sparsity-promoting optimization
                  problems. Our algorithm is particularly suitable to
                  large-scale geophysical inversion problems, such as
                  sparse least-squares reverse-time migration or
                  Kirchoff migration since it allows for a tradeoff
                  between parallel computations, memory allocation,
                  and turnaround times, by working on subsets of the
                  data with different sizes. Comparison of the
                  proposed method for sparse least-squares imaging
                  shows a performance that rivals and even exceeds the
                  performance of state-of-the art one-norm solvers
                  that are able to carry out least-squares migration
                  at the cost of a single migration with all data.},
  keywords = {presentation, SINBAD, SINBADSPRING2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Spring/herrmann2015SINBADfom/herrmann2015SINBADfom.pdf},
  author = {Felix J. Herrmann}
}


@PRESENTATION{lopez2015SINBADdmc,
  title = {Deterministic matrix completion: applications to seismic trace interpolation},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {This talk discusses recent developments in the theory of
                  matrix completion that allow us to analyze how
                  successful a given sub sampling scheme will be for
                  seismic trace interpolation via nuclear norm
                  minimization. By adopting a graph theory
                  perspective, we utilize the spectral gap of our
                  sampling operator as a means to quantify our
                  acquisition design. This approach offers a heuristic
                  technique that is practical and with the potential
                  to achieve trace interpolation via rank penalization
                  in a deterministic manner.},
  keywords = {presentation, SINBAD, SINBADSPRING2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Spring/lopez2015SINBADdmc/lopez2015SINBADdmc.pdf},
  author = {Oscar Lopez and Felix J. Herrmann}
}


@PRESENTATION{oghenekohwo2015SINBADrdc,
  title = {Recent developments in compressive sensing for time-lapse studies},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {We present an overview of the advances we have made with
                  respect to extending compressive sensing ideas to
                  time-lapse studies. We show applications in
                  compressive time-lapse seismic data acquisition,
                  imaging and full-waveform inversion.},
  keywords = {presentation, SINBAD, SINBADSPRING2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Spring/oghenekohwo2015SINBADrdc/oghenekohwo2015SINBADrdc.pdf},
  author = {Felix Oghenekohwo and Rajiv Kumar and Haneet Wason and Ernie Esser and Ning Tu and Felix J. Herrmann}
}


@PRESENTATION{peters2015SINBADrwi,
  title = {Regularizing waveform inversion by projections onto convex sets},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {A framework is proposed for regularizing the waveform
                  inversion problem by projections onto intersections
                  of convex sets. Multiple pieces of prior information
                  about the geology are represented by multiple convex
                  sets, for example limits on the velocity or minimum
                  smoothness conditions on the model. The data-misfit
                  is then minimized, such that the estimated model is
                  always in the intersection of the convex
                  sets. Therefore, it is clear what properties the
                  estimated model will have at each iteration. This
                  approach does not require any quadratic penalties to
                  be used and thus avoids the known problems and
                  limitations of those types of penalties. It is shown
                  that by formulating waveform inversion as a
                  constrained problem, regularization ideas such as
                  Tikhonov regularization and gradient filtering can
                  be incorporated into one framework. The algorithm is
                  generally applicable, in the sense that it works
                  with any (differentiable) objective function and
                  does not require significant additional
                  computation. The method is demonstrated on the
                  inversion of the 2D marine isotropic elastic
                  synthetic seismic benchmark by Chevron using an
                  acoustic modeling code. To highlight the effect of
                  the projections, we apply no data pre-processing.},
  keywords = {presentation, SINBAD, SINBADSPRING2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Spring/peters2015SINBADrwi/peters2015SINBADrwi.pdf},
  author = {Bas Peters and Brendan R. Smithyman and Felix J. Herrmann}
}


@PRESENTATION{peters2015SINBADswr,
  title = {Solving {WRI’s} data-augmented wave equation in {3D}},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {The Wavefield Reconstruction Inversion (WRI) approach to
                  seismic inversion is a topic of active research in
                  the SLIM group. Several variants have been proposed,
                  all of which rely on the solution of a linear
                  least-squares problem with a Helmholtz
                  discretization. While algorithms based on LU or QR
                  factorization are very efficient for 2D problems, 3D
                  problems required the development of a
                  factorization-free iterative method. This talk
                  introduces the current version of the algorithm,
                  which requires significantly less memory and
                  computation then a few months ago and uses existing
                  Helmholtz solvers as an important building block.},
  keywords = {presentation, SINBAD, SINBADSPRING2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Spring/peters2015SINBADswr/peters2015SINBADswr.pdf},
  author = {Bas Peters and Chen Greif and Felix J. Herrmann}
}


@PRESENTATION{wason2015SINBADldr,
  title = {Latest developments in randomized marine acquisition and source separation},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2015},
  abstract = {We present the latest developments in randomized
                  (simultaneous) marine acquisition for time-lapse
                  surveys. We also show a comparison between two
                  source separation algorithms — sparsity-promotion
                  and rank-minimization — for simultaneous data.},
  keywords = {presentation, SINBAD, SINBADSPRING2015, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2015/Spring/wason2015SINBADldr/wason2015SINBADldr.pdf},
  author = {Haneet Wason and Rajiv Kumar and Felix Oghenekohwo and Felix J. Herrmann}
}


%----- 2014 (FALL) -----%

@PRESENTATION{dasilva2014SINBADlss,
  title = {Large-scale seismic data interpolation in a parallel computing environment},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {This talk will outline the details for a parallel
                  implementation of the SPG-LR algorithm for matrix
                  completion and its applications to large-scale
                  seismic data interpolation. Previous work done in
                  our group on LR-based matrix factorization has been
                  shown to be extremely efficient for seismic data
                  interpolation that fits in memory. We extend the
                  SPGL1 framework that this method is built on to
                  instances where the full seismic data volume cannot
                  entirely fit in the memory of one computational node
                  and must be run in a distributed environment. We
                  will also look at off-the-grid regularization for
                  tensor completion using a non-uniform FFT operator
                  and examine its effects on interpolating an
                  irregularly sampled, 3D seismic frequency slice.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/dasilva2014SINBADlss/dasilva2014SINBADlss.pdf},
  author = {Curt da Silva and Felix J. Herrmann}
}


@PRESENTATION{esser2014SINBADlcs,
  title = {A lifted $\ell_1$/$\ell_2$ constraint for sparse blind deconvolution},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {We propose a modification to a sparsity constraint based
                  on the ratio of l1 and l2 norms for solving blind
                  seismic deconvolution problems in which the data
                  consist of linear convolutions of different sparse
                  reflectivities with the same source wavelet. No
                  assumptions are made about the location of the
                  support of either the wavelet or the sparse
                  signals. Minimizing the ratio of l1 and l2 norms has
                  been previously shown to promote sparsity in a
                  variety of applications including blind
                  deconvolution. Most existing implementations are
                  heuristic or require smoothing the l1/l2
                  penalty. Lifted versions of l1/l2 constraints have
                  also been proposed but are still challenging to
                  implement. Inspired by the lifting approach, we
                  propose to split the sparse signals into positive
                  and negative components and apply an l1/l2
                  constraint to the difference, thereby obtaining a
                  constraint that is easy to implement without
                  smoothing the l1 or l2 norms. We show that a method
                  of multipliers implementation of the resulting model
                  can recover source wavelets that are not necessarily
                  minimum phase and approximately reconstruct the
                  sparse reflectivities. It appears to be robust to
                  the initialization and to small amounts of noise in
                  the data. We also discuss extensions to the
                  Estimation of Primaries by Sparse Inversion (EPSI)
                  convolution model.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/esser2014SINBADlcs/esser2014SINBADlcs.pdf},
  author = {Ernie Esser and Tim T.Y. Lin and Rongrong Wang and Felix J. Herrmann}
}


@PRESENTATION{esser2014SINBADtvc,
  title = {Total variation constrained full waveform inversion with continuation},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {Adding a total variation constraint to the Wavefield
                  Reconstruction Inversion model proposed by van
                  Leeuwen and Herrmann can improve robustness and
                  remove artifacts by encouraging piecewise smooth
                  solutions. However, if the parameter controlling the
                  strength of the TV regularization is not well
                  chosen, important details can be lost. We
                  investigate continuation strategies that gradually
                  reduce the strength of the TV constraint so that
                  these important details are eventually allowed back
                  into the solution. Preliminary numerical experiments
                  suggest that this can improve the solution path and
                  lead to better velocity models than if TV
                  regularization were not used during the intermediate
                  steps.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/esser2014SINBADtvc/esser2014SINBADtvc.pdf},
  author = {Ernie Esser and Llu\'{i}s Guasch and Tristan van Leeuwen and Aleksandr Y. Aravkin and Felix J. Herrmann}
}


@PRESENTATION{fang2014SINBADsew,
  title = {Source estimation for {WRI} and its application},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/fang2014SINBADsew/fang2014SINBADsew.pdf},
  author = {Zhilong Fang and Xiang Li and Bas Peters and Felix J. Herrmann}
}


@PRESENTATION{fang2014SINBADuqw,
  title = {Uncertainty quantification for {Wavefield} {Reconstruction} {Inversion}},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {This talk discusses the uncertainty quantification for
                  the wavefield reconstruction inversion. The Hessian
                  and gradient of the WRI can be easily obtained once
                  the wave field is generated. As a result, we use a
                  Newton type McMC method to sample the posterior
                  distribution. Statistical property such as mean,
                  standard deviation, confidence interval are
                  calculated based on the sampled distribution.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/fang2014SINBADuqw/fang2014SINBADuqw.pdf},
  author = {Zhilong Fang and Chia Ying Lee and Curt Da Silva and Felix J. Herrmann and Rachel Kuske}
}


@PRESENTATION{ghadermarzy2014SINBADwrr,
  title = {Wavefield reconstruction via randomized sampling},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {Regular sampling along the time axis is a common
                  approach for seismic imaging. However, seismic data
                  are often spatially undersampled due to economical
                  reasons as well as ground surface
                  limitations. Operational conditions might result in
                  noisy traces or even gaps in coverage and irregular
                  sampling which often leads to image
                  artifacts. Seismic trace interpolation aims to
                  interpolate missing traces in an otherwise regularly
                  sampled seismic data to a complete regular grid. In
                  this talk, we explain some techniques, with low
                  computational complexity, that can be used to
                  improve the results. In particular, we talk about
                  weighting, switching to midpoint-offset domain from
                  frequency-space domain, and uniform versus jittered
                  undersampling. We illustrate the advantages of these
                  modifications using a real seismic line from the
                  Gulf of Suez.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/ghadermarzy2014SINBADwrr/ghadermarzy2014SINBADwrr.pdf},
  author = {Navid Ghadermarzy and Ozgur Yilmaz and Felix J. Herrmann}
}


@PRESENTATION{herrmann2014SINBADldm2,
  title = {Latest developments in randomized ({4D}) acquisition},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {We will present the latest developments in randomized
                  (4D) seismic data acquisition.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/herrmann2014SINBADldm2/herrmann2014SINBADldm2.pdf},
  author = {Felix J. Herrmann}
}


@PRESENTATION{herrmann2014SINBADwri,
  title = {Wavefield {Reconstruction} {Inversion} ({WRI}) - a new take on wave-equation based inversion},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {We discuss a recently proposed novel method for waveform
                  inversion: Wavefield Reconstruction Inversion
                  (WRI). As opposed to conventional FWI --- which
                  attempts to minimize the error between observed and
                  predicted data obtained by solving a wave equation
                  --- WRI reconstructs a wave-field from the data and
                  extracts a model-update from this wavefield by
                  minimizing the wave-equation residual. The method
                  does not require explicit computation of an adjoint
                  wavefield as all the necessary information is
                  contained in the reconstructed wavefield. We show
                  how the corresponding model updates can be
                  interpreted physically analogously to the
                  conventional imaging-condition-based approach.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/herrmann2014SINBADwri/herrmann2014SINBADwri.pdf},
  author = {Felix J. Herrmann}
}


@PRESENTATION{kumar2014SINBADeia,
  title = {Extended images in action --- image gather with surface-related multiples},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {Common image gathers are used in building velocity
                  models, invert anisotropy parameters and to analyze
                  reservoir attributes. Often primary reflections are
                  used to form the image gathers and multiples are
                  typically attenuated in processing. However,
                  researchers have shown that, if correctly used, they
                  can provide useful information about the subsurface
                  during reverse-time migration. In this work, i will
                  show how we can use multiples along with primaries
                  to form the image-gathers.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/kumar2014SINBADeia/kumar2014SINBADeia.pdf},
  author = {Rajiv Kumar and Ning Tu and Tristan van Leeuwen and Felix J. Herrmann}
}


@PRESENTATION{kumar2014SINBADtjm,
  title = {Time-jittered marine acquisition: low-rank v/s sparsity},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {In this work we will show how simultaneous or blended
                  acquisition can be setup as a rank-minimization
                  (deblending) problem. The simultaneous acquisition
                  scenario is a pragmatic time-jittered marine
                  acquisition scheme, where a source vessel (with two
                  airgun arrays) sails across an ocean-bottom array
                  firing at jittered source locations and instances in
                  time, resulting in better spatial sampling and
                  acquisition speedup. We make comparisons with
                  sparsity-promoting based techniques and demonstrate
                  that rank-minimization based debelending techniques
                  are computationally faster and memory
                  efficient. This is joint work with Haneet Wason.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/kumar2014SINBADtjm/kumar2014SINBADtjm.pdf},
  author = {Rajiv Kumar and Haneet Wason and Felix J. Herrmann}
}


@PRESENTATION{lago2014SINBADmri,
  title = {Minimal-residual iterative methods for time-harmonic wave-equation},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {In this work we aim at comparing the performance of some
                  iterative methods for solving the time-harmonic
                  acoustic wave-equation, one of the most challenging
                  problems in the numerical linear algebra
                  community. Widely used in frequency domain full
                  waveform inversion, the discrete wave-equation
                  yields very large, complex, sparse, ill-conditioned
                  linear systems. In this extended abstract we aim at
                  further comparing our recently proposed method,
                  CRMN, against CGMN and another method renown for
                  being very memory efficient, the two-level
                  preconditioner with shifted Laplacian operator. We
                  propose several improvements for the two-level
                  preconditioner by combining this method with CRMN in
                  several senses.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/lago2014SINBADmri/lago2014SINBADmri.pdf},
  author = {Rafael Lago and Felix J. Herrmann}
}


@PRESENTATION{li2014SINBADtsi,
  title = {2D/{3D} time-stepping for imaging and inversion},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {In this talk, we present a SLIM version of 2D/3D
                  time-stepping modeling kernel for inversion and
                  imaging. One key aspect of this framework is that
                  the implementation follows some mathematic
                  principles, such as linear operator test. Aside from
                  obeying mathematic principles, we also developed a
                  strategy that allows us to compute the gradient of
                  conventional FWI objective or RTM imaging without
                  storing extra wavefield. We also show some
                  applications of this framework on the Chevron 2014
                  benchmark elastic synthetic data set.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/li2014SINBADtsi/li2014SINBADtsi.pdf},
  author = {Xiang Li and Felix J. Herrmann}
}


@PRESENTATION{li2014SINBADwmg,
  title = {Why the modified {Gauss-Newton} method?},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {In our earlier work, we develop the modified
                  Gauss-Newton method for FWI, which requires each
                  update of FWI to be sparse in the curvelet
                  domain. Our empirical observation of the MGN method
                  is that it can find us a solution for FWI problem
                  with sparse perturbation of the initial guess
                  without changing the underlying objective. In this
                  talk, we will analyze the MGN method to find out the
                  reason why it can generate a sparse perturbation of
                  the initial model, because sum of spares updates
                  could easily generate non-sparse
                  perturbations. Moreover, we will illustrate when do
                  we expect the modified Gauss-Newton method to yield
                  a solution with sparse perturbation and in what
                  circumstances should we use it in place of other
                  algorithms like standard Gauss-Newton.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/li2014SINBADwmg/li2014SINBADwmg.pdf},
  author = {Xiang Li and Ernie Esser and Felix J. Herrmann}
}


@PRESENTATION{lin2014SINBADdag,
  title = {Dealing with acquisition gaps in {Robust} {EPSI} without interpolation using wavefield auto-convolution},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {Straightforward modifications to the Estimation of
                  Primaries by Sparse Inversion (EPSI) problem allows
                  us to mitigate acquisition holes without any
                  reconstruction of the missing traces, neither prior
                  to nor during the inversion process. This is
                  achieved by simulating the missing multiple
                  contributions with terms involving auto-convolutions
                  of the primary wavefield. In this formulation we no
                  longer need to treat the missing data as another
                  unknown in the inversion process, which is important
                  in eliminating a significant source of possible
                  local minima from attempting to invent data using
                  incorrect primary and multiple models. In this talk
                  we investigate the necessary modifications to the
                  Robust EPSI problem, as well as algorithms that
                  account for the resulting non-linear modelling
                  operator. We will also investigate the
                  reconstruction limits of this approach in relation
                  to acquisition geometry and subsurface
                  characteristics.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/lin2014SINBADdag/lin2014SINBADdag.pdf},
  author = {Tim T.Y. Lin and Felix J. Herrmann}
}


@PRESENTATION{lopez2014SINBADogm,
  title = {Off-the-grid matrix completion},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {This talk discusses a modified rank minimization
                  workflow designed to handle unstructured matrix
                  completion problems. We extend current rank
                  minimization based trace interpolation techniques to
                  optimally handle undersampled irregular data, in
                  such a way that allows us to recover regularized
                  densely sampled data. This is achieved by
                  introducing a regularization operator, that
                  accurately allows us to simulate the effect of an
                  unstructured grid. We provide reconstruction error
                  bounds to demonstrate the potential of our
                  procedure.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/lopez2014SINBADogm/lopez2014SINBADogm.pdf},
  author = {Oscar Lopez and Rajiv Kumar and Ernie Esser and Felix J. Herrmann}
}


@PRESENTATION{lopez2014SINBADrma,
  title = {Rank minimization via alternating optimization},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {This talk discusses the benefits of an alternating
                  optimization approach for seismic trace
                  interpolation via rank minimization. We consider
                  recent factorization approaches to rank minimization
                  problems where we write our low rank matrix in
                  bi-linear form, and modify this workflow by
                  alternating our optimization to handle a single
                  matrix factor at a time. This allows for a more
                  tractable procedure that can better handle highly
                  sub sampled data sets without increasing the time
                  complexity. We demonstrate the potential of this
                  approach with several seismic trace interpolation
                  experiments.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/lopez2014SINBADrma/lopez2014SINBADrma.pdf},
  author = {Oscar Lopez and Rajiv Kumar and Felix J. Herrmann}
}


@PRESENTATION{oghenekohwo2014SINBADrst,
  title = {Randomized subsampling in time-lapse surveys and recovery techniques},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {This talk centres on new insights on how we think of
                  acquisition of time-lapse data using ideas from
                  compressed sensing. Specifically, we will show how
                  simultaneous processing of time-lapse data from
                  randomized subsampling gives more reliable estimates
                  of the vintages and time-lapse signal, compared to
                  parallel processing.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/oghenekohwo2014SINBADrst/oghenekohwo2014SINBADrst.pdf},
  author = {Felix Oghenekohwo and Rajiv Kumar and Haneet Wason and Ernie Esser and Felix J. Herrmann}
}


@PRESENTATION{oghenekohwo2014SINBADuwc,
  title = {Use what`s in common: time-lapse {FWI} with distributed {Compressive} {Sensing}},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {In this talk, we will extend our joint recovery
                  formulation to FWI in time-lapse seismic, using the
                  modified Gauss-Newton approach. Leveraging ideas
                  from distributed compressive sensing, we exploit
                  shared information in the sparse model perturbations
                  of the baseline and monitor data.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/oghenekohwo2014SINBADuwc/oghenekohwo2014SINBADuwc.pdf},
  author = {Felix Oghenekohwo and Rajiv Kumar and Ernie Esser and Felix J. Herrmann}
}


@PRESENTATION{peters2014SINBADcda,
  title = {Challenges and developments arising from {2D} {FWI} of a land {VSP} dataset in the {Permian} {Basin}},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {Full-waveform inversion was carried out to map 3D
                  velocity structures for a site in the Permian Basin
                  of Texas, USA. The data come from co-located surface
                  and 3D VSP seismic surveys that share common source
                  vibration points. This challenging on-land setting
                  has been used as a testbed to evaluate our FWI
                  algorithms and their effectiveness when applied to
                  real-data problems. We discuss the challenges posed
                  by this case study, and several new techniques and
                  methodological refinements that have been developed
                  and tested as a result. These include:
                  regularization and incorporation of multiple
                  datasets in an on-land FWI workflow; restrictions on
                  the types of models allowed (viz., smoothness
                  regularization) by quadratic penalty and by
                  Projected Quasi-Newton (PQN) inversion; corrections
                  to enable 2D FWI and recovery of 3D models; the
                  state of ongoing work in WRI (``Penalty Method''
                  inversion) and the handling of anisotropy.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/peters2014SINBADcda/peters2014SINBADcda.pdf},
  author = {Brendan R. Smithyman and Bas Peters and Felix J. Herrmann}
}


@PRESENTATION{peters2014SINBADiss,
  title = {Iterative solution strategy for least-squares problem with a {PDE}-block},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {Waveform inversion using the Wavefield Reconstruction
                  Inversion (WRI) is a quadratic-penalty based method
                  developed in our group and has shown to have
                  advantages over the reduced Lagrangian based
                  conventional FWI methods. WRI involves no explicit
                  PDE solves, but a least-squares problem with a PDE
                  block. This type of least-squares problem is very
                  challenging to solve using iterative methods. In
                  this talk we show algorithms we developed
                  specifically for this application and which involve
                  a combination of preconditioning, low-rank
                  decomposition and deflation.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/peters2014SINBADiss/peters2014SINBADiss.pdf},
  author = {Bas Peters and Felix J. Herrmann}
}


@PRESENTATION{peters2014SINBADqpb,
  title = {Quadratic-penalty based full-space methods for waveform inversion},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {Waveform inversion problems are commonly solved by
                  eliminating the field variables. Full-space methods
                  store and update the fields, rather than solving for
                  them. This leads to a problem with nice properties,
                  such as free function value evaluation and exact
                  gradients and Hessians at no cost. The existing
                  literature is almost exclusively based on the
                  Lagrangian formulation. We propose to work with a
                  quadratic penalty formulation, which allows us to
                  reduce the storage requirements and gives the Newton
                  system a favourable structure which we exploit to
                  obtain a intrinsically parallel updating scheme for
                  the fields and medium parameters. We will show a
                  comparison with reduced-space methods.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/peters2014SINBADqpb/peters2014SINBADqpb.pdf},
  author = {Bas Peters and Felix J. Herrmann}
}


@PRESENTATION{peters2014SINBADsmp,
  title = {Single- and multi-parameter WRI --- synthetic examples},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {We demonstrate the performance of WRI on a number of
                  single- and multi-parameter synthetic examples. The
                  examples shot that WRI is less reliant on accurate
                  starting models while this formulation is also
                  conducive to multi-parameter inversion.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/peters2014SINBADsmp/peters2014SINBADsmp.pdf},
  author = {Bas Peters and Felix J. Herrmann}
}


@PRESENTATION{tu2014SINBADfis,
  title = {Fast imaging with surface-related multiples by sparse inversion},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {When used correctly, surface-related multiples can
                  provide extra illumination coverage compared with
                  primaries. In this talk, I will discuss how to
                  jointly image primaries and surface-related
                  multiples in a computationally efficient fashion. We
                  bring down the computational cost by two
                  means. First we use wave-equation solvers to
                  implicitly carry out the expensive dense
                  matrix-matrix multiplications in the prediction of
                  surface-related multiples. Second we bring down the
                  simulation cost by subsampling the frequencies and
                  monochromatic source experiments together with
                  curvelet-domain sparsity-promoting and
                  rerandomization. As a result, we obtain
                  true-amplitude least-squares migrated seismic images
                  with computational costs that are comparable to a
                  single RTM with all the data. We demonstrate the
                  efficacy of the proposed method using realistic
                  synthetic examples.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/tu2014SINBADfis/tu2014SINBADfis.pdf},
  author = {Ning Tu and Felix J. Herrmann}
}


@PRESENTATION{tu2014SINBADfis2,
  title = {Fast imaging with source estimation},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {In seismic imaging, an inaccurate estimate of the source
                  wavelet may result in degraded seismic images. While
                  conventional reverse-time migration requires the
                  knowledge of the source wavelet as prior
                  information, we propose to include source estimation
                  in the fast compressive imaging procedure. Using the
                  proposed method, we can obtain seismic images that
                  are comparable to those imaged with the true source
                  wavelet, with computational costs that are
                  comparable to conventional RTM images with all the
                  data. We also extend the proposed method to image
                  data with surface-related multiples, where these
                  multiples help to mitigate the amplitude ambiguity
                  in source estimation. We verify the proposed method
                  using realistic synthetic examples.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/tu2014SINBADfis2/tu2014SINBADfis2.pdf},
  author = {Ning Tu and Aleksandr Y. Aravkin and Tristan van Leeuwen and Tim T.Y. Lin and Felix J. Herrmann}
}


@PRESENTATION{tu2014SINBADind,
  title = {Imaging the {Nelson} data set using surface-related multiples},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {We will present out latest finding on migration with
                  multiples on the Nelson dataset.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/tu2014SINBADind/tu2014SINBADind.pdf},
  author = {Ning Tu and Felix J. Herrmann}
}


@PRESENTATION{tu2014SINBADsis,
  title = {Sparse inversion simplified},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {Implementation of the SPGl1 solver in low-level computer
                  languages such as C or Fortran may sound daunting to
                  many. This talk is tailored for you if you would
                  like to reap benefits from using sparse constraints
                  to regularize your inversion problem, and at the
                  same time want some easy-to-implement algorithm to
                  solve it. We discuss in this talk the linearized
                  Bregman projection method that embraces both
                  ingredients, and show promising applications in fast
                  least-squares imaging and seismic data interpolation
                  problems.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/tu2014SINBADsis/tu2014SINBADsis.pdf},
  author = {Ning Tu and Navid Ghadermarzy and Rajiv Kumar and Felix J. Herrmann}
}


@PRESENTATION{wang2014SINBADdwi,
  title = {Denoising the wavefield inversion problem through source blending and penalty method},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {Previous FWI denoising techniques, such as source
                  blending and stacking, are designed applied to the
                  seismic traces. Hence no physical information is
                  used to improve the fidelity of the reconstruction,
                  even when a good initial model is
                  available. Moreover, traditional source blending
                  requires the existence of common source gathers,
                  which might be missing from the data. In this talk,
                  we incorporate the source blending idea into the
                  penalty method, and extract the synthetic sources
                  corresponding to the principle directions of the
                  gradient update. By blending the sources, we can not
                  only achieve an acceleration and storage efficiency
                  of the penalty method, but also make the
                  reconstruction more robust to the random noise. This
                  method is applicable to any source receiver position
                  but is more efficient if common source gathers are
                  available.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/wang2014SINBADdwi/wang2014SINBADdwi.pdf},
  author = {Rongrong Wang and Felix J. Herrmann}
}


@PRESENTATION{wason2014SINBADrrt,
  title = {Randomization and repeatability in time-lapse marine acquisition},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {During this talk, we will show how randomization
                  techniques from (distributed) compressed sensing
                  affect the way we think about time-lapse surveys.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/wason2014SINBADrrt/wason2014SINBADrrt.pdf},
  author = {Haneet Wason and Felix Oghenekohwo and Felix J. Herrmann}
}


@PRESENTATION{wason2014SINBADsss,
  title = {Source separation via {SVD}-free rank-minimization in the hierarchical semi-separable representation},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {During this talk, we will show a source separation
                  algorithm for blended marine acquisition, where two
                  sources are deployed at different depths (over/under
                  acquisition). The separation method incorporates the
                  Hierarchical Semi-Separable (HSS) structure inside
                  rank-regularized least-squares formulations. We also
                  compare this deblending scheme with the
                  sparsity-promoting one-norm minimization scheme.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/wason2014SINBADsss/wason2014SINBADsss.pdf},
  author = {Haneet Wason and Rajiv Kumar and Felix J. Herrmann}
}


@PRESENTATION{yilmaz2014SINBADadc,
  title = {Analog-to-digital conversion in compressive sampling},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {Compressive sampling theory is now established as an
                  effective method for dimension reduction when the
                  underlying signals (e.g., seismic data) are sparse
                  with respect to some suitable basis or frame (e.g.,
                  curvelets in the case of seismic). One important
                  problem directly related to the acquisition of
                  analog signals is how to perform analog-to-digital
                  conversion. This is directly related to two
                  important issues: (i) how accurately one can acquire
                  a signal using compressive sampling (in terms of bit
                  depth), and (ii) how ``compressed'' is compressive
                  sampling (in terms of the total number of bits one
                  ends up using after acquiring the signal). We will
                  present recent results that provide answers to these
                  questions. Specifically, we provide an
                  analog-to-digital conversion method that achieves
                  nearly optimal compression using cheap analog
                  devices with a large margin for imperfections.},
  keywords = {presentation, SINBAD, SINBADFALL2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Fall/yilmaz2014SINBADadc/yilmaz2014SINBADadc.pdf},
  author = {Ozgur Yilmaz}
}


%----- 2014 (SPRING) -----%

@PRESENTATION{dasilva2014SINBADlrp,
  title = {Low-rank promoting transformations and tensor interpolation - applications to seismic data denoising},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {In this presentation, we extend our previous work in
                  Hierarchical Tucker (HT) tensor completion, which
                  uses an extremely efficient representation for
                  representing high-dimensional tensors exhibiting
                  low-rank structure, to handle subsampled tensors
                  with noisy entries. We consider a `low-noise' case,
                  so that the energies of the noise and the signal are
                  nearly indistinguishable, and a `high-noise' case,
                  in which the noise energy is now scaled to the
                  amplitude of the entire data volume. For the
                  low-noise case in particular, standard
                  trace-by-trace energy comparisons cannot distinguish
                  noise from signal. We examine the behaviour of noise
                  in terms of the singular values along different
                  matricizations of the data, i.e. reshaping of the
                  tensor along different dimensions. By interpreting
                  this effect in the context of tensor completion, we
                  demonstrate the inefficacy of denoising by this
                  method in the source-receiver domain. In light of
                  this observation, we transform the decimated, noisy
                  data in to the midpoint-offset domain, which
                  promotes low-rank behaviour in the signal and
                  high-rank behaviour in the noise. This distinction
                  between signal and noise allows low-rank
                  interpolation to effectively denoise the signal,
                  without knowledge of the noise location, with only a
                  marginal increase in computational cost. We
                  demonstrate the effectiveness of this approach on a
                  4D frequency slice.},
  keywords = {presentation, SINBAD, SINBADSPRING2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Spring/dasilva2014SINBADlrp/dasilva2014SINBADlrp.pdf},
  author = {Curt da Silva and Felix J. Herrmann}
}


@PRESENTATION{esser2014SINBADsgp,
  title = {A scaled gradient projection method for total variation regularized full waveform inversion},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {We propose a modification to the quadratic penalty
                  formulation for seismic full waveform inversion
                  proposed by van Leeuwen and Herrmann that includes
                  convex constraints on the model. In particular, we
                  show how to simultaneously constrain the total
                  variation of the slowness squared while enforcing
                  bound constraints to keep it within a physically
                  realistic range. Synthetic experiments show that
                  including total variation regularization can improve
                  the recovery of a high velocity perturbation to a
                  smooth background model.},
  keywords = {presentation, SINBAD, SINBADSPRING2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Spring/esser2014SINBADsgp/esser2014SINBADsgp.pdf},
  author = {Ernie Esser and Tristan van Leeuwen and Aleksandr Y. Aravkin and Felix J. Herrmann}
}


@PRESENTATION{fang2014SINBADsqn,
  title = {A stochastic quasi-{Newton} {McMC} method for uncertainty quantification of full-waveform inversion},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {In this work, we present a fast McMC method using the
                  stochastic l-BFGS Hessian to quantify the
                  uncertainty of full-waveform inversion. Using the
                  stochastic l-BFGS Hessian, we do not need the
                  assumption that the Hessian of data misfit is low
                  rank and we also reduce the computational cost of
                  estimating the Hessian. Numerical result shows the
                  capability of this fast McMC method.},
  keywords = {presentation, SINBAD, SINBADSPRING2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Spring/fang2014SINBADsqn/fang2014SINBADsqn.pdf},
  author = {Zhilong Fang and Chia Ying Lee and Felix J. Herrmann}
}


@PRESENTATION{herrmann2014SINBADdns,
  title = {{DNOISE III} – the next step},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {During this presentation I will discuss our plans for
                  the NSERC Collaborative Research and Development
                  Grant DNOISE III. With this grant, we aim to match
                  the industry contributions of SINBAD
                  dollar-for-dollar.},
  keywords = {presentation, SINBAD, SINBADSPRING2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Spring/herrmann2014SINBADdns/herrmann2014SINBADdns.pdf},
  author = {Felix J. Herrmann}
}


@PRESENTATION{herrmann2014SINBADldm,
  title = {Latest developments in marine ({4D}) acquisition},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {During this talk, we will show the advantages of
                  randomly dithered marine acquisition with ocean
                  bottom nodes and how randomization techniques from
                  compressive sensing affect the way we think about
                  time-lapse surveys. This is joint work with Felix
                  and Haneet.},
  keywords = {presentation, SINBAD, SINBADSPRING2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Spring/herrmann2014SINBADldm/herrmann2014SINBADldm.pdf},
  author = {Felix J. Herrmann}
}


@PRESENTATION{herrmann2014SINBADrpe,
  title = {Relax the physics & expand the search space – {FWI} via {Wavefield} {Reconstruction} {Inversion}},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {During this talk, we will present a new formulation of
                  full-waveform inversion that combines the best of
                  full-space constrained methods and reduced-space
                  unconstrained methods. Instead of eliminating the
                  constraint, which leads to the reduced adjoint-state
                  method that underpins most formulations of
                  full-waveform inversion, our method relaxes the PDE
                  constraint by replacing it by an additive
                  (least-squares) penalty term. By using the method of
                  variable projection, we arrive at a formulation that
                  alternates between solving for the wavefield, given
                  the velocity model & data, and solving for
                  velocity-model updates, given the wavefields. We
                  named this method Wavefield Reconstruction Inversion
                  (WRI) because it inverts for the model updates by
                  reconstructing the wavefield everywhere given data
                  observed at the receivers and the physics of the
                  wave equation. During the talk, we present this new
                  method and discuss how the increased search space,
                  now consisting of the wavefields and model,
                  mitigates the effects of local minima. We will also
                  discuss recent extensions including multi-parameter
                  inversion and regularization. This is joint work
                  with Tristan van Leeuwen, Bas Peters, and Ernie
                  Esser.},
  keywords = {presentation, SINBAD, SINBADSPRING2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Spring/herrmann2014SINBADrpe/herrmann2014SINBADrpe.pdf},
  author = {Felix J. Herrmann}
}


@PRESENTATION{kumar2014SINBADsfm,
  title = {{SVD}-free matrix completion for seismic data reconstruction},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {Seismic data interpolation via rank-minimization
                  techniques has been recently introduced in the
                  seismic community. All the existing
                  rank-minimization techniques assumes the underlying
                  grid to be regular. Irregularity is one of the
                  common impediments in acquisition. In this work, we
                  studied the effect of irregularity on structured and
                  show that how we can modify the existing techniques
                  to handle it. Other then irregularity, we often have
                  missing data. We also show that we can tackle both
                  regularization and interpolation issue
                  simultaneously. The objective of this work is to
                  extend our existing method of interpolation on
                  structured grid to unstructured grid. We illustrate
                  the advantages of the modification in existing
                  methodology using a seismic line from Gulf of Suez
                  to obtain high quality results for regularization
                  and interpolation, a key application in exploration
                  geophysics.},
  keywords = {presentation, SINBAD, SINBADSPRING2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Spring/kumar2014SINBADsfm/kumar2014SINBADsfm.pdf},
  author = {Rajiv Kumar and Oscar Lopez and Ernie Esser and Felix J. Herrmann}
}


@PRESENTATION{lago2014SINBADhfw,
  title = {Heuristics in full-waveform inversion},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {For many full-waveform inversion techniques, the most
                  computationally intensive step is the computation of
                  a numerical solution for the wave equation on every
                  iteration. In the frequency domain approach, this
                  requires the solution of very large, complex,
                  sparse, ill-conditioned linear systems. In this
                  abstract we bring out attention specifically to CGMN
                  method for solving PDEs, known for being flexible
                  (i.e. it is able to treat equally acoustic data as
                  well as visco-elastic or more complex scenarios)
                  efficient with respect both to memory and
                  computation time, and controllable accuracy of the
                  final approximation. We propose an improvement for
                  the known CGMN method by imposing a minimal residual
                  condition, which incurs in one extra model vector
                  storage. The resulting algorithm called CRMN enjoys
                  several interesting properties as monotonically
                  nonincreasing behaviour of the norm of the residual
                  and minimal residual, guaranteeing optimal
                  convergence for the relative residual criterion. We
                  discuss numerical experiments both in an isolated
                  PDE solve and also within the inversion procedure,
                  showing that in a realistic scenario we can expect a
                  speedup around 25\% when using CRMN rather than
                  CGMN. Joint work with Art Petrenko, Zhilong Fang,
                  Felix J. Herrmann.},
  keywords = {presentation, SINBAD, SINBADSPRING2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Spring/lago2014SINBADhfw/lago2014SINBADhfw.pdf},
  author = {Rafael Lago and Felix J. Herrmann}
}


@PRESENTATION{lin2014SINBADiit,
  title = {Implicit interpolation of trace gaps in {REPSI} using auto-convolution terms},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {It is possible to solve the Estimation of Primaries by
                  Sparse Inversion problem from a sesimic record with
                  large holes without any explicit data
                  reconstruction, by instead simulating the missing
                  multiple contributions with terms involving
                  auto-convolutions of the primary
                  wavefield. Exclusion of the unknown data as an
                  inversion variable from the REPSI process is
                  desireable, since it eliminates a significant source
                  of local minima that arises from attempting to
                  invert for the unobserved traces using primary and
                  multiple models that may be far-away from the true
                  solution. In this talk we investigate the necessary
                  modifications to the REPSI algorithm to account for
                  the resulting non-linear modeling operator, and
                  demonstrate that just a few auto-convolution terms
                  are enough to satisfactorily mitigate the effects of
                  data gaps during the inversion process.},
  keywords = {presentation, SINBAD, SINBADSPRING2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Spring/lin2014SINBADiit/lin2014SINBADiit.pdf},
  author = {Tim T.Y. Lin and Felix J. Herrmann}
}


@PRESENTATION{lin2014SINBADmas,
  title = {Multilevel acceleration strategy for {REPSI}},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {This talk discusses a multilevel inversion strategy that
                  aims to substantially reduce the computational costs
                  of the Robust Estimation of Primaries by Sparse
                  Inversion algorithm. The proposed method solves
                  early iterations of REPSI at very coarse spatial
                  sampling grids while gradually ramping-up the
                  spatial sampling when more accuracy is desired. No
                  changes to the the core implementation of the
                  original algorithm are necessary while in addition
                  only requiring trace decimation, low-pass filtering,
                  and rudimentary interpolation techniques.},
  keywords = {presentation, SINBAD, SINBADSPRING2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Spring/lin2014SINBADmas/lin2014SINBADmas.pdf},
  author = {Tim T.Y. Lin and Felix J. Herrmann}
}


@PRESENTATION{oghenekohwo2014SINBADrsw,
  title = {Randomized sampling without repetition in time-lapse seismic surveys},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {In this talk, we will show a method for acquiring
                  time-lapse data, where we do not have to repeat the
                  survey geometry. Our method works provided our
                  acquisition is randomized, where sources and
                  receivers are at random locations on a computational
                  grid, and provided we know the spatial locations of
                  the shots and receivers. In addition, we show the
                  implications of either (sub)sampling more for the
                  baseline and less for the monitor or vice-versa, and
                  how this sampling scheme affects the 4-D signal.},
  keywords = {presentation, SINBAD, SINBADSPRING2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Spring/oghenekohwo2014SINBADrsw/oghenekohwo2014SINBADrsw.pdf},
  author = {Felix Oghenekohwo and Haneet Wason and Ernie Esser and Felix J. Herrmann}
}


@PRESENTATION{peters2014SINBADmpw,
  title = {Multi-parameter waveform inversion; exploiting the structure of penalty-methods},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {In this talk I consider the problem of inverting
                  waveforms for multiple medium parameters. The
                  governing PDE is chosen to be the Helmholtz equation
                  with compressibility and buoyancy as the
                  unknowns. Both unknowns occur in the same equation
                  and practice has shown it is very hard to estimate
                  both equally accurate; the buoyancy estimate (or
                  density if a slightly different parametrization is
                  used) is typically much smoother than the
                  compressibility (or velocity). Here I introduce a
                  new waveform inversion algorithm: a full Newton-type
                  method based on a penalty method which adds the PDE
                  constraint as a quadratic penalty term. This method
                  updates both the 'wavefields' and medium parameters,
                  without explicitly solving PDE's. One of the main
                  advantages is the availability of a sparse Hessian
                  and exact gradient which are not the result of any
                  PDE solves. We asses if the availability of the
                  Hessian, which includes information about the
                  coupling between the two medium parameters, can help
                  reconstruct both compressibility and buoyancy.},
  keywords = {presentation, SINBAD, SINBADSPRING2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Spring/peters2014SINBADmpw/peters2014SINBADmpw.pdf},
  author = {Bas Peters and Felix J. Herrmann}
}


@PRESENTATION{petrenko2014SINBADaih,
  title = {Accelerating an iterative {Helmholtz} solver with {FPGAs}},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {An implementation of seismic wave simulation on a
                  platform consisting of a conventional host processor
                  and a reconfigurable hardware accelerator is
                  presented. This research is important in the field
                  of exploration for oil and gas resources, where a 3D
                  model of the subsurface is frequently required. By
                  comparing seismic data collected in a real-world
                  survey with synthetic data generated by simulated
                  waves, it is possible to deduce such a
                  model. However this requires many time-consuming
                  simulations with different Earth models to find the
                  one that best fits the measured data. Speeding up
                  the wave simulations would allow more models to be
                  tried, yielding a more accurate estimate of the
                  subsurface. The reconfigurable hardware accelerator
                  employed in this work is a field programmable gate
                  array (FPGA). FPGAs are computer chips that consist
                  of electronic building blocks that the user can
                  configure and reconfigure to represent their
                  algorithm in hardware. Whereas a traditional
                  processor can be viewed as a pipeline for processing
                  instructions, an FPGA is a pipeline for processing
                  data. The chief advantage of the FPGA is that all
                  the instructions in the algorithm are already
                  hardwired onto the chip. This means that execution
                  time depends only on the amount of data to be
                  processed, and not on the complexity of the
                  algorithm. The main contribution is an
                  implementation of the well-known Kaczmarz row
                  projection algorithm on the FPGA, using techniques
                  of dataflow programming. This kernel is used as the
                  preconditioning step of CGMN, a modified version of
                  the conjugate gradients method that is used to solve
                  the time-harmonic acoustic isotropic constant
                  density wave equation. Using one FPGA accelerator,
                  the current implementation allows seismic wave
                  simulations to be performed over twice as fast,
                  compared to running on one Intel Xeon E5-2670
                  core. I also discuss the effect of modifications of
                  the algorithm necessitated by the hardware on the
                  convergence properties of CGMN. Finally, a specific
                  plan for future work is set-out in order to fully
                  exploit the accelerator platform, and my work is set
                  in its larger context},
  keywords = {presentation, SINBAD, SINBADSPRING2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Spring/petrenko2014SINBADaih/petrenko2014SINBADaih.pdf},
  author = {Art Petrenko and Tristan van Leeuwen and Diego Oriato and Simon Tilbury and Felix J. Herrmann}
}


@PRESENTATION{warner2014SINBADawi,
  title = {Adaptive waveform inversion - {FWI} without cycle skipping},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {Conventional FWI minimises the direct differences
                  between observed and predicted seismic
                  datasets. Because seismic data are oscillatory, this
                  approach will suffer from the detrimental effects of
                  cycle skipping if the starting model is
                  inaccurate. We reformulate FWI so that it instead
                  adapts the predicted data to the observed data using
                  Wiener filters, and then iterates to improve the
                  model by forcing the Wiener filters towards zero-lag
                  delta functions. This adaptive FWI scheme is
                  demonstrated on synthetic data where it is shown to
                  be immune to cycle skipping, and is able to invert
                  successfully data for which conventional FWI fails
                  entirely. The new method does not require low
                  frequencies or a highly accurate starting model to
                  be successful. Adaptive FWI has some features in
                  common with wave-equation migration velocity
                  analysis, but it works for all types of arrivals
                  including multiples and refractions, and it does not
                  have the high computational costs of WEMVA in 3D.},
  keywords = {presentation, SINBAD, SINBADSPRING2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Spring/warner2014SINBADawi/warner2014SINBADawi.pdf},
  author = {Mike Warner}
}


@PRESENTATION{zheglova2014SINBADead,
  title = {Exploring applications of depth stepping in seismic inverse problems},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2014},
  abstract = {We are exploring applications of stable depth
                  extrapolation with the full wave equation to imaging
                  and inversion. Depth stepping with full wave
                  equation can be advantageous to the time and
                  frequency domain modelling if special care is taken
                  to stabilize the depth exptrapolator efficiently,
                  since it reduces the higher dimensional modelling
                  problem to a number of lower dimensional
                  subproblems. We are interested in exploring
                  applications in inversion, modelling and
                  imaging. For example, just as the reverse time
                  migration can be shown to be the gradient of the
                  reduced formulation of the full waveform inversion
                  problem, it is interesting to explore whether a
                  formulation of the inversion problem can be achieved
                  whose gradient can be computed using depth stepping
                  techniques. We are also interested in such
                  applications as preconditioning of iterative methods
                  for Helmholtz equation and imaging.},
  keywords = {presentation, SINBAD, SINBADSPRING2014, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2014/Spring/zheglova2014SINBADead/zheglova2014SINBADead.pdf},
  author = {Polina Zheglova and Felix J. Herrmann}
}


%----- 2013 (FALL) -----%

@PRESENTATION{fang2013SINBADsfwi,
  title = {Swift {FWI}},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {In 3D case, there is much more data and modeling is much
                  more expensive. As a result, parallel computing is
                  very important for 3D full-waveform inversion. Both
                  domain decomposition and data decomposition need a
                  large number of parallel computing
                  resources. However, programs based on parallel
                  MATLAB suffer from the limitation of licenses. In
                  order to obtain a MATLAB licenses free solution, we
                  use SWIFT, which is a fast and easy parallel
                  scripting language. Once the original Mat file is
                  complied to an executable file, SWIFT can run the
                  code inside the executable file in parallel without
                  using parallel MATLAB. We use the SWIFT to compute
                  the object functions and gradients of different
                  shots in parallel, and test the parallel 3D FWI code
                  with overthrust data. This is joint work with Thomas
                  Lai, Harsh Juneja, Bas Peters.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/fang2013SINBADsfwi/fang2013SINBADsfwi.pdf},
  author = {Zhilong Fang and Henryk Modzelewski and Tristan van Leeuwen and Thomas Lai and Harsh Juneja and Bas Peters and Felix J. Herrmann}
}


@PRESENTATION{vanleeuwen2013SINBADsda,
  title = {Solving the data-augmented wave equation},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {The recently proposed penalty method promises to
                  mitigate some of the non-linearity inherent in
                  full-waveform inversion by relaxing the requirement
                  that the wave-equation needs to be solved
                  exactly. The basic workflow of this new method is as
                  follows; i) solve an overdetermined wave-equation
                  (the data-augmented wave-equation), where the data
                  serves as additional constraints for the wavefields,
                  ii) compute the wavefield-residual by substituting
                  this wavefield in the wave-equation, and iii)
                  correlate the wavefield with the wavefield-residual
                  to obtain a model-update. As opposed to the
                  conventional workflow, no explicit adjoint solve is
                  needed to compute the model-update. However, instead
                  of solving a wave-equation, we need to solve a
                  data-augmented wave-equation. In this talk we
                  explore some of the challenges of solving this
                  data-augmented wave-equation and review some
                  possible solution strategies for both time and
                  frequency-domain applications.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/vanleeuwen2013SINBADsda/vanleeuwen2013SINBADsda.pdf},
  author = {Tristan van Leeuwen and Bas Peters and Felix J. Herrmann}
}


@PRESENTATION{petrenko2013SINBADaih,
  title = {Accelerating an iterative {Helmholtz} solver with {FPGAs}},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {Solution of the Helmholtz equation is the main
                  computational burden of full-waveform inversion in
                  the frequency domain. For this task we employ the
                  CARP-CG algorithm (Gordon & Gordon 2010), an
                  iterative solver that preconditions the original
                  Helmholtz system into an equivalent symmetric
                  positive definite system and then applies the method
                  of conjugate gradients. Forming the matrix for the
                  new system is not necessary as its multiplicative
                  action on a vector is implemented using a series of
                  projections onto the rows of the original
                  system. Our contribution is implementing CARP-CG for
                  a host + accelerator (FPGA) computing
                  environment. The computational paradigm is one of
                  dataflow: vector and matrix elements are streamed
                  from memory through the accelerator which applies
                  the row projections. The advantage of an FPGA to
                  process streams of data is that unless the algorithm
                  is memory bandwidth limited, computation time is
                  directly proportional to the amount of data. The
                  complexity of the algorithm implemented on the FPGA
                  is irrelevant since all the operations programmed
                  onto the FPGA happen in the same clock tick. In
                  contrast, on a CPU, more complex algorithms require
                  more clock ticks as the instructions are executed
                  sequentially, or with only a small amount of
                  parallelism. Ongoing work porting the CARP-CG
                  algorithm to the accelerator is presented.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/petrenko2013SINBADaih/petrenko2013SINBADaih.pdf},
  author = {Art Petrenko and Tristan van Leeuwen and Felix J. Herrmann and Diego Oriato and Simon Tilbury}
}


@PRESENTATION{lago2013SINBADksf,
  title = {Krylov solvers in frequency domain {FWI}},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {We briefly discuss here several aspects rising from the
                  use of Krylov solvers for frequency domain
                  FWI. Although several powerful preconditioners are
                  in constant development by the linear algebra
                  community targeting this application, some issues as
                  the multishot and multifrequency scenarios as well
                  as advanced Krylov method techniques in combination
                  with these powerful preconditioners are rarely
                  addressed. We provide an overview of some of the
                  recent research on this regard and discuss
                  possibility of use of some of these techniques in
                  the context of an inversion.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/lago2013SINBADksf/lago2013SINBADksf.pdf},
  author = {Rafael Lago and Felix J. Herrmann}
}


@PRESENTATION{yeung2013SINBADcsr,
  title = {Compressed sensing, recovery of signals using random {Turbo} matrices},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {Compressed sensing is an emerging technique that allows
                  us to recover an image using far fewer number of
                  measurements than classical sampling
                  techniques. Designing the measurement matrices with
                  certain properties are critical to this
                  task. Gaussian matrices are most commonly used. We
                  discover a new class of random matrices that can
                  outperform the Gaussian matrices when we are in a
                  situation of taking an outrageously small number of
                  samples.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/yeung2013SINBADcsr/yeung2013SINBADcsr.pdf},
  author = {Enrico Au-Yeung and Ozgur Yilmaz and Felix J. Herrmann}
}


@PRESENTATION{wang2013SINBADnru,
  title = {Noise reduction by using interferometric measurements},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {The interferometric formulation of linear wave-based
                  inversion problem was proposed by Demanet and Jugnon
                  recently. Instead of directly fitting the data, they
                  proposed to fit a subset of the data's
                  cross-correlation. It can be verified that if the
                  full cross-correlation is used, then the problem is
                  equivalent to the usual least square problem. The
                  subsampling, which is usually considered to cause
                  instability to the solution, is surprisingly useful
                  in this setting. Numerical experiments for the
                  inverse source problem and the inverse scatting
                  problem have both suggested that a 'good' sampling
                  strategy can actually increase the stability under
                  modeling error caused by the uncertainty of a
                  kinetic nature. We will study the mathematical
                  mechanism behind this phenomenon, and try to see
                  whether or not there exists a universally 'good'
                  sampling strategy independent of the types of
                  forward operators we use.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/wang2013SINBADnru/wang2013SINBADnru.pdf},
  author = {Rongrong Wang and Bas Peters and Felix J. Herrmann}
}


@PRESENTATION{esser2013SINBADasr,
  title = {Applications of phase retrieval methods to blind seismic deconvolution},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {Phase retrieval is the non-convex optimization problem
                  of recovering a signal from magnitudes of complex
                  linear measurements. Solving convex semi-definite
                  program (SDP) relaxations has been shown to be a
                  robust approach, but it remains too expensive to
                  apply to large problems. We will discuss methods for
                  accelerating computations and explore applications
                  to seismic deconvolution.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/esser2013SINBADasr/esser2013SINBADasr.pdf},
  author = {Ernie Esser and Felix J. Herrmann}
}


@PRESENTATION{hargreaves2013SINBADbfo,
  title = {The bridge from orthogonal to redundant transforms and weighted $\ell_1$ optimization},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {Traditional arguments in synthesis $\ell_1$
                  -optimization require our forward operator to be
                  orthogonal, though we use redundant transforms in
                  practice. These traditional arguments do not
                  translate to redundant transforms, and other
                  arguments require impractical conditions on our
                  effective measurement matrix. Recent theory in
                  one-norm analysis, namely the optimal dual $\ell_1$
                  analysis of Shidong et al, have provided point-wise
                  reconstruction error estimates for synthesis using
                  an equivalence relationship where we can use weaker
                  assumptions. This exposes an important model
                  assumption indicating why analysis might outperform
                  synthesis, for which careful consideration in
                  seismic is necessary, and the need for models such
                  as the cosparse model. In this talk we will discuss
                  these ideas, provide evidence which indicates this
                  theory should generalize to uniform error
                  estimates(and thus not signal dependent), and how
                  redundancy, support information, and weighting play
                  important roles.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/hargreaves2013SINBADbfo/hargreaves2013SINBADbfo.pdf},
  author = {Brock Hargreaves and Ozgur Yilmaz and Felix J. Herrmann}
}


@PRESENTATION{goh2013SINBADttt,
  title = {Taming time through tangents},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {Given two vectors of (possibly) different lengths, the
                  edit distance considers all possible alignments
                  between the two and picks the one that minimizes the
                  number of operations needed to turn one into the
                  other. Though highly non-smooth and riddled with
                  local minima, we show a way to compute the convex
                  envelope of this function, which opens the door to
                  using the approximate edit distance as a surrogate
                  for the L2 distance and comparing vectors of
                  different lengths.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/goh2013SINBADttt/goh2013SINBADttt.pdf},
  author = {Gabriel Goh and Michael P. Friedlander and Felix J. Herrmann}
}


@PRESENTATION{macedo2013SINBADdap,
  title = {A dual approach to {PhaseLift} via gauge programming and bundle methods},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {A feature common to many sparse optimization problems is
                  that the number of variables may be significantly
                  larger than the number of constraints- e.g., the
                  matrix-lifting approach taken by PhaseLift for phase
                  retrieval results in a problem where the number of
                  variables is quadratic in the number of
                  constraints. We consider a duality framework and
                  numerical methods to leverage the relatively small
                  number of constraints. Preliminary numerical results
                  illustrate our approach and its flexibility.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/macedo2013SINBADdap/macedo2013SINBADdap.pdf},
  author = {Ives Macedo and Michael P. Friedlander and Felix J. Herrmann}
}


@PRESENTATION{nutini2013SINBADpcb,
  title = {Putting the curvature back into sparse solvers},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {For many problems in signal and image processing, we
                  seek a sparse solution that solves that
                  approximately solves the problem Ax $\approx$ b,
                  where A is an m-by-n matrix and b is an
                  m-vector. Many of the most used approaches to
                  problem thissuch as iterative soft thresholding
                  SPGL1 andare first-order methods. As a result, these
                  methods can sometimes be slow to converge. In this
                  talk, we present an approach that takes advantage of
                  the easily-obtainable second-order information. By
                  exploiting this available second-order information,
                  we are able to put the curvature back into sparse
                  solvers and improve upon the convergence rates of
                  existing solvers.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/nutini2013SINBADpcb/nutini2013SINBADpcb.pdf},
  author = {Julie Nutini and Michael P. Friedlander and Felix J. Herrmann}
}  


@PRESENTATION{pong2013SINBADppg,
  title = {The proximal-proximal gradient algorithm},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {In many applications, one has to minimize the sum of a
                  smooth loss function modeling misfit and a
                  regularization term inducing structures. In this
                  talk, we consider the case when the regularization
                  is a composition of a convex function, whose
                  proximal mapping is easy to compute, and a nonzero
                  linear map. Such instances arise in system
                  identification and realization problems. In this
                  talk, we present a new algorithm, the
                  proximal-proximal gradient algorithm, which admits
                  easy subproblems. Our algorithm reduces to the
                  proximal gradient algorithm if the linear map is
                  just the identity map, and can be viewed as "very
                  inexact" inexact proximal gradient algorithm. We
                  show that the whole sequence generated from the
                  algorithm converges to an optimal solution, and
                  establish an upper bound on iteration complexity.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/pong2013SINBADppg/pong2013SINBADppg.pdf},
  author = {Ting Kei Pong and Michael P. Friedlander and Felix J. Herrmann}
}  


@PRESENTATION{akalin2013SINBADmtc,
  title = {Matrix and tensor completion for large-scale seismic interpolation: a comparative study},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {Owing to their high dimensionality, interpolating 3D
                  seismic data volumes remains a computationally
                  daunting task. In this work, we outline a
                  comprehensive framework for sampling and
                  interpolating such volumes based on the
                  well-understood theory of Matrix and Tensor
                  completion. This interpolation theory consists of
                  three components major signal structure,
                  structure-destroying sampling, and
                  structure-restoring optimization. By viewing
                  interpolation in the context of this theory, we are
                  able to specify exactly when these approaches are
                  expected to perform well. We also introduce
                  structure-revealing transformations that promote the
                  inherent low-rank structure in seismic data as well
                  as a factorization approach that scales to large
                  problem sizes. Our methods are able to handle
                  large-scale data volumes more accurately and more
                  quickly compared to other more ad-hoc approaches, as
                  we will demonstrate. This is joint work with Curt Da
                  Silva, Rajiv Kumar, Ben Recht, and Felix
                  J. Herrmann.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/akalin2013SINBADmtc/akalin2013SINBADmtc.pdf},
  author = {Okan Akalin and Curt Da Silva and Rajiv Kumar and Ben Recht and Felix J. Herrmann}
}  


@PRESENTATION{dasilva2013SINBADstf,
  title = {Structured tensor formats for missing-trace interpolation and beyond},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {High-dimensional data, alternatively known as tensors,
                  occurs in a variety of seismic problems. By
                  exploiting the fact that seismic data can be well
                  represented as a structured tensor, we design
                  algorithms that operate on much lower dimensional
                  parameters. In this talk, we will review some recent
                  developments in interpolating seismic data volumes
                  in the so-called Hierarchical Tucker format as well
                  as demonstrate the need for such formats when
                  tackling high-dimensional problems such as
                  Uncertainty Quantification.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/dasilva2013SINBADstf/dasilva2013SINBADstf.pdf},
  author = {Curt da Silva and Felix J. Herrmann}
}  


@PRESENTATION{kumar2013SINBADwrs,
  title = {Wavefield reconstruction with {SVD}-free low-rank matrix factorization},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {As shown in past, we can leverage the ideas from the
                  field of compressed sensing to cast problems like
                  seismic data interpolation or sequential shot data
                  recovery from simultaneous data, as a compressed
                  sensing problem. In this work we will show how we
                  can borrow the same ideas of compressed sensing and
                  cast these problems as matrix completion
                  problems. Instead of sparsity we will show that we
                  can exploit the low-rank structure of seismic data
                  to solve these problems. One of the impediments in
                  rank-minimization problem is the computation of
                  singular values. We will also show how we can solve
                  the rank minimization problems SVD-free. The
                  practical application is divided into three parts:
                  1. In case of sequential seismic data acquisition,
                  how jittered subsampling helps to recover the better
                  quality data as compared to random
                  subsampling. 2. How the incorporation of reciprocity
                  principles help to enhance the quality of recovered
                  fully sampled data. 3. How we can recover the
                  sequential source data from simultaneous source
                  data.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/kumar2013SINBADwrs/kumar2013SINBADwrs.pdf},
  author = {Rajiv Kumar and Aleksandr Y. Aravkin and Hassan Mansour and Ernie Esser and Felix J. Herrmann}
}  


@PRESENTATION{ghadermarzy2013SINBADups,
  title = {Using prior support information in approximate message passing algorithms},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {Consider the standard compressed sensing problem. We
                  want to recover sparse compressible aorsignal from
                  few linear measurements. In this talk we investigate
                  recovery performance when we have prior information
                  about the support, i.e., the indices of the non-zero
                  entries, of the signal to be recovered. First we
                  briefly review the results of "weighted $\ell_p$
                  minimization algorithm with p = 1 and 0 < p <
                  1". Then we derive a weighted approximate message
                  passing (AMP) algorithm which incorporates prior
                  support information into the AMP algorithm. We
                  empirically show that this algorithm recovers sparse
                  signals significantly faster than weighted $\ell_1$
                  minimization. We also introduce a reweighting scheme
                  for AMP and weighted AMP which, we observe,
                  substantially improves the recovery conditions of
                  these algorithms. We illustrate our results with
                  extensive numerical experiments on synthetic data
                  and seismic data reconstruction.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/ghadermarzy2013SINBADups/ghadermarzy2013SINBADups.pdf},
  author = {Navid Ghadermarzy and Ozgur Yilmaz and Felix J. Herrmann}
}  


@PRESENTATION{lin2013SINBADbre,
  title = {Bootstrapping robust {EPSI} with coarsely sampled data},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {The EPSI method of surface multiple removal directly
                  inverts for the free-surface operator, i.e., the
                  multiple-free Greens function of the subsurface seismic
                  response. One peculiar feature of this approach is
                  the theoretical independence of the spectrum of the
                  free-surface operator from the spectrum of the
                  observed data. The SRME approach requires coarsely
                  sampled data to be low-pass filtered sufficiently in
                  order to avoid aliasing in multiple contribution
                  gathers, which in turn limits the temporal
                  resolution of the demultipled result. Conversely,
                  such limitations in temporal resolution do not
                  directly apply to the inversion solution of
                  EPSI. This property can in turn be exploited both to
                  significantly lower the cost of EPSI and to mitigate
                  the effect of under sampled data in a controlled
                  way.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/lin2013SINBADbre/lin2013SINBADbre.pdf},
  author = {Tim T.Y. Lin and Felix J. Herrmann}
}  
  

@PRESENTATION{oghenekohwo2013SINBADedt,
  title = {Estimating {4D} differences in time-lapse using randomized sampling techniques},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {Repeatability in the seismic survey and processing has
                  been cited as the main reason for which 4D seismic
                  technology work. In the last decade, concerted
                  efforts have been spent to make the 4D seismic
                  process highly repeatable, without significant
                  success. On the contrary, Compressed Sensing, which
                  is also a relatively new sampling paradigm proposes
                  that one can recover an estimate of a fully sampled
                  signal from a noisy, under-sampled measurements
                  provided the acquisition architecture satisfies some
                  properties. By observing different under-sampled and
                  random measurements from each vintage, corresponding
                  to different acquisition geometries, we show that
                  one can still detect the 4D change in time using
                  recent ideas from Compressed sensing. Using a
                  realistic synthetic model, we show two methods of
                  estimating the 4D difference and compare their
                  relative performance to each other.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/oghenekohwo2013SINBADedt/oghenekohwo2013SINBADedt.pdf},
  author = {Felix Oghenekohwo and Ernie Esser and Felix J. Herrmann}
}  


@PRESENTATION{wason2013SINBADtjm,
  title = {Time-jittered marine sources},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {Current efforts towards dense shot (and/or receiver)
                  sampling and full azimuthal coverage to produce
                  higher-resolution images have led to the deployment
                  of multiple source vessels across the survey area. A
                  step ahead from multi-source seismic acquisition is
                  simultaneous or blended acquisition where different
                  source arrays/vessels fire shots at
                  near-simultaneous or slightly random times. Seismic
                  data acquisition with simultaneous (or blended)
                  sources has helped improve acquisition efficiency
                  and mitigate acquisition related costs. Deblending
                  then aims to recover unblended data, as acquired
                  during conventional acquisition, from blended data
                  since many processing techniques rely on full,
                  regular sampling. We present a simultaneous/blended
                  marine acquisition setup where shots fire at
                  significantly jittered instances in time resulting
                  in jittered shot locations for a given speed of the
                  source vessel. The conventional, unblended data is
                  recovered from the blended, jittered/irregular data
                  by sparsity-promoting inversion using the
                  non-equispaced fast discrete curvelet transform. The
                  optimization scheme aims to deblend the blended data
                  along with regularization and interpolation to a
                  (finer) regular grid.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/wason2013SINBADtjm/wason2013SINBADtjm.pdf},
  author = {Haneet Wason and Felix J. Herrmann}
}  


@PRESENTATION{kumar2013SINBADava,
  title = {Extended images in action: efficient {AVA} via probing},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {Common image gathers (CIG) are an important tool to
                  perform AVA analysis in areas of complex
                  geology. Unfortunately, it is prohibitively very
                  expensive to compute these CIG for all the
                  subsurface points. In this work, we present an
                  efficient way to compute CIG for all subsurface
                  offsets without explicitly calculating the source
                  and receiver wavefields for all the sources. Because
                  the CIG contain all possible subsurface offsets, we
                  compute the angle-domain image gathers by selecting
                  the subsurface offset that is aligned with the local
                  geologic dip. We propose a method to compute the
                  local dip information directly from
                  common-image-point gathers. To assess the quality of
                  the angle-domain common-image-points gathers we
                  compute the angle-dependent reflectivity
                  coefficients and compare them with theoretical
                  reflectivity coefficients yielded by the
                  (linearized) Zoeppritz equations for a few synthetic
                  models.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/kumar2013SINBADava/kumar2013SINBADava.pdf},
  author = {Rajiv Kumar and Tristan van Leeuwen and Felix J. Herrmann}
}  


@PRESENTATION{miao2013SINBADfid,
  title = {Fast imaging via depth stepping with the two-way wave equation},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {In this presentation we propose a fast imaging algorithm
                  via depth stepping with the two-way wave
                  equation. Within the framework of survey sinking, a
                  stabilized depth extrapolation operator is computed
                  using a spectral projector which can efficiently
                  split evanescent wave components. The computation of
                  the spectral projector features with an
                  Hierarchically Semi-Seperable (HSS) matrix
                  representation speeded up polynomial recursion,
                  resulting in an accleration from cubic numerical
                  complexity to linear numerical complexity.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/miao2013SINBADfid/miao2013SINBADfid.pdf},
  author = {Lina Miao and Felix J. Herrmann}
}  


@PRESENTATION{zheglova2013SINBADihss,
  title = {Imaging with hierarchical semi separable matrices},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {Hierarchically Semi Separable (HSS) matrices are (in
                  general) dense matrices that have low rank
                  off-diagonal blocks that can be represented
                  economically. Exploiting a specific structure of HSS
                  representation, fast algorithms have been devised
                  for matrix matrix multiplication, addition and
                  computing a matrix inverse. We are interested in
                  developing fast algorithms for seismic imaging using
                  ideas from this approach. An overview of HSS
                  representation and some methods using HSS
                  representations of operators will be shown in this
                  talk.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/zheglova2013SINBADihss/zheglova2013SINBADihss.pdf},
  author = {Polina Zheglova and Felix J. Herrmann}
}  


@PRESENTATION{tu2013SINBADfim,
  title = {Fast imaging with multiples and source estimation},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {During this talk, we present a computationally efficient
                  (cost of 1-2 RTMs with all data) iterative
                  sparsity-promoting inversion framework where
                  surface-related multiples are jointly imaged with
                  primaries and where the source signature is
                  estimated on the fly. Our imaging algorithm is
                  computationally efficient because it works during
                  each iteration with small independent randomized
                  subsets of data. The multiples are handled by
                  introducing an areal source term that includes the
                  upgoing wavefield. We update the source signature
                  for each iteration using a variable projection
                  method. The resulting algorithm removes imaging
                  artifacts from surface-related multiples, estimates
                  and removes the imprint of the source, recovers true
                  amplitudes, is fast, and robust to linearization
                  errors by virtue of the statistical independence of
                  the subsets of data we are working with at each
                  iteration.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/tu2013SINBADfim/tu2013SINBADfim.pdf},
  author = {Ning Tu and Felix J. Herrmann}
}  


@PRESENTATION{fang2013SINBADuafwi,
  title = {Uncertainty analysis for {FWI}},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {Uncertainty analysis is important for seismic
                  interpretation. Based on the framework of Bayesian,
                  we can analyse different statistic parameters of our
                  FWI result. However, directly sampling the posterior
                  probability density function (pdf) is
                  computationally intractable. In order to make this
                  problem computationally tractable, in this work, we
                  use Gaussian distribution approximation and low rank
                  approximation to generate the posterior
                  pdf. Simultaneous shots are also used to reduce the
                  computational costs.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/fang2013SINBADuafwi/fang2013SINBADuafwi.pdf},
  author = {Zhilong Fang and Felix J. Herrmann}
}  


@PRESENTATION{li2013SINBADmsd,
  title = {Model-space versus data-space {FWI} with the acoustic wave equation},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {Inverting data with elastic phases using an acoustic
                  wave equation can lead to erroneous results,
                  especially when the number of iterations is too
                  high, which may lead to over fitting the
                  data. Several approaches have been proposed to
                  address this issue. Most commonly, people data-
                  filtering operations that are aimed to deemphasize
                  the elastic phases in the data in favor of the
                  acoustic phases. Examples of this approach are
                  nested loops over offset range and Laplace
                  parameters. In this presentation, we discuss two
                  complementary optimization-driven methods where the
                  minimization process decides adaptively which of the
                  data or model components are consistent with the
                  objective. Specifically, we compare the s t misfit
                  function as the data-space alternative and
                  curvelet-domain sparsity promotion as the
                  model-space alternative. Application of these two
                  methods to a realistic synthetic lead to comparable
                  results that we believe can be improved by combining
                  these two methods.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/li2013SINBADmsd/li2013SINBADmsd.pdf},
  author = {Xiang Li and Anais Tamalet and Tristan van Leeuwen and Felix J. Herrmann}
}  


@PRESENTATION{tu2013SINBADmachar,
  title = {{SLIM's} findings on the {Machar} dataset},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {By courtesy of BP, we are able to get some hands-on
                  experience imaging seismic data from the Machar
                  field in North Sea. We performed reverse-time
                  migration and sparsity-promoting migration with
                  source-estimation to this dataset (or a cropped
                  section of the dataset in many cases due to
                  computational constraints), and had some interesting
                  findings. In this presentation, we will show some
                  conclusive results we have got, and explain the key
                  techniques in imaging this dataset.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/tu2013SINBADmachar/tu2013SINBADmachar.pdf},
  author = {Ning Tu and Felix J. Herrmann}
}  


@PRESENTATION{li2013SINBADgom,
  title = {Lessons learned from {Chevron} {Gulf} of {Mexico} data set},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {Chevron Gulf of Mexico data set is very challenging for
                  FWI because of elastic phases, limited offset, lack
                  of low frequencies and salt structure. To overcome
                  these issue, we first use ray-based tomography on
                  the hand-picked first breaks to generate initial
                  model for FWI, and then we apply curvelet-denosing
                  techniques to improve the poor signal-to-noise ratio
                  of the observed data at low frequencies. Finally,
                  Curvelet domain sparsity promoting Gauss-Newton FWI
                  helps to suppress model space artifacts caused by
                  elastic phases. This is joint work with Andrew
                  J. Calvert, Ian Hanlon, Mostafa Javanmehri, Rajiv
                  Kumar, Tristan van Leeuwen, Brendan R. Smithyman,
                  Haneet Wason and Felix J. Herrmann},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/li2013SINBADgom/li2013SINBADgom.pdf},
  author = {Xiang Li and Andrew J. Calvert and Ian Hanlon and Mostafa Javanmehri and Rajiv Kumar and Tristan van Leeuwen and Brendan R. Smithyman and Haneet Wason and Felix J. Herrmann}
}  


@PRESENTATION{warner2013SINBADrfwi,
  title = {Reflection {FWI} with a poor starting model},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/warner2013SINBADrfwi/warner2013SINBADrfwi.pdf},
  author = {Mike Warner}
}  


@PRESENTATION{smithyman2013SINBADprb,
  title = {Phase-residual based quality-control methods and techniques for mitigating cycle skips},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {Most full-waveform inversion algorithms use local
                  optimization methods to iteratively improve the
                  numerical earth model. All of these make the
                  implicit assumption that the the model close to the
                  true earth to avoid cycle. In practice this may not
                  be true. We explore two questions: 1. How do we
                  understand and visualize the cycle skip phenomenon
                  in order to recognize it if it exists? 2. How do we
                  automate this quality control step and use the rich
                  information from multiple data to mitigate cycle
                  skips and avoid local minima?},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/smithyman2013SINBADprb/smithyman2013SINBADprb.pdf},
  author = {Brendan R. Smithyman and Felix J. Herrmann}
}  


@PRESENTATION{fang2013SINBADp3dfwi,
  title = {Parallel {3D} {FWI} with simultaneous shots},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {In this work, we manage to build the workflow for the
                  parallel 3D full-waveform inversion. In the forward
                  simulation part, we generate the Helmholtz matrix in
                  parallel and use parallel CARPCG to solve the
                  Helmholtz equation. In the inversion process,
                  simultaneous shots are used to reduce the
                  computational costs. Additionally, we propose a
                  method to select the number of simultaneous shots
                  and tolerance of CARPCG dynamically, to reach a
                  compromise between the computational costs and
                  accuracy.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/fang2013SINBADp3dfwi/fang2013SINBADp3dfwi.pdf},
  author = {Zhilong Fang and Tristan van Leeuwen and Felix J. Herrmann}
}  


@PRESENTATION{kumar2013SINBADeia,
  title = {Extended images in action: efficient {WEMVA} via randomized probing},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {Extended images as a function of the full subsurface
                  offset are an important tool to perform
                  wave-equation based migration velocity analysis
                  (WEMVA) in areas of complex geology. Unfortunately,
                  computation & storage of these extended images is
                  prohibitively expensive. In this work, we present an
                  efficient way to compute extended images for all
                  subsurface offsets without explicitly calculating
                  the source and receiver wavefields for all the
                  sources. Instead, we calculate actions of extended
                  image volumes on probing vectors that live in the
                  image space. The probing can either be defined as
                  vectors from the Dirac basis, which allows us to
                  form the extended image at the location of the point
                  diffractor, or they can be defined in terms of
                  Gaussian noise. The latter corresponds to sources
                  with random weights firing simultaneously at every
                  grid point. We demonstrate that this probing leads
                  to a computationally efficient implementation of
                  WEMVA. This is joint work with Tristan van Leeuwen.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/kumar2013SINBADeia/kumar2013SINBADeia.pdf},
  author = {Rajiv Kumar and Tristan van Leeuwen and Felix J. Herrmann}
}  


@PRESENTATION{peters2013SINBADepm,
  title = {Examples from the penalty-method},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {A novel penalty method for PDE-constrained optimization
                  was recently proposed by van Leeuwen and Herrmann
                  (2013). The conventional PDE-constrained
                  optimization formulation used in seismic waveform
                  inversion is based on calculating the gradient of
                  the data misfit objective functional via the
                  adjoint-state method, at the cost of two PDE
                  solves. The penalty method requires only one
                  solution of an overdetermined linear system, in the
                  least-squares sense. In this talk some numerical
                  properties of this linear system will be
                  exposed. The penalty method for PDE-constrained
                  optimization involves a parameter, balancing the
                  data misfit and PDE-misfit parts of the objective
                  functional. This talk will address how to select
                  this very important parameter. Some examples will be
                  shown in which the penalty method outperforms the
                  conventional method in non-linear waveform
                  inversion, as well as linearized seismic imaging by
                  migration.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/peters2013SINBADepm/peters2013SINBADepm.pdf},
  author = {Bas Peters and Tristan van Leeuwen and Felix J. Herrmann}
}  


@PRESENTATION{herrmann2013SINBADffwi,
  title = {Frugal {FWI}},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {Seismic waveform inversion aims at obtaining detailed
                  estimates of subsurface medium parameters, such as
                  soundspeed, from seismic data. A formulation in the
                  frequency-domain leads to an optimization problem
                  constrained by a Helmholtz equation with many
                  right-hand sides. Application of this technique in
                  3D precludes the use of factorization techniques to
                  solve the Helmholtz equation due the large number of
                  gridpoints and the bandwidth of the matrix. While
                  many sophisticated pre-conditioned iterative
                  techniques have been developed for the Helmholtz
                  equation, they often include model-specific tuning
                  parameters and are thus not very attractive for
                  inversion since the medium parameters change from
                  one iteration to the next. In this paper, we propose
                  a method for 3D seismic waveform inversion that
                  addresses both the need to efficiently solve the
                  Helmholtz equation as well as the computational cost
                  induced by the many right-hand sides. To solve the
                  Helmholtz equation, we consider a simple generic
                  preconditioned iterative method (CARP-CG) that is
                  well-suited for inversion because of its
                  robustness. We extend this method to a
                  block-iterative method that can efficiently handle
                  multiple right-hand sides. To reduce the
                  computational cost of of the overall optimization
                  procedure, we use recently proposed techniques from
                  stochastic optimization that allow us to work with
                  approximate gradient information. These
                  approximations are obtained by evaluating only a
                  small portion of the right-hand sides and/or by
                  solving the PDE approximately. We propose heuristics
                  to adaptively determine the required accuracy of the
                  PDE solves and the sample-size and illustrate the
                  algorithms on synthetic benchmark models. This is
                  joint work with Tristan van Leeuwen.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/herrmann2013SINBADffwi/herrmann2013SINBADffwi.pdf},
  author = {Felix J. Herrmann}
}  


@PRESENTATION{vanleeuwen2013SINBADrpp,
  title = {Relaxing the physics: a penalty method for full-waveform inversion},
  booktitle = {SINBAD Fall consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {Computationally efficient method of solving partial
                  differential equation (PDE)-constrained optimization
                  problems that occur in (geophysical) inversion
                  problems. The method takes measured data from a
                  physical system as input and minimizes an objective
                  function that depends on an unknown model for the
                  physical parameters, fields, and additional nuisance
                  parameters such as the source function. The
                  invention consists of a minimization procedure
                  involving a cost functional comprising of a
                  data-misfit term and a penalty term that measures
                  how accurately the fields satisfy the PDE. The
                  method is composed of two alternating steps, namely
                  the solution of a system of equations forming the
                  discretization of the data-augmented PDE, and the
                  solution of physical model parameters from the PDE
                  itself given the field that solves the
                  data-augmented system and an estimate for the
                  sources. Compared to all-at-once approaches to
                  PDE-constrained optimization, there is no need to
                  update and store the fields for all sources leading
                  to significant memory savings. As in the all-at-once
                  approach, the proposed method explores a larger
                  search space and is therefore less sensitive to
                  initial estimates for the physical model
                  parameters. Contrary to the reduced formulation, the
                  proposed method does not require the solution of an
                  adjoint PDE, effectively halving the number of PDE
                  solves and memory requirement. As in the reduced
                  formulation, fields can be computed independently
                  and aggregated, possibly in parallel.},
  keywords = {presentation, SINBAD, SINBADFALL2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Fall/vanleeuwen2013SINBADrpp/vanleeuwen2013SINBADrpp.pdf},
  author = {Tristan van Leeuwen and Felix J. Herrmann}
}  


%----- 2013 (SPRING) -----%

@PRESENTATION{dasilva2013SINBADSPRhtuck,
  title = {Hierarchical {Tucker} tensor optimization - applications to {4D} seismic data interpolation},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {In this work, we develop optimization algorithms on the
                  manifold of Hierarchical Tucker (HT) tensors, an
                  extremely efficient format for representing
                  high-dimensional tensors exhibiting particular
                  low-rank structure. With some minor alterations to
                  existing theoretical developments, we develop an
                  optimization framework based on the geometric
                  understanding of HT tensors as a smooth manifold, a
                  generalization of smooth curves/surfaces. Building
                  on the existing research of solving optimization
                  problems on smooth manifolds, we develop Steepest
                  Descent and Conjugate Gradient methods for HT
                  tensors. The resulting algorithms converge quickly,
                  are immediately parallelizable, and do not require
                  the computation of SVDs. We also derive efficient
                  Gauss-Newton based algorithms which converge much
                  faster than standard, first-order methods. We also
                  extend ideas about favourable sampling conditions
                  for missing-data recovery from the field of Matrix
                  Completion to Tensor Completion and demonstrate how
                  the organization of data can affect the success of
                  recovery. As a result, if one has data with randomly
                  missing source pairs, using these ideas, coupled
                  with an efficient solver, one can interpolate
                  large-scale seismic data volumes with missing
                  sources and/or receivers by exploiting the
                  multidimensional dependencies in the data. We are
                  able to recover data volumes amidst extremely high
                  subsampling ratios (in some cases, > 75\%) using this
                  approach and we demonstrate our recovery on a
                  synthetic 5D data set provided to us by BG.},
  keywords = {presentation, SINBAD, SINBADSPRING2013, SLIM, Hierarchical Tucker, Structured Tensor, 3D Data interpolation, structured tensor},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Spring/dasilva2013SINBADSPRhtuck/dasilva2013SINBADSPRhtuck.pdf},
  author = {Curt Da Silva and Felix J. Herrmann}
}


@PRESENTATION{herrmann2013SINBADmlm,
  title = {Mitigating local minima in full-waveform inversion by expanding the search space with the penalty method},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {Wave-equation based inversions, such as full-waveform
                  inversion, are challenging because of their
                  computational costs, memory requirements, and
                  reliance on accurate initial models. To confront
                  these issues, we propose a novel formulation of
                  full-waveform inversion based on a penalty
                  method. In this formulation, the objective function
                  consists of a data-misfit term and a penalty term
                  which measures how accurately the wavefields satisfy
                  the wave-equation. Because we carry out the
                  inversion over a larger search space, including both
                  the model and synthetic wavefields, our approach
                  suffers less from local minima. Our main
                  contribution is the development of an efficient
                  optimization scheme that avoids having to store and
                  update the wavefields by explicit
                  elimination. Compared to existing optimization
                  strategies for full-waveform inversion, our method
                  differers in two main aspects; i. The wavefields are
                  solved from an augmented wave-equation, where the
                  solution is forced to solve the wave-equation and
                  fit the observed data, ii. no adjoint wavefields are
                  required to update the model, which leads to
                  significant computational savings. We demonstrate
                  the validity of our approach by carefully selected
                  examples and discuss possible extensions and future
                  research. This is joint work with Tristan van
                  Leeuwen.},
  keywords = {presentation, SINBAD, SINBADSPRING2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Spring/herrmann2013SINBADmlm/herrmann2013SINBADmlm.pdf},
  author = {Felix J. Herrmann}  
}


@PRESENTATION{herrmann2013SINBADfwi,
  title = {Frugal {FWI}},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {Seismic waveform inversion aims at obtaining detailed
                  estimates of subsurface medium parameters, such as
                  soundspeed, from seismic data. A formulation in the
                  frequency-domain leads to an optimization problem
                  constrained by a Helmholtz equation with many
                  right-hand- sides. Application of this technique in
                  3D precludes the use of factorization techniques to
                  solve the Helmholtz equation due the large number of
                  gridpoints and the bandwidth of the matrix. While
                  many sophisticated pre-conditioned iterative
                  techniques have been developed for the Helmholtz
                  equation, they often include model-specific tuning
                  parameters and are thus not very attractive for
                  inversion since the medium parameters change from
                  one iteration to the next. In this paper, we propose
                  a method for 3D seismic waveform inversion that
                  addresses both the need to efficiently solve the
                  Helmholtz equation as well as the computational cost
                  induced by the many right-hand-sides. To solve the
                  Helmholtz equation, we consider a simple generic
                  preconditioned iterative method (CARP-CG) that is
                  well-suited for inversion because of its
                  robustness. We extend this method to a
                  block-iterative method that can efficiently handle
                  multiple right-hand sides. To reduce the
                  computational cost of of the overall optimization
                  procedure, we use recently proposed techniques from
                  stochastic optimization that allow us to work with
                  approximate gradient information. These
                  approximations are obtained by evaluating only a
                  small portion of the right-hand sides and/or by
                  solving the PDE approximately. We propose heuristics
                  to adaptively determine the required accuracy of the
                  PDE solves and the sample-size and illustrate the
                  algorithms on synthetic benchmark models. This is
                  joint work with Tristan van Leeuwen.},
  keywords = {presentation, SINBAD, SINBADSPRING2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Spring/herrmann2013SINBADfwi/herrmann2013SINBADfwi.pdf},
  author = {Felix J. Herrmann}  
}


@PRESENTATION{herrmann2013SINBADeia,
  title = {Extended images in action},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2013},
  abstract = {Image gathers as a function of subsurface offset are an
                  important tool for migration-velocity and
                  amplitude-versus-angle analysis in areas of complex
                  geology. Traditionally, these gathers are thought of
                  as multidimensional correlations of the source and
                  receiver wavefields. The bottleneck in computing the
                  gathers lies in the fact that one needs to store and
                  compute these wavefields and in correlating the
                  wavefields to obtain the desired image
                  gathers. Therefore, the image gathers are typically
                  only computed for a limited number of subsurface
                  points and for a limited range of subsurface
                  offsets. In this presentation, we offer a new
                  perspective on such gathers by organizing the
                  extended image as a function of all subsurface
                  offsets for all subsurface points in a matrix whose
                  (i,j) entry captures the interaction between
                  gridpoints i and j. Of course, it is infeasible to
                  form and store this matrix. Instead, we propose an
                  efficient algorithm to glean information from the
                  image volume via the action of matrix-vector
                  products with this matrix. We illustrate how this
                  can be used to (i) form conventional image gathers
                  and construct objective functions for automated MVA
                  and (ii) calculate true two-way wave-equation angle
                  gathers that allow us to carry out linearized
                  angle-versus-offset inversion with geologic-dip
                  correction. This is joint work with Tristan van
                  Leeuwen and Rajiv Kumar.},
  keywords = {presentation, SINBAD, SINBADSPRING2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Spring/herrmann2013SINBADeia/herrmann2013SINBADeia.pdf},
  author = {Felix J. Herrmann}  
}


@PRESENTATION{herrmann2013SINBADldc,
  title = {Latest developments on the {Chevron} {GOM} and other datasets},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2013},  
  abstract = {During this talk, we will give a brief overview on our
                  imaging and FWI results on various synthetic and
                  field datasets we have been working on. This is
                  joint work with Andrew Calvert, Brendan R. Smithyman,
                  and the SLIM team.},
  keywords = {presentation, SINBAD, SINBADSPRING2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Spring/herrmann2013SINBADldc/herrmann2013SINBADldc.pdf},
  author = {Felix J. Herrmann}  
}


@PRESENTATION{kumar2013SINBADhss,
  title = {Seismic data interpolation via low-rank matrix factorization in the hierarchical semi-separable representation},
  booktitle = {SINBAD Spring consortium talks},  
  organization = {SINBAD},
  year = {2013},  
  abstract = {Recent developments in matrix rank optimization have
                  allowed for new computational approaches in the
                  field of seismic data interpolation.  One of the
                  main requirements of exploiting rank-minimization
                  approaches is that the target data set should
                  exhibit a low-rank structure. Seismic frequency
                  slices exhibit low-rank structure at the
                  low-frequencies, but not at the high
                  frequencies. This behavior is due to the increase in
                  oscillations as we move from low to high-frequency
                  slices, even though the energy remains focused
                  around the diagonal. Therefore, interpolation via
                  rank minimization in the high-frequency range
                  requires extended formulations that incorporate
                  low-rank structure. We propose an approach for
                  seismic data interpolation which incorporates the
                  Hierarchical Semi-Separable Structure (HSS) inside
                  rank-regularized least-squares formulations for the
                  missing-trace interpolation problem. The proposed
                  approach is suitable for large scale problems, since
                  it avoids SVD computations and uses a low-rank
                  factorized formulation instead. We illustrate the
                  advantages of the new HSS approach by interpolating
                  a seismic line from the Gulf of Suez and compare the
                  reconstruction with conventional rank minimization.},
  keywords = {presentation, SINBAD, SINBADSPRING2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Spring/kumar2013SINBADhss/kumar2013SINBADhss.pdf},
  author = {Rajiv Kumar and Hassan Mansour and Aleksandr Y. Aravkin and Felix J. Herrmann}  
}


@PRESENTATION{lin2013SINBADcsd,
  title = {Cosparse seismic data interpolation},
  booktitle = {SINBAD Spring consortium talks},
  organization = {SINBAD},
  year = {2013},
  abstract = {Over the years we have investigated seismic data
                  interpolation and redatuming algorithms rely on on
                  the assumption that seismic records and images
                  permit sparse approximations under certain
                  representations, such as Curvelet
                  coefficients. Recent findings have suggested that
                  for redundant representations (of which Curvelet is
                  an example), the analysis operator that maps the
                  physical signal to coefficients may also play a
                  crucial role in recovering data from incomplete
                  observations. This insight elevates the significance
                  of a question that often goes unaddressed: is it
                  better for the transform-domain sparsity to be
                  achieved through explicit construction of sparse
                  representations (e.g., by thresholding of small
                  transform-domain coefficients), or by demanding that
                  the algorithm return physical signals which produces
                  sparse coefficients when hit with the forward
                  transform? Recent results show that the two
                  approaches give rise to different solutions when the
                  transform is redundant, and that the latter approach
                  imposes a whole new class of constraints. In
                  particular, the number of zero-valued coefficients
                  given by the analysis operator acting on the signal,
                  referred to as its "cosparsity", have an analogous
                  role to the sparsity of the signal in terms of the
                  coefficients. From this framework, a new
                  reconstruction algorithm is proposed which may allow
                  better reconstruction from subsampled signaled than
                  what the sparsity assumption alone would predict. In
                  this work we apply the new framework and algorithm
                  to the case of seismic data interpolation under the
                  curvelet domain, and show that it admits better
                  reconstruction than some existing L1 sparsity-based
                  methods derived from compressive sensing for a range
                  of subsampling factors. We will also investigate
                  different analysis operators and their impact on
                  both sparsity and cosparsity-based algorithms.},
  keywords = {presentation, SINBAD, SINBADSPRING2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Spring/lin2013SINBADcsd/lin2013SINBADcsd.pdf},
  author = {Tim T.Y. Lin and Felix J. Herrmann}
}


@PRESENTATION{tu2013SINBADvp,
  title = {Fast imaging with multiples and source estimation},
  booktitle = {SINBAD Spring consortium talks},
  organization = {SINBAD},
  year = {2013},
  abstract = {Multiples are usually treated as unwanted components in
                  seismic data. However, if correctly used, they can
                  provide valuable information about the
                  subsurface. In this presentation, I will talk about
                  how to make use of multiples in $\ell_1$ regularized
                  least-squares imaging, and how to estimate the
                  source wavelet on the fly using multiples. Synthetic
                  examples show that using multiples not only helps to
                  retrieve the true-amplitude seismic image and the
                  source wavelet, it also increases the wavenumber
                  contents in the seismic image. By using
                  dimensionality reduction techniques with
                  rerandomization, we also greatly decrease the
                  simulation cost without compromising the image
                  quality.},
  keywords = {presentation, SINBAD, SINBADSPRING2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Spring/tu2013SINBADvp/tu2013SINBADvp.pdf},
  author = {Ning Tu and Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann}
}


@PRESENTATION{tu2013SINBADrerand,
  title = {Controlling linearization errors with rerandomization},
  booktitle = {SINBAD Spring consortium talks},
  organization = {SINBAD},
  year = {2013},      
  abstract = {Least squares migration aims to fit the observed seismic
                  data with data predicted by linearized modelling, by
                  solving an PDE-constrained optimization
                  problem. This problem is challenging mostly because
                  of its prohibitive computational cost. To address
                  the issue, dimensionality reduction techniques were
                  proposed in the literature. However, the solution of
                  the reduced problem can deviate from that of the
                  full problem when there are components in the
                  observed data that cannot be explained by linearized
                  modelling. We solve the problem by rerandomizing our
                  $\ell_1$ regularized inversion. In this
                  presentation, I will explain the method and
                  demonstrate what we can achieve with
                  rerandomization, especially for resolving fine
                  sub-salt structures.},
  keywords = {presentation, SINBAD,  SINBADSPRING2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Spring/tu2013SINBADrerand/tu2013SINBADrerand.pdf},
  author = {Ning Tu and Felix J. Herrmann}
}


@PRESENTATION{wason2013SINBADtjo,
  title = {Time-jittered ocean bottom seismic acquisition},
  booktitle = {SINBAD Spring consortium talks},
  organization = {SINBAD},
  year = {2013},
  abstract = {Leveraging ideas from the field of compressed sensing,
                  we show how simultaneous or blended acquisition can
                  be setup as a -- compressed sensing problem. This
                  helps us to design a pragmatic time-jittered marine
                  acquisition scheme where multiple source vessels
                  sail across an ocean-bottom array firing airguns at
                  -- jittered source locations and instances in time,
                  resulting in better spatial sampling, and speedup
                  acquisition. Furthermore, we can significantly
                  impact the reconstruction quality of conventional
                  seismic data (from jittered data) and demonstrate
                  successful recovery by sparsity promotion. In
                  contrast to random (under)sampling, acquisition via
                  jittered (under)sampling helps in controlling the
                  maximum gap size, which is a practical requirement
                  of wavefield reconstruction with localized
                  sparsifying transforms. Results are illustrated with
                  simulations of time-jittered marine acquisition,
                  which translates to jittered source locations for a
                  given speed of the source vessel.},
  keywords = {presentation, SINBAD, SINBADSPRING2013, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2013/Spring/wason2013SINBADtjo/wason2013SINBADtjo.pdf},
  author = {Haneet Wason and Felix J. Herrmann}
}


%----- 2012 (FALL) -----%

@PRESENTATION{Akalin2012SINBADlss,
  title = {Large scale seismic data interpolation with matrix completion},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  abstract = {Seismic surveys amass large and incomplete data
                  sets, and designing algorithms to interpolate the
                  missing data at very large scales poses a daunting
                  and critical challenge.  We study how to apply
                  scalable matrix completion methods to such
                  interpolation problems.  Recent studies in matrix
                  completion have shown that a matrix that has low
                  rank can be exactly completed when only a small
                  number of observations are available. However, there
                  are two challenges to applying matrix completion to
                  seismic data. Matrix completion is typically applied
                  to two dimensional or dyadic data whereas seismic
                  data is often tensorial. Also successful matrix
                  completion requires a low-rank matrix structure. We
                  address these problems by organizing the seismic
                  data on a matrix grid which exhibits a low-rank
                  structure. This encoding allows us to apply the
                  Jellyfish algorithm, developed at the University of
                  Wisconsin, which achieves state-of-the-art
                  performance for large-scale matrix completion. The
                  proposed framework makes it possible to complete
                  high-SNR interpolations of gigabytes of 4-D seismic
                  data in minutes on standard multicore
                  workstations. Our preliminary experimental results
                  suggest that matrix completion provides a promising
                  new approach to the seismic data interpolation
                  problem.},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/Akalin2012SINBADlss/Akalin2012SINBADlss_pres.pdf},
  author = {Okan Akalin}
}


@PRESENTATION{aravkin2012SINBADenp,
  title = {Estimating nuisance parameters in inverse problems},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  abstract = {Many inverse problems include nuisance parameters
                  which, while not of direct interest, are required to
                  recover primary parameters. In this talk, we present
                  the idea of "projecting out" these variables, and
                  how this idea allows us to design methods for
                  solving a broad class of problems with nuisance
                  parameters, such as variance or degrees of
                  freedom. We then discuss several geophysical
                  applications, including including estimation of
                  unknown variance parameters in the Gaussian model
                  for full waveform inversion, degree of freedom
                  (d.o.f.) parameter estimation in the context of
                  robust imaging problems, and robust source
                  estimation.},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/aravkin2012SINBADenp/aravkin2012SINBADenp_pres.pdf},
  author = {Aleksandr Y. Aravkin}
}


@PRESENTATION{aravkin2012SINBADgsf,
  title = {Generalized {SPGL1}: from theory to applications},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  abstract = {The SPGL1 solver has been effectively used for many
                  geophysical applications, including curvelet data
                  interpolation, imaging, and as a subroutine in full
                  waveform inversion. In this talk, we present an
                  overview of the theoretical foundation of the
                  solver, along with a broad generalization of this
                  foundation. We then introduce several applications,
                  including robust & sparse imaging, sparse
                  deconvolution, and data interpolation by matrix
                  completion.},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/aravkin2012SINBADgsf/aravkin2012SINBADgsf_pres.pdf},
  author = {Aleksandr Y. Aravkin}
}


@PRESENTATION{au-yeung2012SINBADcs,
  title = {Compressed sensing, random {Fourier} matrix and jitter sampling},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  abstract = {Compressed sensing is an emerging signal processing
                  technique that allows signals to be sampled well
                  below the Nyquist rate, when the signal has a sparse
                  representation in an orthonormal basis. By using a
                  random Fourier matrix or a Gaussian matrix as our
                  measurement matrix, we can reconstruct a signal from
                  far fewer measurements than required by Shannon
                  sampling theorem. In this talk, we will discuss the
                  role of uniform versus jitter sampling, both in a
                  theoretical and practical viewpoint.},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/au-yeung2012SINBADcs/au-yeung2012SINBADcs_pres.pdf},
  author = {Enrico Au-Yeung and Hassan Mansour and Ozgur Yilmaz}
}


@PRESENTATION{dasilva2012SINBADhtt,
  title = {Hierarchical {Tucker} tensor optimization - applications to {4D} seismic data interpolation},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  abstract = {There has been a swell of research in the scientific
                  computing community in the last couple of years
                  which tries to extend notions of linear algebra
                  (rank, the SVD, linear systems, etc.) to higher
                  dimensional arrays, or tensors. Much work has been
                  proposed to try to overcome the so called "curse of
                  dimensionality", the O(N^d) storage required for a
                  d-dimensional array, where N is the size of each
                  dimension. The hierarchical Tucker format is one
                  such tensor representation which manages to
                  decompose a hierarchy of dimensions into parameter
                  matrices of very manageable size, requiring at most
                  dNK + (d - 2)K^3 + K^2 parameters, where K is an
                  internal rank parameter. In this work, we extend
                  ideas of matrix completion to the tensor case, where
                  we only know a small number of randomly distributed
                  entries from various 4D frequency slices, and try to
                  recover the fully sampled tensor based on the
                  knowledge that it has low hierarchical tucker rank
                  in a particular arrangement of dimensions. Using
                  this approach, we exploit the multi-dimensional
                  dependencies within the full data in order to
                  achieve very promising interpolation results even
                  from heavily subsampled data.},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/dasilva2012SINBADhtt/dasilva2012SINBADhtt_pres.pdf},
  author = {Curt Da Silva}
}


@PRESENTATION{friedlander2012SINBADrsh,
  title = {Randomized sampling: {How} confident are you?},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  abstract = {At last year's consortium meeting, I described an
                  inexact gradient method and sampling scheme for data
                  fitting. The randomization method has good
                  convergence properties, at least as measured by the
                  distance to the solution----in expectation. But as
                  one insightful critic rightly pointed out, we don't
                  usually observe the expectation, at least not in a
                  single run. In this talk I will characterize the
                  convergence of the method in terms of bounds on the
                  probability of being too far away from the
                  solution.},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/friedlander2012SINBADrsh/friedlander2012SINBADrsh_pres.pdf},
  author = {Michael P. Friedlander}
}


@PRESENTATION{ghadermarzy2012SINBADncc,
  title = {Non-convex compressed sensing using partial support information},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  abstract = {In this talk, we will address the recovery
                  conditions of weighted $\ell_p$ minimization for
                  signal reconstruction from compressed sensing
                  measurements when (possibly inaccurate) partial
                  support information is available. First we will
                  motivate the use of (weighted) $\ell_p$ minimization
                  with $p<1$ and point out its advantages over
                  weighted $\ell_1$ minimization when there is prior
                  information on the support of the signal that is
                  possibly partial and inaccurate. Then we will
                  provide theoretical guarantees of sufficient
                  recovery conditions for weighted $\ell_p$
                  minimization, which are better than those for
                  (unweighted) $\ell_p$ minimization as well as those
                  for weighted $\ell_1$. In the last part of the talk,
                  we will illustrate our results with some numerical
                  experiments stylized applications.},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/ghadermarzy2012SINBADncc/ghadermarzy2012SINBADncc_pres.pdf},
  author = {Navid Ghadermarzy}
}


@PRESENTATION{hargreaves2012SINBADavs,
  title = {Analysis versus synthesis in weighted sparse recovery},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  abstract = {The synthesis model for compressive sensing has been the
                  model of choice for many years and various weighting
                  schemes have been shown to improve it's performance
                  (see Yilmaz, Mansour, and Ghadermarzy
                  talks). However, there is a counterpart model to
                  synthesis, namely the analysis model, which has been
                  less popular but recently attracted more attention
                  (see Lin's talk). In this talk, weighting in the
                  analysis model is discussed and applied to the
                  seismic trace interpolation problem.},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/hargreaves2012SINBADavs/hargreaves2012SINBADavs_pres.pdf},
  author = {Brock Hargreaves}
}


@PRESENTATION{herrmann2012SINBADals,
  title = {Fast sparsity-promoting imaging with message passing},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  abstract = {To meet current-day challenges, exploration
                  seismology increasingly relies on more and more
                  sophisticated algorithms that require multiple paths
                  through all data. This requirement leads to problems
                  because the size of seismic data volumes is
                  increasing exponentially, exposing bottlenecks in IO
                  and computational capability. To overcome these
                  bottlenecks, we follow recent trends in machine
                  learning and compressive sensing by proposing a
                  sparsity-promoting inversion technique that works on
                  small randomized subsets of data only. We boost the
                  performance of this algorithm significantly by
                  modifying a state-of-the-art l1-norm solver to
                  benefit from message passing, which breaks the build
                  up of correlations between model iterates and the
                  randomized linear forward model. We demonstrate the
                  performance of this algorithm on a toy
                  sparse-recovery problem and on a realistic
                  reverse-time-migration example with random source
                  encoding. The improvements in speed, memory use, and
                  output quality are truly remarkable.},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/herrmann2012SINBADals/herrmann2012SINBADals_pres.pdf},
  author = {Felix J. Herrmann}
}


@PRESENTATION{herrmann2012SINBADfwi,
  title = {Our findings on the {Chevron} benchmark dataset},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  abstract = {During this presentation, we will review our findings
                  working with the synthetic GOM data released as part
                  of the post SEG workshop:"Gulf of Mexico Imaging
                  Challenges: What Can Full Waveform Inversion
                  Achieve?". This is joint work with Andrew
                  J. Calvert, Ian Hanlon, Mostafa Javanmehri, Rajiv
                  Kumar, Tristan van Leeuwen, Xiang Li, Brendan
                  R. Smithyman, Eric Takam Takougang, and Haneet
                  Wason.},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/herrmann2012SINBADfwi/herrmann2012SINBADfwi_pres.pdf},
  author = {Andrew J. Calvert and Ian Hanlon and Mostafa Javanmehri and Rajiv Kumar and Tristan van Leeuwen and Xiang Li and Brendan R. Smithyman and Eric Takam Takougang and Haneet Wason and Felix J. Herrmann}
}


@PRESENTATION{herrmann2012SINBADhpc,
  title = {{SLIM's} perspective on {HPC} & big data},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/herrmann2012SINBADhpc/herrmann2012SINBADhpc_pres.pdf},
  author = {Tim T.Y. Lin and Tristan van Leeuwen and Felix J. Herrmann}
}


@PRESENTATION{krislock2012SINBADwsn,
  title = {Wireless sensor network localization},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  abstract = {Locating the position of sensors connected together
                  in a wireless network given only the position of a
                  small number of the sensors and estimates of some of
                  the distances between the sensors is a difficult
                  problem with many modern applications. Within the
                  last few years research in wireless sensor network
                  localization has greatly increased due to the many
                  new applications using wireless sensors, from
                  lightweight sensors used to monitor the environment
                  to ocean-bottom sensors used in geophysical
                  applications. A second reason for this increased
                  interest is our recent ability to efficiently solve
                  these problems using modern semidefinite
                  optimization solvers. We will discuss how
                  semidefinite optimization can be used to solve such
                  problems and possible directions for future work.},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/krislock2012SINBADwsn/krislock2012SINBADwsn_pres.pdf},
  author = {Nathan Krislock}
}


@PRESENTATION{kumar2012SINBADsdi,
  title = {Seismic data interpolation using {SVD} free {Pareto} curve based low rank optimization},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  abstract = {Seismic data acquisition is cursed by missing data
                  caused by physical and/or budget constraints. Aim of
                  interpolation technique is to spatially transform
                  irregularly acquired data to regularly sampled data
                  while maintaining the events coherency. While
                  transform-domain sparsity promotion has proven to be
                  an effective tool to solve this recovery problem,
                  recent developments in Rank penalizing techniques
                  opens new horizon to improved recovery by exploiting
                  low-rank structure. A major downside of current
                  state of the art techniques is their reliance on the
                  SVD of seismic data structures, which can be
                  prohibitively expensive. Fortunately, recent work
                  allows us to circumvent this problem by working with
                  matrix factorizations. We review a novel approach to
                  rank penalization, and successfully apply it to the
                  seismic interpolation problem by exploiting the
                  low-rank structure of seismic data. Experiments for
                  the recovery of 2D and 3D acquisition support the
                  feasibility and potential of the new approach.},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/kumar2012SINBADsdi/kumar2012SINBADsdi_pres.pdf},
  author = {Rajiv Kumar}
}


@PRESENTATION{vanleeuwen2012SINBAD3dfd,
  title = {{3D} frequency-domain waveform inversion using a row-projected {Helmholtz} solver},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  abstract = {3D frequency-domain full waveform inversion relies on
                  being able to efficiently solve the 3D Helmholtz
                  equation.  Iterative methods require sophisticated
                  preconditioners because the Helmholtz matrix is
                  typically indefinite. In the first part of the talk
                  I review a preconditioning technique that is based
                  on row-projections. Notable advantages of this
                  preconditioner over existing ones are that it has
                  low algorithmic complexity, is easily parallelizable
                  and extendable to time-harmonic vector equations. In
                  the second part of the talk I discuss how the
                  row-projected solver can be used in the context of
                  waveform inversion. Key aspects are: the use of
                  block-iterative methods for multiple sources and
                  adapting the accuracy of the solver.},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/vanleeuwen2012SINBAD3dfd/vanleeuwen2012SINBAD3dfd_pres.pdf},
  author = {Tristan van Leeuwen}
}


@PRESENTATION{vanleeuwen2012SINBADyap,
  title = {Yet another perspective on image volumes},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  abstract = {An extended image is defined as the multi-dimensional
                  cross-correlation of the source and receiver
                  wavefields used for imaging. This extended image
                  will reveal velocity errors by de-focusing and can
                  thus be used for velocity analysis. However, for
                  optimal sensitivity to velocity errors, the
                  subsurface offset has to be aligned with the local
                  dip. As this dip is not known a priori, we consider
                  forming the extended image for subsurface offsets in
                  all directions. However, computing and storing such
                  a large image volume is not computationally
                  feasible.  We organize the image volume in a matrix
                  and use matrix-probing techniques to glean
                  information form the matrix without explicitly
                  forming it. A matrix-vector multiply with the
                  image-volume matrix can be performed at the cost of
                  two wave-equation solves and does not require any
                  explicit cross-correlations of the wavefields. Such
                  techniques can also be used to evaluate focusing
                  penalties without forming the whole image volume.
                  Finally, the matrix-viewpoint allows us to derive a
                  2-way equivalent of the DSR equation in a
                  straightforward manner and provides a possible
                  avenue for developing new velocity-continuation
                  techniques.},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/vanleeuwen2012SINBADyap/vanleeuwen2012SINBADyap_pres.pdf},
  author = {Tristan van Leeuwen}
}


@PRESENTATION{lin2012SINBADics,
  title = {An introduction to cosparse signal reconstruction},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  abstract = {Undersampling techniques in exploration seismology
                  usually relies on the assumption that seismic
                  records and images permit sparse approximations
                  under certain representations, such as Curvelet
                  coefficients. Recent findings have suggested that
                  for redundant representations (of which Curvelet is
                  an example), the analysis operator that maps the
                  physical signal to coefficients may also play a
                  crucial role in recovering data from incomplete
                  observations. In particular, the number of
                  zero-valued coefficients given by the analysis
                  operator acting on the signal, referred to as its
                  "cosparsity", have an analogous role to the sparsity
                  of the signal in terms of the coefficients. The
                  cosparsity of the signal permits recovery guarantees
                  that are completely separate from sparsity-based
                  models, and gives rise to distinct sets of
                  reconstruction algorithms and performances compared
                  to sparsity-based approaches. We present in this
                  talk some initial findings on the viability of
                  cosparse reconstruction for a variety of seismic
                  applications that previously relied on sparse signal
                  reconstruction, such as data interpolation and
                  source separation.},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/lin2012SINBADics/lin2012SINBADics_pres.pdf},
  author = {Tim T.Y. Lin}
}


@PRESENTATION{Lin2012SINBADrdr,
  title = {Recent developments on the robust estimation of primaries by sparse inversion},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  abstract = {Robust estimation of primaries by sparse inversion
                  is a next-generation surface multiple removal
                  technique with an objective to truly invert an
                  operator that models the free-surface. Key to the
                  success of this approach is the imposition of a
                  sparsity constraint on the primary impulse response
                  in the time domain. This is accomplished by
                  carefully applying large-scale convex optimization
                  techniques on an extended L1 minimization
                  problem. One of the benefits of our approach is that
                  many extensions to the algorithm can be devised
                  under this optimization framework to improve the
                  quality of the solution given fixed computational
                  costs and mitigating various shortcomings in field
                  data. This talk will first review the basic
                  technique of Robust EPSI and follow with some
                  highlights on recent further developments of the
                  algorithm, including a discussion on the role of
                  regularization by reciprocity and the interpolation
                  of near-offset data, as well as investigations into
                  optimality and robustness to data outliers.},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/Lin2012SINBADrdr/Lin2012SINBADrdr_pres.pdf},
  author = {Tim T.Y. Lin}
}


@PRESENTATION{mansour2012SINBADsti,
  title = {Seismic trace interpolation via sparsity promoting reweighted algorithms},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  abstract = {Missing-trace interpolation aims to reconstruct
                  regularly sampled wavefields from periodically
                  sampled data with gaps caused by physical
                  constraints. While transform-domain sparsity
                  promotion has proven to be an effective tool to
                  solve this recovery problem, current recovery
                  techniques make no use of a priori information on
                  the transform-domain coefficients. To overcome these
                  vulnerabilities in solving the recovery problem for
                  large-scale problems, we propose recovery by
                  weighted one-norm minimization, which exploits
                  correlations between locations of significant
                  coefficients of different partitions, e.g., shot
                  records, common-offset gathers, or frequency slices,
                  of the acquired data. Moreover, in situations where
                  no prior support estimate is available, we propose
                  the WSPGL1 algorithm that outperforms standard
                  $\ell_1$ minimization in finding sparse solutions to
                  underdetermined linear systems of equations. Our
                  algorithm is a modification of the SPGL1 algorithm
                  and enjoys better sparse recovery performance at no
                  additional computational cost. We illustrate the
                  improved recovery using WSPGL1 for randomly
                  subsampled seismic traces.},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/mansour2012SINBADsti/mansour2012SINBADsti_pres.pdf},
  author = {Hassan Mansour}
}


@PRESENTATION{miao2012SINBADasp,
  title = {Accelerating on sparse promoting recovery and its benefits in seismic application},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  abstract = {Sparse promoting recovery problem arises more and more
                  frequently with the broad application of compressed
                  sensing tool in exploration seismology. Because of
                  the curse of dimensionality, the prohibitive
                  computation burden on iteratively evaluating
                  objective functions is one of the key issues that
                  constrain high performance l1 solver. In this paper,
                  we try to further improve the convergence
                  performance of SPGl1, one of the state-of-the-art
                  large scale sparse recovery solver, and as a result
                  limit the number of objective function evaluations
                  by introducing a projected quasi Newton
                  method. Examples showing acceleration on seismic
                  data collection, data processing as well as
                  inversion are included.},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/miao2012SINBADasp/miao2012SINBADasp_pres.pdf},
  author = {Lina Miao}
}


@PRESENTATION{ning2012SINBADfim,
  title = {Fast imaging with multiples},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  abstract = {If correctly used, multiple energy can be mapped to
                  the correct subsurface locations. However, simply
                  applying the cross-correlation imaging condition
                  will introduce non-causal artifacts into the final
                  image. Here we propose an inversion approach to
                  image primaries and multiples simultaneously that
                  yields an artifact-free image. To address the high
                  computational cost associated with inversion, we
                  propose to: i) have the wave-equation solver carry
                  out the multi-dimensional convolutions implicitly,
                  and ii) reduce the number of PDE solves by
                  randomized subsampling. We then propose to improve
                  the overall performance of this algorithm by a
                  process called rerandomization, which helps to
                  cancel the correlation built between model iterate
                  and the subsampling operator. We show the merits of
                  our approach on a number of examples.},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/ning2012SINBADfim/ning2012SINBADfim_pres.pdf},
  author = {Ning Tu}
}


@PRESENTATION{oghenekohwo2012SINBADcs,
  title = {Compressed sensing: a tool for eliminating repeatability in acquisition of {4D} (time-lapse) seismic data},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  abstract = {In 4D (time-lapse) seismic data acquisition, a very
                  significant step is the repeatability of the
                  acquisition process. In other words, the geophones
                  must be placed at the exact location as they were,
                  during baseline survey and acquisition. This
                  condition is required to be able to produce an image
                  of the same location over time and this enhances a
                  proper reservoir characterization. The cost of
                  repeating the seismic acquisition is very expensive,
                  as geophones (receivers) have to be left at the same
                  location over the period for which the data will be
                  acquired. In this talk, we attempt to highlight the
                  effort of Compressed Sensing, to eliminate this
                  condition of repeatability of the acquisition. We
                  show that a random sampling of the shots or random
                  placement of the geophones is able to reproduce the
                  same image over time, hence eliminating any
                  acquisition imprints on the final seismic image.},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/oghenekohwo2012SINBADcs/oghenekohwo2012SINBADcs_pres.pdf},
  author = {Felix Oghenekohwo}
}


@PRESENTATION{peters2012SINBADfde,
  title = {Frequency domain {3D} elastic wave propagation in general anisotropic media},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  abstract = {Elastic wave propagation in 3 spatial dimensions is
                  modeled using a wave equation containing the full
                  stiffness tensor consisting of 21 independent
                  components. This allows modeling in general
                  anisotropic media. The wave equation is discretized
                  on several Cartesian and rotated Cartesian staggered
                  finite-difference grids (using a 2nd order
                  approximation). The grids are linearly combined and,
                  in combination with a antilumped mass strategy,
                  minimize numerical dispersion while requiring a low
                  number of grid points per wavelength. In case not
                  all 21 components need to be modeled, an
                  approximation of the stiffness tensor can be used
                  (e.g., orthorhombic anisotropy, TTI, ...). This
                  results in a linear system of equations, which is
                  solved using an iterative method. The modeling of
                  all 21 components of the stiffness tensor (or an
                  approximation) enables the development of new
                  waveform inversion functionalities.},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/peters2012SINBADfde/peters2012SINBADfde_pres.pdf},
  author = {Bas Peters}
}


@PRESENTATION{petrenko2012SINBADcarp,
  title = {{CARP-CG}: a computational study},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  abstract = {Forward modelling of the wave equation is a key
                  ingredient in seismic full waveform inversion
                  (FWI). Simulation in the time domain and solution of
                  the wave equation in the frequency domain are two
                  competing approaches to modelling. Frequency domain
                  approaches can further be categorized as using
                  either direct or iterative solvers. For 3D FWI,
                  iterative solvers in the frequency domain are
                  attractive, partly because they require less memory
                  than the other methods. This is due to the fact that
                  there is no need to store the wavefield at each time
                  step, or compute a factorization of the Helmholtz
                  operator that will not be as sparse as the original
                  matrix. One iterative solver that has been applied
                  to the Helmholtz equation is CARP-CG. CARP-CG uses
                  Kaczmarz row projections for each block of a domain
                  decomposition scheme to precondition the Helmholtz
                  system into being symmetric and positive
                  semidefinite. The method of conjugate gradients is
                  then used to solve the preconditioned system. We
                  present a comparison of the performance of CARP-CG
                  implemented in several languages (MATLAB, C,
                  FORTRAN, julia) and in two different hardware
                  environments: the LIMA HPC cluster hosted at UBC,
                  and the Checkers cluster which is part of the
                  Westgrid consortium. Parallelization of the
                  algorithm via domain decomposition implemented with
                  MPI (distributed memory) and OMP (shared memory) is
                  also examined.},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/petrenko2012SINBADcarp/petrenko2012SINBADcarp_pres.pdf},
  author = {Art Petrenko}
}


@PRESENTATION{tamalet2012SINBADvpe,
  title = {Variance parameters estimation - application to full waveform inversion},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  abstract = {Many inverse problems include nuisance parameters. While
                  not of direct interest, these parameters are
                  required to recover primary parameters. In order to
                  estimate these nuisance parameters as well as the
                  primary parameters in large-scale inverse problems,
                  a method based on variable projection, which
                  consists in projecting out a subset over the
                  variables, has been developed. We present here the
                  application of this method to the problem of
                  variance parameters estimation in multiple datasets,
                  which is an important problem in many areas
                  including geophysics. More precisely, we apply the
                  method to Full Waveform Inversion and demonstrate
                  the improvement in recovery of the model parameters
                  in the case where the variance of the noise
                  increases with the frequency.},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/tamalet2012SINBADvpe/tamalet2012SINBADvpe_pres.pdf},
  author = {Anais Tamalet}
}


@PRESENTATION{warner2012SINBADafw,
  title = {Anisotropic {3D} full-waveform inversion of the {Tommeliten} {Alpha} field},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  abstract = {We have implemented a robust and practical scheme for
                  anisotropic 3D acoustic full-waveform inversion. We
                  demonstrate this scheme on a field data set,
                  applying it to a four-component ocean-bottom survey
                  over the Tommeliten Alpha field in the North
                  Sea. This shallow-water data set provides good
                  azimuthal coverage to offsets of 7 km, with reduced
                  coverage to a maximum offset of about 11 km. The
                  reservoir lies at the crest of a high-velocity
                  antiformal chalk section, overlain by about 3000 m
                  of clastics within which a low-velocity gas cloud
                  produces a seismic obscured area. We inverted only
                  the hydrophone data, and we retained free-surface
                  multiples and ghosts within the field data. We
                  invert in six narrow frequency bands, in the range 3
                  to 6.5 Hz. At each iteration, we selected only a
                  subset of sources, using a different subset at each
                  iteration; this strategy is more efficient than
                  inverting all the data every iteration. Our starting
                  velocity model was obtained using standard PSDM
                  model building including anisotropic reflection
                  tomography, and contained epsilon values as high as
                  20\%. We have also attempted full-elastic inversion
                  of these data to recover a shallow isotropic model
                  of both p and s-wave velocities. The final FWI
                  velocity model shows a network of shallow
                  high-velocity channels that match similar features
                  in the reflection data. Deeper in the section, the
                  FWI velocity model reveals a sharper and
                  more-intense low-velocity region associated with the
                  gas cloud in which low-velocity fingers match the
                  location of gas-filled faults visible in the
                  reflection data. The resulting velocity model
                  provides a better match to well logs, and better
                  flattens common image gathers, than does the
                  starting model. Reverse-time migration, using the
                  FWI velocity model, provides significant uplift to
                  the migrated image, simplifying the planform of the
                  reservoir section at depth. The workflows, inversion
                  strategy, and algorithms that we have used have
                  broad application to invert a wide-range of
                  analogous field data sets.},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/warner2012SINBADafw/warner2012SINBADafw_pres.pdf},
  author = {Mike Warner}
}


@PRESENTATION{wason2012SINBADobs,
  title = {Ocean bottom seismic acquisition via jittered sampling},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  abstract = {We present a pragmatic marine acquisition scheme
                  where a single (or multiple) vessel sails across an
                  ocean-bottom array firing airguns at ? optimally
                  jittered source locations and instances in
                  time. Following the principles of compressive
                  sensing, we can significantly impact the
                  reconstruction quality of conventional seismic data
                  (from jittered data) and demonstrate successful
                  recovery by sparsity promotion. In contrast to
                  random (under)sampling, acquisition via jittered
                  (under)sampling helps in controlling the maximum gap
                  size, which is a practical requirement of wavefield
                  reconstruction with localized sparsifying
                  transforms. Results are illustrated with simulations
                  of optimally jittered marine acquisition, and
                  periodic time-dithering marine acquisition.},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/wason2012SINBADobs/wason2012SINBADobs_pres.pdf},
  author = {Haneet Wason}
}


@PRESENTATION{xiang2012SINBADfgn,
  title = {Fast {Gauss-Newton} full-waveform inversion with sparsity regularization},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  abstract = {Full-waveform inversion (FWI) can be considered as a
                  controlled data fitting process, in which we
                  approximately fit observed data by iteratively
                  updating the initial velocity model, we expect the
                  final model can reveal subsurface structure till the
                  wavefield misfit can converge to designed tolerance.
                  The conventional FWI approach is expensive since it
                  requires the inversion of a linear system, which
                  involves extremely large multi-experiment data
                  volumes. To overcome this issue we percent a
                  curvetlet based sparsity-promoting Gauss-Newton
                  inversion method. In this presentation we invert for
                  the model updates by replacing the normal
                  Gauss-Newton linearized subproblem for subsampled
                  FWI with a sparsity promoting FWI formulation. We
                  speed up the algorithm and avoid over fitting the
                  data by solving the problem approximately. Aside
                  from this, we control wavefield dispersion by
                  gradually increasing grid size as we move to higher
                  frequencies. Our approach is successful because it
                  reduces the size of seismic data volumes without
                  loss of information. With this reduction, we can
                  compute a Newton-like update with the reduced data
                  volume at the cost of roughly one gradient update
                  for the fully sampled wavefield.},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/xiang2012SINBADfgn/xiang2012SINBADfgn_pres.pdf},
  author = {Xiang Li}
}


@PRESENTATION{xiang2012SINBADweb,
  title = {Wave-equation based inversion with joint sparsity promotion},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/xiang2012SINBADweb/xiang2012SINBADweb_pres.pdf},
  author = {Xiang Li}
}


@PRESENTATION{yilmaz2012SINBADwms,
  title = {Weighted methods in sparse recovery},
  booktitle = {SINBAD Fall consortium talks},
  year = {2012},
  organization = {SINBAD},
  abstract = {In the recent years we have successfully employed
                  "weighted" algorithms to recover sparse signals from
                  few linear, non-adaptive measurements. The general
                  principle here is to use prior knowledge about the
                  signal to be recovered, e.g., approximate locations
                  of large-in-magnitude transform coefficients, if
                  such information is available. An example for this
                  is the use of weighted 1-norm minimization to
                  improve wavefield reconstruction from randomized
                  (sub)sampling. We will review these results and
                  outline some new directions we have explored during
                  the last year, such as weighted non-convex sparse
                  recovery (see Ghadermarzy's talk), weighted
                  analysis-based recovery (see Hargreaves's talk), and
                  a weighted randomized Kaczmarz algorithm for solving
                  large overdetermined systems of equations that are
                  known to admit a (nearly) sparse solution. Various
                  examples in seismic will be shown.},
  keywords = {presentation, SINBAD, SINBADFALL2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Fall/yilmaz2012SINBADwms/yilmaz2012SINBADwms_pres.pdf},
  author = {Ozgur Yilmaz}
}


%----- 2012 (SPRING) -----%

@PRESENTATION{aravkin2012SINBADipu,
  title = {Inverse problems using {Student's} t},
  booktitle = {SINBAD Spring consortium talks},
  year = {2012},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADSPRING2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Spring/aravkin2012SINBADipu/aravkin2012SINBADipu_pres.pdf},
  author = {Aleksandr Y. Aravkin}
}


@PRESENTATION{dasilva2012SINBADrdp,
  title = {Recent developments in preconditioning the wave-equation {Hessian}},
  booktitle = {SINBAD Spring consortium talks},
  year = {2012},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADSPRING2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Spring/dasilva2012SINBADrdp/dasilva2012SINBADrdp_pres.pdf},
  author = {Curt Da Silva}
}


@PRESENTATION{herrmann2012SINBADlds,
  title = {Latest developments in seismic-data recovery},
  booktitle = {SINBAD Spring consortium talks},
  year = {2012},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADSPRING2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Spring/herrmann2012SINBADlds/herrmann2012SINBADlds_pres.pdf},
  author = {Felix J. Herrmann}
}


@PRESENTATION{herrmann2012SINBADsls,
  title = {Supercool(ed) least-squares imaging: latest insights in sparsity-promoting migration},
  booktitle = {SINBAD Spring consortium talks},
  year = {2012},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADSPRING2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Spring/herrmann2012SINBADsls/herrmann2012SINBADsls_pres.pdf},
  author = {Felix J. Herrmann}
}


@PRESENTATION{ning2012SINBADSPRINGfim,
  title = {Fast imaging with multiples by sparse inversion},
  booktitle = {SINBAD Spring consortium talks},
  year = {2012},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADSPRING2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Spring/ning2012SINBADSPRINGfim/ning2012SINBADSPRINGfim_pres.pdf},
  author = {Ning Tu}
}


@PRESENTATION{vanleeuwen2012SINBADoof,
  title = {An object-oriented framework for frequency-domain {FWI}},
  booktitle = {SINBAD Spring consortium talks},
  year = {2012},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADSPRING2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Spring/vanleeuwen2012SINBADoof/vanleeuwen2012SINBADoof_pres.pdf},
  author = {Tristan van Leeuwen}
}


@PRESENTATION{wason2012SINBADrma,
  title = {Randomized marine acquisition for ocean-bottom surveys},
  booktitle = {SINBAD Spring consortium talks},
  year = {2012},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADSPRING2012, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2012/Spring/wason2012SINBADrma/wason2012SINBADrma_pres.pdf},
  author = {Haneet Wason}
}


%----- 2011 (FALL) -----%

@PRESENTATION{aravkin2011SINBADesp,
  title = {Extensions to sparsity promotion},
  booktitle = {SINBAD Fall consortium talks},
  year = {2011},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2011, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Fall/aravkin2011SINBADesp/aravkin2011SINBADesp_pres.pdf},
  author = {Aleksandr Y. Aravkin}
}


@PRESENTATION{aravkin2011SINBADrfwi,
  title = {Robust {FWI} using {Student's} t & robust source estimation},
  booktitle = {SINBAD Fall consortium talks},
  year = {2011},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2011, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Fall/aravkin2011SINBADrfwi/aravkin2011SINBADrfwi_pres.pdf},
  author = {Aleksandr Y. Aravkin}
}


@PRESENTATION{aravkin2011SINBADrsp,
  title = {A randomized, sparsity promoting, Gauss-Newton algorithm for seismic waveform inversion},
  booktitle = {SINBAD Fall consortium talks},
  year = {2011},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2011, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Fall/aravkin2011SINBADrsp/aravkin2011SINBADrsp_pres.pdf},
  url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Fall/aravkin2011SINBADrsp/li2011SINBADrsp_pres.pdf},
  author = {Aleksandr Y. Aravkin and Xiang Li}
}


@PRESENTATION{dasilva2011SINBADrdp,
  title = {Recent developments in preconditioning the {FWI} {Hessian}},
  booktitle = {SINBAD Fall consortium talks},
  year = {2011},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2011, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Fall/dasilva2011SINBADrdp/dasilva2011SINBADrdp_pres.pdf},
  author = {Curt Da Silva}
}


@PRESENTATION{friedlander2011SINBADrir,
  title = {Robust inversion, data-fitting, and inexact gradient methods},
  booktitle = {SINBAD Fall consortium talks},
  year = {2011},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2011, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Fall/friedlander2011SINBADrir/friedlander2011SINBADrir_pres.pdf},
  author = {Michael P. Friedlander}
}


@PRESENTATION{herrmann2011SINBADcoc,
  title = {Challenges and opportunities for compressive sensing in seismic acquisition},
  booktitle = {SINBAD Fall consortium talks},
  year = {2011},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2011, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Fall/herrmann2011SINBADcoc/herrmann2011SINBADcoc_pres.pdf},
  author = {Felix J. Herrmann}
}


@PRESENTATION{herrmann2011SINBADcos,
  title = {Challenges and opportunities in sparse wavefield inversion},
  booktitle = {SINBAD Fall consortium talks},
  year = {2011},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2011, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Fall/herrmann2011SINBADcos/herrmann2011SINBADcos_pres.pdf},
  author = {Felix J. Herrmann}
}


@PRESENTATION{herrmann2011SINBADoverview,
  title = {Welcome and overview of {SINBAD} & {DNOISE}},
  booktitle = {SINBAD Fall consortium talks},
  year = {2011},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2011, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Fall/herrmann2011SINBADoverview/herrmann2011SINBADoverview_pres.pdf},
  author = {Felix J. Herrmann}
}


@PRESENTATION{herrmann2011SINBADrnr,
  title = {To redraw or not to redraw: recent insights in randomized dimensionality reduction for inversion},
  booktitle = {SINBAD Fall consortium talks},
  year = {2011},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2011, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Fall/herrmann2011SINBADrnr/herrmann2011SINBADrnr_pres.pdf},
  author = {Felix J. Herrmann}
}


@PRESENTATION{jumah2011SINBADdrEPSI,
  title = {Dimensionality-reduced estimation of primaries by sparse inversion},
  booktitle = {SINBAD Fall consortium talks},
  year = {2011},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2011, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Fall/jumah2011SINBADdrEPSI/jumah2011SINBADdrEPSI_pres.pdf},
  author = {Bander Jumah}
}

@PRESENTATION{li2011SINBADels,
  title = {Efficient least-squares imaging with sparsity promotion and compressive sensing},
  booktitle = {SINBAD Fall consortium talks},
  year = {2011},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2011, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Fall/li2011SINBADels/li2011SINBADels_pres.pdf},
  author = {Xiang Li}
}

@PRESENTATION{lin2011SINBADirEPSI,
  title = {Inside the robust {EPSI} formulation},
  booktitle = {SINBAD Fall consortium talks},
  year = {2011},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2011, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Fall/lin2011SINBADirEPSI/lin2011SINBADirEPSI_pres.pdf},
  author = {Tim T.Y. Lin}
}


@PRESENTATION{lin2011SINBADslim,
  title = {{SLIM's} software design principles},
  booktitle = {SINBAD Fall consortium talks},
  year = {2011},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2011, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Fall/lin2011SINBADslim/lin2011SINBADslim_pres.pdf},
  author = {Tim T.Y. Lin}
}


@PRESENTATION{mansour2011SINBADwcw,
  title = {Why do curvelets work?},
  booktitle = {SINBAD Fall consortium talks},
  year = {2011},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2011, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Fall/mansour2011SINBADwcw/mansour2011SINBADwcw_pres.pdf},
  author = {Hassan Mansour}
}


@PRESENTATION{min2011SINBADpss,
  title = {Parameter-selection strategy for density in frequency-domain elastic waveform inversion},
  booktitle = {SINBAD Fall consortium talks},
  year = {2011},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2011, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Fall/min2011SINBADpss/min2011SINBADpss_pres.pdf},
  author = {Dong-Joo Min}
}


@PRESENTATION{ning2011SINBADmsr,
  title = {Migration from surface-related multiples},
  booktitle = {SINBAD Fall consortium talks},
  year = {2011},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2011, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Fall/ning2011SINBADmsr/ning2011SINBADmsr_pres.pdf},
  author = {Ning Tu}
}


@PRESENTATION{vanderneut2011SINBADirs,
  title = {Interferometric redatuming by sparse inversion},
  booktitle = {SINBAD Fall consortium talks},
  year = {2011},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2011, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Fall/vanderneut2011SINBADirs/vanderneut2011SINBADirs_pres.pdf},
  author = {Joost van der Neut}
}


@PRESENTATION{vanleeuwen2011SINBADfwi,
  title = {Fast waveform inversion without source encoding},
  booktitle = {SINBAD Fall consortium talks},
  year = {2011},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2011, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Fall/vanleeuwen2011SINBADfwi/vanleeuwen2011SINBADfwi_pres.pdf},
  author = {Tristan van Leeuwen}
}


@PRESENTATION{vanleeuwen2011SINBADoofwi,
  title = {Wavefield modelling and inversion in {Matlab}},
  booktitle = {SINBAD Fall consortium talks},
  year = {2011},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2011, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Fall/vanleeuwen2011SINBADoofwi/vanleeuwen2011SINBADoofwi_pres.pdf},
  author = {Tristan van Leeuwen}
}


@PRESENTATION{vanleeuwen2011SINBADtem,
  title = {Towards extended modelling for velocity inversion},
  booktitle = {SINBAD Fall consortium talks},
  year = {2011},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2011, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Fall/vanleeuwen2011SINBADtem/vanleeuwen2011SINBADtem_pres.pdf},
  author = {Tristan van Leeuwen}
}


@PRESENTATION{wason2011SINBADode,
  title = {Only dither: efficient marine acquisition ``without" simultaneous sourcing},
  booktitle = {SINBAD Fall consortium talks},
  year = {2011},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2011, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Fall/wason2011SINBADode/wason2011SINBADode_pres.pdf},
  author = {Haneet Wason}
}


@PRESENTATION{yilmaz2011SINBADcsp,
  title = {Compressed sensing with prior information},
  booktitle = {SINBAD Fall consortium talks},
  year = {2011},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2011, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Fall/yilmaz2011SINBADcsp/yilmaz2011SINBADcsp_pres.pdf},
  author = {Ozgur Yilmaz}
}


%----- 2011 (SPRING) -----%

@PRESENTATION{aravkin2011SINBADSPRINGrfwi,
  title = {Robust {FWI} using {Student's} t-distribution},
  booktitle = {SINBAD Spring consortium talks},
  year = {2011},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADSPRING2011, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Spring/aravkin2011SINBADSPRINGrfwi/aravkin2011SINBADSPRINGrfwi_pres.pdf},
  author = {Aleksandr Y. Aravkin}
}


@PRESENTATION{herrmann2011SINBADdre,
  title = {Dimensionality-reduced estimation of primaries by sparse inversion},
  booktitle = {SINBAD Spring consortium talks},
  year = {2011},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBASPRING2011, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Spring/herrmann2011SINBADdre/herrmann2011SINBADdre_pres.pdf},
  author = {Felix J. Herrmann}
}


@PRESENTATION{lin2011SINBADrep,
  title = {Robust {EPSI} in a curvelet-like representation domain},
  booktitle = {SINBAD Spring consortium talks},
  year = {2011},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADSPRING2011, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Spring/lin2011SINBADrep/lin2011SINBADrep_pres.pdf},
  author = {Tim T.Y. Lin}
}


@PRESENTATION{vanleeuwen2011SINBADfwiso,
  title = {A hybrid stochastic-deterministic method for waveform inversion},
  booktitle = {SINBAD Spring consortium talks},
  year = {2011},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADSPRING2011, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Spring/vanleeuwen2011SINBADfwiso/vanleeuwen2011SINBADfwiso_pres.pdf},
  author = {Tristan van Leeuwen}
}


@PRESENTATION{vanleeuwen2011SINBADpei,
  title = {Probing the extended image volume},
  booktitle = {SINBAD Spring consortium talks},
  year = {2011},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADSPRING2011, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2011/Spring/vanleeuwen2011SINBADpei/vanleeuwen2011SINBADpei_pres.pdf},
  author = {Tristan van Leeuwen}
}


%----- 2010 (FALL) -----%

@PRESENTATION{almatar2010SINBADesf,
  title = {Estimation of surface-free data by curvelet-domain matched filtering and sparse inversion},
  booktitle = {SINBAD Fall consortium talks},
  year = {2010},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2010, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2010/Fall/almatar2010SINBADesf/almatar2010SINBADesf_pres.pdf},
  author = {Mufeed H. AlMatar}
}


@PRESENTATION{aravkin2010SINBADesf,
  title = {Exploiting sparsity in full-waveform inversion: nonlinear basis pursuit denoise algorithm},
  booktitle = {SINBAD Fall consortium talks},
  year = {2010},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2010, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2010/Fall/aravkin2010SINBADesf/aravkin2010SINBADesf_pres.pdf},
  author = {Aleksandr Y. Aravkin}
}


@PRESENTATION{aravkin2010SINBADicc,
  title = {Introduction to convex composite optimization},
  booktitle = {SINBAD Fall consortium talks},
  year = {2010},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2010, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2010/Fall/aravkin2010SINBADicc/aravkin2010SINBADicc_pres.pdf},
  author = {Aleksandr Y. Aravkin}
}


@PRESENTATION{friedlander2010SINBADaso,
  title = {Algorithms for sparse optimization},
  booktitle = {SINBAD Fall consortium talks},
  year = {2010},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2010, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2010/Fall/friedlander2010SINBADaso/friedlander2010SINBADaso_pres.pdf},
  author = {Michael P. Friedlander}
}


@PRESENTATION{friedlander2010SINBADisl,
  title = {Introduction to {Spot}: a linear-operator toolbox},
  booktitle = {SINBAD Fall consortium talks},
  year = {2010},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2010, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2010/Fall/friedlander2010SINBADisl/friedlander2010SINBADisl_pres.pdf},
  author = {Michael P. Friedlander}
}


@PRESENTATION{herrmann2010SINBADcss,
  title = {Compressive sensing and sparse recovery in exploration seismology},
  booktitle = {SINBAD Fall consortium talks},
  year = {2010},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2010, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2010/Fall/herrmann2010SINBADcss/herrmann2010SINBADcss_pres.pdf},
  author = {Felix J. Herrmann}
}


@PRESENTATION{herrmann2010SINBADdrf,
  title = {Dimensionality reduction for full-waveform inversion},
  booktitle = {SINBAD Fall consortium talks},
  year = {2010},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2010, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2010/Fall/herrmann2010SINBADdrf/herrmann2010SINBADdrf_pres.pdf},
  author = {Felix J. Herrmann}
}


@PRESENTATION{kumar2010SINBADpoe,
  title = {Parallelizing operations with ease using Parallel {SPOT}},
  booktitle = {SINBAD Fall consortium talks},
  year = {2010},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2010, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2010/Fall/kumar2010SINBADpoe/kumar2010SINBADpoe_pres.pdf},
  author = {Nameet Kumar}
}


@PRESENTATION{li2010SINBADci,
  title = {Compressive imaging},
  booktitle = {SINBAD Fall consortium talks},
  year = {2010},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2010, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2010/Fall/li2010SINBADci/li2010SINBADci_pres.pdf},
  author = {Xiang Li}
}


@PRESENTATION{li2010SINBADfwi,
  title = {Full-waveform inversion with randomized {L1} recovery for the model updates},
  booktitle = {SINBAD Fall consortium talks},
  year = {2010},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2010, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2010/Fall/li2010SINBADfwi/li2010SINBADfwi_pres.pdf},
  author = {Xiang Li}
}


@PRESENTATION{lin2010SINBADlib,
  title = {Leveraging informed blind deconvolution techniques for the estimation of primaries by sparse inversion},
  booktitle = {SINBAD Fall consortium talks},
  year = {2010},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2010, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2010/Fall/lin2010SINBADlib/lin2010SINBADlib_pres.pdf},
  author = {Tim T.Y. Lin}
}


@PRESENTATION{lin2010SINBADsol,
  title = {Sparse optimization and the {L1} norm},
  booktitle = {SINBAD Fall consortium talks},
  year = {2010},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2010, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2010/Fall/lin2010SINBADsol/lin2010SINBADsol_pres.pdf},
  author = {Tim T.Y. Lin}
}


@PRESENTATION{lin2010SINBADsre,
  title = {Software release: estimation of primaries by {L1} inversion},
  booktitle = {SINBAD Fall consortium talks},
  year = {2010},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2010, SLIM},
  author = {Tim T.Y. Lin}
}


@PRESENTATION{mansour2010SINBADrcs,
  title = {Recovering compressively sampled signals using partial support information},
  booktitle = {SINBAD Fall consortium talks},
  year = {2010},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2010, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2010/Fall/mansour2010SINBADrcs/mansour2010SINBADrcs_pres.pdf},
  author = {Hassan Mansour}
}


@PRESENTATION{modzelewski2010SINBADsra,
  title = {Software releases and architecture},
  booktitle = {SINBAD Fall consortium talks},
  year = {2010},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2010, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2010/Fall/modzelewski2010SINBADsra/modzelewski2010SINBADsra_pres.pdf},
  author = {Henryk Modzelewski}
}


@PRESENTATION{moghaddam2010SINBADrfwi,
  title = {Randomized full-waveform inversion},
  booktitle = {SINBAD Fall consortium talks},
  year = {2010},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2010, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2010/Fall/moghaddam2010SINBADrfwi/moghaddam2010SINBADrfwi_pres.pdf},
  author = {Peyman P. Moghaddam}
}


@PRESENTATION{pacteau2010SINBADkpo,
  title = {Kronecker product optimization in {SPOT}},
  booktitle = {SINBAD Fall consortium talks},
  year = {2010},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2010, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2010/Fall/pacteau2010SINBADkpo/pacteau2010SINBADkpo_pres.pdf},
  author = {Sebastien Pacteau}
}


@PRESENTATION{saab2010SINBADcsk,
  title = {Compressed sensing using {Kronecker} products},
  booktitle = {SINBAD Fall consortium talks},
  year = {2010},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2010, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2010/Fall/saab2010SINBADcsk/saab2010SINBADcsk_pres.pdf},
  author = {Rayan Saab}
}


@PRESENTATION{schmidt2010SINBADhsd,
  title = {Hybrid stochastic-deterministic methods},
  booktitle = {SINBAD Fall consortium talks},
  year = {2010},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2010, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2010/Fall/schmidt2010SINBADhsd/schmidt2010SINBADhsd_pres.pdf},
  author = {Mark Schmidt}
}


@PRESENTATION{tu2010SINBADspm,
  title = {Sparsity promoting migration with surface-related multiples},
  booktitle = {SINBAD Fall consortium talks},
  year = {2010},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2010, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2010/Fall/tu2010SINBADspm/tu2010SINBADspm_pres.pdf},
  author = {Ning Tu}
}


@PRESENTATION{vanleeuwen2010SINBADwis,
  title = {Waveform inversion by stochastic optimization},
  booktitle = {SINBAD Fall consortium talks},
  year = {2010},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2010, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2010/Fall/vanleeuwen2010SINBADwis/vanleeuwen2010SINBADwis_pres.pdf},
  author = {Tristan van Leeuwen}
}


@PRESENTATION{wason2010SINBADssd,
  title = {Sequential source data recovery from simultaneous acquisition through transform-domain sparsity promotion - curvelet versus shearlet transform},
  booktitle = {SINBAD Fall consortium talks},
  year = {2010},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2010, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2010/Fall/wason2010SINBADssd/wason2010SINBADssd_pres.pdf},
  author = {Haneet Wason}
}


@PRESENTATION{yilmaz2010SINBADsac,
  title = {Sparse approximations and compressive sensing: an overview},
  booktitle = {SINBAD Fall consortium talks},
  year = {2010},
  organization = {SINBAD},
  keywords = {presentation, SINBAD, SINBADFALL2010, SLIM},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2010/Fall/yilmaz2010SINBADsac/yilmaz2010SINBADsac_pres.pdf},
  author = {Ozgur Yilmaz}
}


%----- 2010 (SPRING) -----%


% This file was created with JabRef 2.9.
% Encoding: MacRoman

@MASTERSTHESIS{zhang2018THlss,
  author = {Yiming Zhang},
  title = {Large-scale seismic data compression: application to full waveform inversion and extended image volume},
  school = {The University of British Columbia},
  year = {2018},
  month = {04},
  address = {Vancouver},
  abstract = {Conventional oil and gas fields are increasingly difficult to explore and im- age, resulting in a call for more complex wave-equation based inversion al- gorithms that require dense long-o↵set samplings. Consequently, there is an exponential growth in the size of data volumes and prohibitive demands on computational resources. In this work, we propose a method to com- press and process seismic data directly in a low-rank tensor format, which drastically reduces the amount of storage required to represent the data. We first outline how seismic data exhibits low-rank structure in a particular transform-domain, which can be exploited to compress the dense data in one extremely storage-efficient tensor format when the data is fully sampled. In the more realistic case of missing data, we can use interpolation techniques based on the same tensor format to recover fully sampled data volume in compressed form. In either case, once we have our data represented in its compressed tensor form, we design an algorithm to extract source or receiver gathers directly from the compressed parameters. This extraction process can be done on-the-fly directly on the compressed data, in the full wave- form inversion context, and does not require scanning through the entire dataset in order to form shot gathers. To the best of our knowledge, this work is one of the first major contributions to working with seismic data applications directly in the compressed domain without reconstructing the entire data volume. We use a stochastic inversion approach, which works with small subsets of source experiments at each iteration, further to reduce the computational and memory costs of full waveform inversion. We also demonstrate how this data compression and extraction technique can be ap- plied to forming full subsurface image gathers through probing techniques.},
  keywords = {MSc, thesis, compression, FWI, extended image volume, on-the-fly, multilinear algebra, Hierarchical Tucker},
  note = {(MSc)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2018/zhang2018THlss/zhang2018THlss.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Thesis/2018/zhang2018THlss/zhang2018THlss_pres.pdf}
}

@MASTERSTHESIS{bougher2016THmla,
  author = {Ben B. Bougher},
  title = {Machine learning applications to geophysical data analysis},
  school = {The University of British Columbia},
  year = {2016},
  month = {08},
  address = {Vancouver},
  abstract = {The sedimentary layers of the Earth are a complex
                  amorphous material formed from chaotic, turbulent,
                  and random natural processes. Exploration
                  geophysicists use a combination of assumptions,
                  approximate physical models, and trained pattern
                  recognition to extract useful information from
                  complex remote sensing data such as seismic and well
                  logs. In this thesis I investigate supervised and
                  unsupervised machine learning models in geophysical
                  data analysis and present two novel applications to
                  exploration geophysics. Firstly, interpreted well
                  logs from the Trenton-Black River study are used to
                  train a classifier that results in a success rate of
                  67\% at predicting stratigraphic units from gamma
                  ray logs. I use the scattering transform, a
                  multiscale analysis transform, to extract
                  discriminating features to feed a K-nearest
                  neighbour classifier. A second experiment frames a
                  conventional pre-stack seismic data characterization
                  workflow as an unsupervised machine learning problem
                  that is free from physical
                  assumptions. Conventionally, the Shuey model is used
                  to fit the angle dependent reflectivity response of
                  seismic data. I instead use principle component
                  based approaches to learn projections from the data
                  that improve classification. Results on the Marmousi
                  II elastic model and an industry field dataset show
                  that unsupervised learning models can be effective
                  at segmenting hydrocarbon reservoirs from seismic
                  data.},
  keywords = {MSc, thesis, machine learning, PCA, AVA, well logs, scattering transform},
  note = {(MSc)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2016/bougher2016THmla/bougher2016THmla.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Thesis/2016/bougher2016THmla/bougher2016THmla_pres.pdf}
}


@MASTERSTHESIS{li2015THwmd,
  author = {Xiaowei Li},
  title = {A weighted $\ell_1$-minimization for distributed compressive sensing},
  school = {The University of British Columbia},
  year = {2015},
  month = {09},
  address = {Vancouver},
  abstract = {Distributed Compressive Sensing (DCS) studies the
                  recovery of jointly sparse signals. Compared to
                  separate recovery, the joint recovery algorithms in
                  DCS are usually more effective as they make use of
                  the joint sparsity. In this thesis, we study a
                  weighted l1-minimization algorithm for the joint
                  sparsity model JSM-1 proposed by Baron et al. Our
                  analysis gives a sufficient null space property for
                  the joint sparse recovery. Furthermore, this
                  property can be extended to stable and robust
                  settings. We also presents some numerical
                  experiments for this algorithm.},
  keywords = {MSc, thesis, weighted $\ell_1$, distributed compressive sensing},
  note = {(MSc)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2015/li2015THwmd/li2015THwmd.pdf}
}


@MASTERSTHESIS{hargreaves2014THssr,
  author = {Brock Hargreaves},
  title = {Sparse signal recovery: analysis and synthesis formulations with prior support information},
  school = {The University of British Columbia},
  year = {2014},
  month = {04},
  address = {Vancouver},
  abstract = {The synthesis model for signal recovery has been the
                  model of choice for many years in compressive
                  sensing. Various weighting schemes using prior
                  support information to adjust the objective function
                  associated with the synthesis model have been shown
                  to improve the recovery of the signal in terms of
                  accuracy. Generally, even with no prior knowledge of
                  the support, iterative methods can build support
                  estimates and incorporate that into the recovery
                  which has also been shown to increase the speed and
                  accuracy of the recovery. However when the original
                  signal is sparse with respect to a redundant
                  dictionary (rather than an orthonormal basis) there
                  is a counterpart model to synthesis, namely the
                  analysis model, which has been less popular but has
                  recently attracted more attention. The analysis
                  model is much less understood and thus there are
                  fewer theorems available in both the context of
                  non-weighted and weighted signal recovery. In this
                  thesis, we investigate weighting in both the
                  analysis model and synthesis model in weighted
                  $\ell_1$-minimization. Theoretical guarantees on
                  reconstruction and various weighting strategies for
                  each model are discussed. We give conditions for
                  weighted synthesis recovery with frames which do not
                  require strict incoherency conditions, this is based
                  on recent results of regular synthesis with frames
                  using optimal dual $\ell_1$ analysis. A novel
                  weighting technique is introduced in the analysis
                  case which outperforms its traditional counterparts
                  in the case of seismic wavefield reconstruction. We
                  also introduce a weighted split Bregman algorithm
                  for analysis and optimal dual analysis. We then
                  investigate these techniques on seismic data and
                  synthetically created test data using a variety of
                  frames.},
  keywords = {MSc, thesis, sparse, analysis, synthesis, weighted $\ell_1$},
  note = {(MSc)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2014/hargreaves2014THssr/hargreaves2014THssr.pdf}
}


@MASTERSTHESIS{petrenko2014THaih,
  author = {Art Petrenko},
  title = {Accelerating an iterative {Helmholtz} solver using reconfigurable hardware},
  school = {The University of British Columbia},
  year = {2014},
  month = {04},
  address = {Vancouver},
  abstract = {An implementation of seismic wave simulation on a
                  platform consisting of a conventional host processor
                  and a reconfigurable hardware accelerator is
                  presented. This research is important in the field
                  of exploration for oil and gas resources, where a 3D
                  model of the subsurface of the Earth is frequently
                  required. By comparing seismic data collected in a
                  real-world survey with synthetic data generated by
                  simulated waves, it is possible to deduce such a
                  model. However this requires many time-consuming
                  simulations with different Earth models to find the
                  one that best fits the measured data. Speeding up
                  the wave simulations would allow more models to be
                  tried, yielding a more accurate estimate of the
                  subsurface. The reconfigurable hardware accelerator
                  employed in this work is a field programmable gate
                  array (FPGA). FPGAs are computer chips that consist
                  of electronic building blocks that the user can
                  configure and reconfigure to represent their
                  algorithm in hardware. Whereas a traditional
                  processor can be viewed as a pipeline for processing
                  instructions, an FPGA is a pipeline for processing
                  data. The chief advantage of the FPGA is that all
                  the instructions in the algorithm are already
                  hardwired onto the chip. This means that execution
                  time depends only on the amount of data to be
                  processed, and not on the complexity of the
                  algorithm. The main contribution is an
                  implementation of the well-known Kaczmarz row
                  projection algorithm on the FPGA, using techniques
                  of dataflow programming. This kernel is used as the
                  preconditioning step of CGMN, a modified version of
                  the conjugate gradients method that is used to solve
                  the time-harmonic acoustic isotropic constant
                  density wave equation. Using one FPGA-based
                  accelerator, the current implementation allows
                  seismic wave simulations to be performed over twice
                  as fast, compared to running on one Intel Xeon
                  E5-2670 core. I also discuss the effect of
                  modifications of the algorithm necessitated by the
                  hardware on the convergence properties of CGMN.
                  Finally, a specific plan for future work is set-out
                  in order to fully exploit the accelerator platform,
                  and the work is set in its larger context.},
  keywords = {CG, CGMN, FPGA, Helmholtz, Kaczmarz, linear solver, Maxeler, MSc, thesis, wave equation},
  note = {(MSc)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2014/petrenko2014THaih/petrenko2014THaih.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Thesis/2014/petrenko2014THaih/petrenko2014THaih_pres.pdf}
}


@MASTERSTHESIS{miao2014THesi,
  author = {Lina Miao},
  title = {Efficient seismic imaging with spectral projector and joint sparsity},
  school = {The University of British Columbia},
  year = {2014},
  month = {03},
  address = {Vancouver},
  abstract = {In this thesis, we investigate the potential of
                  improving the eciency of seismic imaging with two
                  advanced techniques: the spectral projector and the
                  'joint sparsity'. The spectral projector offers an
                  eigenvalue decomposition free computation routine
                  that can filter out unstable evanescent wave com-
                  ponents during wave equation based depth
                  extrapolation. 'Joint sparsity' aims to improve on
                  the pure sparsity promoting recovery by making use
                  of additional structure information of the
                  signal. Besides, a new sparsity optimization
                  algorithm - PQNL1 - is proposed to improve both
                  theoretical convergence rate and practical
                  performance for extremely large seismic imaging
                  problems.},
  keywords = {MSc, thesis},
  note = {(MSc)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2014/miao2014THesi/miao2014THesi.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Thesis/2014/miao2014THesi/miao2014THesi_pres.pdf}
}


@MASTERSTHESIS{ghadermarzy2013THups,
  author = {Navid Ghadermarzy},
  title = {Using prior support information in compressed sensing},
  school = {The University of British Columbia},
  year = {2013},
  month = {08},
  address = {Vancouver},
  abstract = {Compressed sensing is a data acquisition technique that
                  entails recovering estimates of sparse and
                  compressible signals from $n$ linear measurements,
                  significantly fewer than the signal ambient
                  dimension $N$. In this thesis we show how we can
                  reduce the required number of measurements even
                  further if we incorporate prior information about
                  the signal into the reconstruction
                  algorithm. Specifically, we study certain weighted
                  nonconvex $\ell_p$ minimization algorithms and a
                  weighted approximate message passing algorithm. In
                  Chapter 1 we describe compressed sensing as a
                  practicable signal acquisition method in application
                  and introduce the generic sparse approximation
                  problem. Then we review some of the algorithms used
                  in compressed sensing literature and briefly
                  introduce the method we used to incorporate prior
                  support information into these problems. In Chapter
                  2 we derive sufficient conditions for stable and
                  robust recovery using weighted $\ell_p$ minimization
                  and show that these conditions are better than those
                  for recovery by regular $\ell_p$ and weighted
                  $\ell_1$. We present extensive numerical
                  experiments, both on synthetic examples and on
                  audio, and seismic signals. In Chapter 3 we derive
                  weighted AMP algorithm which iteratively solves the
                  weighted $\ell_1$ minimization. We also introduce a
                  reweighting scheme for weighted AMP algorithms which
                  enhances the recovery performance of weighted
                  AMP. We also apply these algorithms on synthetic
                  experiments and on real audio signals.},
  keywords = {MSc, thesis, compressed sensing, weighted $\ell_1$},
  note = {(MSc)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2013/ghadermarzy2013THups.pdf}
}


@MASTERSTHESIS{johnson2013THswr,
  author = {James Johnson},
  title = {Seismic wavefield reconstruction using reciprocity},
  school = {The University of British Columbia},
  year = {2013},
  month = {03},
  address = {Vancouver},
  abstract = {The primary focus of most reflection seismic surveys is
                  to help locate hydrocarbon recourses. Due to an ever
                  increasing scarcity of these recourses, we must
                  increase the size and quality of our seismic
                  surveys. However, processing such large seismic data
                  volumes to accurately recover earth properties is a
                  painstaking and computationally intensive
                  process. Due to the way reflection seismic surveys
                  are conducted there are often holes in the collected
                  data, where traces are not recorded. This can be due
                  to physical or cost constraints. For some of the
                  initial stages of processing these missing traces
                  are of little consequence. However processes like
                  multiple prediction and removal, interferometric
                  ground roll prediction, and migration require
                  densely sampled data on a regular grid. Thus the
                  need to interpolate undersampled data cannot be
                  ignored. Using the fact that reflection seismic data
                  sets obey a reciprocal relationship in source and
                  receiver locations, combined with recent advances in
                  the field of compressed sensing, we show that
                  properly regularized the wavefield reconstruction
                  problem can be solved with a high degree of
                  accuracy. We exploit the compressible nature of
                  seismic data in the curvelet domain to solve
                  regularized l1 recovery problems that seek to match
                  the measured data and enforce the above mentioned
                  reciprocity. Using our method we were able to
                  achieve results with a 20.45 dB signal to noise
                  ratio when reconstructing a marine data set that had
                  50\% of its traces decimated. This is a 13.44 dB
                  improvement over using the same method run without
                  taking reciprocity into account.},
  keywords = {MSc, thesis},
  note = {(MSc)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2013/johnson2013THswr.pdf}
}


@MASTERSTHESIS{alhashim09THsdp,
  author = {Fadhel Abbas Alhashim},
  title = {Seismic data processing with the parallel windowed curvelet transform},
  school = {The University of British Columbia},
  year = {2009},
  month = {08},
  address = {Vancouver},
  abstract = {The process of obtaining high quality seismic images is
                  very challenging when exploring new areas that have
                  high complexities. The to be processed seismic data
                  comes from the field noisy and commonly incomplete.
                  Recently, major advances were accomplished in the
                  area of coherent noise removal, for example, Surface
                  Related Multiple Elimination (SRME). Predictive
                  multiple elimination methods, such as SRME, consist
                  of two steps: The first step is the prediction step,
                  in this step multiples are predicted from the
                  seismic data. The second step is the separation step
                  in which primary reflection and surface related
                  multiples are separated, this involves predicted
                  multiples from the first step to be matched with the
                  true multiples in the data and eventually removed. A
                  recent robust Bayesian wavefield separation method
                  have been recently introduced to improve on the
                  separation by matching methods. This method utilizes
                  the effectiveness of using the multi scale and multi
                  angular curvelet transform in processing seismic
                  images. The method produced excellent results and
                  improved multiple removal. A considerable problem in
                  the seismic processing field is the fact that
                  seismic data are large and require a correspondingly
                  large memory size and processing time. The fact that
                  curvelets are redundant also increases the need for
                  large memory to process seismic data. In this thesis
                  we propose a parallel approach based windowing
                  operator that divides large seismic data into
                  smaller more managable datasets that can fit in
                  memory so that it is possible to apply the Bayesian
                  separation process in parallel with minimal harm to
                  the image quality and data integrity. However, by
                  dividing the data, we introduce discontinuities. We
                  take these discontinuities into account and compare
                  two ways that different windows may communicate.
                  The first method is to communicate edge information
                  at only two steps, namely, data scattering and
                  gathering processes while applying the multiple
                  separation on each window separately. The second
                  method is to define our windowing operator as a
                  global operator, which exchanges window edge
                  information at each forward and inverse curvelet
                  transform.  We discuss the trade off between the two
                  methods trying to minimize complexity and I/O time
                  spent in the process. We test our windowing operator
                  on a seismic denoising problem and then apply the
                  windowing operator on our sparse-domain Bayesian
                  primary-multiple separation.},
  keywords = {MSc},
  note = {(MSc)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2009/alhashim09THsdp.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Thesis/2009/alhashim09THsdp_pres.pdf}
}


@MASTERSTHESIS{jumah2012THdre,
  author = {Bander Jumah},
  title = {Dimensionality-reduced estimation of primaries by sparse inversion},
  school = {The University of British Columbia},
  year = {2012},
  month = {02},
  address = {Vancouver},
  abstract = {Data-driven methods—such as the estimation of primaries
                  by sparse inversion suffer from the curse of
                  dimensionality that leads to disproportional growth
                  in computational and storage demands when moving to
                  realistic 3D field data. To remove this fundamental
                  impediment, we propose a dimensionality-reduction
                  technique where the data matrix is approximated
                  adaptively by a randomized low-rank
                  factorization. Compared to conventional methods,
                  which need passes through all the data possibly
                  including on-the-fly interpolations for each
                  iteration, our approach has the advantage that the
                  passes is reduced to one to three. In addition, the
                  low-rank matrix factorization leads to considerable
                  reductions in storage and computational costs of the
                  matrix multiplies required by the sparse
                  inversion. Application of the proposed formalism to
                  synthetic and real data shows that significant
                  performance improvements in speed and memory use are
                  achievable at a low computational overhead required
                  by the low-rank factorization.},
  keywords = {MSc, thesis},
  note = {(MSc)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2012/jumah2012THdre.pdf}
}


@MASTERSTHESIS{almatar10THesd,
  author = {Mufeed H. AlMatar},
  title = {Estimation of surface-free data by curvelet-domain matched filtering and sparse inversion},
  school = {The University of British Columbia},
  year = {2010},
  month = {12},
  address = {Vancouver},
  abstract = {A recent robust multiple-elimination technique, based on
                  the underlying principle that relates primary
                  impulse response to total upgoing wavefield, tries
                  to change the paradigm that sees surface-related
                  multiples as noise that needs to be removed from the
                  data prior to imaging. This technique, estimation of
                  primaries by sparse inversion (EPSI), (van
                  Groenestijn and Verschuur, 2009; Lin and Herrmann,
                  2009), proposes an inversion procedure during which
                  the source function and surface- free impulse
                  response are directly calculated from the upgoing
                  wavefield using an alternating optimization
                  procedure. EPSI hinges on a delicate interplay
                  between surface-related multiples and pri-
                  maries. Finite aperture and other imperfections may
                  violate this relationship. In this thesis, we
                  investigate how to make EPSI more robust by
                  incorporating curvelet-domain matching in its
                  formulation. Compared to surface-related multiple
                  removal (SRME), where curvelet-domain matching was
                  used successfully, incorporating this step has the
                  additional advantage that matches multiples to
                  multiples rather than predicated multiples to total
                  data as in SRME.},
  keywords = {MSc},
  note = {(MSc)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2010/almatar10THesd.pdf}
}


@MASTERSTHESIS{dupuis05THssc,
  author = {Catherine Dupuis},
  title = {Seismic singularity characterization with redundant dictionaries},
  school = {The University of British Columbia},
  year = {2005},
  month = {07},
  address = {Vancouver},
  abstract = {We consider seismic signals as a superposition of
                  waveforms parameterized by their fractional-
                  orders. Each waveform models the reflection of a
                  seismic wave at a particular transition between two
                  lithological layers in the subsurface. The location
                  of the waveforms in the seismic signal corresponds
                  to the depth of the transitions in the subsurface,
                  whereas their fractional-order constitutes a measure
                  of the sharpness of the transitions. By considering
                  fractional-order transitions, we generalize the
                  zero-order transition model of the conventional
                  deconvolution problem, and aim at capturing the
                  different types of transitions. The goal is to
                  delineate and characterize transitions from seismic
                  signals by recovering the locations and
                  fractional-orders of its corresponding
                  waveforms. This problem has received increasing
                  interest, and several methods have been proposed,
                  including multi- and monoscale analysis based on
                  Mallat{\textquoteright}s wavelet transform modulus
                  maxima, and seismic atomic decomposition. We propose
                  a new method based on a two-step approach, which
                  divides the initial problem of delineating and
                  characterizing transitions over the whole seismic
                  signal, into two easier sub-problems. The algorithm
                  first partitions the seismic signal into its major
                  components, and then estimates the fractional-orders
                  and locations of each component. Both steps are
                  based on the sparse decomposition of seismic signals
                  in overcomplete dictionaries of waveforms parameter-
                  ized by their fractional-orders, and involve
                  $\ell_1$ minimizations solved by an iterative
                  thresholding algorithm. We present the method and
                  show numerical results on both synthetic and real
                  data.},
  keywords = {MSc},
  note = {(MSc)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2005/dupuis05THssc.pdf}
}


@MASTERSTHESIS{kumar09THins,
  author = {Vishal Kumar},
  title = {Incoherent noise suppression and deconvolution using curvelet-domain sparsity},
  school = {The University of British Columbia},
  year = {2009},
  month = {06},
  address = {Vancouver},
  abstract = {Curvelets are a recently introduced transform domain
                  that belongs to a family of multiscale and also
                  multidirectional data expansions. As such, curvelets
                  can be applied to resolution of the issues of
                  complicated seismic wavefronts. We make use of this
                  multiscale, multidirectional and hence sparsifying
                  ability of the curvelet transform to suppress
                  incoherent noise from crustal data where the
                  signal-to-noise ratio is low and to develop an
                  improved deconvolution procedure. Incoherent noise
                  present in seismic reflection data corrupts the
                  quality of the signal and can often lead to
                  misinterpretation. The curvelet domain lends itself
                  particularly well for denoising because coherent
                  seismic energy maps to a relatively small number of
                  significant curvelet coefficents.},
  keywords = {MSc},
  note = {(MSc)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2009/kumar09THins.pdf}
}


@MASTERSTHESIS{lebed08THssr,
  author = {Evgeniy Lebed},
  title = {Sparse signal recovery in a transform domain},
  school = {The University of British Columbia},
  year = {2008},
  month = {08},
  address = {Vancouver},
  abstract = {The ability to efficiently and sparsely represent
                  seismic data is becoming an increasingly important
                  problem in geophysics. Over the last thirty years
                  many transforms such as wavelets, curvelets,
                  contourlets, surfacelets, shearlets, and many other
                  types of x-lets have been developed. Such transform
                  were leveraged to resolve this issue of sparse
                  representations. In this work we compare the
                  properties of four of these commonly used
                  transforms, namely the shift-invariant wavelets,
                  complex wavelets, curvelets and surfacelets. We also
                  explore the performance of these transforms for the
                  problem of recovering seismic wavefields from
                  incomplete measurements.},
  keywords = {MSc},
  note = {(MSc)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2008/lebed08THssr.pdf}
}


@MASTERSTHESIS{maysami08THlcs,
  author = {Mohammad Maysami},
  title = {Lithology constraints from seismic waveforms: application to opal-{A} to opal-{CT} transition},
  school = {The University of British Columbia},
  year = {2008},
  month = {02},
  address = {Vancouver},
  abstract = {In this work, we present a new method for seismic
                  waveform characterization, which is aimed at
                  extracting detailed litho-stratigraphical
                  information from seismic data. We attempt to
                  estimate the lithological attributes from seismic
                  data according to our parametric representation of
                  stratigraphical horizons, where the parameter values
                  provide us with a direct link to nature of
                  lithological transitions. We test our method on a
                  seismic dataset with a strong diagenetic transition
                  (opal-A to opal-CT transition).  Given some
                  information from cutting samples of well, we use a
                  percolation-based model to construct the elastic
                  profile of lithological transitions.  Our goal is to
                  match parametric representation for the diagenetic
                  transition in both real data and synthetic data
                  given by these elastic profiles. This match may be
                  interpreted as a well-seismic tie, which reveals
                  lithological information about stratigraphical
                  horizons.},
  keywords = {MSc},
  note = {(MSc)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2008/maysami08THlcs.pdf}
}


@MASTERSTHESIS{yarham08THsgs,
  author = {Carson Yarham},
  title = {Seismic ground-roll separation using sparsity promoting $\ell_1$ minimization},
  school = {The University of British Columbia},
  year = {2008},
  month = {05},
  address = {Vancouver},
  abstract = {The removal of coherent noise generated by surface waves
                  in land based seismic is a prerequisite to imaging
                  the subsurface. These surface waves, termed as
                  ground roll, overlay important reflector information
                  in both the t-x and f-k domains. Standard techniques
                  of ground roll removal commonly alter reflector
                  information as a consequence of the ground roll
                  removal. We propose the combined use of the curvelet
                  domain as a sparsifying basis in which to perform
                  signal separation techniques that can preserve
                  reflector information while increasing ground roll
                  removal. We examine two signal separation
                  techniques, a block-coordinate relaxation method and
                  a Bayesian separation method. The derivations and
                  background for both methods are presented and the
                  parameter sensitivity is examined. Both methods are
                  shown to be effective in certain situations
                  regarding synthetic data and erroneous surface wave
                  predictions. The block-coordinate relaxation method
                  is shown to have ma jor weaknesses when dealing with
                  seismic signal separation in the presence of noise
                  and with the production of artifacts and reflector
                  degradation. The Bayesian separation method is shown
                  to improve overall separation for both seismic and
                  real data. The Bayesian separation scheme is used on
                  a real data set with a surface wave prediction
                  containing reflector information. It is shown to
                  improve the signal separation by recovering
                  reflector information while improving the surface
                  wave removal. The abstract contains a separate real
                  data example where both the block-coordinate
                  relaxation method and the Bayesian separation method
                  are compared.},
  keywords = {MSc},
  note = {(MSc)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2008/yarham08THsgs.pdf}
}

@MASTERSTHESIS{fenelon08msc,
  author = {Lloyd Fenelon},
  title = {Nonequispaced discrete curvelet transform for seismic data reconstruction},
  howpublished = {BSc thesis, Ecole Nationale Superieure De Physique de Strasbourg},
  month = {08},
  year = {2008},
  abstract = {Physical constraints during seismic acquisitions lead to
                  incomplete seismic datasets. Curvelet Reconstruction
                  with Sparsity promoting Inversion (CRSI) is one of
                  the most efficient interpolation method available to
                  recover complete datasets from data with missing
                  traces. The method uses in its definition the
                  curvelet transform which is well suited to process
                  seismic data. However, its main shortcoming is to
                  not be able to provide an accurate result if the
                  data are acquired at irregular positions. This come
                  from the curvelet transform implementation which
                  cannot handle this type of data. In this thesis the
                  implementation of the curvelet transform is modified
                  to offer the possibility to CRSI to give better
                  representation of seismic data for high quality
                  seismic imaging.},
  bdsk-url-1 = {http://slim.gatech.edu/Publications/Public/Theses/2008/fenelon08msc.pdf},
  date-added = {2008-09-03 16:18:08 -0700},
  date-modified = {2008-09-03 16:25:10 -0700},
  keywords = {SLIM, BSc},
  pdf = {http://slim.gatech.edu/Publications/Public/Theses/2008/fenelon08msc.pdf}
}



% This file was created with JabRef 2.9.
% Encoding: MacRoman

%-----2020-----%

@TECHREPORT{siahkoohi2020TRfuqf,
  author = {Ali Siahkoohi and Gabrio Rizzuti and Philipp A. Witte and Felix J. Herrmann},
  title = {Faster Uncertainty Quantification for Inverse Problems with Conditional Normalizing Flows},
  year = {2020},
  month = {07},
  number = {TR-CSE-2020-2},
  institution = {Georgia Institute of Technology},
  abstract = {In inverse problems, we often have access to data consisting of
paired samples $(x,y)\sim p_{X,Y}(x,y)$ where $y$ are partial observations of
a physical system, and $x$ represents the unknowns of the problem. Under
these circumstances, we can employ supervised training to learn a solution
$x$ and its uncertainty from the observations $y$. We refer to this problem
as the "supervised" case. However, the data $y\sim p_{Y}(y)$ collected at one
point could be distributed differently than observations $y'\sim p_{Y}'(y')$,
relevant for a current set of problems. In the context of Bayesian inference,
we propose a two-step scheme, which makes use of normalizing flows and joint
data to train a conditional generator $q_{\theta}(x|y)$ to approximate the
target posterior density $p_{X|Y}(x|y)$. Additionally, this preliminary phase
provides a density function $q_{\theta}(x|y)$, which can be recast as a prior
for the "unsupervised" problem, e.g.~when only the observations $y'\sim
p_{Y}'(y')$, a likelihood model $y'|x$, and a prior on $x'$ are known. We
then train another invertible generator with output density $q'_{\phi}(x|y')$
specifically for $y'$, allowing us to sample from the posterior
$p_{X|Y}'(x|y')$. We present some synthetic results that demonstrate
considerable training speedup when reusing the pretrained network
$q_{\theta}(x|y')$ as a warm start or preconditioning for approximating
$p_{X|Y}'(x|y')$, instead of learning from scratch. This training modality
can be interpreted as an instance of transfer learning. This result is
particularly relevant for large-scale inverse problems that employ expensive
numerical simulations.},
  keywords = {deep learning, invertible networks, uncertainty quantification},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2020/siahkoohi2020TRfuqf/siahkoohi2020TRfuqf.html}
}

@TECHREPORT{louboutin2020SEGtwri,
  author = {Mathias Louboutin and Gabrio Rizzuti and Felix J. Herrmann},
  title = {Time-domain Wavefield Reconstruction Inversion in a TTI medium},
  year = {2020},
  month = {4},
  number = {TR-CSE-2020-1},
  institution = {Georgia Institute of Technology},
  abstract = {We introduce a generalization of time-domain wavefield
reconstruction inversion to anisotropic acoustic modeling. Wavefield
reconstruction inversion has been extensively researched in recent years for
its ability to mitigate cycle skipping. The original method was formulated in
the frequency domain with acoustic isotropic physics. However,
frequency-domain modeling requires sophisticated iterative solvers that are
difficult to scale to industrial-size problems and more realistic physical
assumptions, such as tilted transverse isotropy, object of this study. The
work presented here is based on a recently proposed dual formulation of
wavefield reconstruction inversion, which allows time-domain propagator that
are suitable to both large scales and more accurate physics.},
  keywords = {FWI, WRI, anisotropy, TTI},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2020/louboutin2020SEGtwri/louboutin2020SEGtwri.html},
  url2 = {https://arxiv.org/pdf/2004.07355.pdf}
}

@TECHREPORT{louboutin2020SCsta,
  author = {Mathias Louboutin and Fabio Luporini and Philipp A. Witte and Rhodri
Nelson and George Bisbas and Jan Thorbecke and Felix J. Herrmann and Gerard
Gorman},
  title = {Scaling through abstractions – high-performance vectorial wave
simulations for seismic inversion with Devito},
  year = {2020},
  month = {4},
  number = {TR-CSE-2020-3},
  institution = {Georgia Institute of Technology},
  abstract = {[Devito] is an open-source Python project based on domain-specific
language and compiler technology. Driven by the requirements of rapid HPC
applications development in exploration seismology, the language and compiler
have evolved significantly since inception. Sophisticated boundary
conditions, tensor contractions, sparse operations and features such as
staggered grids and sub-domains are all supported; operators of essentially
arbitrary complexity can be generated. To accommodate this flexibility whilst
ensuring performance, data dependency analysis is utilized to schedule loops
and detect computational-properties such as parallelism. In this article, the
generation and simulation of MPI-parallel propagators (along with their
adjoints) for the pseudo-acoustic wave-equation in tilted transverse
isotropic media and the elastic wave-equation are presented. Simulations are
carried out on industry scale synthetic models in a HPC Cloud system and
reach a performance of 28TFLOP/s, hence demonstrating Devito's suitability
for production-grade seismic inversion problems.},
  keywords = {HPC, Devito, finite-difference, large-scale, RTM, elastic, TTI},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2020/louboutin2020SCsta/louboutin2020SCsta.html}
}

%-----2019-----%

@TECHREPORT{siahkoohi2019TRnna,
  author = {Ali Siahkoohi and Mathias Louboutin and Felix J. Herrmann},
  title = {Neural network augmented wave-equation simulation},
  year = {2019},
  month = {09},
  number = {TR-CSE-2019-1},
  institution = {Georgia Institute of Technology},
  abstract = {Accurate forward modeling is important for solving inverse
problems. An inaccurate wave-equation simulation, as a forward operator, will
offset the results obtained via inversion. In this work, we consider the case
where we deal with incomplete physics. One proxy of incomplete physics is an
inaccurate discretization of Laplacian in simulation of wave equation via
finite-difference method. We exploit intrinsic one-to-one similarities
between timestepping algorithm with Convolutional Neural Networks (CNNs), and
propose to intersperse CNNs between low-fidelity timesteps. Augmenting neural
networks with low-fidelity timestepping algorithms may allow us to take large
timesteps while limiting the numerical dispersion artifacts. While simulating
the wave-equation with low-fidelity timestepping algorithm, by correcting the
wavefield several time during propagation, we hope to limit the numerical
dispersion artifact introduced by a poor discretization of the Laplacian. As
a proof of concept, we demonstrate this principle by correcting for numerical
dispersion by keeping the velocity model fixed, and varying the source
locations to generate training and testing pairs for our supervised learning
algorithm.},
  keywords = {wave equation, deep learning, finite difference},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2019/siahkoohi2019TRnna/siahkoohi2019TRnna.html},
  url2 = {https://arxiv.org/pdf/1910.00925.pdf}
}

%-----2017-----%

@TECHREPORT{louboutin2016OGHPCocp,
  author = {Mathias Louboutin and Gerard Gorman and Felix J. Herrmann},
  title = {Optimizing the computational performance of time-domain modelling---leveraging multiple right-hand-sides},
  year = {2017},
  number = {TR-EOAS-2017-2},
  institution = {UBC},
  abstract = {Exploration geophysics heavily relies upon fast solvers
                  for the wave-equation and its adjoint. The main
                  computational cost of a wave-equation solver is to
                  compute the Laplacian, or more complex
                  finite-difference operators, at every time step. The
                  performance of many discretizations is limited by
                  the relatively low operational intensity (number of
                  floating point operations divided by memory traffic)
                  of the finite-difference stencil. Solving the
                  wave-equation for multiple sources/right-hand-sides
                  (RHSs) at once mitigates this problem by increasing
                  the operational intensity. This is implemented by
                  rewriting the classical matrix-vector product into a
                  matrix-matrix product where each column of the
                  second matrix represent the solution wavefield for
                  each given source. This minor modification to the
                  solver is shown to achieve a 2-4 times speedup
                  compared to a single source solver. We concentrate
                  in this paper on acoustic modelling, but our
                  approach can easily be extended to anisotropic or
                  elastic cases for both forward and adjoint
                  modelling.},
  keywords = {finite differences, HPC, modelling, time domain},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2017/louboutin2016OGHPCocp/louboutin2016OGHPCocp.pdf}
}


@TECHREPORT{witte2017TRcls,
  author = {Philipp A. Witte and Mengmeng Yang and Felix J. Herrmann},
  title = {Compressive least-squares migration with source estimation},
  year = {2017},
  number = {TR-EOAS-2017-3},
  institution = {UBC},
  abstract = {Least-squares reverse-time migration is a powerful
                  approach for true-amplitude seismic imaging of
                  complex geological structures. The successful
                  application of this method is hindered by its
                  exceedingly large computational cost and required
                  prior knowledge of the generally unknown source
                  wavelet. We address these problems by introducing an
                  algorithm for low-cost sparsity-promoting
                  least-squares migration with source estimation. We
                  adapt a recent algorithm from sparse optimization,
                  which allows to work with randomized subsets of
                  shots during each iteration of least-squares
                  migration, while still converging to an
                  artifact-free solution. We modify the algorithm to
                  incorporate on-the-fly source estimation through
                  variable projection, which lets us estimate the
                  wavelet without additional PDE solves. The algorithm
                  is easy to implement and allows imaging at a
                  fraction of the cost of conventional least squares
                  reverse-time migration, requiring only around two
                  passes trough the data, making the method feasible
                  for large-scale imaging problems with unknown source
                  wavelets.},
  keywords = {LSRTM, source estimation, sparsity, migration, time domain},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2017/witte2017TRcls/witte2017TRcls.html}
}


@TECHREPORT{witte2016OGHPClst,
  author = {Philipp A. Witte and Mathias Louboutin and Gerard Gorman and Felix J. Herrmann},
  title = {A large-scale time-domain seismic modeling and inversion workflow in {Julia}},
  year = {2017},
  number = {TR-EOAS-2017-1},
  institution = {UBC},
  abstract = {We present our initial steps towards the development of
                  a large-scale seismic modeling workflow in Julia
                  that provides a framework for wave equation based
                  inversion methods like full waveform inversion or
                  least squares migration. Our framework is based on
                  the Devito, a finite difference domain specific
                  language compiler that generates highly optimized
                  and parallel code. We develop a flexible workflow
                  that is based on abstract matrixfree linear
                  operators and enables developers to write code that
                  closely resembles the underlying math, while at the
                  same time leveraging highly optimized wave equation
                  solvers, allowing us to solve large-scale
                  three-dimensional inverse problems.},
  keywords = {HPC, inversion, modelling},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2017/witte2016OGHPClst/witte2016OGHPClst.pdf}
}


%-----2016-----%

@TECHREPORT{louboutin2016SEGocp,
  author = {Mathias Louboutin and Gerard Gorman and Felix J. Herrmann},
  title = {Optimizing the computational performance and maintainability of time-domain modelling---leveraging multiple right-hand-sides},
  year = {2016},
  month = {06},
  number = {TR-EOAS-2016-2},
  institution = {UBC},
  abstract = {Numerical solvers for the wave equation are a key
                  component of Full-Waveform Inversion (FWI) and
                  Reverse-Time Migration (RTM). The main computational
                  cost of a wave-equation solver stems from the
                  computation of the Laplacian at each time step. When
                  using a finite difference discretization this can be
                  characterized as a structured grid computation
                  within Colella's Seven Dwarfs. Independent of the
                  degree of parallelization the performance will be
                  limited by the relatively low operational intensity
                  (number of operations divided by memory traffic) of
                  finite-difference stencils, that is so say that the
                  method is memory bandwidth bound. For this reason
                  many developers have focused on porting their code
                  to platforms that have higher memory bandwidth, such
                  as GPU's, or put significant effort into highly
                  intrusive optimisations. However, these
                  optimisations rarely strike the right performance vs
                  productivity balance as the software becomes less
                  maintainable and extensible. By solving the wave
                  equation for multiple sources/right-hand-sides
                  (RHSs) at once, we overcome this problem arriving at
                  a time-stepping solver with higher operational
                  intensity. In essence, we arrive at this result by
                  turning the usual matrix-vector products into a
                  matrix-matrix products where the first matrix
                  implements the discretized wave equation and each
                  column of the second matrix contain separate
                  wavefields for each given source. By making this
                  relatively minor change to the solver we readily
                  achieved a $\times{2}$ speedup. While we limit
                  ourselves to acoustic modeling, our approach can
                  easily be extended to the anisotropic or elastic
                  cases.},
  keywords = {modelling, 3D, time domain},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2016/louboutin2016SEGocp/louboutin2016SEGocp.html}
}


@TECHREPORT{herrmann2016SLBors,
  author = {Felix J. Herrmann},
  title = {Overview research at the {SINBAD Consortium}},
  year = {2016},
  month = {03},
  number = {TR-EOAS-2016-1},
  institution = {UBC},
  keywords = {presentation, SLIM},
  note = {Presented at a seminar at Schlumberger Gould, Cambridge on March 17, 2016.},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2016/SLB/herrmann2016SLBors/herrmann2016SLBors.pdf}
}


%-----2015-----%

@TECHREPORT{kumar2015EAGElse,
  author = {Rajiv Kumar and Ning Tu and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Least-squares extended imaging with surface-related multiples},
  year = {2015},
  month = {01},
  number = {TR-EOAS-2015-1},
  institution = {UBC},
  abstract = {Common image gathers are used in building velocity
                  models, inverting for anisotropy parameters, and
                  analyzing reservoir attributes. Typically, only
                  primary reflections are used to form image gathers
                  as multiples can cause artifacts that interfere with
                  the events of interest. However, it has been shown
                  that multiples can actually provide extra
                  illumination of the subsurface, especially for
                  delineating the near-surface features. In this
                  paper, we aim to form common image gathers directly
                  from the data with surface related multiples by
                  applying concepts that have been used to
                  successfully deal with surface-related multiples in
                  imaging. We achieve this by effectively inverting an
                  extended migration operator. This results in
                  extended images with better near-surface
                  illumination that are free of artifacts that can
                  hamper velocity analysis. In addition, being able to
                  generate extended images directly from the total
                  data avoids the need for (time-consuming)
                  pre-processing. Synthetic examples on a layered
                  model show that the proposed formulation is
                  promising.},
  keywords = {surface-related multiples, image gathers},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2015/kumar2015EAGElse/kumar2015EAGElse.html}
}


@TECHREPORT{kumar2015EAGEtjm,
  author = {Rajiv Kumar and Haneet Wason and Felix J. Herrmann},
  title = {Time-jittered marine acquisition: low-rank v/s sparsity},
  year = {2015},
  month = {01},
  number = {TR-EOAS-2015-2},
  institution = {UBC},
  abstract = {Time-jittered simultaneous marine acquisition has been
                  recognized as an economic way of improving the
                  spatial sampling, and speedup acquisition, where a
                  single (or multiple) source vessel fires at --
                  jittered source locations and instances in time. It
                  has been shown in the past that this problem can be
                  setup as a -- compressed sensing problem, where
                  conventional seismic data is reconstructed from
                  blended data via a sparsity-promoting optimization
                  formulation. While the recovery quality of deblended
                  data is very good, the recovery process is
                  computationally very expensive. In this paper, we
                  present a computationally efficient
                  rank-minimization algorithm to deblend the seismic
                  data. The proposed algorithm is suitable for
                  large-scale seismic data, since it avoids SVD
                  computations and uses a low-rank factorized
                  formulation instead. Results are illustrated with
                  simulations of time-jittered marine acquisition,
                  which translates to jittered source locations for a
                  given speed of the source vessel, for a single
                  source vessel with two airgun arrays.},
  keywords = {deblending, low-rank},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2015/kumar2015EAGEtjm/kumar2015EAGEtjm.html}
}


@TECHREPORT{lago2015EAGEtrg,
  author = {Rafael Lago and Felix J. Herrmann},
  title = {Towards a robust geometric multigrid scheme for {Helmholtz} equation},
  year = {2015},
  month = {01},
  number = {TR-EOAS-2015-3},
  institution = {UBC},
  abstract = {We discuss an improvement of existing multigrid
                  techniques for the solution of the time harmonic
                  wave equation targeting application to seismic
                  inversion and imaging, using non-traditional
                  smoothing and coarse correction techniques, namely
                  the CGMN and CRMN methods. We aim at developing a
                  multigrid scheme to be used as a preconditioner for
                  FGMRES showing less sensibility to changes in the
                  discretization of the operator. We compare this
                  multigrid scheme with recent developments in the
                  multigrid field obtaining very satisfactory
                  results. Our numerical experiments using SEG/EAGE
                  Overthrust velocity model showing not only more
                  robustness when switching from a basic 7 points
                  stencil to a more compact 27 points stencil, but
                  also a considerable reduction in the number of
                  preconditioning steps required to attain
                  convergence, a result encouraging further
                  investigation.},
  keywords = {Helmholtz, multigrid, CGMN, CRMN},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2015/lago2015EAGEtrg/lago2015EAGEtrg.pdf}
}


@TECHREPORT{peters2015EAGErwi,
  author = {Bas Peters and Brendan R. Smithyman and Felix J. Herrmann},
  title = {Regularizing waveform inversion by projection onto intersections of convex sets},
  year = {2015},
  month = {01},
  number = {TR-EOAS-2015-4},
  institution = {UBC},
  abstract = {A framework is proposed for regularizing the waveform
                  inversion problem by projections onto intersections
                  of convex sets. Multiple pieces of prior information
                  about the geology are represented by multiple convex
                  sets, for example limits on the velocity or minimum
                  smoothness conditions on the model. The data-misfit
                  is then minimized, such that the estimated model is
                  always in the intersection of the convex
                  sets. Therefore, it is clear what properties the
                  estimated model will have at each iteration. This
                  approach does not require any quadratic penalties to
                  be used and thus avoids the known problems and
                  limitations of those types of penalties. It is shown
                  that by formulating waveform inversion as a
                  constrained problem, regularization ideas such as
                  Tikhonov regularization and gradient filtering can
                  be incorporated into one framework. The algorithm is
                  generally applicable, in the sense that it works
                  with any (differentiable) objective function,
                  several gradient and quasi-Newton based solvers and
                  does not require significant additional
                  computation. The method is demonstrated on the
                  inversion of very noisy synthetic data and vertical
                  seismic profiling field data.},
  keywords = {waveform inversion, regularization, convex sets},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2015/peters2015EAGErwi/peters2015EAGErwi.html}
}


@TECHREPORT{esser2015tvwri,
  author = {Ernie Esser and Lluís Guasch and Tristan van Leeuwen and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Total variation regularization strategies in full waveform inversion for improving robustness to noise, limited data and poor initializations},
  year = {2015},
  month = {06},
  number = {TR-EOAS-2015-5},
  institution = {UBC},
  abstract = {We propose an extended full waveform inversion
                  formulation that includes convex constraints on the
                  model. In particular, we show how to simultaneously
                  constrain the total variation of the slowness
                  squared while enforcing bound constraints to keep it
                  within a physically realistic range. Synthetic
                  experiments show that including total variation
                  regularization can improve the recovery of a high
                  velocity perturbation to a smooth background model,
                  removing artifacts caused by noise and limited data.
                  Total variation-like constraints can make the
                  inversion results significantly more robust to a
                  poor initial model, leading to reasonable results in
                  some cases where unconstrained variants of the
                  method completely fail. Numerical results are
                  presented for portions of the SEG/EAGE salt model
                  and the 2004 BP velocity
                  benchmark. ***Disclaimer.*** *This technical report
                  is ongoing work (and posted as is except for the
                  addition of another author) of the late John "Ernie"
                  Esser (May 19, 1980 - March 8, 2015), who passed
                  away under tragic circumstances. We will work hard
                  to finalize and submit this work to the peer-review
                  literature.* Felix J. Herrmann},
  keywords = {Wavefield Reconstruction Inversion, total-variation, hinge loss, cones constraints},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2015/esser2015tvwri/esser2015tvwri.html}
}


@TECHREPORT{witte2015TRoam,
  author = {Philipp A. Witte and Mathias Louboutin and Felix J. Herrmann},
  title = {Overview on anisotropic modeling and inversion},
  year = {2015},
  month = {08},
  number = {TR-EOAS-2015-6},
  institution = {UBC},
  abstract = {This note provides an overview on strategies for
                  modeling and inversion with the anisotropic wave
                  equation. Since linear and non-linear inversion
                  methods like least squares RTM and Full Waveform
                  Inversion depend on matching observed field data
                  with synthetically modelled data, accounting for
                  anisotropy effects is necessary in order to
                  accurately match waveforms at long offsets and
                  propagation times. In this note, the two main
                  strategies for anisotropic modelling by solving
                  either a pseudo acoustic wave equation or a pure
                  quasi-P-wave equation are discussed and an inversion
                  workflow using the pure quasi-P-wave equation is
                  provided. In particular, we derive the exact adjoint
                  of the anisotropic forward modelling and jacobian
                  operator and give a detailled describtion of their
                  implementation. The anistropic FWI workflow is
                  tested on a sythetic data example.},
  keywords = {full waveform inversion, anisotropy, modeling},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2015/witte2015TRoam/witte2015TRoam.html}
}


@TECHREPORT{peters2015SEGrwi,
  author = {Bas Peters and Zhilong Fang and Brendan R. Smithyman and Felix J. Herrmann},
  title = {Regularizing waveform inversion by projections onto convex sets --- application to the {2D} {Chevron} 2014 synthetic blind-test dataset},
  year = {2015},
  month = {06},
  number = {TR-EOAS-2015-7},
  institution = {UBC},
  abstract = {A framework is proposed for regularizing the waveform
                  inversion problem by projections onto intersections
                  of convex sets. Multiple pieces of prior information
                  about the geology are represented by multiple convex
                  sets, for example limits on the velocity or minimum
                  smoothness conditions on the model. The data-misfit
                  is then minimized, such that the estimated model is
                  always in the intersection of the convex
                  sets. Therefore, it is clear what properties the
                  estimated model will have at each iteration. This
                  approach does not require any quadratic penalties to
                  be used and thus avoids the known problems and
                  limitations of those types of penalties. It is shown
                  that by formulating waveform inversion as a
                  constrained problem, regularization ideas such as
                  Tikhonov regularization and gradient filtering can
                  be incorporated into one framework. The algorithm is
                  generally applicable, in the sense that it works
                  with any (differentiable) objective function and
                  does not require significant additional
                  computation. The method is demonstrated on the
                  inversion of the 2D marine isotropic elastic
                  synthetic seismic benchmark by Chevron using an
                  acoustic modeling code. To highlight the effect of
                  the projections, we apply no data pre-processing.},
  keywords = {SEG, Waveform inversion, regularization, projection, blind-test, Wavefield Reconstruction Inversion},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2015/peters2015SEGrwi/peters2015SEGrwi.html}
}


%-----2014-----%

@TECHREPORT{esser2014sln,
  author = {Ernie Esser},
  title = {Some lifting notes},
  year = {2014},
  month = {02},
  number = {TR-EOAS-2014-1},
  institution = {UBC},
  keywords = {lifting, nonconvex quadratic problems, convex semidefinite programming, low rank},
  note = {written on February 15, 2014},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2014/esser2014sln/esser2014sln.pdf}
}


@TECHREPORT{herrmann2014pmpde,
  author = {Felix J. Herrmann and Tristan van Leeuwen},
  title = {A penalty method for {PDE}-constrained optimization},
  institution = {UBC},
  year = {2014},
  month = {10},
  day = {30},
  number = {WO 2014/172787},
  type = {Patent},
  yearfiled = {2014},
  monthfiled = {04},
  dayfiled = {23},
  abstract = {The invention is directed to a computer-implemented
                  method for obtaining a physical model having
                  physical model parameters wherein solutions to one
                  or more partial-differential-equations (PDE's) are
                  calculated ans wherein (i) an appropriate initial
                  model is selected, (ii) setup a system of equations
                  referred to as the data-augmented PDE for the field,
                  comprising of the discretized PDE, the sampling
                  operator, the source function and the observed data,
                  and solve the data-augmented PDE in a suitable
                  manner to obtain a field that both satisfies the PDE
                  and fits the data to some degree, (iii) setup a
                  system of equations by using the PDE, the source
                  function and the field obtained in step (ii) and
                  solve this system of equations in a suitable manner
                  to obtain an update of the physical model parameters
                  and repeat steps (ii)-(iii) until a predetermined
                  stopping criterion is met.},
  keywords = {penalty method, optimization, patent},
  note = {(International publication date 30 October 2014. International publication number WO 2014/172787.)},
  url = {https://slim.gatech.edu/Publications/Public/Patents/2014/herrmann2014pmpde/herrmann2014pmpde_WO2014172787.pdf},
  url2 = {http://patentscope.wipo.int/search/en/WO2014172787}
}


@TECHREPORT{slim2014NSERCapp,
  author = {Felix J. Herrmann},
  title = {{NSERC} 2014 {DNOISE} application},
  year = {2014},
  number = {TR-EOAS-2014-7},
  institution = {UBC},
  abstract = {This current proposal describes a comprehensive
                  five-year continuation of our research project in
                  dynamic nonlinear optimization for imaging in
                  seismic exploration (DNOISE). DNOISE III—Exploration
                  Seismology in the Petascale Age builds on the proven
                  track record of our multidisciplinary research team
                  that conducts transformative research in the fields
                  of seismic-data acquisition, processing, and
                  wave-equation based inversion. The overarching goals
                  of the DNOISE series of projects can be simply
                  summarized as: ``How to image more deeply and with
                  more detail?" and ``How to do more with less data?"
                  Also, to help overcome the current substantial
                  challenges in the oil and gas industry, we maintain
                  this focus with more specific follow-up questions
                  such as: ``How can we control costs and remove
                  acquisition-related artifacts in 3D (time-lapse)
                  seismic data sets?" and ``How can we replace
                  conventional seismic data processing with
                  wave-equation based inversion, control computational
                  costs, assess uncertainties, extract reservoir
                  information and remove sensitivity to starting
                  models?" To answer these questions, we have
                  assembled an expanded cross-disciplinary research
                  team with backgrounds in scientific computing (SC),
                  machine learning (ML), compressive sensing (CS),
                  hardware design, and computational and observational
                  exploration seismology (ES). With this team, we will
                  continue to drive innovations in ES by utilizing our
                  unparalleled access to high-performance computing
                  (HPC), our expertise and experience in CS and
                  wave-equation based inversion (WEI) and our proven
                  abilities in incorporating our research findings
                  into practical scalable software of our inversion
                  solutions.},
  keywords = {NSERC, DNOISE},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/NSERC/2014/DNOISEIII/crd.html}
}

@TECHREPORT{tschannen2014tdl,
  author = {Valentin Tschannen and Zhilong Fang and Felix J. Herrmann},
  title = {Time domain least squares migration and dimensionality reduction},
  institution = {UBC},
  year = {2014},
  month = {06},
  number = {TR-EOAS-2014-9},
  abstract = {Least-squares seismic migration (LSM) is a wave equation based linearized inversion problem relying on the minimization of a least-squares misfit function, with respect to the medium perturbation, between recorded and modeled wavefields. Today’s challenges in Hydrocarbon ex- ploration are to build high resolution images of more and more complicated geological reservoirs, which requires to handle very large systems of equations. The extreme size of the problem com- bined with the fact that it is ill-conditioned make LSM not yet feasible for industrial purposes. To overcome this "curse of dimensionality", dimension reduction and divide-and-conquer tech- niques that aim to decrease the computation time and the required memory, while conserving the image quality, have recently been developed. By borrowing ideas from stochastic optimiza- tion and compressive sensing, the imaging problem is reformulated as an L1-regularized, sparsity promoted LSM. The idea is to take advantage of the compressibility of the model perturbation in the curvelet domain and to work on series of smaller subproblems each involving a small ran- domized subset of data. We try two different subset sampling strategies, artificial randomized simultaneous sources experiments ("supershots") and drawing sequential shots firing at random source locations. These subsets are changed after each subproblem is solved. In both cases we obtain good migration results at significantly reduced computational cost. Application of these methods to a complicated synthetic model yields to encouraging results, underlining the usefulness of sparsity promotion and randomization in time stepping formulation.
},
  keywords = {Wave equation migration, sparsity promotion, compressive sensing, stochastic optimization},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2014/tschannen2014tdl/tschannen2014tdl.pdf}
}

@TECHREPORT{esser2014SEGsgp,
  author = {Ernie Esser and Tristan van Leeuwen and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {A scaled gradient projection method for total variation regularized full waveform inversion},
  year = {2014},
  month = {04},
  number = {TR-EOAS-2014-2},
  institution = {UBC},
  abstract = {We propose an extended full waveform inversion
                  formulation that includes convex constraints on the
                  model. In particular, we show how to simultaneously
                  constrain the total variation of the slowness
                  squared while enforcing bound constraints to keep it
                  within a physically realistic range. Synthetic
                  experiments show that including total variation
                  regularization can improve the recovery of a high
                  velocity perturbation to a smooth background model.},
  keywords = {full waveform inversion, convex constraints, total variation regularization},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2014/esser2014SEGsgp/esser2014SEGsgp.html}
}


@TECHREPORT{zfang2014SEGsqn,
  author = {Zhilong Fang and Felix J. Herrmann},
  title = {A stochastic quasi-Newton {McMC} method for uncertainty quantification of full-waveform inversion},
  year = {2014},
  month = {04},
  number = {TR-EOAS-2014-6},  
  institution = {UBC},
  abstract = {In this work we propose a stochastic quasi-Newton Markov
                  chain Monte Carlo (McMC) method to quantify the
                  uncertainty of full-waveform inversion (FWI). We
                  formulate the uncertainty quantification problem in
                  the framework of the Bayesian inference, which
                  formulates the posterior probability as the
                  conditional probability of the model given the
                  observed data. The Metropolis-Hasting algorithm is
                  used to generate samples satisfying the posterior
                  probability density function (pdf) to quantify the
                  uncertainty. However it suffers from the challenge
                  to construct a proposal distribution that
                  simultaneously provides a good representation of the
                  true posterior pdf and is easy to manipulate. To
                  address this challenge, we propose a stochastic
                  quasi-Newton McMC method, which relies on the fact
                  that the Hessian of the deterministic problem is
                  equivalent to the inverse of the covariance matrix
                  of the posterior pdf. The l-BFGS (limited-memory
                  Broyden–Fletcher–Goldfarb–Shanno) Hessian is used to
                  approximate the inverse of the covariance matrix
                  efficiently, and the randomized source sub-sampling
                  strategy is used to reduce the computational cost of
                  evaluating the posterior pdf and constructing the
                  l-BFGS Hessian. Numerical experiments show the
                  capability of this stochastic quasi-Newton McMC
                  method to quantify the uncertainty of FWI with a
                  considerable low cost.},
  keywords = {FWI, uncertainty quantification, quasi-Newton, McMC},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2014/zfang2014SEGsqn/zfang2014SEGsqn.html}
}


@TECHREPORT{wang2014SEGfwi,
  author = {Rongrong Wang and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Full waveform inversion with interferometric measurements},
  year = {2014},
  month = {04},
  number = {TR-EOAS-2014-5},
  institution = {UBC},
  abstract = {In this note, we design new misfit functions for
                  full-waveform inversion by using interferometric
                  measurements to reduce sensitivity to phase
                  errors. Though established within a completely
                  different setting from the linear case, we obtain a
                  similar observation: the interferometry can improve
                  robustness under certain modeling errors. Moreover,
                  in order to deal with errors on both source and
                  receiver sides, we propose a higher order
                  interferometry, which, as a generalization of the
                  usual definition, involves the cross correlation of
                  four traces. A proof of principle simulations is
                  included on a stylized example.},
  keywords = {FWI},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2014/wang2014SEGfwi/wang2014SEGfwi.html}
}


@TECHREPORT{kumar2014SEGmcu,
  author = {Rajiv Kumar and Oscar Lopez and Ernie Esser and Felix J. Herrmann},
  title = {Matrix completion on unstructured grids : 2-D seismic data regularization and interpolation},
  year = {2014},
  month = {04},
  number = {TR-EOAS-2014-3},
  institution = {UBC},
  abstract = {Seismic data interpolation via rank-minimization
                  techniques has been recently introduced in the
                  seismic community. All the existing
                  rank-minimization techniques assume the recording
                  locations to be on a regular grid, e.g. sampled
                  periodically, but seismic data are typically
                  irregularly sampled along spatial axes. Other than
                  the irregularity of the sampled grid, we often have
                  missing data. In this paper, we study the effect of
                  grid irregularity to conduct matrix completion on a
                  regular grid for unstructured data. We propose an
                  improvement of existing rank-minimization techniques
                  to do regularization. We also demonstrate that we
                  can perform seismic data regularization and
                  interpolation simultaneously. We illustrate the
                  advantages of the modification using a real seismic
                  line from the Gulf of Suez to obtain high quality
                  results for regularization and interpolation, a key
                  application in exploration geophysics.},
  keywords = {regularization, interpolation, matrix completion, NFFT},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2014/kumar2014SEGmcu/kumar2014SEGmcu.html}
}


@TECHREPORT{smithyman2014SEGjfw,
  author = {Brendan R. Smithyman and Bas Peters and Bryan DeVault and Felix J. Herrmann},
  title = {Joint full-waveform inversion of on-land surface and {VSP} data from the {Permian} {Basin}},
  year = {2014},
  month = {04},
  number = {TR-EOAS-2014-4},
  institution = {UBC},
  abstract = {Full-waveform Inversion is applied to generate a
                  high-resolution model of P-wave velocity for a site
                  in the Permian Basin, Texas, USA. This investigation
                  jointly inverts seismic waveforms from a surface 3-D
                  vibroseis surface seismic survey and a co-located
                  3-D Vertical Seismic Profiling (VSP) survey, which
                  shared common source Vibration Points (VPs). The
                  resulting velocity model captures features that were
                  not resolvable by conventional migration velocity
                  analysis.},
  keywords = {full-waveform inversion, seismic, land, vibroseis, downhole receivers},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2014/smithyman2014SEGjfw/smithyman2014SEGjfw.html}
}


@TECHREPORT{slim2014NSERCpr,
  author = {Felix J. Herrmann},
  title = {{NSERC} 2014 {DNOISE} progress report},
  year = {2014},
  number = {TR-EOAS-2014-8},
  institution = {UBC},
  abstract = {As we entered the second half of the DNOISE II project,
                  we are happy to report that we have made significant
                  progress on several fronts. Firstly, our work on
                  seismic data acquisition with compressive sensing is
                  becoming widely recognized. For instance,
                  ConocoPhilips ran a highly successful field trial on
                  Marine acquisition with compressive sensing and
                  obtained significant improvements compared to
                  standard production (see figure below). Moreover,
                  one of the main outcomes of this year’s EAGE
                  workshop was that industry is ready to adapt
                  randomized sampling as a new acquisition
                  paradigm. Needless to say this is a big success for
                  what we have been trying to accomplish with DNOISE
                  II. Finally, we have made a breakthrough in the
                  application of randomized sampling in 4-D seismic,
                  which is receiving a lot of interest from
                  industry. Secondly, our work on large-scale
                  optimization in the context of wave-equation based
                  inversion is also increasingly widely adapted. For
                  instance, our batching techniques are making the
                  difference between making a loss or profit for a
                  large contractor company active in the area of
                  full-waveform inversion. We also continued to make
                  progress in exciting new directions that go beyond
                  sparsity promotion and which allow us to exploit
                  other types of structure within the data, such as
                  low-rank for matrices or hierarchical Tucker formats
                  for tensors. Application of these techniques show
                  excellent results and in certain cases, such as
                  source separation problems with small dithering,
                  show significant improvements over transform-domain
                  methods. Thirdly, we continued to make significant
                  progress in wave-equation based inversion. We
                  extended our new penalty-based formulation now
                  called Wavefield Reconstruction Inversion/Imaging to
                  include total-variation regularization and density
                  variations. We also continued to make progress on
                  multiples, imaging with multiples and 3-D
                  full-waveform inversion. Statoil is the latest
                  company to join and we have several other companies
                  that have shown a keen interest. We also received
                  substantial in-kind contributions including a
                  license to WesternGeco’s iOmega and HPC equipment
                  discounts. After many years of support BP decided
                  unfortunately to no longer support SINBAD quoting
                  financial headwind related to the Deep horizon
                  disaster. On a more positive note, we are extremely
                  happy to report major progress on our efforts to
                  secure access to high-performance compute, including
                  renewed funding from NSERC and our involvement in
                  the International Inversion Initiative in Brazil. 9
                  peer-reviewed journal publications have resulted
                  from our work within the reporting period, with a
                  further 6 submitted, and DNOISE members disseminated
                  the results of our research at 49 major national and
                  international conference presentations. On the HQP
                  training side, 4 MSc students have recently
                  graduated, with one obtaining a position with CGG
                  Calgary, and we added 4 postdocs and 3 PhD students
                  to our team in September 2014, greatly increasing
                  our research capacity. As can be seen from the
                  report below, we are well on schedule and on certain
                  topics well beyond the milestones included in the
                  original proposal. With the purchase of the new
                  cluster we expect to see a surge of activity in
                  extending our algorithms to 3D. With this increased
                  capacity, we continue to be in an excellent position
                  to make fundamental contributions to the fields of
                  seismic data acquisition, processing, and
                  wave-equation based inversion. In the sections
                  below, we give a detailed overview of the research
                  and publication activities of the different members
                  of the group and how these relate to the objectives
                  of the grant, to industrial uptake, and to
                  outreach. Unless stated otherwise the students and
                  PDFs are (co)-supervised by the PI. We refer to the
                  publications section 4.0 for a complete list of our
                  presentations, conference proceedings, and journal
                  publications. We also refer to our mindmap, which
                  clearly establishes connections between the
                  different research topics we have embarked upon as
                  part of the DNOISE II project.},
  keywords = {NSERC, DNOISE},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/NSERC/2014/Progress_Report_2014.html}
}


%-----2013-----%

@TECHREPORT{li2013EAGEwebmplijsp,
  author = {Xiang Li and Felix J. Herrmann},
  title = {Wave-equation based multi-parameter linearized inversion with joint-sparsity promotion},
  year = {2013},
  number = {TR-EOAS-2013-1},  
  institution = {UBC},
  abstract = {The successful application of linearized inversion is
                  affected by the prohibitive size of the data,
                  computational resources required, and how accurately
                  the model parameters reflects the real Earth
                  properties. The issue of data size and computational
                  resources can be addressed by combining ideas from
                  sparsity promoting and stochastic optimization,
                  which can allow us to invert model perturbation with
                  a small subset of the data, yielding a few PDE
                  solves for the inversion. In this abstract, we are
                  aiming at addressing the issue of accuracy of model
                  parameters by inverting density and velocity
                  simultaneously rather than only using velocity. As a
                  matter of face, the effects of density and velocity
                  variations towards the wavefield are very similar,
                  which will cause energy leakage between density and
                  velocity images. To overcome this issue, we proposed
                  a incoherence enhanced method that can reduce the
                  similarity between the effect of density and
                  velocity. Moreover, the location of structural
                  variations in velocity and density are often
                  overlapped in geological setting, thus in this
                  abstract, we also exploit this property with
                  joint-sparsity promoting to further improve the
                  imaging result.},
  keywords = {linearized inversion, incoherence enhancement, joint-sparsity},
  month = {01},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2013/li2013EAGEwebmplijsp/li2013EAGEwebmplijsp.pdf}
}


@TECHREPORT{kumar2013ICMLlr,
  author = {Aleksandr Y. Aravkin and Rajiv Kumar and Hassan Mansour and Ben Recht and Felix J. Herrmann},
  title = {An {SVD}-free {Pareto} curve approach to rank minimization},
  year = {2013},
  number = {TR-EOAS-2013-2},  
  institution = {UBC},
  abstract = {Recent SVD-free matrix factorization formulations have
                  enabled rank optimization for extremely large-scale
                  systems (millions of rows and columns). In this
                  paper, we consider rank-regularized formulations
                  that only require a target data-fitting error level,
                  and propose an algorithm for the corresponding
                  problem. We illustrate the advantages of the new
                  approach using the Netflix problem, and use it to
                  obtain high quality results for seismic trace
                  interpolation, a key application in exploration
                  geophysics. We show that factor rank can be easily
                  adjusted as the inversion proceeds, and propose a
                  weighted extension that allows known subspace
                  information to improve the results of matrix
                  completion formulations. Using these methods, we
                  obtain high-quality reconstructions for large scale
                  seismic interpolation problems with real data.},
  keywords = {interpolation, low rank},
  month = {02},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2013/kumar2013ICMLlr/kumar2013ICMLlr.pdf}
}


@TECHREPORT{oghenekohwo2013SEGtlswrs,
  author = {Felix Oghenekohwo and Felix J. Herrmann},
  title = {Time-lapse seismics with randomized sampling},
  year = {2013},
  number = {TR-EOAS-2013-3},  
  institution = {UBC},
  abstract = {In time-lapse or 4D seismics, repeatability of the
                  acquisition is a very crucial step, as we do not
                  want spurious events that are not there. In this
                  paper, we propose an approach which avoids any
                  requirement to repeat the surveys, by using
                  randomized sampling technique which allows us to be
                  more efficient in the acquisition. Our method
                  applies to sampling data using ocean bottom nodes
                  (OBN) as receivers. We test the efficacy of our
                  proposed randomized acquisition geometry for
                  time-lapse survey on two different models. In the
                  first example, model properties does not change with
                  time, while in the second example, model exhibit a
                  time-lapse effect which may be caused by the
                  migration of fluid within the reservoir. We perform
                  two types of randomized sampling - uniform
                  randomized sampling and jittered sampling to
                  visualize the effects of non-repeatability in
                  time-lapse survey. We observe that jittered
                  randomized sampling is a more efficient method
                  compared to randomized sampling, due to it's
                  requirement to control the maximum spacing between
                  the receivers. The results are presented, in the
                  image space, as a least-squares migration of the
                  model perturbation and they are shown for a subset
                  of a synthetic model - the Marmousi model},
  keywords = {acquisition, time-lapse, migration},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2013/oghenekohwo2013SEGtlswrs/oghenekohwo2013SEGtlswrs.pdf}
}


@TECHREPORT{petrenko2013SEGsaoc,
  author = {Art Petrenko and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Software acceleration of {CARP}, an iterative linear solver and preconditioner},
  year = {2013},
  number = {TR-EOAS-2013-4},  
  institution = {UBC},
  abstract = {We present the results of software optimization of a
                  row-wise preconditioner (Component Averaged Row
                  Projections) for the method of conjugate gradients,
                  which is used to solve the diagonally banded
                  Helmholtz system representing frequency domain,
                  isotropic acoustic seismic wave simulation. We
                  demonstrate that in our application, a
                  preconditioner bound to one processor core and
                  accessing memory contiguously reduces execution time
                  by 7\% for matrices having on the order of 108
                  non-zeros. For reference we note that our C
                  implementation is over 80 times faster than the
                  corresponding code written for a high-level
                  numerical analysis language.},
  keywords = {Helmholtz equation, Kaczmarz, software, wave propagation, frequency-domain},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2013/petrenko2013SEGsaoc/petrenko2013SEGsaoc.pdf}
}


@TECHREPORT{slim2013NSERCpr,
  author = {Felix J. Herrmann},
  title = {{NSERC} 2013 {DNOISE} progress report},
  year = {2013},
  number = {TR-EOAS-2013-5},  
  institution = {UBC},
  abstract = {As we enter the second half of the DNOISE II project, we
                  are happy to report that we have made significant
                  progress on several fronts. Firstly, our work on
                  seismic data acquisition with compressive sensing is
                  becoming widely recognized, reflected in adaptations
                  of this technology by industry and in this year’s
                  SEG Karcher award, which went to Gilles Hennenfent,
                  who was one of the researchers who started working
                  in this area in our group. As this report shows, we
                  continued to make progress on this topic with
                  numerous presentations, publications, and software
                  releases. Secondly, our work on large-scale
                  optimization is also widely adapted and instrumental
                  to the different research areas on the grant. In
                  particular, we are excited about new directions that
                  go beyond sparsity promotion and which allow us to
                  exploit other types of structure within the data,
                  such as low-rank. Over the near future, we expect to
                  see a body of new research based on these findings
                  touching acquisition as well as the wave-equation
                  based inversion aspects of our research
                  program. Thirdly, we are also very happy to report
                  that we continued to make substantial progress in
                  wave-equation base inversion. In particular, we
                  would like to mention successes in the areas of
                  acceleration of sparsity-promoting imaging with
                  source estimation and multiples and in theoretical
                  as well as practical aspects of full-waveform
                  inversion. We derived a highly practical and
                  economic formulation of 3-D FWI and we also came up
                  with a complete new formulation of FWI, which
                  mitigates issues related to cycle skipping. Finally,
                  we made a lot of progress applying our algorithm to
                  industrial datasets, which has been well received by
                  industry. Our findings show that FWI is still an
                  immature technology calling for more theoretical
                  input and for the development of practical
                  workflows. Over the last year our work cumulated in
                  14 peer-reviewed journal publications, 5 submitted
                  journal publications, 13 (+ 9) extended abstracts,
                  32 talks at international conferences, and 6
                  software packages. Finally, we are happy to report
                  that we have been joined by several new companies,
                  namely, ION Geophysical, CGG, and Woodside. At this
                  midpoint of the Grant, we are also happy to report
                  that we are well on schedule to meet the milestones
                  included in the original proposal. Given our wide
                  range of expertise and our plans to replace our
                  compute cluster, we continue to be in an excellent
                  position to make fundamental contributions to the
                  fields of seismic data acquisition, processing, and
                  wave-equation based inversion.},
  keywords = {NSERC, DNOISE},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/NSERC/2013/Progress_Report_2013.pdf}
}


@TECHREPORT{vanLeeuwen2013Penalty2,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {A penalty method for {PDE}-constrained optimization},
  year = {2013},
  number = {TR-EOAS-2013-6},  
  institution = {UBC},
  abstract = {We present a method for solving PDE constrained
                  optimization problems based on a penalty
                  formulation. This method aims to combine advantages
                  of both full-space and reduced methods by exploiting
                  a large search-space (consisting of both control and
                  state variables) while allowing for an efficient
                  implementation that avoids storing and updating the
                  state-variables. This leads to a method that has
                  roughly the same per-iteration complexity as
                  conventional reduced approaches while dening an
                  objective that is less non-linear in the control
                  variable by implicitly relaxing the constraint. We
                  apply the method to a seismic inverse problem where
                  it leads to a particularly ecient implementation
                  when compared to a conventional reduced approach as
                  it avoids the use of adjoint
                  state-variables. Numerical examples illustrate the
                  approach and suggest that the proposed formulation
                  can indeed mitigate some of the well-known problems
                  with local minima in the seismic inverse problem.},
  keywords = {waveform inversion, optimization},
  month = {04},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2013/vanLeeuwen2013Penalty2/vanLeeuwen2013Penalty2.pdf}
}


%-----2012-----%

@TECHREPORT{rajiv2012SEGFRM,
  author = {Rajiv Kumar and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Fast methods for rank minimization with applications to seismic-data interpolation},
  institution = {Department of Earth and Ocean Sciences},
  year = {2012},
  number = {TR-EOAS-2012-3},
  address = {University of British Columbia, Vancouver},
  abstract = {Rank penalizing techniques are an important direction in
                  seismic inverse problems, since they allow improved
                  recovery by exploiting low-rank structure. A major
                  downside of current state of the art techniques is
                  their reliance on the SVD of seismic data
                  structures, which can be prohibitively
                  expensive. Fortunately, recent work allows us to
                  circumvent this problem by working with matrix
                  factorizations. We review a novel approach to rank
                  penalization, and successfully apply it to the
                  seismic interpolation problem by exploiting the
                  low-rank structure of seismic data in the
                  midpoint-offset domain. Experiments for the recovery
                  of 2D monochromatic data matrices and seismic lines
                  represented as 3D volumes support the feasibility
                  and potential of the new approach.},
  keywords = {rank, optimization, seismic data interpolation},
  month = {04},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2012/rajiv2012SEGFRM/rajiv2012SEGFRM.pdf}
}


@TECHREPORT{slim2012NSERCpr,
  author = {Felix J. Herrmann},
  title = {{NSERC} 2012 {DNOISE} progress report},
  year = {2012},
  number = {TR-EOAS-2012-4},
  institution = {UBC},
  keywords = {NSERC, DNOISE},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/NSERC/2012/Progress_Report_2012.pdf}
}


@TECHREPORT{vanleeuwen2012CGMN,
  author = {Tristan van Leeuwen},
  title = {Fourier analysis of the {CGMN} method for solving the {Helmholtz} equation},
  year = {2012},
  number = {TR-EOAS-2012-1},
  institution = {Department of Earth, Ocean and Atmospheric Sciences},
  address = {The University of British Columbia, Vancouver},
  abstract = {The Helmholtz equation arises in many applications, such
                  as seismic and medical imaging. These application
                  are characterized by the need to propagate many
                  wavelengths through an inhomogeneous medium. The
                  typical size of the problems in 3D applications
                  precludes the use of direct factorization to solve
                  the equation and hence iterative methods are used in
                  practice. For higher wavenumbers, the system becomes
                  increasingly indefinite and thus good
                  preconditioners need to be constructed. In this note
                  we consider an accelerated Kazcmarz method (CGMN)
                  and present an expression for the resulting
                  iteration matrix. This iteration matrix can be used
                  to analyze the convergence of the CGMN method. In
                  particular, we present a Fourier analysis for the
                  method applied to the 1D Helmholtz equation. This
                  analysis suggests an optimal choice of the
                  relaxation parameter. Finally, we present some
                  numerical experiments.},
  keywords = {Helmholtz equation, modelling},
  url = {http://arxiv.org/abs/1210.2644},
}


@TECHREPORT{vanleeuwen2012SEGparallel,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {A parallel, object-oriented framework for frequency-domain wavefield imaging and inversion.},
  institution = {Department of Earth and Ocean Sciences},
  year = {2012},
  number = {TR-EOAS-2012-2},
  address = {University of British Columbia, Vancouver},
  abstract = {We present a parallel object-oriented matrix-free
                  framework for frequency-domain seismic modeling,
                  imaging and inversion. The key aspects of the
                  framework are its modularity and level of
                  abstraction, which allows us to write code that
                  reflects the underlying mathematical structure and
                  develop unit-tests that guarantee the fidelity of
                  the code. By overloading standard linear-algebra
                  operations, such as matrix-vector multiplications,
                  we can use standard optimization packages to work
                  with our code without any modification. This leads
                  to a scalable testbed on which new methods can be
                  rapidly prototyped and tested on medium-sized 2D
                  problems. Although our current implementation uses
                  (parallel) Matlab, all of these design principles
                  can also be met by using lower-level languages which
                  is important when we want to scale to realistic 3D
                  problems. We present some numerical examples on
                  synthetic data.},
  keywords = {modeling, imaging, inversion, SEG},
  month = {04},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2012/vanleeuwen2012SEGparallel/vanleeuwen2012SEGparallel.pdf}
}

  
@TECHREPORT{vanleeuwen2012smii,
  author = {Tristan van Leeuwen},
  title = {A parallel matrix-free framework for frequency-domain seismic modelling, imaging and inversion in Matlab},
  year = {2012},
  number = {TR-EOAS-2012-5},
  abstract = {I present a parallel matrix-free framework for
                  frequency-domain seismic modeling, imaging and
                  inversion. The framework provides basic building
                  blocks for designing and testing optimization-based
                  formulations of both linear and non-linear seismic
                  in- verse problems. By overloading standard
                  linear-algebra operations, such as matrix-vector
                  multiplications, standard optimization packages can
                  be used to work with the code without any
                  modification. This leads to a scalable testbed on
                  which new methods can be rapidly prototyped and
                  tested on medium-sized 2D problems. I present some
                  numerical examples on both linear and non-linear
                  seismic inverse problems.},
  keywords = {seismic imaging, optimization, Matlab, object-oriented programming},
  month = {07},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2012/vanleeuwen2012smii/vanleeuwen2012smii.pdf}
}


%-----2011-----%

@TECHREPORT{slim2011NSERCpr,
  author = {Felix J. Herrmann},
  title = {{NSERC} 2011 {DNOISE} progress report},
  year = {2011},
  number = {TR-EOAS-2011-1},
  institution = {UBC},
  abstract = {The main thrust of the DNOISE project is focused on the
                  following researth themes: [1] seismic acquisition
                  design and recovery from incomplete data with the
                  goal to reduce acquisition costs while increasing
                  the spatial bandwidth and aperture of seismic data;
                  [2] Removal of the 'surface nonlinearity' by
                  simultaneous estimation of the source signature and
                  the surface-free Green's function by inverting the
                  surface-related multiple prediction operator; [3]
                  Reduction of the computational complexity of
                  full-waveform inversion (FWI) by randomized
                  dimensionality reduction; [4] "Convexification" of
                  FWI to remove or at least diminish the adverse
                  effects of non-uniqueness that has plagued FWI since
                  its inception; The first three themes are directed
                  towards removing major impediments faced by FWI
                  related to the costs of acquiring data, the
                  computational costs of processing and inverting
                  data, and to issues with source calibration and
                  surface-related multiples. The final theme is more
                  'blue sky' and tries to incorporate ideas from
                  migration-velocity analysis into the formulation of
                  full-waveform inversion. Aside from these themes, we
                  will continue to work on seismic data acquisition
                  schemes that favor sparsity-promoting recovery and
                  on the development of large-scale solvers using
                  recent developments in convex and stochastic
                  optimization.},
  keywords = {NSERC, DNOISE},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/NSERC/2011/nserc-2011-dnoise-progress-report.pdf}
}


%-----2010-----%

@TECHREPORT{almatar10SEGesfd,
  author = {Mufeed H. AlMatar and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Estimation of surface-free data by curvelet-domain matched filtering and sparse inversion},
  institution = {Department of Earth and Ocean Sciences},
  year = {2010},
  number = {TR-EOAS-2010-1},
  address = {University of British Columbia, Vancouver},
  abstract = {Matching seismic wavefields and images lies at the heart
                  of many pre-post-processing steps part of seismic
                  imaging- whether one is matching predicted wavefield
                  components, such as multiples, to the actual
                  to-be-separated wavefield components present in the
                  data or whether one is aiming to restore migration
                  amplitudes by scaling, using an image-to-remigrated-
                  image matching procedure to calculate the scaling
                  coefficients. The success of these wavefield
                  matching procedures depends on our ability to (i)
                  control possible overfitting, which may lead to
                  accidental removal of energy or to inaccurate
                  image-amplitude corrections, (ii) handle data or
                  images with nonunique dips, and (iii) apply
                  subsequent wavefield separations or migraton
                  amplitude corrections stably. In this paper, we show
                  that the curvelet transform allows us to address all
                  these issues by im- posing smoothness in phase
                  space, by using their capability to handle
                  conflicting dips, and by leveraging their ability to
                  represent seismic data and images sparsely. This
                  latter property renders curvelet-domain sparsity
                  promotion an effective prior.},
  keywords = {SEG},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2010/almatar10SEGesfd/almatar10SEGesfd.pdf}
}


@TECHREPORT{herrmann2010SEGerc,
  author = {Felix J. Herrmann},
  title = {Empirical recovery conditions for seismic sampling},
  institution = {Department of Earth and Ocean Sciences, UBC},
  year = {2010},
  number = {TR-EOAS-2010-2},
  abstract = {In this paper, we offer an alternative sampling method
                  leveraging recent insights from compressive sensing
                  towards seismic acquisition and processing for data
                  that are traditionally considered to be
                  undersampled. The main outcome of this approach is a
                  new technology where acquisition and processing
                  related costs are no longer determined by overly
                  stringent sampling criteria, such as Nyquist. At the
                  heart of our approach lies randomized incoherent
                  sampling that breaks subsampling related
                  interferences by turning them into harmless noise,
                  which we subsequently remove by promoting
                  transform-domain sparsity. Now, costs no longer grow
                  with resolution and dimensionality of the survey
                  area, but instead depend on transform-domain
                  sparsity only. Our contribution is twofold. First,
                  we demonstrate by means of carefully designed
                  numerical experiments that compressive sensing can
                  successfully be adapted to seismic
                  acquisition. Second, we show that accurate recovery
                  can be accomplished for compressively sampled data
                  volumes sizes that exceed the size of conventional
                  transform-domain data volumes by only a small
                  factor. Because compressive sensing combines
                  transformation and encoding by a single linear
                  encoding step, this technology is directly
                  applicable to acquisition and to dimensionality
                  reduction during processing. In either case,
                  sampling, storage, and processing costs scale with
                  transform-domain sparsity.},
  url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2010/herrmann10SEGerc/herrmann10SEGerc.pdf}
}


@TECHREPORT{slim2010NSERCapp,
  author = {Felix J. Herrmann},
  title = {{NSERC} 2010 {DNOISE} application},
  year = {2010},
  number = {TR-EOAS-2010-3},
  institution = {UBC},
  abstract = {DNOISE II: Dynamic Nonlinear Optimization for Imaging in
                  Seismic Exploration is a multidisciplinary research
                  project that involves faculty from the Mathematics,
                  Computer Science, and Earth and Ocean Sciences
                  Departments at the University of British Columbia.
                  DNOISE II constitutes a transformative research
                  program towards a new paradigm in seismic
                  exploration where the acquisition- and
                  processing-related costs are no longer determined by
                  the survey area and discretization but by
                  transform-domain sparsity of the final result. In
                  this approach, we rid ourselves from the
                  confinements of conventional overly stringent
                  sampling criteria that call for regular sampling
                  with sequentiual sources at Nyquist rates. By
                  adapting the principles of compressive sensing,
                  DNOISE II promotes a ground-up formulation for
                  seismic imaging where adverse subsampling-related
                  artifacts are removed by intelligent
                  simultaneous-acquisition design and recovery by
                  transform-domain sparsity promotion. This
                  development---in conjunction with our track records
                  in sparse recovery and time-harmonic Helmholtz
                  solvers---puts us in an unique position to deliver
                  on fundamental breakthroughs in the development and
                  implementation of the next-generation of processing,
                  imaging, and full-waveform inversion solutions.},
  keywords = {NSERC, DNOISE},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/NSERC/2010/nserc-2010-dnoise-application.pdf}
}


%-----2009-----%

@TECHREPORT{tang09TRdtr,
  author = {Gang Tang and Reza Shahidi and Jianwei Ma},
  title = {Design of two-dimensional randomized sampling schemes for curvelet-based sparsity-promoting seismic data recovery},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2009},
  number = {TR-EOAS-2009-3},
  abstract = {The tasks of sampling, compression and reconstruction
                  are very common and often necessary in seismic data
                  processing due to the large size of seismic
                  data. Curvelet-based Recovery by Sparsity-promoting
                  Inversion, motivated by the newly developed theory
                  of compressive sensing, is among the best recovery
                  strategies for seismic data. The incomplete data
                  input to this curvelet-based recovery is determined
                  by randomized sampling of the original complete
                  data. Unlike usual regular undersampling, randomized
                  sampling can convert aliases to easy-to-eliminate
                  noise, thus facilitating the process of
                  reconstruction of the complete data from the
                  incomplete data. Randomized sampling methods such as
                  jittered sampling have been developed in the past
                  that are suitable for curvelet-based recovery,
                  however most have only been applied to sampling in
                  one dimension. Considering that seismic datasets are
                  usually higher dimensional and extremely large, in
                  the present paper, we extend the 1D version of
                  jittered sampling to two dimensions, both with
                  underlying Cartesian and hexagonal grids. We also
                  study separable and non-separable two dimensional
                  jittered sampling, the former referring to the
                  Kronecker product of two one-dimensional jittered
                  samplings. These different categories of jittered
                  sampling are compared against one another in terms
                  of signal-to-noise ratio and visual quality, from
                  which we find that jittered hexagonal sampling is
                  better than jittered Cartesian sampling, while fully
                  non-separable jittered sampling is better than
                  separable sampling. Because in the image processing
                  and computer graphics literature, sampling patterns
                  with blue-noise spectra are found to be ideal to
                  avoid aliasing, we also introduce two other
                  randomized sampling methods, possessing sampling
                  spectra with beneficial blue noise characteristics,
                  Poisson Disk sampling and Farthest Point
                  sampling. We compare these methods, and apply the
                  introduced sampling methodologies to higher
                  dimensional curvelet-based reconstruction. These
                  sampling schemes are shown to lead to better results
                  from CRSI compared to the other more traditional
                  sampling protocols, e.g. regular subsampling.},
  url = {https://slim.gatech.edu/Publications/Public/Journals/2009/tang09TRdtr/tang09TRdtr.pdf}
}


%-----2008-----%

@TECHREPORT{hennenfent08TRori,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {One-norm regularized inversion: learning from the {Pareto} curve},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2008},
  number = {TR-EOAS-2008-5},
  abstract = {Geophysical inverse problems typically involve a trade
                  off between data misfit and some prior. Pareto
                  curves trace the optimal trade off between these two
                  competing aims. These curves are commonly used in
                  problems with two-norm priors where they are plotted
                  on a log-log scale and are known as L-curves. For
                  other priors, such as the sparsity-promoting one
                  norm, Pareto curves remain relatively
                  unexplored. First, we show how these curves provide
                  an objective criterion to gauge how robust one-norm
                  solvers are when they are limited by a maximum
                  number of matrix-vector products that they can
                  perform. Second, we use Pareto curves and their
                  properties to define and compute one-norm
                  compressibilities. We argue this notion is key to
                  understand one-norm regularized inversion. Third, we
                  illustrate the correlation between the one-norm
                  compressibility and the performance of Fourier and
                  curvelet reconstructions with sparsity promoting
                  inversion.},
  keywords = {SLIM},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2008/hennenfent08TRori/hennenfent08TRori.pdf}
}


@TECHREPORT{lebed08TRhgg,
  author = {Evgeniy Lebed and Felix J. Herrmann},
  title = {A hitchhiker's guide to the galaxy of transform-domain sparsification},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2008},
  number = {TR-EOAS-2008-4},
  abstract = {The ability to efficiently and sparsely represent
                  seismic data is becoming an increasingly important
                  problem in geophysics. Over the last decade many
                  transforms such as wavelets, curvelets, contourlets,
                  surfacelets, shearlets, and many other types of
                  'x-lets' have been developed to try to resolve this
                  issue. In this abstract we compare the properties of
                  four of these commonly used transforms, namely the
                  shift-invariant wavelets, complex wavelets,
                  curvelets and surfacelets. We also briefly explore
                  the performance of these transforms for the problem
                  of recovering seismic wavefields from incomplete
                  measurements.},
  keywords = {SLIM},
  url = {https://slim.gatech.edu/Publications/Public/TechReport/2008/lebed08TRhgg/lebed08TRhgg.pdf}
}


@TECHREPORT{vandenberg08gsv,
  author = {Ewout {van den Berg} and Mark Schmidt and Michael P. Friedlander and K. Murphy},
  title = {Group sparsity via linear-time projection},
  year = {2008},
  number = {TR-2008-09},
  month = {06},
  abstract = {We present an efficient spectral projected-gradient
                  algorithm for optimization subject to a group
                  one-norm constraint. Our approach is based on a
                  novel linear-time algorithm for Euclidean projection
                  onto the one- and group one-norm
                  constraints. Numerical experiments on large data
                  sets suggest that the proposed method is
                  substantially more efficient and scalable than
                  existing methods.},
  institution = {UBC - Department of Computer Science},
  keywords = {SLIM, optimization},
  url = {http://www.optimization-online.org/DB_FILE/2008/07/2056.pdf}
}


%-----2007-----%

@TECHREPORT{vandenberg07TRipr,
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  title = {In pursuit of a root},
  institution = {Department of Computer Science},
  year = {2007},
  month = {06},
  number = {TR-EOAS-2007-19},
  address = {University of British Columbia, Vancouver},
  abstract = {The basis pursuit technique is used to find a minimum
                  one-norm solution of an underdetermined
                  least-squares problem. Basis pursuit denoise fits
                  the least-squares problem only approximately, and a
                  single parameter determines a curve that traces the
                  trade-off between the least-squares fit and the
                  one-norm of the solution. We show that the function
                  that describes this curve is convex and continuously
                  differentiable over all points of interest. The dual
                  solution of a least-squares problem with an explicit
                  one-norm constraint gives function and derivative
                  information needed for a root-finding method. As a
                  result, we can compute arbitrary points on this
                  curve. Numerical experiments demonstrate that our
                  method, which relies on only matrix-vector
                  operations, scales well to large problems.},
  url = {http://www.optimization-online.org/DB_HTML/2007/06/1708.html}
}

% This file was created with JabRef 2.9.
% Encoding: MacRoman

%-----2021-----%

@UNPUBLISHED{louboutin2021SEGulm,
  author = {Mathias Louboutin and Felix J. Herrmann},
  title = {Ultra-low memory seismic inversion with randomized trace estimation},
  year = {2021},
  month = {04},
  abstract = {Inspired by recent work on extended image volumes that lays the
ground for randomized probing of extremely large seismic wavefield matrices,
we present a memory frugal and computationally efficient inversion
methodology that uses techniques from randomized linear algebra. By means of
a carefully selected realistic synthetic example, we demonstrate that we are
capable of achieving competitive inversion results at a fraction of the
memory cost of conventional full-waveform inversion with limited
computational overhead. By exchanging memory for negligible computational
overhead, we open with the presented technology the door towards the use of
low-memory accelerators such as GPUs.},
  keywords = {FWI, Randomized linear algebra, HPC, Inversion},
  note = {Submitted to SEG 2021},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2021/louboutin2021SEGulm/louboutinp.html},
  software = {https://github.com/slimgroup/TimeProbeSeismic}
}

@UNPUBLISHED{siahkoohi2021SEGlbe,
  author = {Ali Siahkoohi and Felix J. Herrmann},
  title = {Learning by example: fast reliability-aware seismic imaging with normalizing flows},
  year = {2021},
  month = {04},
  abstract = {Uncertainty quantification provides quantitative measures on the reliability
of candidate solutions of ill-posed inverse problems. Due to their sequential
nature, Monte Carlo sampling methods require large numbers of sampling steps
for accurate Bayesian inference and are often computationally infeasible for
large-scale inverse problems, such as seismic imaging. Our main contribution
is a data-driven variational inference approach where we train a normalizing
flow (NF), a type of invertible neural net, capable of cheaply sampling the
posterior distribution given previously unseen seismic data from neighboring
surveys. To arrive at this result, we train the NF on pairs of low- and
high-fidelity migrated images. In our numerical example, we obtain
high-fidelity images from the Parihaka dataset and low-fidelity images are
derived from these images through the process of demigration, followed by
adding noise and migration. During inference, given shot records from a new
neighboring seismic survey, we first compute the reverse-time migration
image. Next, by feeding this low-fidelity migrated image to the NF we gain
access to samples from the posterior distribution virtually for free. We use
these samples to compute a high-fidelity image including a first assessment
of the image's reliability. To our knowledge, this is the first attempt to
train a conditional network on what we know from neighboring images to
improve the current and assess its reliability.},
  keywords = {Variational Inference, Seismic Imaging, Normalizing Flows, Deep Learning},
  note = {Submitted to SEG 2021},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2021/siahkoohi2021SEGlbe/abstract.html},
  software = {https://github.com/slimgroup/Software.SEG2021}
}

%-----2020-----%

@UNPUBLISHED{lopez2020gsb,
  author = {Oscar Lopez and Rajiv Kumar and Nick Moldoveanu and Felix J. Herrmann},
  title = {Graph Spectrum Based Seismic Survey Design},
  year = {2020},
  month = {12},
  abstract = {Randomized sampling techniques have become increasingly useful in seismic data acquisition and processing, allowing practitioners to achieve dense wavefield reconstruction from a substantially reduced number of field samples. However, typical designs studied in the low-rank matrix recovery and compressive sensing literature are difficult to achieve by standard industry hardware. For practical purposes, a compromise between stochastic and realizable samples is needed. In this paper, we propose a deterministic and computationally cheap tool to alleviate randomized acquisition design, prior to survey deployment and large-scale optimization. We consider universal and deterministic matrix completion results in the context of seismology, where a bipartite graph representation of the source-receiver layout allows for the respective spectral gap to act as a quality metric for wavefield reconstruction. We provide realistic survey design scenarios to demonstrate the utility of the spectral gap for successful seismic data acquisition via low-rank and sparse signal recovery.},
  keywords = {acquisition, compressed sensing, data reconstruction, survey design, algorithm},
  note = {Submitted to Geophysics},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/lopez2020gsb/lopez2020gsb.pdf}
}

@UNPUBLISHED{bisbas2020IPDPStbf,
  author = {George Bisbas and Fabio Luporini and Mathias Louboutin and Rhodri
Nelson and Gerard Gorman and Paul H. J. Kelly},
  title = {Temporal blocking of finite-difference stencil operators with sparse
"off-the-grid" sources},
  year = {2020},
  month = {10},
  abstract = {Stencil kernels dominate a range of scientific applications
including seismic and medical imaging, image processing, and neural networks.
Temporal blocking is a performance optimisation that aims to reduce the
required memory bandwidth of stencil computations by re-using data from the
cache for multiple time steps. It has already been shown to be beneficial for
this class of algorithms. However, optimising stencils for practical
applications remains challenging. These computations often include sparsely
located operators, not aligned with the computational grid ("off-the-grid").
For example, our work is motivated by sources that inject a wavefield and
measurements interpolating grid values. The resulting data dependencies make
the adoption of temporal blocking much more challenging. We propose a
methodology to inspect these data dependencies and reorder the computation,
leading to performance gains in stencil codes where temporal blocking has not
been applicable. We implement this novel scheme in the Devito domain-specific
compiler toolchain. Devito implements a domain-specific language embedded in
Python to generate optimised partial differential equation solvers using the
finite-difference method from high-level symbolic problem definitions. We
evaluate our scheme using isotropic acoustic, anisotropic acoustic and
isotropic elastic wave propagators of industrial significance. Performance
evaluation, after auto-tuning, shows that this enables substantial
performance improvement through temporal blocking, over highly-optimised
vectorized spatially-blocked code of up to 1.6x.},
  keywords = {Finite-difference, wave-equation, HPC, performance},
  note = {Submitted to IPDPS},
  url = {https://arxiv.org/pdf/2010.10248.pdf}
}

@UNPUBLISHED{sharan2020lsh,
  author = {Shashin Sharan and Yijun Zhang and Oscar Lopez and Felix J. Herrmann},
  title = {Large scale high-frequency wavefield reconstruction with recursively weighted matrix factorizations},
  year = {2020},
  month = {10},
  abstract = {Acquiring seismic data on a regular periodic fine grid is challenging. By exploiting the low-rank approximation property of fully sampled seismic data in some transform domain, low-rank matrix completion offers a scalable way to reconstruct seismic data on a regular periodic fine grid from coarsely randomly sampled data acquired in the field. While wavefield reconstruction have been applied successfully at the lower end of the spectrum, its performance deteriorates at the higher frequencies where the low-rank assumption no longer holds rendering this type of wavefield reconstruction ineffective in situations where high resolution images are desired. We overcome this shortcoming by exploiting similarities between adjacent frequency slices explicitly. During low-rank matrix factorization, these similarities translate to alignment of subspaces of the factors, a notion we propose to employ as we reconstruct monochromatic frequency slices recursively starting at the low frequencies. While this idea is relatively simple in its core, to turn this recent insight into a successful scalable wavefield reconstruction scheme for 3D seismic requires a number of important steps. First, we need to move the weighting matrices, which encapsulate the prior information from adjacent frequency slices, from the objective to the data misfit constraint. This move considerably improves the performance of the weighted low-rank matrix factorization on which our wavefield reconstructions is based. Secondly, we introduce approximations that allow us to decouple computations on a row-by-row and column-by-column basis, which in turn allow to parallelize the alternating optimization on which our low-rank factorization relies. The combination of weighting and decoupling leads to a computationally feasible full-azimuth wavefield reconstruction scheme that scales to industry-scale problem sizes. We demonstrate the performance of the proposed parallel algorithm on a 2D field data and on a 3D synthetic dataset. In both cases our approach produces high-fidelity broadband wavefield reconstructions from severely subsampled data.},
  keywords = {5D reconstruction, compressed sensing, frequency-domain, parallel, signal processing},
  note = {Submitted to Geophysics},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/sharan2020lsh/sharan2020lsh.html}
}

@UNPUBLISHED{rizzuti2020dfw,
  author = {Gabrio Rizzuti and Mathias Louboutin and Rongrong Wang and Felix J. Herrmann},
  title = {A dual formulation of wavefield reconstruction inversion for
large-scale seismic inversion},
  year = {2020},
  month = {10},
  abstract = {Most of the seismic inversion techniques currently proposed focus
on robustness with respect to the background model choice or inaccurate
physical modeling assumptions, but are not apt to large-scale 3D
applications. On the other hand, methods that are computationally feasible
for industrial problems, such as full waveform inversion, are notoriously
bogged down by local minima and require adequate starting models. We propose
a novel solution that is both scalable and less sensitive to starting model
or inaccurate physics when compared to full waveform inversion. The method is
based on a dual (Lagrangian) reformulation of the classical wavefield
reconstruction inversion, whose robustness with respect to local minima is
well documented in the literature. However, it is not suited to 3D, as it
leverages expensive frequency-domain solvers for the wave equation. The
proposed reformulation allows the deployment of state-of-the-art time-domain
finite-difference methods, and is computationally mature for industrial scale
problems.},
  keywords = {3D, Full-waveform inversion, Wave equation, Finite-difference},
  note = {Submitted to Geophysics},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/rizzuti2020dfw/rizzuti2020dfw.html}
}

@UNPUBLISHED{kukreja2020lcc,
  author = {Navjot Kukreja and Jan Hueckelheim and Mathias Louboutin and John Washbourne and Paul H. J. Kelly and Gerard J. Gorman},
  title = {Lossy Checkpoint Compression in Full Waveform Inversion},
  year = {2020},
  month = {9},
  abstract = {This paper proposes a new method that combines check-pointing
methods with error-controlled lossy compression for large-scale
high-performance Full-Waveform Inversion (FWI), an inverse problem commonly
used in geophysical exploration. This combination can significantly reduce
data movement, allowing a reduction in run time as well as peak memory. In
the Exascale computing era, frequent data transfer (e.g., memory bandwidth,
PCIe bandwidth for GPUs, or network) is the performance bottleneck rather
than the peak FLOPS of the processing unit. Like many other adjoint-based
optimization problems, FWI is costly in terms of the number of floating-point
operations, large memory footprint during backpropagation, and data transfer
overheads. Past work for adjoint methods has developed checkpointing methods
that reduce the peak memory requirements during backpropagation at the cost
of additional floating-point computations. Combining this traditional
checkpointing with error-controlled lossy compression, we explore the
three-way tradeoff between memory, precision, and time to solution. We
investigate how approximation errors introduced by lossy compression of the
forward solution impact the objective function gradient and final inverted
solution. Empirical results from these numerical experiments indicate that
high lossy-compression rates (compression factors ranging up to 100) have a
relatively minor impact on convergence rates and the quality of the final
solution.},
  keywords = {Lossy compression, Full waveform inversion, checkpointing, memory},
  note = {Submitted to GMD},
  url = {https://arxiv.org/pdf/2009.12623.pdf}
}
% This file was created with JabRef 2.6.
% Encoding: MacRoman

% This file was created with JabRef 2.8.1.
% Encoding: MacRoman

%----- 2021 -----%

@ARTICLE{yang2020lrpo,
  author = {Mengmeng Yang and Marie Graff and Rajiv Kumar and Felix J. Herrmann},
  title = {Low-rank representation of omnidirectional subsurface extended image
volumes},
  journal = {Geophysics},
  year = {2021},
  month = {01},
  volume = {86},
  number = {3},
  pages = {1-41},
  abstract = {Subsurface-offset gathers play an increasingly important role in seismic
imaging. These gathers are used during velocity model building and inversion
of rock properties from amplitude variations. While powerful, these gathers
come with high computational and storage demands to form and manipulate these
high dimensional objects. This explains why only limited numbers of image
gathers are computed over a limited offset range. We avoid these high costs
by working with highly compressed low-rank factorizations. We arrive at these
factorizations via a combination of probings with the double two-way wave
equation and randomized singular value decompositions. In turn, the resulting
factorizations give us access to all subsurface offsets without having to
form the full extended image volumes. The latter is computationally
prohibitive because extended image volumes are quadratic in image size. As a
result, we can easily handle situations where conventional horizontal offset
gathers are no longer focused. More importantly, the factorization also
provides a mechanism to use the invariance relation of extended image volumes
for velocity continuation. With this technique, extended image volumes for
one background velocity model can directly be mapped to those of another
background velocity model. Our low-rank factorization inherits this
invariance property so we incur factorization costs only once when examining
different imaging scenarios. Because all imaging experiments only involve the
factors, they are computationally cheap with costs that scale with the rank
of the factorization. We validate our methodology on 2D synthetics including
a challenging imaging example with salt. Our experiments show that our
low-rank factorization parameterizes extended image volumes naturally.
Instead of brute force explicit cross-correlations between shifted source and receiver
wavefields, our approach relies on the underlying linear-algebra structure
that enables us to work with these objects without incurring unfeasible
demands on computation and storage.},
  keywords = {extended image volumes, low rank, randomized linear algebra, power
schemes, invariance relationship},
  note = {(Geophysics)},
  doi = {10.1190/geo2020-0152.1},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2021/yang2020lrpo/Paper_final.html}
}

%----- 2020 -----%

@ARTICLE{yang2020tdsp,
  author = {Mengmeng Yang and Zhilong Fang and Philipp A. Witte and Felix J. Herrmann},
  title = {Time-domain sparsity promoting least-squares reverse time migration
with source estimation},
  journal = {Geophysical Prospecting},
  year = {2020},
  month = {08},
  volume = {68},
  number = {9},
  pages = {2697-2711},
  abstract = {Least-squares reverse time migration is well-known for its capability to generate artifact-free true-amplitude subsurface images through fitting observed data in the least-squares sense. However, when applied to realistic imaging problems, this approach is faced with issues related to overfitting and excessive computational costs induced by many wave-equation solves. The fact that the source function is unknown complicates this situation even further. Motivated by recent results in stochastic optimization and transform-domain sparsity-promotion, we demonstrate that the computational costs of inversion can be reduced significantly while avoiding imaging artifacts and restoring amplitudes. While powerfull, these new approaches do require accurate information on the source-time function, which is often lacking. Without this information, the imaging quality deteriorates rapidly. We address this issue by presenting an approach where the source-time function is estimated on the fly through a technique known as variable projection. Aside from introducing negligible computational overhead, the proposed method is shown to perform well on imaging problems with noisy data and problems that involve complex settings such as salt. In either case, the presented method produces high resolution high-amplitude fidelity images including an estimates for the source-time function. In addition, due to its use of stochastic optimization, we arrive at these images at roughly one to two times the cost of conventional reverse time migration involving all data.},
  keywords = {sparsity inversion, source estimation, penalty},
  note = {(Geophysical Prospecting)},
  doi = {10.1111/1365-2478.13021},
  url = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalProspecting/2020/yang2020tdsp/yang2020tdsp.html}
}

@ARTICLE{daskalakis2019SIAMJISasr,
  author = {Emmanouil Daskalakis and Felix J. Herrmann and Rachel Kuske},
  title = {Accelerating Sparse Recovery by Reducing Chatter},
  journal = {SIAM Journal on Imaging Sciences},
  year = {2020},
  month = {07},
  volume = {13},
  number = {3},
  pages = {1211–1239},
  abstract = {Compressive Sensing has driven a resurgence of sparse recovery algorithms with
  l_1-norm minimization. While these minimizations are relatively well understood for small underdetermined, possibly inconsistent systems, their behavior for large over-determined and inconsistent systems has received much less attention. Specifically, we focus on large systems where computational restrictions call for algorithms that use randomized subsets of rows that are touched a limited number of times. In that regime, l_1-norm minimization algorithms exhibit unwanted fluctuations near the desired solution, and the Linear Bregman iterations are no exception. We explain this observed lack of performance in terms of chatter, a well-known phenomena observed in non-smooth dynamical systems, where intermediate solutions wander between different states stifling convergence. By identifying chatter as the culprit, we modify the Bregman iterations with chatter reducing adaptive element-wise step lengths in combination with potential support detection via threshold crossing. We demonstrate the performance of our algorithm on carefully selected stylized examples and a realistic seismic imaging problem involving millions of unknowns and matrix-free matrix-vector products that involve expensive wave-equation solves.},
  keywords = {sparsity promotion, inconsistent linear systems, Kacmarz, linearized Bregman
dynamical systems, non-smooth dynamics, chatter},
  note = {(SIAM Journal on Imaging Sciences)},
  doi = {10.1137/19M129111X},
  url = {https://slim.gatech.edu/Publications/Public/Journals/SIAMJournalOnImagingSciences/2020/daskalakis2019SIAMJISasr/daskalakis2019SIAMJISasr.pdf}
}

@ARTICLE{luporini2018aap,
  author = {Fabio Luporini and Michael Lange and Mathias Louboutin and Navjot Kukreja and Jan Huckelheim and Charles Yount and Philipp A. Witte and Paul H. J. Kelly and Gerard J. Gorman and Felix J. Herrmann},
  title = {Architecture and performance of Devito, a system for automated stencil computation},
  journal = {ACM Trans. Math. Softw.},
  year = {2020},
  month = {04},
  volume = {46},
  number = {1},
  abstract = {Stencil computations are a key part of many high-performance computing applications, such as image processing, convolutional neural networks, and finite-difference solvers for partial differential equations. Devito is a framework capable of generating highly-optimized code given symbolic equations expressed in Python, specialized in, but not limited to, affine (stencil) codes. The lowering process – from mathematical equations down to C++ code – is
performed by the Devito compiler through a series of intermediate representations. Several performance optimizations are introduced, including advanced common sub-expressions elimination, tiling and parallelization. Some of these are obtained through well-established stencil optimizers, integrated in the back-end of the Devito compiler. The architecture of the Devito compiler, as well as the performance optimizations that are applied when generating code, are presented. The effectiveness of such performance optimizations is demonstrated using operators drawn from seismic imaging applications.},
  keywords = {Stencil, finite difference method, symbolic processing, structured grid, compiler, performance optimization},
  note = {(ACM Trans. Math. Softw.)},
  doi = {10.1145/3374916},
  url = {https://slim.gatech.edu/Publications/Public/Journals/ACMTOMS/2020/luporini2018aap/luporini2018aap.pdf}
}

@ARTICLE{witte2019TPDedas,
  author = {Philipp A. Witte and Mathias Louboutin and Henryk Modzelewski and Charles Jones and James Selvage and Felix J. Herrmann},
  title = {An Event-Driven Approach to Serverless Seismic Imaging in the Cloud},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  year = {2020},
  month = {03},
  volume = {31},
  number = {9},
  pages={2032-2049},
  abstract = {Adapting the cloud for high-performance computing (HPC) is a
challenging task, as software for HPC applications hinges on fast network
connections and is sensitive to hardware failures. Using cloud infrastructure
to recreate conventional HPC clusters is therefore in many cases an
infeasible solution for migrating HPC applications to the cloud. As an
alternative to the generic lift and shift approach, we consider the specific
application of seismic imaging and demonstrate a serverless and event-driven
pproach for running large-scale instances of this problem in the cloud.
Instead of permanently running compute instances, our workflow is based on a
serverless architecture with high throughput batch computing and event-driven
computations, in which computational resources are only running as long as
they are utilized. We demonstrate that this approach is very flexible and
allows for resilient and nested levels of parallelization, including domain
decomposition for solving the underlying partial differential equations.
While the event-driven approach introduces some overhead as computational
resources are repeatedly restarted, it inherently provides resilience to
instance shut-downs and allows a significant reduction of cost by avoiding
idle instances, thus making the cloud a viable alternative to on-premise
clusters for large-scale seismic imaging.},
  keywords = {cloud, imaging, serverless, event-driven, lsrtm},
  note = {(IEEE Transactions on Parallel and Distributed Systems)},
  doi={10.1109/TPDS.2020.2982626},
  url = {https://slim.gatech.edu/Publications/Public/Journals/IEEETPDS/2020/witte2019TPDedas/witte2019TPDedas.html}
}

%----- 2019 -----%

@ARTICLE{siahkoohi2019itl,
  author = {Ali Siahkoohi and Mathias Louboutin and Felix J. Herrmann},
  title = {The importance of transfer learning in seismic modeling and imaging},
  journal = {Geophysics},
  year = {2019},
  abstract = {Accurate forward modeling is essential for solving inverse problems
in exploration seismology. Unfortunately, it is often not possible to afford being
physically or numerically accurate. To overcome this conundrum, we make use of raw and processed data from nearby surveys. We propose to use this data, consisting of shot records or velocity models, to pre-train a neural network to
correct for the effects of, for instance, the free surface or numerical dispersion, both of which can be considered as proxies for incomplete or inaccurate physics. Given this pre-trained neural network, we apply transfer learning to finetune this pre-trained neural network so it performs well on its task of mapping low-cost, but low-fidelity, solutions to high-fidelity solutions for the current survey. As long as we can limit ourselves during finetuning
to using only a small fraction of high-fidelity data, we gain processing the current survey while using information from nearby surveys. We demonstrate this principle by removing surface-related multiples and ghosts from shot records and the effects of numerical dispersion from migrated images and wave simulations},
  keywords = {deep learning, transfer learning, modeling, imaging, SRME},
  doi = {10.1190/geo2019-0056.1},
  note = {(Accepted in GEOPHYSICS)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2019/siahkoohi2019itl/siahkoohi2019itl.html}
}

@ARTICLE{kukreja2019PASCccd,
  author = {Navjot Kukreja and Jan Huckelheim and Mathias Louboutin and Paul Hovland and Gerard Gorman},
  title = {Combining checkpointing and data compression to accelerate adjoint-based optimization problems},
  journal = {Euro-Par 2019: Parallel Processing},
  year = {2019},
  pages = {87-100},
  publisher = {Springer International Publishing},
  abstract = {Seismic inversion and imaging are adjoint-based optimization problems that processes up to terabytes of data, regularly exceeding the memory capacity of available computers. Data compression is an effective strategy to reduce this memory requirement by a certain factor, particularly if some loss in accuracy is acceptable. A popular alternative is checkpointing, where data is stored at selected points in time, and values at other times are recomputed as needed from the last stored state. This allows arbitrarily large adjoint computations with limited memory, at the cost of additional recomputations. In this paper we combine compression and checkpointing for the first time to compute a realistic seismic inversion. The combination of checkpointing and compression allows larger adjoint computations compared to using only compression, and reduces the recomputation overhead significantly compared to using only checkpointing.},
  keywords = {Adjoint-state, FD, checkpointing, compression, HPC, inverse problems},
  doi = {10.1007/978-3-030-29400-7_7},
  note = {(Euro-Par 2019: Parallel Processing)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/PASC/2019/kukreja2019PASCccd/kukreja2019PASCccd.pdf}
}

@ARTICLE{peters2019aos,
  author = {Bas Peters and Felix J. Herrmann},
  title = {Algorithms and software for projections onto intersections of convex
               and non-convex sets with applications to inverse problems},
  journal = {arXiv e-prints},
  year = {2019},
  month = {03},
  abstract = {We propose algorithms and software for computing projections onto the intersection of multiple convex and non-convex constraint sets. The software package, called SetIntersectionProjection, is intended for the regularization of inverse problems in physical parameter estimation and image processing. The primary design criterion is working with multiple sets, which allows us to solve inverse problems with multiple pieces of prior knowledge. Our algorithms outperform the well known Dykstra's algorithm when individual sets are not easy to project onto because we exploit similarities between constraint sets. Other design choices that make the software fast and practical to use, include recently developed automatic selection methods for auxiliary algorithm parameters, fine and coarse grained parallelism, and a multilevel acceleration scheme. We provide implementation details and examples that show how the software can be used to regularize inverse problems. Results show that we benefit from working with all available prior information and are not limited to one or two regularizers because of algorithmic, computational, or hyper-parameter selection issues.},
  keywords = {software, algorithm},
  eprint={1902.09699},
  note = {(arXiv)},
  url = {http://arxiv.org/abs/1902.09699}
}

@ARTICLE{peters2019gms,
  author = {Bas Peters and Felix J. Herrmann},
  title = {Generalized Minkowski sets for the regularization of inverse problems},
  journal = {arXiv e-prints},
  year = {2019},
  month = {03},
  abstract = {Many works on inverse problems in the imaging sciences consider regularization via one or more penalty
functions or constraint sets. When the models/images are not easily described using one or a few penalty
functions/constraints, additive model descriptions for regularization lead to better imaging results. These
include cartoon-texture decomposition, morphological component analysis, and robust principal component
analysis; methods that typically rely on penalty functions. We propose a regularization framework, based
on the Minkowski set, that merges the strengths of additive models and constrained formulations. We
generalize the Minkowski set, such that the model parameters are the sum of two components, each of
which is constrained to an intersection of sets. Furthermore, the sum of the components is also an element
of another intersection of sets. These generalizations allow us to include multiple pieces of prior knowledge
on each of the components, as well as on the sum of components, which is necessary to ensure physical
feasibility of partial-differential-equation based parameters estimation problems. We derive the projection
operation onto the generalized Minkowski sets and construct an algorithm based on the alternating
direction method of multipliers. We illustrate how we benefit from using more prior knowledge in the
form of the generalized Minkowski set using seismic waveform inversion and video background-anomaly
separation.},
  keywords = {inverse},
  eprint={1903.03942},
  note = {(arXiv)},
  url = {http://arxiv.org/abs/1903.03942}
}

@ARTICLE{witte2018alf,
  author = {Philipp A. Witte and Mathias Louboutin and Navjot Kukreja and Fabio Luporini and Michael Lange and Gerard J. Gorman and Felix J. Herrmann},
  title = {A large-scale framework for symbolic implementations of seismic inversion algorithms in Julia},
  journal = {Geophysics},
  volume = {84},
  number = {3},
  pages = {F57-F71},
  year = {2019},
  month = {03},
  abstract = {Writing software packages for seismic inversion is a very challenging task, since problems such as full-waveform inversion or least-squares imaging are both algorithmically and computationally demanding due to the large number of unknown parameters and the fact that we are propagating waves over many wavelengths. Software frameworks therefore need to combine both versatility and performance to provide geophysicists with the means and flexibility to implement complex algorithms that scale to exceedingly large 3D problems. Following these principles, we introduce the Julia Devito Inversion framework, an open-source software package in Julia for large-scale seismic modeling and inversion based on Devito, a domain-specific language compiler for automatic code generation. The framework consists of matrix-free linear operators for implementing seismic inversion algorithms that closely resembles the mathematical notation, a flexible resilient parallelization and an interface to Devito for generating optimized stencil code to solve the underlying wave equations. In comparison to many manually optimized industry codes written in low-level languages, our software is built on the idea of independent layers of abstractions and user interfaces with symbolic operators, making it possible to manage both the complexity of algorithms and performance optimizations, while preserving modularity, which allows for a level of expressiveness needed to formulate a broad range of wave-equation-based inversion problems. Through a series of numerical examples, we demonstrate that this allows users to implement algorithms for waveform inversion and imaging as simple Julia scripts that scale to large-scale 3D problems; thus providing a truly performant research and production framework.},
  keywords = {FWI, LSRTM, modeling, inversion, software},
  doi = {10.1190/geo2018-0174.1},
  note = {(Geophysics)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2019/witte2018alf/witte2018alf.pdf}
}

@ARTICLE{witte2018cls,
  author = {Philipp A. Witte and Mathias Louboutin and Fabio Luporini and Gerard J. Gorman and Felix J. Herrmann},
  title = {Compressive least-squares migration with on-the-fly Fourier transforms},
  journal = {Geophysics},
  volume = {84},
  number = {5},
  pages = {R655-R672},
  year = {2019},
  month = {08},
  abstract = {Least-squares reverse-time migration is a powerful approach for true amplitude seismic imaging of complex geological structures, but the successful application of this method is currently hindered by its enormous computational cost, as well as high memory requirements for computing the gradient of the objective function. We tackle these problems by introducing an algorithm for low-cost sparsity-promoting least-squares migration using on-the-fly Fourier transforms. We formulate the least-squares migration objective function in the frequency domain and compute gradients for randomized subsets of shot records and frequencies, thus significantly reducing data movement and the number of overall wave equations solves. By using on-the-fly Fourier transforms, we can compute an arbitrary number of monochromatic frequency-domain wavefields with a time-domain modeling code, instead of having to solve individual Helmholtz equations for each frequency, which quickly becomes computationally infeasible when moving to high frequencies. Our numerical examples demonstrate that compressive imaging with on-the-fly Fourier transforms provides a fast and memory-efficient alternative to time-domain imaging with optimal checkpointing, whose memory requirements for a fixed background model and source wavelet is independent of the number of time steps. Instead, memory and additional computational cost grow with the number of frequencies and determine the amount of subsampling artifacts and crosstalk. In contrast to optimal checkpointing, this offers the possibility to trade both memory and computational cost for image quality or a larger number of iterations and is advantageous in new computing environments such as the cloud, where compute is often cheaper than memory and data movement.},
  keywords = {least squares migration, Fourier, sparsity-promotion},
  doi = {10.1190/geo2018-0490.1},
  note = {(Geophysics)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2019/witte2018cls/witte2018cls.pdf}
}

@ARTICLE{witte2019ecl,
  author = {Philipp A. Witte and Mathias Louboutin and Navjot Kukreja and Fabio Luporini and Michael Lange and Gerard J. Gorman and Felix J. Herrmann},
  title = {Geophysics Bright Spots: Efficient coding of large-scale seismic
inversion algorithms},
  journal = {The Leading Edge},
  volume = {38},
  number = {6},
  pages = {482-484},
  year = {2019},
  abstract = {In “A large-scale framework for symbolic implementations of seismic inversion algorithms in Julia,” Witte et al. describe new developments in how to code complex geophysical algorithms in a concise way. Subsurface seismic imaging and parameter estimation are among the most computationally challenging problems in the scientific community. Codes for solving seismic inverse problems, such as FWI or least-squares reverse time migration (LS-RTM), need to be highly optimized, but at the same time, facilitate the implementation of complex optimization algorithms. Traditionally, production-level codes in the oil and gas industry were exclusively written in low-level languages, such as C or Fortran, with extensive amounts of manual performance optimizations, thus making code maintenance, debugging, and adoption of new algorithms prohibitively challenging. Witte et al. present a paradigm of software engineering for seismic inverse problems based on symbolic user interfaces and code generation with automated performance optimization. Inspired by recent deep learning frameworks, the Julia Devito inversion framework (JUDI; an open-source software package) combines high-level abstractions for expressing seismic inversion algorithms with a domain-specific language compiler called Devito for solving the underlying wave equations. Devito's generated code is compiled just in time and outperforms codes with manual performance optimizations. JUDI utilizes Julia's high-level parallelization, making the software easily adaptable to a variety of computing environments such as densely connected HPC clusters or the cloud. The numerical examples (Figure 3) demonstrate the ability to implement a variety of complex algorithms for FWI and LS-RTM in a few lines of Julia code and run it on large-scale 3D models. The paper concludes that abstractions and performance are not mutually exclusive, and use of symbolic user interfaces can facilitate the implementation of new and innovative seismic inversion algorithms.},
  keywords = {bright spots, inversion, large-scale, julia},
  doi = {10.1190/tle38060482.1},
  note = {(The Leading Edge)},
  url = {https://library.seg.org/doi/10.1190/tle38060482.1}
}

@ARTICLE{kumar2018toi,
  author = {Rajiv Kumar and Marie Graff-Kray and Ivan Vasconcelos and Felix J. Herrmann},
  title = {Target-oriented imaging using extended image volumes—a low-rank factorization approach},
  journal = {Geophysical Prospecting},
  volume = {67},
  number = {5},
  pages = {1312-1328},
  year = {2019},
  abstract = {Imaging in geological challenging environments has led to new developments, including the idea of generating reflection responses by means of interferometric redatuming at a given target datum in the subsurface, when the target datum lies beneath a complex overburden. One way to perform this redatuming is via conventional model-based wave-equation techniques. But those techniques can be computationally expensive for large-scale seismic
problems since the number of wave-equation solves is equal to two-times the number of
sources involved during seismic data acquisition. Also conventional shot-profile
techniques require lots of memory to save full subsurface extended image volumes. Therefore,
they only form subsurface image volumes in either horizontal or vertical directions. We now present a
randomized singular value decomposition based approach built upon the matrix
probing scheme, which takes advantage of the algebraic structure of the extended
imaging system. This low-rank representation enables us to overcome both the computational
cost associated with the number of wave-equation solutions and memory usage due to explicit
storage of full subsurface extended image volumes employed by conventional migration
methods. Experimental results on complex geological models demonstrate the efficacy of
the proposed methodology and allow practical reflection-based extended imaging for large-scale 5D seismic data.},
  keywords = {randomized linear algebra, extended image volumes, target-oriented imaging},
  doi = {10.1111/1365-2478.12779},
  note = {(Geophysical Prospecting)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalProspecting/2019/kumar2018toi/kumar2018toi.html}
}

@ARTICLE{louboutin2018dae,
  author = {Mathias Louboutin and Michael Lange and Fabio Luporini and Navjot Kukreja and Philipp A. Witte and Felix J. Herrmann and Paulius Velesko and Gerard J. Gorman},
  title = {Devito (v3.1.0): an embedded domain-specific language for finite differences and geophysical exploration},
  journal = {Geoscientific Model Development},
  year = {2019},
  abstract = {We introduce Devito, a new domain-specific language for implementing
high-performance finite difference partial differential equation solvers.
The motivating application is exploration seismology where methods such as
Full-Waveform Inversion and Reverse-Time Migration are used to
invert terabytes of seismic data to create images of the earth's
subsurface. Even using modern supercomputers, it can take weeks to process a
single seismic survey and create a useful subsurface image. The computational cost is dominated by the numerical solution of wave equations and their corresponding adjoints. Therefore, a great deal of effort is invested in aggressively optimizing the performance of these wave-equation propagators for different computer architectures. Additionally, the actual set of partial differential equations being solved and their numerical discretization is under constant innovation as increasingly realistic representations of the physics are developed, further ratcheting up the cost of practical solvers. By embedding a domain-specific language within Python and making heavy use of SymPy, a symbolic mathematics library, we make it possible to develop finite difference simulators quickly using a
syntax that strongly resembles the mathematics. The Devito compiler reads this code and applies a wide range of analysis to generate highly optimized and parallel code. This approach can reduce the development time of a verified and optimized solver from months to days.},
  keywords = {wave-equation, modeling, finite-differences, HPC},
  doi = {10.5194/gmd-12-1165-2019},
  note = {(Geoscientific Model Development)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/GMD/2019/louboutin2018dae/louboutin2018dae.pdf}
}

@ARTICLE{silva2018alr,
  author = {Curt Da Silva and Yiming Zhang and Rajiv Kumar and Felix J. Herrmann},
  title = {Applications of low-rank compressed seismic data to full waveform inversion and extended image volumes},
  journal = {Geophysics},
  year = {2019},
  abstract = { Conventional oil and gas fields are increasingly difficult to explore and image, resulting in the call for more complex wave-equation based inversion algorithms that require dense long-offset samplings. Consequently, there is an exponential growth in the size of data volumes and prohibitive demands on computational resources. In this work, we propose a method to compress and process seismic data directly in a low-rank tensor format, which drastically reduces the amount of storage required to represent the data. We first outline how seismic data exhibits low-rank structure in a particular transform-domain, which can be exploited to compress the dense data in one extremely storage-efficient tensor format when the data is fully sampled. In the more realistic case of missing data, we can use efficient interpolation techniques to approximate the fully sampled volume in compressed form. In either case, once we have our data represented in its compressed tensor form, we design an algorithm to extract source or receiver gathers directly from the compressed parameters. This extraction process can be done on-the-fly directly on the compressed data, in a full waveform inversion context, and does not require scanning through the entire dataset in order to form shot gathers. To the best of our knowledge, this work is one of the first major contributions to working with seismic data applications directly in the compressed domain without reconstructing the entire data volume. We use a stochastic inversion approach, which works with small subsets of source experiments at each iteration, further reducing the computational and memory costs of full waveform inversion. We also demonstrate how this data compression and extraction technique can be applied to forming full subsurface image gathers through probing techniques.},
  keywords = {low rank, tensor, common shot/receiver gather, full waveform inversion, extended image volume},
  doi = {10.1190/geo2018-0116.1},
  note = {Accepted on January 22, 2019.},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2019/silva2018alr/silva2018alr.pdf}
}

@ARTICLE{dasilva2017uls,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {A unified {2D/3D} large scale software environment for nonlinear inverse problems},
  journal = {ACM Transactions on Mathematical Software},
  year = {2019},
  abstract = {Large scale parameter estimation problems are some of
                  the most computationally demanding problems. An
                  academic researcher's domain-specific knowledge
                  often precludes that of software design, which
                  results in software frameworks for inversion that
                  are technically correct, but not scalable to
                  realistically-sized problems. On the other hand, the
                  computational demands of the problem for realistic
                  problems result in industrial codebases that are
                  geared solely for performance, rather than
                  comprehensibility or flexibility. We propose a new
                  software design that bridges the gap between these
                  two seemingly disparate worlds. A hierarchical and
                  modular design allows a user to delve into as much
                  detail as she desires, while using high performance
                  primitives at the lower levels. Our code has the
                  added benefit of actually reflecting the underlying
                  mathematics of the problem, which lowers the
                  cognitive load on user using it and reduces the
                  initial startup period before a researcher can be
                  fully productive. We also introduce a new
                  preconditioner for the Helmholtz equation that is
                  suitable for fault-tolerant distributed
                  systems. Numerical experiments on a variety of 2D
                  and 3D test problems demonstrate the effectiveness
                  of this approach on scaling algorithms from small to
                  large scale problems with minimal code changes.},
  keywords = {optimization, PDE-constrained inversion, large scale, matlab},
  note = {Accepted on January 27, 2019.},
  url = {https://slim.gatech.edu/Publications/Public/Journals/ACMTOMS/2019/dasilva2017uls/dasilva2017uls.html}
}

%----- 2018 -----%

@ARTICLE{liu2018ssi,
  author = {Michelle Liu and Rajiv Kumar and Eldad Haber and Aleksandr Y. Aravkin},
  title = {Simultaneous-shot inversion for PDE-constrained optimization problems with missing data},
  journal = {Inverse Problems},
  volume = {35},
  number = {2},
  pages = {025003},
  year = {2018},
  abstract = {Stochastic optimization is key to efficient inversion in
PDE-constrained optimization. Using `simultaneous shots', or random
superposition of source terms, works very well in simple acquisition
geometries where all sources see all receivers, but this rarely occurs in practice. We
develop an approach that interpolates data to an ideal acquisition geometry
while solving the inverse problem using simultaneous shots. The approach is
formulated as a joint inverse problem, combining ideas from low-rank
interpolation with full-waveform inversion. Results using synthetic
experiments illustrate the flexibility and efficiency of the approach.},
  keywords = {Optimization, low-rank interpolation, full-waveform inversion},
  doi = {10.1088/1361-6420/aaf317},
  note = {(Inverse Problems)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/InverseProblems/2018/liu2018ssi/liu2018ssi.pdf}
}


@ARTICLE{fang2017sewri,
  author = {Zhilong Fang and Rongrong Wang and Felix J. Herrmann},
  title = {Source estimation for wavefield-reconstruction inversion},
  journal = {Geophysics},
  volume = {83},
  number = {4},
  pages = {R345-R359},
  year = {2018},
  abstract = {Source estimation is essential to all the wave-equation-based seismic inversions, including full-waveform inversion and the recently proposed wavefield-reconstruction inversion. When the source estimation is inaccurate, errors will propagate into the predicted data and introduce additional data misfit. As a consequence, inversion results that minimize this data misfit may become erroneous. To mitigate the errors introduced by the incorrect and pre-estimated sources, an embedded procedure that updates sources along with medium parameters is necessary for the inversion. So far, such a procedure is still missing in the context of wavefield-reconstruction inversion, a method that is, in many situations, less prone to local minima related to the so-called cycle skipping, compared to full-waveform inversion through exact data-fitting. While wavefield-reconstruction inversion indeed helps to mitigate issues related to cycle skipping by extending the search space with wavefields as auxiliary variables, it relies on having access to the correct source functions. In this paper, we remove the requirement of having the accurate source functions by proposing a source estimation technique specifically designed for wavefield-reconstruction inversion. To achieve this task, we consider the source functions as unknown variables and arrive at an objective function that depends on the medium parameters, wavefields, and source functions. During each iteration, we apply the so-called variable projection method to simultaneously project out the source functions and wavefields. After the projection, we obtain a reduced objective function that only depends on the medium parameters and invert for the unknown medium parameters by minimizing this reduced objective. Numerical experiments illustrate that this approach can produce accurate estimates of the unknown medium parameters without any prior information of the source functions.},
  keywords = {WRI, source, estimation, variable projection, FWI},
  doi = {10.1190/geo2017-0700.1},
  note = {(Geophysics)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2018/fang2017sewri/fang2017sewri.html}
}

@ARTICLE{fang2017uqfip,
  author = {Zhilong Fang and Curt Da Silva and Rachel Kuske and Felix J. Herrmann},
  title = {Uncertainty quantification for inverse problems with weak partial-differential-equation constraints},
  journal = {Geophysics},
  volume = {83},
  number = {6},
  pages = {R629-R647},
  year = {2018},
  abstract = { In a statistical inverse problem, the objective is a complete
              statistical description of unknown parameters from noisy observations in order to quantify uncertainties of the parameters of interest. We consider inverse problems with partial-differential-equation-constraints, which are applicable to
              a variety of seismic problems. Bayesian inference is one of the most widely-used approaches to precisely quantify statistics through a posterior distribution, incorporating uncertainties in observed data, modeling kernel, and prior knowledge of the parameters. Typically when formulating the posterior distribution, the partial-differential-equation-constraints are required to be exactly satisfied, resulting in a highly nonlinear forward map and a posterior distribution with many local maxima. These drawbacks make it difficult to find an appropriate approximation for the posterior distribution. Another complicating factor is that traditional Markov chain Monte Carlo methods are known to converge slowly for realistically sized problems. In this work, we relax the partial-differential-equation-constraints by introducing an auxiliary variable, which allows for Gaussian deviations in the partial-differential-equations. Thus, we obtain a new bilinear posterior distribution consisting of both data and partial-differential-equation misfit terms. We illustrate that for a particular range of variance choices for the partial-differential-equation misfit term, the new posterior distribution has fewer modes and can be well-approximated by a Gaussian distribution, which can then be sampled in a straightforward manner. Since it is prohibitively expensive to explicitly construct the dense covariance matrix of the Gaussian approximation for intermediate to large-scale problems, we present a method to implicitly construct it, which enables efficient sampling. We apply this framework to two-dimensional seismic inverse problems with 1,800 and 92,455 unknown parameters. The results illustrate that our framework can produce comparable statistical quantities to those produced by conventional Markov chain Monte Carlo type methods while requiring far fewer partial-differential-equation solves, which are the main computational bottlenecks in these problems.},
  keywords = {UQ, FWI, acoustic, weak-constraint},
  doi = {10.1190/geo2017-0824.1},
  note = {(Geophysics)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2018/fang2017uqfip/fang2017uqfip.html}
}

@ARTICLE{peters2018pmf,
  author = {Bas Peters and Brendan R. Smithyman and Felix J. Herrmann},
  title = {Projection methods and applications for seismic nonlinear inverse problems with multiple constraints},
  journal = {Geophysics},
  volume = {84},
  number = {2},
  pages = {R251-R269},
  year = {2018},
  month = {02},
  abstract = {Nonlinear inverse problems are often hampered by non-uniqueness and local minima because of missing low frequencies and far offsets in the data, lack of access to good starting models, noise, and modeling errors. A well-known approach to counter these deficiencies is to include prior information on the unknown model, which regularizes the inverse problem. While conventional regularization methods have resulted in enormous progress in ill-posed (geophysical) inverse problems, challenges remain when the prior information consists of multiple pieces. To handle this situation, we propose an optimization framework that allows us to add multiple pieces of prior information in the form of constraints. Compared to additive regularization penalties, constraints have a number of advantages making them more suitable for inverse problems such as full-waveform inversion. The proposed framework is rigorous because it offers assurances that multiple constraints are imposed uniquely at each iteration, irrespective of the order in which they are invoked. To project onto the intersection of multiple sets uniquely, we employ Dykstra’s algorithm that scales to large problems and does not rely on trade-off parameters. In that sense, our approach differs substantially from approaches such as Tikhonov regularization, penalty methods, and gradient filtering. None of these offer assurances, which makes them less suitable to full-waveform inversion where unrealistic intermediate results effectively derail the iterative inversion process. By working with intersections of sets, we keep expensive objective and gradient calculations unaltered, separate from projections, and we also avoid trade-off parameters. These features allow for easy integration into existing code bases. In addition to more predictable behavior, working with constraints also allows for heuristics where we built up the complexity of the model gradually by relaxing the constraints. This strategy helps to avoid convergence to local minima that represent unrealistic models. We illustrate this unique feature with examples of varying complexity.},
  keywords = {full-waveform inversion, optimization, constraints, regularization, projection, intersection},
  doi = {10.1190/geo2018-0192.1},
  note = {(Geophysics)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2018/peters2018pmf/peters2018pmf.html}
}

@ARTICLE{sharan2018fsp,
  author = {Shashin Sharan and Rongrong Wang and Felix J. Herrmann},
  title = {Fast sparsity-promoting microseismic source estimation},
  journal = {Geophysical Journal International},
  volume = {216},
  number = {1},
  pages = {164-181},
  year = {2019},
  month = {01},
  abstract = {Microseismic events are generated during hydraulic fracturing of unconventional reservoirs and carry information on fracture locations and the origin times associated with these microseismic events. For drilling purposes and to prevent hazardous situations, we need to have accurate knowledge on the fracture locations as well as on their size, and their density. Because microseismic waves can travel far distances, microseismic data collected at the surface and or in boreholes can help us to monitor hydraulic fracturing. While so-called back propagation or time-reversal methods are able to focus recorded energy back onto the sources when a reasonable velocity model is available, these methods suffer from blurring especially in situations where the data acquisition suffers from lack of aperture, sparse sampling, and noise. As a result, these methods typically cannot resolve sources in close proximity, a desired feature since we need this information if we want to follow the fracture evolution in space and time. In that situation, we need to estimate the locations and the associated source-time functions for closely spaced microseismic sources along the active fractures. To overcome the limitations of time-reversal methods, we propose a wave-equation based inversion approach where we invert for the complete source wavefield in both space and time. By promoting sparsity on the source wavefield in space, we negate the effects of non-radiating sources during the inversion and obtain high-resolution intensity plots and high-fidelity estimates for the source-time functions. We obtain these results relatively fast by accelerating the linearized Bregman method with a dual formulation. Through experiments, we demonstrate that our method is computationally feasible, robust to noise, and works for closely spaced sources with overlapping source-time functions in complex geological settings.},
  keywords = {Waveform inversion, Joint inversion, induced seismicity},
  doi = {10.1093/gji/ggy415},
  note = {(Geophysical Journal International)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalJournalInternational/2018/sharan2018fsp/sharan2018fsp.pdf}
}

@ARTICLE{witte2018fwip3,
  author = {Philipp A. Witte and Mathias Louboutin and Keegan Lensink and Michael Lange and Navjot Kukreja and Fabio Luporini and Gerard Gorman and Felix J. Herrmann},
  title = {Full-Waveform Inversion - Part 3: optimization},
  journal = {The  Leading  Edge},
  volume = {37},
  number = {2},
  pages = {142-145},
  year = {2018},
  month = {01},
  abstract = {This tutorial is the third part of a full-waveform inversion (FWI) tutorial series with a step-by-step walkthrough of setting up forward and adjoint wave equations and building a basic FWI inversion framework. For discretizing and solving wave equations, we use Devito, a Python-based domain-specific language for automated generation of finite-difference code (Lange et al., 2016). The first two parts of this tutorial (Louboutin et al., 2017, 2018) demonstrated how to solve the acoustic wave equation for modeling seismic shot records and how to compute the gradient of the FWI objective function using the adjoint-state method. With these two key ingredients, we will now build an inversion framework that can be used to minimize the FWI least-squares objective function.},
  keywords = {devito, finite-differences, FWI, Modeling, tutorial, inversion},
  doi  = {10.1190/tle37020142.1},
  note = {(The Leading Edge)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/TheLeadingEdge/2018/witte2018fwip3/witte2018fwip3.html}
}

%----- 2017 -----%

@ARTICLE{kumar2017hrc,
  author = {Rajiv Kumar and Haneet Wason and Shashin Sharan and Felix J. Herrmann},
  title = {Highly repeatable {3D} compressive full-azimuth towed-streamer time-lapse acquisition --- a numerical feasibility study at scale},
  journal = {The  Leading  Edge},
  volume = {36},
  number = {8},
  pages = {677-687},
  year = {2017},
  month = {08},
  abstract = {Most conventional 3D time-lapse (or 4D) acquisitions are
                  ocean-bottom cable (OBC) or ocean-bottom node (OBN)
                  surveys since these surveys are relatively easy to
                  replicate compared to towed-streamer surveys. To
                  attain high degrees of repeatability, survey
                  replicability and dense periodic sampling has become
                  the norm for 4D surveys that renders this technology
                  expensive. Conventional towed-streamer acquisitions
                  suffer from limited illumination of subsurface due
                  to narrow azimuth. Although, acquisition techniques
                  such as multi-azimuth, wide-azimuth, rich-azimuth
                  acquisition, etc., have been developed to illuminate
                  the subsurface from all possible angles, these
                  techniques can be prohibitively expensive for
                  densely sampled surveys. This leads to uneven
                  sampling, i.e., dense receiver and coarse source
                  sampling or vice-versa, in order to make these
                  acquisitions more affordable. Motivated by the
                  design principles of Compressive Sensing (CS), we
                  acquire economic, randomly subsampled (or
                  compressive) and simultaneous towed-streamer
                  time-lapse data without the need of replicating the
                  surveys. We recover densely sampled time-lapse data
                  on one and the same periodic grid by using a
                  joint-recovery model (JRM) that exploits shared
                  information among different time-lapse recordings,
                  coupled with a computationally cheap and scalable
                  rank-minimization technique. The acquisition is low
                  cost since we have subsampled measurements (about
                  70\% subsampled), simulated with a simultaneous
                  long-offset acquisition configuration of two source
                  vessels travelling across a survey area at random
                  azimuths. We analyze the performance of our proposed
                  compressive acquisition and subsequent recovery
                  strategy by conducting a synthetic, at scale,
                  seismic experiment on a 3D time-lapse model
                  containing geological features such as channel
                  systems, dipping and faulted beds, unconformities
                  and a gas cloud. Our findings indicate that the
                  insistence on replicability between surveys and the
                  need for OBC/OBN 4D surveys can, perhaps, be
                  relaxed. Moreover, this is a natural next step
                  beyond the successful CS acquisition examples
                  discussed in this special issue.},
  keywords = {time-lapse seismic, marine, 3D, simultaneous long offset, CS, rank minimization},
  doi = {10.1190/tle36080677.1},
  note = {(The Leading Edge)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/TheLeadingEdge/2017/kumar2017hrc/kumar2017hrc.html}
}

@ARTICLE{louboutin2017fwi,
  author = {Mathias Louboutin and Philipp A. Witte and Michael Lange and Navjot Kukreja and Fabio Luporini and Gerard Gorman and Felix J. Herrmann},
  title = {Full-Waveform Inversion - Part 1: forward modeling},
  journal = {The  Leading  Edge},
  volume = {36},
  number = {12},
  pages = {1033-1036},
  year = {2017},
  month = {12},
  abstract = {Since its re-introduction by Pratt (1999), full-waveform inversion (FWI) has gained a lot of attention in geophysical exploration because of its ability to build high resolution velocity models more or less automatically in areas of complex geology. While there is an extensive and growing literature on the topic, publications focus mostly on technical aspects, making this topic inaccessible for a broader audience due to the lack of simple introductory resources for newcomers to geophysics. We will accomplish this by providing a hands-on walkthrough of FWI using Devito (Lange et al. 2016), a system based on domain-specific languages that automatically generates code for time-domainfinite-differences.},
  keywords = {finite-differences, devito, tutorial, FWI, modeling},
  doi  = {10.1190/tle36121033.1},
  note = {(The Leading Edge)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/TheLeadingEdge/2017/louboutin2017fwi/louboutin2017fwi.html}
}

@ARTICLE{louboutin2017fwip2,
  author = {Mathias Louboutin and Philipp A. Witte and Michael Lange and Navjot Kukreja and Fabio Luporini and Gerard Gorman and Felix J. Herrmann},
  title = {Full-Waveform Inversion - Part 2: adjoint modeling},
  journal = {The  Leading  Edge},
  volume = {37},
  number = {1},
  pages = {69-72},
  year = {2018},
  month = {01},
  abstract = {This tutorial is the second part of a three part tutorial series on full-waveform inversion (FWI), in which we provide a step by step walk through of setting up forward and adjoint wave equation solvers and an optimization framework for inversion. In part 1 (Louboutin et al., 2017), we demonstrated how to discretize the acoustic wave equation and how to set up a basic forward modeling scheme using Devito, a domain-specific language (DSL) in Python for automated finite-difference (FD) computations (Lange et al., 2016). Devito allows us to define wave equations as symbolic Python expressions (Meurer et al., 2017), from which optimized FD stencil code is automatically generated at run time. In part 1, we show how we can use Devito to set up and solve acoustic wave equations with (impulsive) seismic sources and sample wavefields at the receiver locations to model shot records.},
  keywords = {tutorial, devito, finite-difference, acoustic, FWI},
  doi  = {10.1190/tle37010069.1},
  note = {(The Leading Edge)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/TheLeadingEdge/2018/louboutin2017fwip2/louboutin2017fwip2.html}
}

@ARTICLE{oghenekohwo2017hrt,
  author = {Felix Oghenekohwo and Felix J. Herrmann},
  title = {Highly repeatable time-lapse seismic with distributed {Compressive} {Sensing}---mitigating effects of calibration errors},
  journal = {The  Leading  Edge},
  year = {2017},
  month = {08},
  volume = {36},
  number = {8},
  pages = {688-694},
  abstract = {Recently, we demonstrated that combining joint recovery
                  with low-cost non-replicated randomized sampling
                  tailored to time-lapse seismic can give us access to
                  high fidelity, highly repeatable, dense prestack
                  vintages, and high-grade time-lapse. To arrive at
                  this result, we assumed well-calibrated
                  surveys---i.e., we presumed accurate post-plot
                  source/receiver positions. Unfortunately, in
                  practice seismic surveys are prone to calibration
                  errors, which are unknown deviations between actual
                  and post-plot acquisition geometry. By means of
                  synthetic experiments, we analyze the possible
                  impact of these errors on vintages and on time-lapse
                  data obtained with our joint recovery model from
                  compressively sampled surveys. Supported by these
                  experiments, we demonstrate that highly repeatable
                  time-lapse vintages are attainable despite the
                  presence of unknown calibration errors in the
                  positions of the shots. We assess the repeatability
                  quantitatively for two scenarios by studying the
                  impact of calibration errors on conventional dense
                  but irregularly sampled surveys and on low-cost
                  compressed surveys. To separate time-lapse effects
                  from calibration issues, we consider the idealized
                  case where the subsurface remains unchanged and the
                  practical situation where time-lapse changes are
                  restricted to a subset of the data. In both cases,
                  the quality of the recovered vintages and time-lapse
                  decreases gracefully for low-cost compressed surveys
                  with increasing calibration errors. Conversely, the
                  quality of vintages from expensive densely
                  periodically sampled surveys decreases more rapidly
                  as unknown and difficult to control calibration
                  errors increase.},
  keywords = {time-lapse seismic, marine, random sampling, calibration errors, joint-recovery method},
  doi  = {10.1190/tle36080688.1},
  note = {(The Leading Edge)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/TheLeadingEdge/2017/oghenekohwo2017hrt/oghenekohwo2017hrt.html}
}

@ARTICLE{kumar2016bls,
  author = {Rajiv Kumar and Oscar Lopez and Damek Davis and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Beating level-set methods for {5D} seismic data interpolation: a primal-dual alternating approach},
  journal = {IEEE Transactions on Computational Imaging},
  year = {2017},
  month = {04},
  abstract = {Acquisition cost is a crucial bottleneck for seismic
                  workflows, and low-rank formulations for data
                  interpolation allow practitioners to 'fill in' data
                  volumes from critically subsampled data acquired in
                  the field. Tremendous size of seismic data volumes
                  required for seismic processing remains a major
                  challenge for these techniques. Residual-constrained
                  formulations require less parameter tuning when the
                  target noise floor is known. We propose a new
                  approach to solve residual constrained formulations
                  for interpolation. We represent the data volume in a
                  compressed manner using low-rank matrix factors, and
                  build a block-coordinate algorithm with constrained
                  convex subproblems that are solved with a
                  primal-dual splitting scheme. The develop
                  optimization framework works on the whole seismic
                  temporal frequency slices and does not require
                  windowing or non-trivial sorting of seismic
                  data. The new approach is competitive with state of
                  the art level-set algorithms that interchange the
                  role of objectives with constraints. We use the new
                  algorithm to successfully interpolate a large scale
                  5D seismic data volume (upto 1010 data-points),
                  generated from the geologically complex synthetic 3D
                  Compass velocity model, where 80\% of the data has
                  been removed. We also develop a robust extension of
                  the primal-dual approach to deal with the outliers
                  (or noise) in the data.},
  keywords = {matrix completion, seismic data, seismic trace interpolation, alternating minimization, primal-dual splitting},
  note = {(published online in IEEE Transactions on Computational Imaging)},
  doi = {10.1109/TCI.2017.2693966},
  url = {https://slim.gatech.edu/Publications/Public/Journals/IEEETransComputationalImaging/2017/kumar2016bls/kumar2016bls.pdf}
}


@ARTICLE{louboutin2016ppf,
  author = {Mathias Louboutin and Michael Lange and Felix J. Herrmann and Navjot Kukreja and Gerard Gorman},
  title = {Performance prediction of finite-difference solvers for different computer architectures},
  journal = {Computers \& Geosciences},
  year = {2017},
  month = {08},
  volume = {105},
  pages = {148-157},
  abstract = {The life-cycle of a partial differential equation (PDE)
                  solver is often characterized by three development
                  phases: the development of a stable numerical
                  discretization; development of a correct (verified)
                  implementation; and the optimization of the
                  implementation for different computer
                  architectures. Often it is only after significant
                  time and effort has been invested that the
                  performance bottlenecks of a PDE solver are fully
                  understood, and the precise details varies between
                  different computer architectures. One way to
                  mitigate this issue is to establish a reliable
                  performance model that allows a numerical analyst to
                  make reliable predictions of how well a numerical
                  method would perform on a given computer
                  architecture, before embarking upon potentially long
                  and expensive implementation and optimization
                  phases. The availability of a reliable performance
                  model also saves developer effort as it both informs
                  the developer on what kind of optimisations are
                  beneficial, and when the maximum expected
                  performance has been reached and optimisation work
                  should stop. We show how discretization of a
                  wave-equation can be theoretically studied to
                  understand the performance limitations of the method
                  on modern computer architectures. We focus on the
                  roofline model, now broadly used in the
                  high-performance computing community, which
                  considers the achievable performance in terms of the
                  peak memory bandwidth and peak floating point
                  performance of a computer with respect to
                  algorithmic choices. A first principles analysis of
                  operational intensity for key time-stepping
                  finite-difference algorithms is presented. With this
                  information available at the time of algorithm
                  design, the expected performance on target computer
                  systems can be used as a driver for algorithm
                  design. },
  keywords = {finite differences, performance, modeling, HPC},
  note = {(Computers \& Geosciences)},
  doi = {https://doi.org/10.1016/j.cageo.2017.04.014},
  url = {https://slim.gatech.edu/Publications/Public/Journals/ComputersAndGeosciences/2016/louboutin2016ppf/louboutin2016ppf.pdf}
}


@ARTICLE{oghenekohwo2016GEOPctl,
  author = {Felix Oghenekohwo and Haneet Wason and Ernie Esser and Felix J. Herrmann},
  title = {Low-cost time-lapse seismic with distributed compressive sensing---{Part} 1: exploiting common information among the vintages},
  journal = {Geophysics},
  year = {2017},
  month = {05},
  volume = {82},
  number = {3},
  pages = {P1-P13},
  abstract = {Time-lapse seismic is a powerful technology for
                  monitoring a variety of subsurface changes due to
                  reservoir fluid flow. However, the practice can be
                  technically challenging when one seeks to acquire
                  colocated time-lapse surveys with high degrees of
                  replicability among the shot locations. We have
                  determined that under "ideal" circumstances, in
                  which we ignore errors related to taking
                  measurements off the grid, high-quality prestack
                  data can be obtained from randomized subsampled
                  measurements that are observed from surveys in which
                  we choose not to revisit the same randomly
                  subsampled on-the-grid shot locations. Our
                  acquisition is low cost because our measurements are
                  subsampled. We have found that the recovered finely
                  sampled prestack baseline and monitor data actually
                  improve significantly when the same on-the-grid shot
                  locations are not revisited. We achieve this result
                  by using the fact that different time-lapse data
                  share information and that nonreplicated
                  (on-the-grid) acquisitions can add information when
                  prestack data are recovered jointly. Whenever the
                  time-lapse data exhibit joint structure---i.e., they
                  are compressible in some transform domain and share
                  information---sparsity-promoting recovery of the
                  "common component" and "innovations," with respect
                  to this common component, outperforms independent
                  recovery of the prestack baseline and monitor
                  data. The recovered time-lapse data are of high
                  enough quality to serve as the input to extract
                  poststack attributes used to compute time-lapse
                  differences. Without joint recovery, artifacts---due
                  to the randomized subsampling---lead to
                  deterioration of the degree of repeatability of the
                  time-lapse data. We tested this method by carrying
                  out experiments with reliable statistics from
                  thousands of repeated experiments. We also confirmed
                  that high degrees of repeatability are achievable
                  for an ocean-bottom cable survey acquired with
                  time-jittered continuous recording.},
  keywords = {acquisition, time-lapse seismic, marine, random sampling, joint recovery method},
  note = {(Geophysics)},
  doi = {10.1190/geo2016-0076.1},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2017/oghenekohwo2016GEOPctl/oghenekohwo2016GEOPctl.html}
}


@ARTICLE{peters2016cvp,
  author = {Bas Peters and Felix J. Herrmann},
  title = {Constraints versus penalties for edge-preserving full-waveform inversion},
  journal = {The Leading Edge},
  year = {2017},
  month = {01},
  volume = {36},
  number = {1},
  pages = {94-100},
  abstract = {Full-waveform inversion is challenging in complex
                  geologic areas. Even when provided with an accurate
                  starting model, the inversion algorithms often
                  struggle to update the velocity model. Compared with
                  other areas in applied geophysics, including prior
                  information in full-waveform inversion is still in
                  its relative infancy. In part, this is due to the
                  fact that it is difficult to incorporate prior
                  information that relates to geologic settings where
                  strong discontinuities in the velocity model
                  dominate, because these settings call for nonsmooth
                  regularizations. We tackle this problem by including
                  constraints on the spatial variations and value
                  ranges of the inverted velocities, as opposed to
                  adding penalties to the objective, which is more
                  customary in mainstream geophysical inversion. By
                  demonstrating the lack of predictability of
                  edge-preserving inversion when the regularization is
                  in the form of an added penalty term, we advocate
                  the inclusion of constraints instead. Our examples
                  show that the latter leads to more predictable
                  results and to significant improvements in the
                  delineation of salt bodies when these constraints
                  are relaxed gradually in combination with extending
                  the search space to approximately fit the observed
                  data but not the noise.},
  keywords = {full waveform inversion, salt dome, algorithm, noise, optimization, total variation, penalty methods},
  note = {(The Leading Edge)},
  doi = {10.1190/tle36010094.1},
  url = {https://slim.gatech.edu/Publications/Public/Journals/TheLeadingEdge/2016/peters2016cvp/peters2016cvp.html}
}


@ARTICLE{wason2016GEOPctl,
  author = {Haneet Wason and Felix Oghenekohwo and Felix J. Herrmann},
  title = {Low-cost time-lapse seismic with distributed compressive sensing---{Part} 2: impact on repeatability},
  journal = {Geophysics},
  year = {2017},
  month = {05},
  volume = {82},
  number = {3},
  pages = {P15-P30},
  abstract = {Irregular or off-the-grid spatial sampling of sources
                  and receivers is inevitable in field seismic
                  acquisitions. Consequently, time-lapse surveys
                  become particularly expensive since current
                  practices aim to replicate densely sampled surveys
                  for monitoring changes occurring in the reservoir
                  due to hydrocarbon production. We demonstrate that
                  under certain circumstances, high-quality prestack
                  data can be obtained from cheap randomized
                  subsampled measurements that are observed from
                  nonreplicated surveys. We extend our time-jittered
                  marine acquisition to time-lapse surveys by
                  designing acquisition on irregular spatial grids
                  that render simultaneous, subsampled and irregular
                  measurements. Using the fact that different
                  time-lapse data share information and that
                  nonreplicated surveys add information when prestack
                  data are recovered jointly, we recover periodic
                  densely sampled and colocated prestack data by
                  adapting the recovery method to incorporate a
                  regularization operator that maps traces from an
                  irregular spatial grid to a regular periodic
                  grid. The recovery method is, therefore, a combined
                  operation of regularization, interpolation
                  (estimating missing fine-grid traces from subsampled
                  coarse-grid data), and source separation (unraveling
                  overlapping shot records). By relaxing the
                  insistence on replicability between surveys, we find
                  that recovery of the time-lapse difference shows
                  little variability for realistic field scenarios of
                  slightly nonreplicated surveys that suffer from
                  unavoidable natural deviations in spatial sampling
                  of shots (or receivers) and pragmatic
                  compressed-sensing based nonreplicated surveys when
                  compared to the "ideal" scenario of exact
                  replicability between surveys. Moreover, the
                  recovered densely sampled prestack baseline and
                  monitor data improve significantly when the
                  acquisitions are not replicated, and hence can serve
                  as input to extract poststack attributes used to
                  compute time-lapse differences. Our observations are
                  based on experiments conducted for an ocean-bottom
                  cable survey acquired with time-jittered continuous
                  recording assuming source equalization (or same
                  source signature) for the time-lapse surveys and no
                  changes in wave heights, water column velocities or
                  temperature and salinity profiles, etc.},
  keywords = {marine acquisition, time-lapse seismic, off-the-grid recovery, random sampling, joint recovery method, optimization},
  note = {(Geophysics)},
  doi = {10.1190/geo2016-0252.1},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2017/wason2016GEOPctl/wason2016GEOPctl.html}
}


@ARTICLE{yarman2017GPmbd,
  author = {Can Evren Yarman and Rajiv Kumar and James Rickett},
  title = {A model based data driven dictionary learning for seismic data representation},
  journal = {Geophysical Prospecting},
  year = {2017},
  month = {04},
  abstract = {Planar waves events recorded in a seismic array can be
                  represented as lines in the Fourier domain. However,
                  in the real-world seismic events usually have
                  curvature or amplitude variability that means their
                  Fourier transforms are no longer strictly linear,
                  but rather occupy conic regions of the Fourier
                  domain that are narrow at low frequencies, but
                  broaden at high frequencies where the effect of
                  curvature becomes more pronounced. One can consider
                  these regions as localised "signal cones". In this
                  work, we consider a space-time variable signal cone
                  to model the seismic data. The variability of the
                  signal cone is obtained through scaling, slanting
                  and translation of the kernel for cone-limited
                  (C-limited) functions (functions whose Fourier
                  transform lives within a cone) or C-Gaussian
                  function (a multivariate function whose Fourier
                  transform decays exponentially with respect to
                  slowness and frequency) which constitutes our
                  dictionary. We find a discrete number of scaling,
                  slanting and translation parameters from a continuum
                  by optimally matching the data. This is a nonlinear
                  optimization problem which we address by a fixed
                  point method which utilizes a variable projection
                  method with e1 constraints on the linear parameters
                  and bound constraints on the nonlinear
                  parameters. We observe that slow decay and
                  oscillatory behavior of the kernel for C-limited
                  functions constitute bottlenecks for the
                  optimization problem which we partially overcome by
                  the C-Gaussian function. We demonstrate our method
                  through an interpolation example. We present the
                  interpolation result using the estimated parameters
                  obtained from the proposed method and compare it
                  with those obtained using sparsity promoting
                  curvelet decomposition, matching pursuit Fourier
                  interpolation and sparsity promoting plane wave
                  decomposition methods.},
  keywords = {dictionary learning, kernel method, reproducing kernel, variational method, nonlinear optimization, variable projection},
  note = {(published online in Geophysical Prospecting)},
  doi = {10.1111/1365-2478.12533},
  url = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalProspecting/2017/yarman2017GPmbd/yarman2017GPmbd.pdf}
}


%----- 2016 -----%

@ARTICLE{bougher2015CSEGust,
  author = {Ben B. Bougher and Felix J. Herrmann},
  title = {Using the scattering transform to predict stratigraphic units from well logs},
  journal = {CSEG Recorder},
  year = {2016},
  month = {01},
  volume = {41},
  number = {1},
  pages = {22-25},
  abstract = {Much of geophysical interpretation relies on trained
                  pattern recognition of signals and images, a
                  workflow that can be modeled by supervised machine
                  learning. A challenge of supervised learning is
                  determining a physically meaningful feature set that
                  can successfully classify the data. Defined by a
                  network of cascading wavelets, the scattering
                  transform provides a non-linear multiscale analysis
                  that has deep connections to the fractal statistics
                  of the signal. Interestingly, the scattering
                  transform takes the form of a pre-trained
                  convolutional neural network. This paper uses the
                  scattering transform to extract features from well
                  logs in order to train a classifier that can predict
                  stratigraphic units. The methodology is tested on
                  interpreted well logs from Trenton-Black River
                  project and initial results are presented.},
  keywords = {machine learning, scattering transform, well logs},
  note = {(CSEG Recorder)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/CSEGRecorder/2016/bougher2015CSEGust/bougher2015CSEGust.html},
  url2 = {http://csegrecorder.com/articles/view/using-the-scattering-transform-to-predict-strat-units-from-well-logs}
}


@ARTICLE{esser2016TLEcwi,
  author = {Ernie Esser and Llu\'{i}s Guasch and Felix J. Herrmann and Mike Warner},
  title = {Constrained waveform inversion for automatic salt flooding},
  journal = {The Leading Edge},
  year = {2016},
  month = {03},
  volume = {35},
  number = {3},
  pages = {235-239},
  abstract = {Given appropriate data acquisition, processing to remove
                  nonprimary arrivals, and use of an accurate
                  migration algorithm, it is the quality of the
                  subsurface velocity model that typically controls
                  the quality of imaging that can be obtained from
                  salt-affected seismic data. Full-waveform inversion
                  has the potential to improve the accuracy,
                  resolution, repeatability, and speed with which such
                  velocity models can be generated, but, in the
                  absence of an accurate starting model, that
                  potential is difficult to realize in
                  practice. Presented are successful inversion
                  results, obtained from synthetic subsalt models,
                  using a robust full-waveform inversion code that
                  includes constraints upon the set of allowable earth
                  models. These constraints include limitations on the
                  total variation of the velocity of the model and,
                  most significantly, on the asymmetric variation of
                  velocity with depth such that negative velocity
                  excursions are limited. During the iteration, these
                  constraints are relaxed progressively so that the
                  final model is driven principally by the seismic
                  data, but the constraints act to steer the inversion
                  path away from local minima in its early
                  stages. This methodology is applied to portions of
                  the 2004 BP benchmark and Phase I SEAM salt models,
                  recovering an accurate model of the salt body,
                  including its base and flanks, and an accurate model
                  of the subsalt velocity structure, starting from
                  one-dimensional velocity models that are severely
                  cycle skipped. This approach removes entirely the
                  requirement to pick salt boundaries from migrated
                  seismic data, and acts as a form of automatic salt
                  and sediment flooding during full-waveform
                  inversion.},
  keywords = {high-contrast and high velocity, salt, FWI, velocity-model building, total-variation minimization},
  note = {(The Leading Edge)},
  doi = {10.1190/tle35030235.1},
  url = {http://dx.doi.org/10.1190/tle35030235.1}
}


@ARTICLE{esser2016tvr,
  author = {Ernie Esser and Llu\'{i}s Guasch and Tristan van Leeuwen and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Total-variation regularization strategies in full-waveform inversion},
  journal = {SIAM Journal on Imaging Sciences},
  year = {2018},
  volume = {11},
  number = {1},
  pages = {376-406},
  abstract = {We propose an extended full-waveform inversion
                  formulation that includes general convex constraints
                  on the model. Though the full problem is highly
                  nonconvex, the overarching optimization scheme
                  arrives at geologically plausible results by solving
                  a sequence of relaxed and warm-started constrained
                  convex subproblems. The combination of box,
                  total-variation, and successively relaxed asymmetric
                  total-variation constraints allows us to steer free
                  from parasitic local minima while keeping the
                  estimated physical parameters laterally continuous
                  and in a physically realistic range. For accurate
                  starting models, numerical experiments carried out
                  on the challenging 2004 BP velocity benchmark
                  demonstrate that bound and total-variation
                  constraints improve the inversion result
                  significantly by removing inversion artifacts,
                  related to source encoding, and by clearly improved
                  delineation of top, bottom, and flanks of a
                  high-velocity high-contrast salt inclusion. The
                  experiments also show that for poor starting models
                  these two constraints by themselves are insufficient
                  to detect the bottom of high-velocity inclusions
                  such as salt. Inclusion of the one-sided asymmetric
                  total-variation constraint overcomes this issue by
                  discouraging velocity lows to buildup during the
                  early stages of the inversion. To the author's
                  knowledge the presented algorithm is the first to
                  successfully remove the imprint of local minima
                  caused by poor starting models and band-width
                  limited finite aperture data.},
  keywords = {full-waveform inversion, constrained optimization, total variation, salt, hinge loss},
  note = {(SIAM Journal on Imaging Sciences)},
  note2 = {(Computing Research Repository)},
  doi = {10.1137/17M111328X},
  url = {https://slim.gatech.edu/Publications/Public/Journals/SIAMJournalOnImagingSciences/2018/esser2016tvr/esser2016tvr.pdf},
  url2 = {https://doi.org/10.1137/17M111328X},
  url3 = {https://arxiv.org/abs/1608.06159}
}


@ARTICLE{lin2015scatterEPSI,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Estimation of primaries by sparse inversion with scattering-based multiple predictions for data with large gaps},
  journal = {Geophysics},
  year = {2016},
  month = {05},
  volume = {81},
  number = {3},
  pages = {V183-V197},
  abstract = {We propose to solve the Estimation of Primaries by
                  Sparse Inversion problem without any explicit data
                  reconstruction, from a seismic record with missing
                  near-offsets and large holes in the acquisition grid
                  which does not produce aliasing but otherwise causes
                  large errors in the multiple prediction. Exclusion
                  of the unknown data as an inversion variable from
                  the process is desirable, since it sidesteps
                  possible issues arising from fitting observations
                  that are also unknowns in the same inversion
                  problem. Instead, we simulate their multiple
                  contributions by augmenting the forward prediction
                  model for the total wavefield with a scattering
                  series that mimics the action of the free surface
                  reflector within the confines of the unobserved
                  trace locations. Each term in this scattering series
                  simply involves convolution of the predicted
                  wavefield once more with the current estimated
                  surface-free Green's function at these unobserved
                  locations. We investigate the necessary
                  modifications to the primary estimation algorithm to
                  account for the resulting nonlinearity in the
                  modeling operator, and also demonstrate that just a
                  few scattering terms are enough to satisfactorily
                  mitigate the effects of near-offset data gaps during
                  the inversion process. Numerical experiments on
                  synthetic data show that the final derived method
                  can significantly outperform explicit data
                  reconstruction for large near-offset gaps. This is
                  achieved with a similar computational cost and
                  better memory efficiency compared to explicit data
                  reconstruction. We also show on real data that our
                  scheme outperforms pre-interpolation of the
                  near-offset gap.},
  keywords = {multiples, REPSI, EPSI, scattering, sparsity, optimization},
  note = {(Geophysics)},
  doi = {10.1190/geo2015-0263.1},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2016/lin2015scatterEPSI/lin2015scatterEPSI.html}
}


@ARTICLE{li2015GEOPmgn,
  author = {Xiang Li and Ernie Esser and Felix J. Herrmann},
  title = {Modified {Gauss}-{Newton} full-waveform inversion explained---why sparsity-promoting updates do matter},
  journal = {Geophysics},
  year = {2016},
  month = {05},
  volume = {81},
  number = {3},
  pages = {R125-R138},
  abstract = {Full-waveform inversion can be formulated as a nonlinear
                  least-squares optimization problem. This non-convex
                  problem can be extremely computationally expensive
                  because it requires repeatedly solving large linear
                  systems that correspond to discretized partial
                  differential equations. Randomized subsampling
                  techniques allow us to work with small subsets of
                  (monochromatic) source experiments, reducing the
                  computational cost. However, this subsampling
                  weakens subsurface illumination and introduces
                  subsampling related incoherent artifacts. These
                  subsampling-related artifacts---in conjunction with
                  local minima that are known to plague full-waveform
                  inversion---motivate us to come up with a technique
                  to "regularize" this problem. Following earlier
                  work, we take advantage of the fact that curvelets
                  represent subsurface models and perturbations
                  parsimoniously. At first impulse promoting sparsity
                  on the model directly seems the most natural way to
                  proceed, but we will demonstrate that in certain
                  cases it can be advantageous to promote sparsity on
                  the Gauss-Newton updates instead. While constraining
                  the one-norm of the descent directions does not
                  change not change the underlying full-waveform
                  inversion objective, the constrained model updates
                  remain descent directions, remove
                  subsampling-related artifacts and improve the
                  overall inversion result. We empirically observe
                  this phenomenon in situations where the different
                  model updates occur at roughly the same locations in
                  the curvelet domain. We further investigate and
                  analyze this phenomenon, where nonlinear inversions
                  benefit from sparsity-promoting constraints on the
                  updates, by means of a set of carefully selected
                  examples including phase retrieval and full-waveform
                  inversion. In all cases, we observe a faster decay
                  of the residual and model error as a function of the
                  number of iterations.},
  keywords = {full-waveform inversion, Gauss-Newton method, sparsity promotion},
  note = {(Geophysics)},
  doi = {10.1190/geo2015-0266.1},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2016/li2015GEOPmgn/li2015GEOPmgn.pdf}
}


@ARTICLE{lopez2015IEEEogl,
  author = {Oscar Lopez and Rajiv Kumar and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Off-the-grid low-rank matrix recovery and seismic data reconstruction},
  journal = {IEEE Journal of Selected Topics in Signal Processing},
  year = {2016},
  month = {06},
  volume = {10},
  number = {4},
  pages = {658-671},
  abstract = {Matrix sensing problems capitalize on the knowledge that
                  a data matrix of interest exhibits low rank
                  properties. This low dimensional structure often
                  arises because the data matrix is obtained by
                  sampling a smooth function on a regular (or
                  structured) grid. However, in many practical
                  situations the measurements are taken on an
                  irregular grid (that is accurately known). This
                  results in an "unstructured data matrix" that is
                  less fit for the low rank model in comparison to its
                  regular counterpart and therefore subject to
                  degraded reconstruction via rank penalization
                  techniques. In this paper, we propose and analyze a
                  modified low-rank matrix recovery work-flow that
                  admits unstructured observations. By incorporating a
                  regularization operator which accurately maps
                  structured data to unstructured data, into the
                  nuclear-norm minimization problem, we are able to
                  compensate for data irregularity. Furthermore, by
                  construction our formulation yields output that is
                  supported on a structured grid. We establish
                  recovery error bounds for our methodology and offer
                  matrix sensing and matrix completion numerical
                  experiments including applications to seismic trace
                  interpolation to demonstrate the potential of the
                  approach.},
  keywords = {matrix sensing, matrix completion, nuclear-norm relaxation, nonuniform discrete Fourier transform, data regularization, seismic data, seismic trace interpolation},
  note = {(IEEE Journal of Selected Topics in Signal Processing)},
  doi = {10.1109/JSTSP.2016.2555482},
  url = {https://slim.gatech.edu/Publications/Public/Journals/IEEESignalProcessingMagazine/2016/lopez2015IEEEogl/lopez2015IEEEogl.pdf},
  url2 = {http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7454674}
}


@ARTICLE{tu2015GJIsem,
  author = {Ning Tu and Aleksandr Y. Aravkin and Tristan van Leeuwen and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Source estimation with surface-related multiples---fast ambiguity-resolved seismic imaging},
  journal = {Geophysical Journal International},
  year = {2016},
  month = {03},
  volume = {205},
  pages = {1492-1511},
  abstract = {We address the problem of obtaining a reliable seismic
                  image without prior knowledge of the source wavelet,
                  especially from data that contain strong
                  surface-related multiples. Conventional reverse-time
                  migration requires prior knowledge of the source
                  wavelet, which is either technically or
                  computationally challenging to accurately determine;
                  inaccurate estimates of the source wavelet can
                  result in seriously degraded reverse-time migrated
                  images, and therefore wrong geological
                  interpretations. To solve this problem, we present a
                  "wavelet-free" imaging procedure that simultaneously
                  inverts for the source wavelet and the seismic
                  image, by tightly integrating source estimation into
                  a fast least-squares imaging framework, namely
                  compressive imaging, given a reasonably accurate
                  background velocity model. However, this joint
                  inversion problem is difficult to solve as it is
                  plagued with local minima and the ambiguity with
                  respect to amplitude scalings because of the
                  multiplicative, and therefore nonlinear, appearance
                  of the source wavelet in the otherwise linear
                  formalism. We have found a way to solve this
                  nonlinear joint-inversion problem using a technique
                  called variable projection, and a way to overcome
                  the scaling ambiguity by including surface-related
                  multiples in our imaging procedure following recent
                  developments in surface-related multiple prediction
                  by sparse inversion. As a result, we obtain without
                  prior knowledge of the source wavelet
                  high-resolution seismic images, comparable in
                  quality to images obtained assuming the true source
                  wavelet is known. By leveraging the computationally
                  efficient compressive-imaging methodology, these
                  results are obtained at affordable computational
                  costs compared with conventional processing work
                  flows that include surface-related multiple removal
                  and reverse-time migration.},
  keywords = {inverse theory, time series analysis, computational seismology, wave propagation, free surface, multiples, seismic imaging},
  note = {(published online in Geophysical Journal International)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalJournalInternational/2016/tu2015GJIsem/tu2015GJIsem.pdf},
  url2 = {http://gji.oxfordjournals.org/content/205/3/1492.full.pdf?keytype=ref&ijkey=zFydRGQpiJzeCS9}
}


@ARTICLE{vanleeuwen2015GPWEMVA,
  author = {Tristan van Leeuwen and Rajiv Kumar and Felix J. Herrmann},
  title = {Enabling affordable omnidirectional subsurface extended image volumes via probing},
  journal = {Geophysical Prospecting},
  year = {2017},
  month = {03},
  volume = {65},
  number = {2},
  pages = {385-406},
  abstract = {Image gathers as a function of subsurface offset are an
                  important tool for the inference of rock properties
                  and velocity analysis in areas of complex
                  geology. Traditionally, these gathers are thought of
                  as multidimensional correlations of the source and
                  receiver wavefields. The bottleneck in computing
                  these gathers lies in the fact that one needs to
                  store, compute, and correlate these wavefields for
                  all shots in order to obtain the desired image
                  gathers. Therefore, the image gathers are typically
                  only computed for a limited number of subsurface
                  points and for a limited range of subsurface
                  offsets, which may cause problems in complex
                  geological areas with large geologic dips. We
                  overcome increasing computational and storage costs
                  of extended image volumes by introducing a
                  formulation that avoids explicit storage and removes
                  the customary and expensive loop over shots found in
                  conventional extended imaging. As a result, we end
                  up with a matrix–vector formulation from which
                  different image gathers can be formed and with which
                  amplitude-versus-angle and wave-equation migration
                  velocity analysis can be performed without requiring
                  prior information on the geologic dips. Aside from
                  demonstrating the formation of two-way extended
                  image gathers for different purposes and at greatly
                  reduced costs, we also present a new approach to
                  conduct automatic wave-equation-based
                  migration-velocity analysis. Instead of focusing in
                  particular offset directions and preselected subsets
                  of subsurface points, our method focuses every
                  subsurface point for all subsurface offset
                  directions using a randomized probing technique. As
                  a consequence, we obtain good velocity models at low
                  cost for complex models without the need to provide
                  information on the geologic dips.},
  keywords = {migration velocity analysis, AVA, stochastic optimization},
  note = {(Geophysical Prospecting)},
  doi = {10.1111/1365-2478.12418},
  url = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalProspecting/2016/vanleeuwen2015GPWEMVA/vanleeuwen2015GPWEMVA.pdf}
}


%----- 2015 -----%

@ARTICLE{dasilva2014htuck,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Optimization on the {Hierarchical} {Tucker} manifold - applications to tensor completion},
  journal = {Linear Algebra and its Applications},
  year = {2015},
  month = {09},
  volume = {481},
  pages = {131-173},
  abstract = {In this work, we develop an optimization framework for
                  problems whose solutions are well-approximated by
                  Hierarchical Tucker (HT) tensors, an efficient
                  structured tensor format based on recursive subspace
                  factorizations. By exploiting the smooth manifold
                  structure of these tensors, we construct standard
                  optimization algorithms such as Steepest Descent and
                  Conjugate Gradient for completing tensors from
                  missing entries. Our algorithmic framework is fast
                  and scalable to large problem sizes as we do not
                  require SVDs on the ambient tensor space, as
                  required by other methods. Moreover, we exploit the
                  structure of the Gramian matrices associated with
                  the HT format to regularize our problem, reducing
                  overfitting for high subsampling ratios. We also
                  find that the organization of the tensor can have a
                  major impact on completion from realistic seismic
                  acquisition geometries. These samplings are far from
                  idealized randomized samplings that are usually
                  considered in the literature but are realizable in
                  practical scenarios. Using these algorithms, we
                  successfully interpolate large-scale seismic data
                  sets and demonstrate the competitive computational
                  scaling of our algorithms as the problem sizes
                  grow.},
  keywords = {hierarchical tucker tensors, tensor completion, Riemannian manifold optimization, Gauss–Newton, differential geometry, low-rank tensor},
  note = {(Linear Algebra and its Applications)},
  doi = {10.1016/j.laa.2015.04.015},
  url = {https://slim.gatech.edu/Publications/Public/Journals/LinearAlgebraAndItsApplications/2015/dasilva2014htuck/dasilva2014htuck.pdf},
  url2 = {http://www.sciencedirect.com/science/article/pii/S0024379515002530}
}


@ARTICLE{kumar2014GEOPemc,
  author = {Rajiv Kumar and Curt Da Silva and Okan Akalin and Aleksandr Y. Aravkin and Hassan Mansour and Ben Recht and Felix J. Herrmann},
  title = {Efficient matrix completion for seismic data reconstruction},
  journal = {Geophysics},
  year = {2015},
  month = {09},
  volume = {80},
  number = {5},
  pages = {V97-V114},
  abstract = {Despite recent developments in improved acquisition,
                  seismic data often remain undersampled along source
                  and receiver coordinates, resulting in incomplete
                  data for key applications such as migration and
                  multiple prediction. We have interpreted the
                  missing-trace interpolation problem in the context
                  of matrix completion (MC), and developed three
                  practical principles for using low-rank optimization
                  techniques to recover seismic data. Specifically, we
                  strive for recovery scenarios wherein the original
                  signal is low rank and the subsampling scheme
                  increases the singular values of the matrix. We use
                  an optimization program that restores this low-rank
                  structure to recover the full volume. Omitting one
                  or more of these principles can lead to poor
                  interpolation results, as we found
                  experimentally. In light of this theory, we
                  compensate for the high-rank behavior of data in the
                  source-receiver domain by using the midpoint-offset
                  transformation for 2D data and a source-receiver
                  permutation for 3D data to reduce the overall
                  singular values. Simultaneously, to work with
                  computationally feasible algorithms for large-scale
                  data, we use a factorization-based approach to MC,
                  which significantly speeds up the computations
                  compared with repeated singular value decompositions
                  without reducing the recovery quality. In the
                  context of our theory and experiments, we also find
                  that windowing the data too aggressively can have
                  adverse effects on the recovery quality. To overcome
                  this problem, we carried out our interpolations for
                  each frequency independently while working with the
                  entire frequency slice. The result is a
                  computationally efficient, theoretically motivated
                  framework for interpolating missing-trace data. Our
                  tests on realistic 2D and 3D seismic data sets show
                  that our method compares favorably in terms of
                  computational speed and recovery quality with
                  existing curvelet- and tensor-based techniques.},
  keywords = {2D, 3D, algorithm, interpolation, signal processing, low-rank},
  note = {(Geophysics)},
  doi = {10.1190/geo2014-0369.1},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2015/kumar2014GEOPemc/kumar2014GEOPemc.pdf},
  url2 = {http://library.seg.org/doi/abs/10.1190/geo2014-0369.1}
}


@ARTICLE{kumar2015sss,
  author = {Rajiv Kumar and Haneet Wason and Felix J. Herrmann},
  title = {Source separation for simultaneous towed-streamer marine acquisition --- a compressed sensing approach},
  journal = {Geophysics},
  year = {2015},
  month = {11},
  volume = {80},
  number = {6},
  pages = {WD73-WD88},
  abstract = {Simultaneous marine acquisition is an economic way to
                  sample seismic data and speed up acquisition,
                  wherein single or multiple source vessels fire
                  sources at near-simultaneous or slightly random
                  times, resulting in overlapping shot records. The
                  current paradigm for simultaneous towed-streamer
                  marine acquisition incorporates “low variability” in
                  source firing times, i.e., 0 ≤ 1 or 2 s because the
                  sources and receivers are moving. This results in a
                  low degree of randomness in simultaneous data, which
                  is challenging to separate (into its constituent
                  sources) using compressed-sensing-based separation
                  techniques because randomization is key to
                  successful recovery via compressed sensing. We have
                  addressed the challenge of source separation for
                  simultaneous towed-streamer acquisitions via two
                  compressed-sensing-based approaches, i.e., sparsity
                  promotion and rank minimization. We have evaluated
                  the performance of the sparsity-promotion- and
                  rank-minimization-based techniques by simulating two
                  simultaneous towed-streamer acquisition scenarios,
                  i.e., over/under and simultaneous long offset. A
                  field data example from the Gulf of Suez for the
                  over/under acquisition scenario was also
                  developed. We observed that the proposed approaches
                  gave good and comparable recovery qualities of the
                  separated sources, but the rank-minimization
                  technique outperformed the sparsity-promoting
                  technique in terms of the computational time and
                  memory. We also compared these two techniques with
                  the normal-moveout-based median-filtering-type
                  approach, which had comparable results.},
  keywords = {acquisition, 2D, inversion, marine, source separation, optimization, sparsity, rank},
  note = {(Geophysics)},
  doi = {10.1190/geo2015-0108.1},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2015/kumar2015sss/kumar2015sss_revised.pdf},
  url2 = {http://library.seg.org/doi/abs/10.1190/geo2015-0108.1}
}


@ARTICLE{tu2014fis,
  author = {Ning Tu and Felix J. Herrmann},
  title = {Fast imaging with surface-related multiples by sparse inversion},
  journal = {Geophysical Journal International},
  year = {2015},
  month = {04},
  volume = {201},
  number = {1},
  pages = {304-317},
  abstract = {In marine exploration seismology, surface-related
                  multiples are usually treated as noise mainly
                  because subsequent processing steps, such as
                  migration velocity analysis and imaging, require
                  multiple-free data. Failure to remove these
                  wavefield components from the data may lead to
                  erroneous estimates for migration velocity or result
                  in strong coherent artefacts that interfere with the
                  imaged reflectors. However, multiples can carry
                  complementary information compared to primaries, as
                  they interact with the free surface and are
                  therefore exposed more to the subsurface. Recent
                  work has shown that when processed correctly
                  multiples can improve seismic illumination. Given a
                  sufficiently accurate background velocity model and
                  an estimate for the source signature, we propose a
                  new and computationally efficient linearized
                  inversion procedure based on two-way wave equations,
                  which produces accurate images of the subsurface
                  from the total upgoing wavefield including
                  surface-related multiples. Modelling of the
                  surface-related multiples in the proposed method
                  derives from the well-known surface-related multiple
                  elimination method. We incur a minimal overhead from
                  incorporating the multiples by having the
                  wave-equation solver carry out the multiple
                  predictions via the inclusion of an areal source
                  instead of expensive dense matrix-matrix
                  multiplications. By using subsampling techniques, we
                  obtain high-quality true-amplitude least-squares
                  migrated images at computational costs of roughly a
                  single reverse-time migration (RTM) with all the
                  data. These images are virtually free of coherent
                  artefacts from multiples. Proper inversion of the
                  multiples would be computationally infeasible
                  without using these techniques that significantly
                  bring down the cost. By promoting sparsity in the
                  curvelet domain and using rerandomization, out
                  method gains improved robustness to errors in the
                  background velocity model, and errors incurred in
                  the linearization of the wave equation with respect
                  to the model. We demonstrate the superior
                  performance of the proposed method compared to the
                  conventional RTM using realistic synthetic
                  examples.},
  keywords = {multiples, inversion, Kaczmarz, compressive sensing, curvelet, approximate message passing},
  note = {(Geophysical Journal International)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalJournalInternational/2014/tu2014fis/tu2014fis.pdf},
  url2 = {http://gji.oxfordjournals.org/cgi/content/full/ggv020?ijkey=pKzrSXbtxjlOjzd&keytype=ref}
}


@ARTICLE{tu2015TLEfls,
  author = {Ning Tu and Felix J. Herrmann},
  title = {Fast least-squares imaging with surface-related multiples: application to a {North}-{Sea} data set},
  journal = {The Leading Edge},
  year = {2015},
  month = {07},
  volume = {34},
  number = {7},
  pages = {788–794},
  abstract = {In marine seismic acquisition, surface-related multiples
                  constitute a significant portion of the acquired
                  data. Typically, multiples are removed during
                  early-stage data-processing as they can lead to
                  phantom reflectors during migration that may result
                  in erroneous geological interpretations. However, if
                  properly dealt with, multiples can provide valuable
                  extra information and complement primaries in
                  illuminating the subsurface. In this article, we
                  demonstrate the limitation of the reverse-time
                  migration in imaging these multiples, and present an
                  alternative inversion procedure that is
                  computationally efficient, that jointly maps both
                  primaries and multiples to the true reflectors, and
                  where the source function is estimated on the
                  fly. As a result, we obtain high-quality, mostly
                  artifact-free, broad-band images where the imprint
                  of the source-function are partly removed at a
                  computationally affordable expense compared to the
                  combined costs of the wave-equation based
                  surface-related multiple elimination and the
                  reverse-time migration. We achieve all this by
                  including the total downgoing wavefields as areal
                  sources in the least-squares migration in
                  combination with curvelet-domain sparsity promotion.
                  We apply the proposed method to a shallow-water
                  marine data set from the North sea, which contains
                  abundant short-period surface-related multiples, and
                  show its efficacy in eliminating coherent imaging
                  artifacts associated with these multiples. We also
                  demonstrate the benefits of joint imaging of
                  primaries and multiples compared to imaging these
                  signal components separately.},
  keywords = {migration, multiples, inversion, least squares, field data},
  note = {(The Leading Edge)},
  doi = {10.1190/tle34070788.1},
  url = {https://slim.gatech.edu/Publications/Public/Journals/TheLeadingEdge/2015/tu2015TLEfls/tu2015TLEfls.html}
}


@ARTICLE{vanleeuwen2015IPpmp,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {A penalty method for {PDE}-constrained optimization in inverse problems},
  journal = {Inverse Problems},
  year = {2015},
  month = {12},
  volume = {32},
  number = {1},
  pages = {015007},
  abstract = {Many inverse and parameter estimation problems can be
                  written as PDE-constrained optimization
                  problems. The goal is to infer the parameters,
                  typically coefficients of the PDE, from partial
                  measurements of the solutions of the PDE for several
                  right-hand sides. Such PDE-constrained problems can
                  be solved by finding a stationary point of the
                  Lagrangian, which entails simultaneously updating
                  the parameters and the (adjoint) state
                  variables. For large-scale problems, such an
                  all-at-once approach is not feasible as it requires
                  storing all the state variables. In this case one
                  usually resorts to a reduced approach where the
                  constraints are explicitly eliminated (at each
                  iteration) by solving the PDEs. These two
                  approaches, and variations thereof, are the main
                  workhorses for solving PDE-constrained optimization
                  problems arising from inverse problems. In this
                  paper, we present an alternative method that aims to
                  combine the advantages of both approaches. Our
                  method is based on a quadratic penalty formulation
                  of the constrained optimization problem. By
                  eliminating the state variable, we develop an
                  efficient algorithm that has roughly the same
                  computational complexity as the conventional reduced
                  approach while exploiting a larger search
                  space. Numerical results show that this method
                  indeed reduces some of the nonlinearity of the
                  problem and is less sensitive to the initial
                  iterate.},
  keywords = {penalty method, PDE, optimization, inverse problems},
  note = {(Inverse Problems)},
  url = {https://slim.gatech.edu/Publications/Public/Journals/InverseProblems/2015/vanleeuwen2015IPpmp/vanleeuwen2015IPpmp.pdf},
  url2 = {http://stacks.iop.org/0266-5611/32/i=1/a=015007}
}


%----- 2014 -----%

@ARTICLE{vanLeeuwen2014SISC3Dfds,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {{3D} frequency-domain seismic inversion with controlled sloppiness},
  journal = {SIAM Journal on Scientific Computing},
  year = {2014},
  month = {10},
  volume = {36},
  number = {5},
  pages = {S192-S217},
  abstract = {Seismic waveform inversion aims at obtaining detailed
                  estimates of subsurface medium parameters, such as
                  the spatial distribution of soundspeed, from
                  multiexperiment seismic data. A formulation of this
                  inverse problem in the frequency domain leads to an
                  optimization problem constrained by a Helmholtz
                  equation with many right-hand sides. Application of
                  this technique to industry-scale problems faces
                  several challenges: First, we need to solve the
                  Helmholtz equation for high wave numbers over large
                  computational domains. Second, the data consist of
                  many independent experiments, leading to a large
                  number of PDE solves. This results in high
                  computational complexity both in terms of memory and
                  CPU time as well as input/output costs. Finally, the
                  inverse problem is highly nonlinear and a lot of art
                  goes into preprocessing and regularization. Ideally,
                  an inversion needs to be run several times with
                  different initial guesses and/or tuning
                  parameters. In this paper, we discuss the
                  requirements of the various components (PDE solver,
                  optimization method, \dots) when applied to
                  large-scale three-dimensional seismic waveform
                  inversion and combine several existing approaches
                  into a flexible inversion scheme for seismic
                  waveform inversion. The scheme is based on the idea
                  that in the early stages of the inversion we do not
                  need all the data or very accurate PDE solves. We
                  base our method on an existing preconditioned Krylov
                  solver (CARP-CG) and use ideas from stochastic
                  optimization to formulate a gradient-based
                  (quasi-Newton) optimization algorithm that works
                  with small subsets of the right-hand sides and uses
                  inexact PDE solves for the gradient calculations. We
                  propose novel heuristics to adaptively control both
                  the accuracy and the number of right-hand sides. We
                  illustrate the algorithms on synthetic benchmark
                  models for which significant computational gains can
                  be made without being sensitive to noise and without
                  losing the accuracy of the inverted model.},
  keywords = {seismic inversion, Helmholtz equation, preconditioning, Kaczmarz method, inexact gradient, block-cg},
  note = {(SISC)},
  doi = {10.1137/130918629},
  url = {http://epubs.siam.org/doi/abs/10.1137/130918629},
  url2 = {https://slim.gatech.edu/Publications/Public/Journals/SIAMJournalOnScientificComputing/2014/vanLeeuwen2014SISC3Dfds/vanLeeuwen2014SISC3Dfds.pdf}
}


@ARTICLE{aravkin2014SISCfmd,
  author = {Aleksandr Y. Aravkin and Rajiv Kumar and Hassan Mansour and Ben Recht and Felix J. Herrmann},
  title = {Fast methods for denoising matrix completion formulations, with applications to robust seismic data interpolation},
  journal = {SIAM Journal on Scientific Computing},
  year = {2014},
  month = {10},
  volume = {36},
  number = {5},
  pages = {S237-S266},
  abstract = {Recent SVD-free matrix factorization formulations have
                  enabled rank minimization for systems with millions
                  of rows and columns, paving the way for matrix
                  completion in extremely large-scale applications,
                  such as seismic data interpolation. In this paper,
                  we consider matrix completion formulations designed
                  to hit a target data-fitting error level provided by
                  the user, and we propose an algorithm called LR-BPDN
                  that is able to exploit factorized formulations to
                  solve the corresponding optimization problem. Since
                  practitioners typically have strong prior knowledge
                  about target error level, this innovation makes it
                  easy to apply the algorithm in practice, leaving
                  only the factor rank to be determined. Within the
                  established framework, we propose two extensions
                  that are highly relevant to solving practical
                  challenges of data interpolation. First, we propose
                  a weighted extension that allows known subspace
                  information to improve the results of matrix
                  completion formulations. We show how this weighting
                  can be used in the context of frequency
                  continuation, an essential aspect to seismic data
                  interpolation. Second, we propose matrix completion
                  formulations that are robust to large measurement
                  errors in the available data. We illustrate the
                  advantages of LR-BPDN on collaborative filtering
                  problems using the MovieLens 1M and 10M and Netflix
                  100M datasets. Then we use the new method, along
                  with its robust and subspace reweighted extensions,
                  to obtain high-quality reconstructions for
                  large-scale seismic interpolation problems with real
                  data, even in the presence of data contamination.},
  keywords = {matrix completion, factorized formulations, seismic trace interpolation, variational optimization, robust statistics},
  note = {(SISC)},
  doi = {10.1137/130919210},
  url = {https://slim.gatech.edu/Publications/Public/Journals/SIAMJournalOnScientificComputing/2014/aravkin2014SISCfmd/aravkin2014SISCfmd.pdf},
  url2 = {http://epubs.siam.org/doi/abs/10.1137/130919210}
}


@ARTICLE{vanLeeuwen2014GEOPcav,
  author = {Tristan van Leeuwen and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Comment on: “Application of the variable projection scheme for frequency-domain full-waveform inversion” (M. Li, J. Rickett, and A. Abubakar, Geophysics, 78, no. 6, R249–R257)},
  journal = {Geophysics},
  year = {2014},
  month = {05},
  volume = {79},
  number = {3},
  pages = {X11-X17},
  keywords = {waveform inversion, variable projection},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2014/vanLeeuwen2014GEOPcav/vanLeeuwen2014GEOPcav.pdf},
  doi = {10.1190/geo2013-0466.1},
  note = {(discussion by Tristan van Leeuwen, Aleksandr Y. Aravkin, and Felix J. Herrmann)}
}


@ARTICLE{jumah2014GPdre,
  author = {Bander Jumah and Felix J. Herrmann},
  title = {Dimensionality-reduced estimation of primaries by sparse inversion},
  journal = {Geophysical Prospecting},
  year = {2014},
  month = {09},
  volume = {62},
  number = {5},
  pages = {972-993},
  abstract = {Wave-equation based methods, such as the estimation of
                  primaries by sparse inversion, have been successful
                  in the mitigation of the adverse effects of
                  surface-related multiples on seismic imaging and
                  migration-velocity analysis. However, the reliance
                  of these methods on multidimensional convolutions
                  with fully sampled data exposes the ‘curse of
                  dimensionality’, which leads to disproportional
                  growth in computational and storage demands when
                  moving to realistic 3D field data. To remove this
                  fundamental impediment, we propose a
                  dimensionality-reduction technique where the ‘data
                  matrix’ is approximated adaptively by a randomized
                  low-rank factorization. Compared to conventional
                  methods, which need for each iteration passage
                  through all data possibly requiring on-the-fly
                  interpolation, our randomized approach has the
                  advantage that the total number of passes is reduced
                  to only one to three. In addition, the low-rank
                  matrix factorization leads to considerable
                  reductions in storage and computational costs of the
                  matrix multiplies required by the sparse
                  inversion. Application of the proposed method to
                  two-dimensional synthetic and real data shows that
                  significant performance improvements in speed and
                  memory use are achievable at a low computational
                  up-front cost required by the low-rank
                  factorization.},
  keywords = {sparse inversion, factorization, primaries},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/1365-2478.12113/abstract},
  url2 = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalProspecting/2014/jumah2014GPdre/jumah2014GPdre.pdf},
  doi = {10.1111/1365-2478.12113},
  note = {(Geophysical Prospecting)}
}


@ARTICLE{ghadermarzy2013ncs,
  author = {Navid Ghadermarzy and Hassan Mansour and Ozgur Yilmaz},
  title = {Non-convex compressed sensing using partial support information},
  journal = {Journal of Sampling Theory in Signal and Image Processing},
  year = {2014},
  volume = {13},
  number = {3},
  pages = {249-270},
  abstract = {In this paper we address the recovery conditions of
                  weighted $\ell_p$ minimization for signal reconstruction
                  from compressed sensing measurements when partial
                  support in- formation is available. We show that
                  weighted $\ell_p$ minimization with 0 < p < 1 is stable
                  and robust under weaker sufficient conditions
                  compared to weighted $\ell_1$ minimization. Moreover, the
                  sufficient recovery conditions of weighted $\ell_p$ are
                  weaker than those of regular $\ell_p$ minimization if at
                  least 50\% of the support estimate is accurate. We
                  also review some algorithms which exist to solve the
                  non-convex $\ell_p$ problem and illustrate our results
                  with numerical experiments.},
  keywords = {Compressed sensing, weighted $\ell_p$, nonconvex optimization, sparse reconstruction},
  url = {http://arxiv.org/abs/1311.3773}
}


@ARTICLE{herrmann2013TLEffwi,
  author = {Felix J. Herrmann and Andrew J. Calvert and Ian Hanlon and Mostafa Javanmehri and Rajiv Kumar and Tristan van Leeuwen and Xiang Li and Brendan R. Smithyman and Eric Takam Takougang and Haneet Wason},
  title = {Frugal full-waveform inversion: from theory to a practical algorithm},
  journal = {The Leading Edge},
  year = {2013},
  month = {09},
  volume = {32},
  number = {9},
  pages = {1082-1092},
  abstract = {As conventional oil and gas fields are maturing, our
                  profession is challenged to come up with the
                  next-generation of more and more sophisticated
                  exploration tools. In exploration seismology this
                  trend has led to the emergence of wave-equation
                  based inversion technologies such as reverse-time
                  migration and full-waveform inversion. While
                  significant progress has been made in wave-equation
                  based inversion, major challenges remain in the
                  development of robust and computationally feasible
                  workflows that give reliable results in
                  geophysically challenging areas that may include
                  ultra-low shear velocity zones or high-velocity
                  salt. Moreover, sub-salt production carries risks
                  that needs mitigation, which raises the bar from
                  creating sub-salt images to inverting for sub-salt
                  overpressure.},
  keywords = {waveform inversion, optimization},
  url = {https://slim.gatech.edu/Publications/Public/Journals/TheLeadingEdge/2013/herrmann2013ffwi/herrmann2013ffwi.html},
  doi = {10.1190/tle32091082.1}
}


@ARTICLE{vanLeeuwen2013GJImlm,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {Mitigating local minima in full-waveform inversion by expanding the search space},
  journal = {Geophysical Journal International},
  year = {2013},
  month = {10},
  volume = {195},
  pages = {661-667},
  abstract = {Wave equation based inversions, such as full-waveform
                  inversion and reverse-time migration, are
                  challenging because of their computational costs,
                  memory requirements and reliance on accurate initial
                  models. To confront these issues, we propose a novel
                  formulation of wave equation based inversion based
                  on a penalty method. In this formulation, the
                  objective function consists of a data-misfit term
                  and a penalty term, which measures how accurately
                  the wavefields satisfy the wave equation. This new
                  approach is a major departure from current
                  formulations where forward and adjoint wavefields,
                  which both satisfy the wave equation, are correlated
                  to compute updates for the unknown model
                  parameters. Instead, we carry out the inversions
                  over two alternating steps during which we first
                  estimate the wavefield everywhere, given the current
                  model parameters, source and observed data, followed
                  by a second step during which we update the model
                  parameters, given the estimate for the wavefield
                  everywhere and the source. Because the inversion
                  involves both the synthetic wavefields and the
                  medium parameters, its search space is enlarged so
                  that it suffers less from local minima. Compared to
                  other formulations that extend the search space of
                  wave equation based inversion, our method differs in
                  several aspects, namely (i) it avoids storage and
                  updates of the synthetic wavefields because we
                  calculate these explicitly by finding solutions that
                  obey the wave equation and fit the observed data and
                  (ii) no adjoint wavefields are required to update
                  the model, instead our updates are calculated from
                  these solutions directly, which leads to significant
                  computational savings. We demonstrate the validity
                  of our approach by carefully selected examples and
                  discuss possible extensions and future research.},
  url = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalJournalInternational/2013/vanLeeuwen2013GJImlm/vanLeeuwen2013mlm.pdf},
  doi = {10.1093/gji/ggt258},
  eprint = {http://gji.oxfordjournals.org/content/early/2013/07/30/gji.ggt258.full.pdf+html}
}


@ARTICLE{mansour2013GEOPiwr,
  author = {Hassan Mansour and Felix J. Herrmann and Ozgur Yilmaz},
  title = {Improved wavefield reconstruction from randomized sampling via weighted one-norm minimization},
  journal = {Geophysics},
  year = {2013},
  month = {08},
  volume = {78},
  number = {5},
  pages = {V193-V206},
  abstract = {Missing-trace interpolation aims to recover the gaps
                  caused by physical obstacles or deliberate
                  subsampling to control acquisition costs in
                  otherwise regularly sampled seismic
                  wavefields. Although transform-domain sparsity
                  promotion has proven to be an effective tool to
                  solve this recovery problem, current recovery
                  techniques do not fully utilize a priori information
                  derived from the locations of the transform-domain
                  coefficients, especially when curvelet domain
                  sparsity is exploited. We use recovery by weighted
                  one-norm minimization, which exploits correlations
                  between the locations of significant curvelet
                  coefficients of different partitions, e.g., shot
                  records, common-offset gathers, or frequency slices
                  of the acquired data. We use these correlations to
                  define a sequence of 2D curvelet-based recovery
                  problems that exploit 3D continuity exhibited by
                  seismic wavefields without relying on the highly
                  redundant 3D curvelet transform. To test the
                  performance of our weighted algorithm, we compared
                  recoveries from different data sorting and
                  partitioning scenarios for a seismic line from the
                  Gulf of Suez. These tests demonstrated that our
                  method is superior to standard $\ell_1$ minimization
                  in terms of antialiasing capability, reconstruction
                  quality and computational memory requirements.},
  keywords = {trace interpolation, weighted one-norm minimization, compressed sensing, randomized sampling},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2013/mansour2013GEOPiwr/mansour2013GEOPiwr.pdf},
  doi = {10.1190/geo2012-0383.1}
}


@ARTICLE{lin2013GEOPrepsi,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Robust estimation of primaries by sparse inversion via one-norm minimization},
  journal = {Geophysics},
  year = {2013},
  month = {05},
  volume = {78},
  number = {3},
  pages = {R133-R150},
  abstract = {A recently proposed method called estimation of
                  primaries by sparse inversion (EPSI) avoids the need
                  for adaptive subtraction of approximate multiple
                  predictions by directly inverting for the
                  multiple-free subsurface impulse response as a
                  collection of band-limited spikes. Although it can
                  be shown that the correct primary impulse response
                  is obtained through the sparsest possible solution,
                  the original EPSI algorithm was not designed to take
                  advantage of this result, and instead it relies on a
                  multitude of inversion parameters, such as the level
                  of sparsity per gradient update. We proposed and
                  tested a new algorithm, named robust EPSI, in which
                  we make obtaining the sparsest solution an explicit
                  goal. Our approach remains a gradient-based approach
                  like the original algorithm, but it is derived from
                  a new biconvex optimization framework based on an
                  extended basis-pursuit denoising
                  formulation. Furthermore, because it is based on a
                  general framework, robust EPSI can recover the
                  impulse response in transform domains, such as
                  sparsifying curvelet-based representations, without
                  changing the underlying algorithm. We discovered
                  that the sparsity-minimizing objective of our
                  formulation enabled it to operate successfully on a
                  variety of synthetic and field marine data sets
                  without excessive tweaking of inversion
                  parameters. We also found that recovering the
                  solution in alternate sparsity domains can
                  significantly improve the quality of the directly
                  estimated primaries, especially for weaker
                  late-arrival events. In addition, we found that
                  robust EPSI produces a more artifact-free impulse
                  response compared to the original algorithm.},
  keywords = {multiples, optimization, sparsity, waveform inversion, pareto, biconvex, algorithm, EPSI},
  doi = {10.1190/geo2012-0097.1},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2013/lin2013GEOPrepsi/lin2013GEOPrepsi.pdf}
}


@ARTICLE{shahidi2013GPars,
  author = {Reza Shahidi and Gang Tang and Jianwei Ma and Felix J. Herrmann},
  title = {Application of randomized sampling schemes to curvelet-based sparsity-promoting seismic data recovery},
  journal = {Geophysical Prospecting},
  volume = {61},
  number = {5},
  pages = {973-997},
  year = {2013},
  month = {09},
  abstract = {Reconstruction of seismic data is routinely used to
                  improve the quality and resolution of seismic data
                  from incomplete acquired seismic
                  recordings. Curvelet-based Recovery by
                  Sparsity-promoting Inversion, adapted from the
                  recently-developed theory of compressive sensing, is
                  one such kind of reconstruction, especially good for
                  recovery of undersampled seismic data. Like
                  traditional Fourier-based methods, it performs best
                  when used in conjunction with randomized
                  subsampling, which converts aliases from the usual
                  regular periodic subsampling into easy-to-eliminate
                  noise. By virtue of its ability to control gap size,
                  along with the random and irregular nature of its
                  sampling pattern, jittered (sub)sampling is one
                  proven method that has been used successfully for
                  the determination of geophone positions along a
                  seismic line. In this paper, we extend jittered
                  sampling to two-dimensional acquisition design, a
                  more difficult problem, with both underlying
                  Cartesian, and hexagonal grids. We also study what
                  we term separable and non-separable two-dimensional
                  jittered samplings. We find hexagonal jittered
                  sampling performs better than Cartesian jittered
                  sampling, while fully non-separable jittered
                  sampling performs better than separable jittered
                  sampling. Two other 2D randomized sampling methods,
                  Poisson Disk sampling and Farthest Point sampling,
                  both known to possess blue-noise spectra, are also
                  shown to perform well.},
  keywords = {Geophysical Prospecting, randomized sampling, curvelets},
  url = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalProspecting/2013/shahidi2013GPars/shahidi2013GPars.pdf},
  doi = {10.1111/1365-2478.12050}
}


@ARTICLE{moghaddam2013GEOPnoa,
  author = {Peyman P. Moghaddam and Henk Keers and Felix J. Herrmann and Wim A. Mulder},
  title = {A new optimization approach for source-encoding full-waveform inversion},
  journal = {Geophysics},
  year = {2013},
  month = {05},
  volume = {78},
  number = {3},
  pages = {R125-R132},
  abstract = {Waveform inversion is the method of choice for
                  determining highly heterogeneous subsurface
                  structure. However, conventional waveform inversion
                  requires that the wavefield for each source is
                  computed separately. This makes it very expensive
                  for realistic 3D seismic surveys. Source-encoding
                  waveform inversion, in which the sources are
                  modelled simultaneously, is considerably faster than
                  conventional waveform inversion but suffers from
                  artifacts. These artifacts can partly be removed by
                  assigning random weights to the source
                  wavefields. We found that the misfit function, and
                  therefore also its gradient, for source-encoding
                  waveform inversion is an unbiased random estimation
                  of the misfit function used in conventional waveform
                  inversion. We found a new method of source-encoding
                  waveform inversion which takes into account the
                  random nature of the gradients used in the
                  optimization. In this new method, the gradient at
                  each iteration is a weighted average of past
                  gradients such that the most recent gradients have
                  the largest weights with exponential decay. This way
                  we damped the random fluctuations of the gradient by
                  incorporating information from the previous
                  iterations. We compare this new method with existing
                  source-encoding waveform inversion methods as well
                  as conventional waveform inversion and found that
                  the model misfit reduction is faster and smoother
                  than those of existing source-encoding waveform
                  inversion methods, and it approaches the model
                  misfit reduction obtained in conventional waveform
                  inversion.},
  keywords = {Geophysics, FWI, optimization, source encoding},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2013/moghaddam2013GEOPnoa/moghaddam2013GEOPnoa.pdf},
  doi = {10.1190/GEO2012-0090.1}
}


@ARTICLE{vanderneut2013GJIirs,
  author = {Joost {van der Neut} and Felix J. Herrmann},
  title = {Interferometric redatuming by sparse inversion},
  journal = {Geophysical Journal International},
  year = {2013},
  month = {02},
  volume = {192},
  pages = {666-670},
  abstract = {Assuming that exact transmission responses are known
                  between the surface and a particular depth level in
                  the subsurface, seismic sources can be effectively
                  mapped to that level by a process called
                  interferometric redatuming. After redatuming, the
                  obtained wavefields can be used for imaging below
                  this particular depth level. Interferometric
                  redatuming consists of two steps, namely (i) the
                  decomposition of the observed wavefields into up-
                  and down-going constituents and (ii) a
                  multidimensional deconvolution of the up- and
                  downgoing wavefields. While this method works in
                  theory, sensitivity to noise and artifacts due to
                  incomplete acquisition call for a different
                  formulation. In this letter, we demonstrate the
                  benefits of formulating the two steps that undergird
                  interferometric redatuming in terms of a
                  transform-domain sparsity-promoting program. By
                  exploiting compressibility of seismic wavefields in
                  the curvelet domain, we not only become robust with
                  respect to noise but we are also able to remove
                  certain artifacts while preserving the frequency
                  content. These improvements lead to a better image
                  of the target from the redatumed data.},
  keywords = {Controlled source seismology, interferometry, inverse theory},
  url = {http://gji.oxfordjournals.org/content/192/2/666}
}


@ARTICLE{berg2008SJSCpareto,
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  title = {Probing the {Pareto} frontier for basis pursuit solutions},
  journal = {SIAM Journal on Scientific Computing},
  year = {2008},
  volume = {31},
  pages = {890-912},
  number = {2},
  month = {01},
  abstract = {The basis pursuit problem seeks a minimum one-norm
                  solution of an underdetermined least-squares
                  problem. Basis pursuit denoise (BPDN) fits the
                  least-squares problem only approximately, and a
                  single parameter determines a curve that traces the
                  optimal trade-off between the least-squares fit and
                  the one-norm of the solution. We prove that this
                  curve is convex and continuously differentiable over
                  all points of interest, and show that it gives an
                  explicit relationship to two other optimization
                  problems closely related to BPDN. We describe a
                  root-finding algorithm for finding arbitrary points
                  on this curve; the algorithm is suitable for
                  problems that are large scale and for those that are
                  in the complex domain. At each iteration, a spectral
                  gradient-projection method approximately minimizes a
                  least-squares problem with an explicit one-norm
                  constraint. Only matrix-vector operations are
                  required. The primal-dual solution of this problem
                  gives function and derivative information needed for
                  the root-finding method. Numerical experiments on a
                  comprehensive set of test problems demonstrate that
                  the method scales well to large problems.},
  keywords = {basis pursuit, convex program, duality, Newton{\textquoteright}s method, one-norm regularization, projected gradient, root-finding, sparse solutions, optimization},
  doi = {10.1137/080714488},
  publisher = {SIAM},
  url = {https://slim.gatech.edu/Publications/Public/Journals/SIAMJournalOnScientificComputing/2008/vanderberg08SIAMptp/vanderberg08SIAMptp.pdf}
}


@ARTICLE{vandenberg2010IEEEter,
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  title = {Theoretical and empirical results for recovery from multiple measurements},
  journal = {IEEE Transactions on Information Theory},
  year = {2010},
  month = {05},
  volume = {56},
  number = {5},
  pages = {2516-2527},
  abstract = {The joint-sparse recovery problem aims to recover, from
                  sets of compressed measurements, unknown sparse
                  matrices with nonzero entries restricted to a subset
                  of rows. This is an extension of the
                  single-measurement-vector (SMV) problem widely
                  studied in compressed sensing. We analyze the
                  recovery properties for two types of recovery
                  algorithms. First, we show that recovery using
                  sum-of-norm minimization cannot exceed the uniform
                  recovery rate of sequential SMV using L1
                  minimization, and that there are problems that can
                  be solved with one approach but not with the
                  other. Second, we analyze the performance of the
                  ReMBo algorithm [M. Mishali and Y. Eldar, IEEE
                  Trans. Sig. Proc., 56 (2008)] in combination with L1
                  minimization, and show how recovery improves as more
                  measurements are taken. From this analysis it
                  follows that having more measurements than number of
                  nonzero rows does not improve the potential
                  theoretical recovery rate.},
  keywords = {convex optimization, joint sparsity, multiple channels, sparse recovery},
  doi = {10.1109/TIT.2010.2043876},
  url = {http://www.math.ucdavis.edu/%7Empf/2010-joint-sparsity.html}
}


@ARTICLE{vandenberg2009ACMstf,
  author = {Ewout {van den Berg} and Michael P. Friedlander and Gilles Hennenfent and Felix J. Herrmann and Rayan Saab and Ozgur Yilmaz},
  title = {Sparco: a testing framework for sparse reconstruction},
  journal = {ACM Transactions on Mathematical Software},
  year = {2009},
  month = {02},
  volume = {35},
  number = {4},
  pages = {1-16},
  abstract = {Sparco is a framework for testing and benchmarking
                  algorithms for sparse reconstruction. It includes a
                  large collection of sparse reconstruction problems
                  drawn from the imaging, compressed sensing, and
                  geophysics literature. Sparco is also a framework
                  for implementing new test problems and can be used
                  as a tool for reproducible research. Sparco is
                  implemented entirely in Matlab, and is released as
                  open-source software under the GNU Public License.},
  keywords = {compressed sensing, sparse recovery, linear operators},
  url = {http://doi.acm.org/10.1145/1462173.1462178}
}


@ARTICLE{aravkin2012IPNuisance,
  author = {Aleksandr Y. Aravkin and Tristan {van Leeuwen}},
  title = {Estimating nuisance parameters in inverse problems},
  journal = {Inverse Problems},
  year = {2012},
  volume = {28},
  number = {11},
  month = {10},
  abstract = {Many inverse problems include nuisance parameters which,
                  while not of direct interest, are required to
                  recover primary parameters. Structure present in
                  these problems allows efficient optimization
                  strategies - a well known example is variable
                  projection, where nonlinear least squares problems
                  which are linear in some parameters can be very
                  efficiently optimized. In this paper, we extend the
                  idea of projecting out a subset over the variables
                  to a broad class of maximum likelihood (ML) and
                  maximum a posteriori likelihood (MAP) problems with
                  nuisance parameters, such as variance or degrees of
                  freedom. As a result, we are able to incorporate
                  nuisance parameter estimation into large-scale
                  constrained and unconstrained inverse problem
                  formulations. We apply the approach to a variety of
                  problems, including estimation of unknown variance
                  parameters in the Gaussian model, degree of freedom
                  (d.o.f.)  parameter estimation in the context of
                  robust inverse problems, automatic calibration, and
                  optimal experimental design. Using numerical
                  examples, we demonstrate improvement in recovery of
                  primary parameters for several large- scale inverse
                  problems. The proposed approach is compatible with a
                  wide variety of algorithms and formulations, and its
                  implementation requires only minor modifications to
                  existing algorithms.},
  keywords = {full waveform inversion, students t, variance},
  doi = {10.1088/0266-5611/28/11/115016},
  url = {http://arxiv.org/abs/1206.6532},
  url2 = {https://slim.gatech.edu/Publications/Public/Journals/InverseProblems/2012/aravkin2012IPNuisance/aravkin2012IPNuisance.pdf},
  url3 = {http://iopscience.iop.org/0266-5611/28/11/115016/}
}


@ARTICLE{Aravkin11TRridr,
  author = {Aleksandr Y. Aravkin and Michael P. Friedlander and Felix J. Herrmann and Tristan van Leeuwen},
  title = {Robust inversion, dimensionality reduction, and randomized sampling},
  journal = {Mathematical Programming},
  year = {2012},
  volume = {134},
  pages = {101-125},
  number = {1},
  month = {08},
  abstract = {We consider a class of inverse problems in which the
                  forward model is the solution operator to linear
                  ODEs or PDEs. This class admits several
                  dimensionality-reduction techniques based on data
                  averaging or sampling, which are especially useful
                  for large-scale problems.  We survey these
                  approaches and their connection to stochastic
                  optimization.  The data-averaging approach is only
                  viable, however, for a least-squares misfit, which
                  is sensitive to outliers in the data and artifacts
                  unexplained by the forward model. This motivates us
                  to propose a robust formulation based on the
                  Student's t-distribution of the error. We
                  demonstrate how the corresponding penalty function,
                  together with the sampling approach, can obtain good
                  results for a large-scale seismic inverse problem
                  with 50 \% corrupted data.},
  keywords = {inverse problems, seismic inversion, stochastic optimization, robust
	estimation, optimization, FWI},
  doi = {10.1007/s10107-012-0571-6},
  url = {http://www.springerlink.com/content/35rwr101h5736340/},
  url2 = {https://slim.gatech.edu/Publications/Public/Journals/MathematicalProgramming/aravkin2012MPrid/aravkin2012MPrid.pdf}
}


@ARTICLE{bernabe2004JGRpas,
  author = {Y. Bernab{\'e} and U. Mok and B. Evans and Felix J. Herrmann},
  title = {Permeability and storativity of binary mixtures of high-and low-porosity materials},
  journal = {Journal of Geophysical Research: Solid Earth},
  year = {2004},
  volume = {109},
  pages = {B12207},
  month = {12},
  abstract = {As a first step toward determining the mixing laws for
                  the transport properties of rocks, we prepared
                  binary mixtures of high- and low-permeability
                  materials by isostatically hot-pressing mixtures of
                  fine powders of calcite and quartz. The resulting
                  rocks were marbles containing varying concentrations
                  of dispersed quartz grains. Pores were present
                  throughout the rock, but the largest ones were
                  preferentially associated with the quartz particles,
                  leading us to characterize the material as being
                  composed of two phases, one with high permeability
                  and the second with low permeability. We measured
                  the permeability and storativity of these materials
                  using the oscillating flow technique, while
                  systematically varying the effective pressure and
                  the period and amplitude of the input fluid
                  oscillation. Control measurements performed using
                  the steady state flow and pulse decay techniques
                  agreed well with the oscillating flow tests. The
                  hydraulic properties of the marbles were highly
                  sensitive to the volume fraction of the
                  high-permeability phase (directly related to the
                  quartz content). Below a critical quartz content,
                  slightly less than 20 wt \%, the high-permeability
                  volume elements were disconnected, and the overall
                  permeability was low. Above the critical quartz
                  content the high-permeability volume elements formed
                  throughgoing paths, and permeability increased
                  sharply.  We numerically simulated fluid flow
                  through binary materials and found that permeability
                  approximately obeys a percolation-based mixing law,
                  consistent with the measured permeability of the
                  calcite-quartz aggregates.},
  keywords = {permeability, porosity, SLIM, modeling},
  doi = {10.1029/2004JB003111},
  url = {https://slim.gatech.edu/Publications/Public/Journals/JournalOfGeophysicalResearch/2004/bernabe04JGRpas/bernabe04JGRpas.pdf},
  url2 = {http://onlinelibrary.wiley.com/doi/10.1029/2004JB003111/abstract}
}


@ARTICLE{mansour2012IEEETITrcs,
  author = {Michael P. Friedlander and Hassan Mansour and Rayan Saab and Ozgur Yilmaz},
  title = {Recovering compressively sampled signals using partial support information},
  journal = {IEEE Transactions on Information Theory},
  year = {2012},
  volume = {58},
  pages = {1122-1134},
  number = {2},
  month = {02},
  abstract = {We study recovery conditions of weighted $\ell_1$
                  minimization for signal reconstruction from
                  compressed sensing measurements when partial support
                  information is available. We show that if at least
                  50\% of the (partial) support information is
                  accurate, then weighted $\ell_1$ minimization is
                  stable and robust under weaker sufficient conditions
                  than the analogous conditions for standard $\ell_1$
                  minimization.  Moreover, weighted $\ell_1$
                  minimization provides better upper bounds on the
                  reconstruction error in terms of the measurement
                  noise and the compressibility of the signal to be
                  recovered. We illustrate our results with extensive
                  numerical experiments on synthetic data and real
                  audio and video signals.},
  address = {University of British Columbia, Vancouver},
  institution = {Department of Computer Science},
  keywords = {compressive sensing},
  doi = {10.1109/TIT.2011.2167214},
  url = {https://slim.gatech.edu/Publications/Public/Journals/IEEETransInformationTheory/2012/mansour2012IEEETITrcs/mansour2012IEEETITrcs.pdf}
}


@ARTICLE{vandenberg2011SIAMsol,
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  title = {Sparse optimization with least-squares constraints},
  journal = {SIAM Journal on Optimization},
  year = {2011},
  month = {11},
  volume = {21},
  number = {4},
  pages = {1201–1229},
  abstract = {The use of convex optimization for the recovery of
                  sparse signals from incomplete or compressed data is
                  now common practice. Motivated by the success of
                  basis pursuit in recovering sparse vectors, new
                  formulations have been proposed that take advantage
                  of different types of sparsity. In this paper we
                  propose an efficient algorithm for solving a general
                  class of sparsifying formulations. For several
                  common types of sparsity we provide applications,
                  along with details on how to apply the algorithm,
                  and experimental results.},
  keywords = {basis pursuit, compressed sensing, convex program,
                  duality, group sparsity, matrix completion, Newton’s
                  method, root-finding, sparse solutions},
  doi = {10.1137/100785028},
  url = {http://www.math.ucdavis.edu/%7Empf/2010-sparse-optimization-with-least-squares.html}
}


@ARTICLE{Friedlander11TRhdm,
  author = {Michael P. Friedlander and Mark Schmidt},
  title = {Hybrid deterministic-stochastic methods for data fitting},
  journal = {SIAM Journal on Scientific Computing},
  year = {2012},
  volume = {34},
  pages = {A1380-A1405},
  number = {3},
  month = {01},
  abstract = {Many structured data-fitting applications require the
                  solution of an optimization problem involving a sum
                  over a potentially large number of
                  measurements. Incremental gradient algorithms (both
                  deterministic and randomized) offer inexpensive
                  iterations by sampling only subsets of the terms in
                  the sum. These methods can make great progress
                  initially, but often slow as they approach a
                  solution. In contrast, full gradient methods achieve
                  steady convergence at the expense of evaluating the
                  full objective and gradient on each iteration. We
                  explore hybrid methods that exhibit the benefits of
                  both approaches. Rate of convergence analysis and
                  numerical experiments illustrate the potential for
                  the approach.},
  keywords = {optimization},
  doi = {10.1137/110830629}
}


@ARTICLE{friedlander2007SJOero,
  author = {Michael P. Friedlander and P. Tseng},
  title = {Exact regularization of convex programs},
  journal = {SIAM Journal on Optimization},
  year = {2007},
  volume = {18},
  pages = {1326-1350},
  number = {4},
  month = {05},
  abstract = {The regularization of a convex program is exact if all
                  solutions of the regularized problem are also
                  solutions of the original problem for all values of
                  the regularization parameter below some positive
                  threshold. For a general convex program, we show
                  that the regularization is exact if and only if a
                  certain selection problem has a Lagrange
                  multiplier. Moreover, the regularization parameter
                  threshold is inversely related to the Lagrange
                  multiplier. We use this result to generalize an
                  exact regularization result of Ferris and
                  Mangasarian [Appl. Math.  Optim., 23(1991),
                  pp. 266{\textendash}273] involving a linearized
                  selection problem. We also use it to derive
                  necessary and sufficient conditions for exact
                  penalization, similar to those obtained by Bertsekas
                  [Math. Programming, 9(1975), pp. 87{\textendash}99]
                  and by Bertsekas, Nedi , Ozdaglar [Convex Analysis
                  and Optimization, Athena Scientific, Belmont, MA,
                  2003]. When the regularization is not exact, we
                  derive error bounds on the distance from the
                  regularized solution to the original solution
                  set. We also show that existence of a
                  {\textquoteleft}{\textquoteleft}weak sharp
                  minimum{\textquoteright}{\textquoteright} is in some
                  sense close to being necessary for exact
                  regularization. We illustrate the main result with
                  numerical experiments on the l1 regularization of
                  benchmark (degenerate) linear programs and
                  semidefinite/second-order cone programs. The
                  experiments demonstrate the usefulness of l1
                  regularization in finding sparse solutions.},
  keywords = {SLIM,Optimization},
  doi = {10.1137/060675320}
}


@ARTICLE{friedlander2007TASdtd,
  author = {Michael P. Friedlander and M. A. Saunders},
  title = {Discussion: the {Dantzig} selector: statistical estimation when p is much larger than n},
  journal = {The Annals of Statistics},
  year = {2007},
  volume = {35},
  pages = {2385-2391},
  number = {6},
  month = {03},
  keywords = {dantzig, SLIM, statistics},
  doi = {10.1214/009053607000000479}
}


@ARTICLE{haber10TRemp,
  author = {Eldad Haber and Matthias Chung and Felix J. Herrmann},
  title = {An effective method for parameter estimation with {PDE} constraints with multiple right hand sides},
  journal = {SIAM Journal on Optimization},
  year = {2012},
  volume = {22},
  number = {3},
  month = {07},
  abstract = {Often, parameter estimation problems of
                  parameter-dependent PDEs involve multiple right-hand
                  sides. The computational cost and memory
                  requirements of such problems increase linearly with
                  the number of right-hand sides. For many
                  applications this is the main bottleneck of the
                  computation.  In this paper we show that problems
                  with multiple right-hand sides can be reformulated
                  as stochastic programming problems by combining the
                  right-hand sides into a few „simultaneous”
                  sources. This effectively reduces the cost of the
                  forward problem and results in problems that are
                  much cheaper to solve. We discuss two solution
                  methodologies: namely sample average approximation
                  and stochastic approximation. To illustrate the
                  effectiveness of our approach we present two model
                  problems, direct current resistivity and seismic
                  tomography.},
  keywords = {SLIM, FWI, optimization},
  url = {http://dx.doi.org/10.1137/11081126X}
}


@ARTICLE{hennenfent2008GEOPnii,
  author = {Gilles Hennenfent and Ewout {van den Berg} and Michael P. Friedlander and Felix J. Herrmann},
  title = {New insights into one-norm solvers from the {Pareto} curve},
  journal = {Geophysics},
  year = {2008},
  month = {07},
  volume = {73},
  number = {4},
  pages = {A23-A26},
  abstract = {Geophysical inverse problems typically involve a trade
                  off between data misfit and some prior. Pareto
                  curves trace the optimal trade off between these two
                  competing aims. These curves are commonly used in
                  problems with two-norm priors where they are plotted
                  on a log-log scale and are known as L-curves. For
                  other priors, such as the sparsity-promoting one
                  norm, Pareto curves remain relatively unexplored. We
                  show how these curves lead to new insights in
                  one-norm regularization. First, we confirm the
                  theoretical properties of smoothness and convexity
                  of these curves from a stylized and a geophysical
                  example. Second, we exploit these crucial properties
                  to approximate the Pareto curve for a large-scale
                  problem. Third, we show how Pareto curves provide an
                  objective criterion to gauge how different one-norm
                  solvers advance towards the solution.},
  keywords = {Pareto, SLIM, Geophysics, optimization, acquisition, processing},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2008/hennenfent08GEOnii/hennenfent08GEOnii.pdf},
  doi = {10.1190/1.2944169}
}


@ARTICLE{hennenfent2010GEOPnct,
  author = {Gilles Hennenfent and Lloyd Fenelon and Felix J. Herrmann},
  title = {Nonequispaced curvelet transform for seismic data reconstruction: a sparsity-promoting approach},
  journal = {Geophysics},
  year = {2010},
  volume = {75},
  pages = {WB203-WB210},
  number = {6},
  month = {12},
  abstract = {We extend our earlier work on the nonequispaced fast
                  discrete curvelet transform (NFDCT) and introduce a
                  second generation of the transform. This new
                  generation differs from the previous one by the
                  approach taken to compute accurate curvelet
                  coefficients from irregularly sampled data. The
                  first generation relies on accurate Fourier
                  coefficients obtained by an l2-regularized inversion
                  of the nonequispaced fast Fourier transform (FFT)
                  whereas the second is based on a direct
                  l1-regularized inversion of the operator that links
                  curvelet coefficients to irregular data. Also, by
                  construction the second generation NFDCT is lossless
                  unlike the first generation NFDCT. This property is
                  particularly attractive for processing irregularly
                  sampled seismic data in the curvelet domain and
                  bringing them back to their irregular record-ing
                  locations with high fidelity. Secondly, we combine
                  the second generation NFDCT with the standard fast
                  discrete curvelet transform (FDCT) to form a new
                  curvelet-based method, coined nonequispaced curvelet
                  reconstruction with sparsity-promoting inversion
                  (NCRSI) for the regularization and interpolation of
                  irregularly sampled data. We demonstrate that for a
                  pure regularization problem the reconstruction is
                  very accurate. The signal-to-reconstruction error
                  ratio in our example is above 40 dB. We also conduct
                  combined interpolation and regularization
                  experiments. The reconstructions for synthetic data
                  are accurate, particularly when the recording
                  locations are optimally jittered. The reconstruction
                  in our real data example shows amplitudes along the
                  main wavefronts smoothly varying with limited
                  acquisition imprint.},
  keywords = {curvelet transforms, data acquisition, geophysical techniques, seismology, SLIM, processing},
  doi = {10.1190/1.3494032},
  publisher = {SEG},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2010/hennenfent2010GEOPnct/hennenfent2010GEOPnct.pdf}
}


@ARTICLE{hennenfent2008GEOPsdw,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Simply denoise: wavefield reconstruction via jittered undersampling},
  journal = {Geophysics},
  year = {2008},
  volume = {73},
  pages = {V19-V28},
  number = {3},
  month = {05},
  abstract = {In this paper, we present a new discrete undersampling
                  scheme designed to favor wavefield reconstruction by
                  sparsity-promoting inversion with transform elements
                  that are localized in the Fourier domain. Our work
                  is motivated by empirical observations in the
                  seismic community, corroborated by recent results
                  from compressive sampling, which indicate favorable
                  (wavefield) reconstructions from random as opposed
                  to regular undersampling. As predicted by theory,
                  random undersampling renders coherent aliases into
                  harmless incoherent random noise, effectively
                  turning the interpolation problem into a much
                  simpler denoising problem. A practical requirement
                  of wavefield reconstruction with localized
                  sparsifying transforms is the control on the maximum
                  gap size. Unfortunately, random undersampling does
                  not provide such a control and the main purpose of
                  this paper is to introduce a sampling scheme, coined
                  jittered undersampling, that shares the benefits of
                  random sampling, while offering control on the
                  maximum gap size. Our contribution of jittered
                  sub-Nyquist sampling proves to be key in the
                  formulation of a versatile wavefield
                  sparsity-promoting recovery scheme that follows the
                  principles of compressive sampling. After studying
                  the behavior of the jittered undersampling scheme in
                  the Fourier domain, its performance is studied for
                  curvelet recovery by sparsity-promoting inversion
                  (CRSI). Our findings on synthetic and real seismic
                  data indicate an improvement of several decibels
                  over recovery from regularly-undersampled data for
                  the same amount of data collected.},
  html_version = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2008/hennenfent08GEOsdw/paper_html/paper.html},
  keywords = {sampling, Geophysics, SLIM, acquisition, processing, optimization, compressive sensing},
  doi = {10.1190/1.2841038},
  publisher = {SEG},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2008/hennenfent08GEOsdw/hennenfent08GEOsdw.pdf}
}


@ARTICLE{hennenfent2006CiSEsdn,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Seismic denoising with nonuniformly sampled curvelets},
  journal = {Computing in Science \& Engineering},
  year = {2006},
  volume = {8},
  number = {3},
  pages = {16-25},
  month = {05},
  abstract = {The authors present an extension of the fast discrete
                  curvelet transform (FDCT) to nonuniformly sampled
                  data. This extension not only restores curvelet
                  compression rates for nonuniformly sampled data but
                  also removes noise and maps the data to a regular
                  grid.},
  keywords = {CiSE, processing},
  doi = {10.1109/MCSE.2006.49},
  url = {https://slim.gatech.edu/Publications/Public/Journals/CiSE/2006/hennenfent06CiSEsdn/hennenfent06CiSEsdn.pdf }
}


@ARTICLE{herrmann2012IIsi,
  author = {Felix J. Herrmann},
  title = {Seismic advances},
  journal = {International Innovation},
  year = {2013},
  pages = {46-49},
  month = {01},
  abstract = {Current seismic exploration techniques are hampered by
                  bottlenecks in data sampling and processing due to
                  challenges in data collection, demand for more data
                  and the increasing need to study highly complex
                  geological settings. Professor Felix J. Herrmann's
                  group is developing novel techniques to overcome
                  these barriers which could greatly benefit the
                  hydrocarbon industry.},
  keywords = {seismic exploration techniques, compressive sensing, wave-equation-based
	data mining, dynamic nonlinear optimization},
  url = {https://slim.gatech.edu/Publications/Public/Journals/InternationalInnovation/2012/herrmann2012IIsi/herrmann2012IIsi.pdf}
}


@ARTICLE{herrmann2010GEOPrsg,
  author = {Felix J. Herrmann},
  title = {Randomized sampling and sparsity: getting more information from fewer samples},
  journal = {Geophysics},
  year = {2010},
  volume = {75},
  pages = {WB173-WB187},
  number = {6},
  month = {12},
  abstract = {Many seismic exploration techniques rely on the
                  collection of massive data volumes that are
                  subsequently mined for information during
                  processing.  Although this approach has been
                  extremely successful in the past, current efforts
                  toward higher-resolution images in increasingly
                  complicated regions of the earth continue to reveal
                  fundamental shortcomings in our workflows. Chiefly
                  among these is the so-called
                  {\textquotedblleft}curse of
                  dimensionality{\textquotedblright} exemplified by
                  Nyquist{\textquoteright}s sampling criterion, which
                  disproportionately strains current acquisition and
                  processing systems as the size and desired
                  resolution of our survey areas continue to
                  increase. We offer an alternative sampling method
                  leveraging recent insights from compressive sensing
                  toward seismic acquisition and processing for data
                  that are traditionally considered to be
                  undersampled. The main outcome of this approach is a
                  new technology where acquisition and processing
                  related costs are no longer determined by overly
                  stringent sampling criteria, such as Nyquist. At the
                  heart of our approach lies randomized incoherent
                  sampling that breaks subsampling related
                  interferences by turning them into harmless noise,
                  which we subsequently remove by promoting
                  transform-domain sparsity. Now, costs no longer grow
                  significantly with resolution and dimensionality of
                  the survey area, but instead depend only on
                  transform-domain sparsity. Our contribution is
                  twofold. First, we demonstrate by means of carefully
                  designed numerical experiments that compressive
                  sensing can successfully be adapted to seismic
                  exploration.  Second, we show that accurate recovery
                  can be accomplished for compressively sampled data
                  volumes sizes that exceed the size of conventional
                  transform-domain data volumes by only a small
                  factor. Because compressive sensing combines
                  transformation and encoding by a single linear
                  encoding step, this technology is directly
                  applicable to acquisition and to dimensionality
                  reduction during processing. In either case,
                  sampling, storage, and processing costs scale with
                  transform-domain sparsity. We illustrate this
                  principle by means of number of case studies.},
  keywords = {data acquisition, geophysical techniques, Nyquist criterion, sampling
	methods, seismology, SLIM, acquisition, compressive sensing, optimization},
  doi = {10.1190/1.3506147},
  publisher = {SEG},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2010/herrmann2010GEOPrsg/herrmann2010GEOPrsg.pdf}
}


@ARTICLE{herrmann2005ICAEsdb,
  author = {Felix J. Herrmann},
  title = {Seismic deconvolution by atomic decomposition: a parametric approach with sparseness constraints},
  journal = {Integrated Computer-Aided Engineering},
  year = {2005},
  volume = {12},
  pages = {69-90},
  number = {1},
  month = {01},
  abstract = {In this paper an alternative approach to the blind
                  seismic deconvolution problem is presented that aims
                  for two goals namely recovering the location and
                  relative strength of seismic reflectors, possibly
                  with super-localization, as well as obtaining
                  detailed parametric characterizations for the
                  reflectors. We hope to accomplish these goals by
                  decomposing seismic data into a redundant dictionary
                  of parameterized waveforms designed to closely match
                  the properties of reflection events associated with
                  sedimentary records. In particular, our method
                  allows for highly intermittent non-Gaussian records
                  yielding a reflectivity that can no longer be
                  described by a stationary random process or by a
                  spike train. Instead, we propose a reflector
                  parameterization that not only recovers the
                  reflector{\textquoteright}s location and relative
                  strength but which also captures reflector
                  attributes such as its local scaling, sharpness and
                  instantaneous phase-delay. The first set of
                  parameters delineates the stratigraphy whereas the
                  second provides information on the lithology. As a
                  consequence of the redundant parameterization,
                  finding the matching waveforms from the dictionary
                  involves the solution of an ill-posed problem. Two
                  complementary sparseness-imposing methods Matching
                  and Basis Pursuit are compared for our dictionary
                  and applied to seismic data.},
  address = {Amsterdam, The Netherlands},
  issn = {1069-2509},
  keywords = {deconvolution, SLIM, processing, modelling},
  publisher = {IOS Press},
  url = {http://dl.acm.org/citation.cfm?id=1238980.1238986},
  url2 = {https://slim.gatech.edu/Publications/Public/Journals/IntegratedComputerAidedEngineering/2005/herrmann2005ICAEsdb/herrmann2005ICAEsdb.pdf}
}


@ARTICLE{herrmann2004GJIssa,
  author = {Felix J. Herrmann and Y. Bernab\'e },
  title = {Seismic singularities at upper-mantle phase transitions: a site percolation model},
  journal = {Geophysical Journal International},
  year = {2004},
  volume = {159},
  pages = {949-960},
  number = {3},
  month = {12},
  abstract = {Mineralogical phase transitions are usually invoked to
                  account for the sharpness of globally observed
                  upper-mantle seismic discontinuities. We propose a
                  percolation-based model for the elastic properties
                  of the phase mixture in the coexistence regions
                  associated with these transitions. The major
                  consequence of the model is that the elastic moduli
                  (but not the density) display a singularity at the
                  percolation threshold of the high-pressure
                  phase. This model not only explains the sharp but
                  continuous change in seismic velocities across the
                  phase transition, but also predicts its abruptness
                  and scale invariance, which are characterized by a
                  non-integral scale exponent. Using the
                  receiver-function approach and new, powerful
                  signal-processing techniques, we quantitatively
                  determine the singularity exponent from recordings
                  of converted seismic waves at two Australian
                  stations (CAN and WRAB). Using the estimated values,
                  we construct velocity{\textendash}depth profiles
                  across the singularities and verify that the
                  calculated converted waveforms match the
                  observations under CAN. Finally, we point out a
                  series of additional predictions that may provide
                  new insights into the physics and fine structure of
                  the upper-mantle transition zone.},
  keywords = {percolation, SLIM, modelling},
  doi = {10.1111/j.1365-246X.2004.02464.x}
}


@ARTICLE{herrmann2007GJInlp,
  author = {Felix J. Herrmann and Urs Boeniger and D. J. Verschuur},
  title = {Non-linear primary-multiple separation with directional curvelet frames},
  journal = {Geophysical Journal International},
  year = {2007},
  month = {08},
  volume = {170},
  number = {2},
  pages = {781-799},
  abstract = {Predictive multiple suppression methods consist of two
                  main steps: a prediction step, during which
                  multiples are predicted from seismic data, and a
                  primary-multiple separation step, during which the
                  predicted multiples are
                  {\textquoteright}matched{\textquoteright} with the
                  true multiples in the data and subsequently
                  removed. This second separation step, which we will
                  call the estimation step, is crucial in practice: an
                  incorrect separation will cause residual multiple
                  energy in the result or may lead to a distortion of
                  the primaries, or both. To reduce these adverse
                  effects, a new transformed-domain method is proposed
                  where primaries and multiples are separated rather
                  than matched. This separation is carried out on the
                  basis of differences in the multiscale and
                  multidirectional characteristics of these two signal
                  components. Our method uses the curvelet transform,
                  which maps multidimensional data volumes into almost
                  orthogonal localized multidimensional prototype
                  waveforms that vary in directional and
                  spatio-temporal content. Primaries-only and
                  multiples-only signal components are recovered from
                  the total data volume by a non-linear optimization
                  scheme that is stable under noisy input data. During
                  the optimization, the two signal components are
                  separated by enhancing sparseness (through weighted
                  l1-norms) in the transformed domain subject to
                  fitting the observed data as the sum of the
                  separated components to within a user-defined
                  tolerance level. Whenever, during the optimization,
                  the estimates for the primaries in the transformed
                  domain correlate with the predictions for the
                  multiples, the recovery of the coefficients for the
                  estimated primaries will be suppressed while for
                  regions where the correlation is small the method
                  seeks the sparsest set of coefficients that
                  represent the estimation for the primaries. Our
                  algorithm does not seek a matched filter and as such
                  it differs fundamentally from traditional adaptive
                  subtraction methods. The method derives its
                  stability from the sparseness obtained by a
                  non-parametric (i.e. not depending on a parametrized
                  physical model) multiscale and multidirectional
                  overcomplete signal representation. This sparsity
                  serves as prior information and allows for a
                  Bayesian interpretation of our method during which
                  the log-likelihood function is minimized while the
                  two signal components are assumed to be given by a
                  superposition of prototype waveforms, drawn
                  independently from a probability function that is
                  weighted by the predicted primaries and
                  multiples. In this paper, the predictions are based
                  on the data-driven surface-related multiple
                  elimination method. Synthetic and field data
                  examples show a clean separation leading to a
                  considerable improvement in multiple suppression
                  compared to the conventional method of adaptive
                  matched filtering. This improved separation
                  translates into an improved stack.},
  keywords = {signal separation, SLIM, processing},
  doi = {10.1111/j.1365-246X.2007.03360.x},
  url = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalJournalInternational/2007/herrmann2007GJInlp/herrmann2007GJInlp.pdf}
}


@ARTICLE{herrmann2009GEOPcbm,
  author = {Felix J. Herrmann and Cody R. Brown and Yogi A. Erlangga and Peyman P. Moghaddam},
  title = {Curvelet-based migration preconditioning and scaling},
  journal = {Geophysics},
  year = {2009},
  volume = {74},
  pages = {A41-A46},
  month = {07-08},
  abstract = {The extremely large size of typical seismic imaging
                  problems has been one of the major stumbling blocks
                  for iterative techniques to attain accurate
                  migration amplitudes. These iterative methods are
                  important because they complement theoretical
                  approaches that are hampered by difficulties to
                  control problems such as finite-acquisition
                  aperture, source-receiver frequency response, and
                  directivity. To solve these problems, we apply
                  preconditioning, which significantly improves
                  convergence of least-squares migration. We discuss
                  different levels of preconditioning that range from
                  corrections for the order of the migration operator
                  to corrections for spherical spreading, and position
                  and reflector-dip dependent amplitude errors. While
                  the first two corrections correspond to simple
                  scalings in the Fourier and physical domain, the
                  third correction requires phase-space (space spanned
                  by location and dip) scaling, which we carry out
                  with curvelets. We show that our combined
                  preconditioner leads to a significant improvement of
                  the convergence of least-squares
                  {\textquoteleft}wave-equation{\textquoteright}
                  migration on a line from the SEG AA{\textquoteright}
                  salt model.},
  keywords = {migration, SLIM, imaging},
  doi = {10.1190/1.3124753},
  url = {http://geophysics.geoscienceworld.org/content/74/4/A41.abstract},
  url2 = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2009/herrmann2009GEOPcbm/herrmann2009GEOPcbm.pdf}
}


@ARTICLE{herrmann2009GEOPcsf,
  author = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title = {Compressive simultaneous full-waveform simulation},
  journal = {Geophysics},
  year = {2009},
  volume = {74},
  number = {4},
  pages = {A35-A40},
  month = {07-08},
  abstract = {The fact that computational complexity of wavefield
                  simulation is proportional to the size of the
                  discretized model and acquisition geometry, and not
                  to the complexity of the simulated wavefield, is a
                  major impediment within seismic imaging. By turning
                  simulation into a compressive sensing
                  problem{\textendash}-where simulated data is
                  recovered from a relatively small number of
                  independent simultaneous sources{\textendash}-we
                  remove this impediment by showing that compressively
                  sampling a simulation is equivalent to compressively
                  sampling the sources, followed by solving a reduced
                  system. As in compressive sensing, this allows for a
                  reduction in sampling rate and hence in simulation
                  costs. We demonstrate this principle for the
                  time-harmonic Helmholtz solver. The solution is
                  computed by inverting the reduced system, followed
                  by a recovery of the full wavefield with a sparsity
                  promoting program. Depending on the
                  wavefield{\textquoteright}s sparsity, this approach
                  can lead to significant cost reductions, in
                  particular when combined with the implicit
                  preconditioned Helmholtz solver, which is known to
                  converge even for decreasing mesh sizes and
                  increasing angular frequencies. These properties
                  make our scheme a viable alternative to explicit
                  time-domain finite-differences.},
  keywords = {full-waveform, SLIM, modelling, compressive sensing},
  doi = {10.1190/1.3115122},
  url = {http://library.seg.org/doi/abs/10.1190/1.3115122},
  url2 = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2009/herrmann2009GEOPcsf/herrmann2009GEOPcsf.pdf}
}


@ARTICLE{Herrmann11TRfcd,
  author = {Felix J. Herrmann and Michael P. Friedlander and Ozgur Yilmaz},
  title = {Fighting the curse of dimensionality: compressive sensing in exploration seismology},
  journal = {Signal Processing Magazine, IEEE},
  year = {2012},
  volume = {29},
  pages = {88-100},
  number = {3},
  month = {05},
  abstract = {Many seismic exploration techniques rely on the
                  collection of massive data volumes that are mined
                  for information during processing. This approach has
                  been extremely successful, but current efforts
                  toward higher resolution images in increasingly
                  complicated regions of Earth continue to reveal
                  fundamental shortcomings in our typical workflows.
                  The "curse" of dimensionality is the main roadblock
                  and is exemplified by Nyquist's sampling criterion,
                  which disproportionately strains current acquisition
                  and processing systems as the size and desired
                  resolution of our survey areas continues to
                  increase.},
  issn = {1053-5888},
  keywords = {Earth, Nyquist sampling criterion, dimensionality curse, higher-resolution images, massive data volumes, seismic exploration techniques, strains current acquisition system, strains current processing system, geographic information systems, seismology},
  doi = {10.1109/MSP.2012.2185859},
  url = {https://slim.gatech.edu/Publications/Public/Journals/IEEESignalProcessingMagazine/2012/Herrmann11TRfcd/Herrmann11TRfcd.pdf}
}


@ARTICLE{herrmann2008GJInps,
  author = {Felix J. Herrmann and Gilles Hennenfent},
  title = {Non-parametric seismic data recovery with curvelet frames},
  journal = {Geophysical Journal International},
  year = {2008},
  volume = {173},
  pages = {233-248},
  month = {04},
  abstract = {Seismic data recovery from data with missing traces on
                  otherwise regular acquisition grids forms a crucial
                  step in the seismic processing flow. For instance,
                  unsuccessful recovery leads to imaging artifacts and
                  to erroneous predictions for the multiples,
                  adversely affecting the performance of multiple
                  elimination. A non-parametric transform-based
                  recovery method is presented that exploits the
                  compression of seismic data volumes by recently
                  developed curvelet frames. The elements of this
                  transform are multidimensional and directional and
                  locally resemble wavefronts present in the data,
                  which leads to a compressible representation for
                  seismic data. This compression enables us to
                  formulate a new curvelet-based seismic data recovery
                  algorithm through sparsity-promoting inversion. The
                  concept of sparsity-promoting inversion is in itself
                  not new to geophysics. However, the recent insights
                  from the field of {\textquoteleft}compressed
                  sensing{\textquoteright} are new since they clearly
                  identify the three main ingredients that go into a
                  successful formulation of a recovery problem, namely
                  a sparsifying transform, a sampling strategy that
                  subdues coherent aliases and a sparsity-promoting
                  program that recovers the largest entries of the
                  curvelet-domain vector while explaining the
                  measurements. These concepts are illustrated with a
                  stylized experiment that stresses the importance of
                  the degree of compression by the sparsifying
                  transform. With these findings, a curvelet-based
                  recovery algorithms is developed, which recovers
                  seismic wavefields from seismic data volumes with
                  large percentages of traces missing. During this
                  construction, we benefit from the main three
                  ingredients of compressive sampling, namely the
                  curvelet compression of seismic data, the existence
                  of a favorable sampling scheme and the formulation
                  of a large-scale sparsity-promoting solver based on
                  a cooling method. The recovery performs well on
                  synthetic as well as real data and performs better
                  by virtue of the sparsifying property of
                  curvelets. Our results are applicable to other areas
                  such as global seismology.},
  keywords = {curvelet transform, reconstruction, SLIM, acquisition},
  url = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalJournalInternational/2008/herrmann2008GJInps.pdf},
  doi = {10.1111/j.1365-246X.2007.03698.x}
}


@ARTICLE{herrmann11GPelsqIm,
  author = {Felix J. Herrmann and Xiang Li},
  title = {Efficient least-squares imaging with sparsity promotion and compressive sensing},
  journal = {Geophysical Prospecting},
  year = {2012},
  volume = {60},
  pages = {696-712},
  number = {4},
  month = {07},
  abstract = {Seismic imaging is a linearized inversion problem
                  relying on the minimization of a least-squares
                  misfit functional as a function of the medium
                  perturbation. The success of this procedure hinges
                  on our ability to handle large systems of
                  equations---whose size grows exponentially with the
                  demand for higher resolution images in more and more
                  complicated areas---and our ability to invert these
                  systems given a limited amount of computational
                  resources. To overcome this "curse of
                  dimensionality" in problem size and computational
                  complexity, we propose a combination of randomized
                  dimensionality-reduction and divide-and-conquer
                  techniques. This approach allows us to take
                  advantage of sophisticated sparsity-promoting
                  solvers that work on a series of smaller subproblems
                  each involving a small randomized subset of
                  data. These subsets correspond to artificial
                  simultaneous-source experiments made of random
                  superpositions of sequential-source experiments. By
                  changing these subsets after each subproblem is
                  solved, we are able to attain an inversion quality
                  that is competitive while requiring fewer
                  computational, and possibly, fewer acquisition
                  resources. Application of this concept to a
                  controlled series of experiments showed the validity
                  of our approach and the relationship between its
                  efficiency---by reducing the number of sources and
                  hence the number of wave-equation solves---and the
                  image quality. Application of our
                  dimensionality-reduction methodology with sparsity
                  promotion to a complicated synthetic with well-log
                  constrained structure also yields excellent results
                  underlining the importance of sparsity promotion.},
  address = {University of British Columbia, Vancouver},
  keywords = {SLIM, imaging, optimization, compressive sensing},
  doi = {10.1111/j.1365-2478.2011.01041.x},
  url = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalProspecting/2012/herrmann11GPelsqIm/herrmann11GPelsqIm.pdf},
  url2 = {http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2478.2011.01041.x/full}
}


@ARTICLE{herrmann2008ACHAsac,
  author = {Felix J. Herrmann and Peyman P. Moghaddam and Christiaan C. Stolk},
  title = {Sparsity- and continuity-promoting seismic image recovery with curvelet frames},
  journal = {Applied and Computational Harmonic Analysis},
  year = {2008},
  volume = {24},
  pages = {150-173},
  number = {2},
  month = {03},
  abstract = {A nonlinear singularity-preserving solution to seismic
                  image recovery with sparseness and continuity
                  constraints is proposed. We observe that curvelets,
                  as a directional frame expansion, lead to sparsity
                  of seismic images and exhibit invariance under the
                  normal operator of the linearized imaging
                  problem. Based on this observation we derive a
                  method for stable recovery of the migration
                  amplitudes from noisy data. The method corrects the
                  amplitudes during a post-processing step after
                  migration, such that the main additional cost is one
                  ap- plication of the normal operator, i.e. a
                  modeling followed by a migration. Asymptotically
                  this normal operator corresponds to a
                  pseudodifferential operator, for which a convenient
                  diagonal approximation in the curvelet domain is
                  derived, including a bound for its error and a
                  method for the estimation of the diagonal from a
                  compound operator consisting of discrete
                  implementations for the scattering operator and its
                  adjoint the migration operator. The solution is
                  formulated as a nonlinear optimization problem where
                  sparsity in the curvelet domain as well as
                  continuity along the imaged reflectors are jointly
                  promoted. To enhance sparsity, the $ell_1$-norm on the
                  curvelet coefficients is minimized, while continuity
                  is promoted by minimizing an anisotropic diffusion
                  norm on the image. The performance of the recovery
                  scheme is evaluated with a time-reversed
                  {\textquoteright}wave-equation{\textquoteright}
                  migration code on synthetic datasets, including the
                  complex SEG/EAGE AA salt model.},
  keywords = {curvelet transform, imaging, SLIM, processing},
  doi = {10.1016/j.acha.2007.06.007},
  url = {https://slim.gatech.edu/Publications/Public/Journals/ACHA/2008/herrmann2008ACHAsac/herrmann2008ACHAsac.pdf}
}


@ARTICLE{herrmann2008GEOPcbs,
  author = {Felix J. Herrmann and Deli Wang and Gilles Hennenfent and Peyman P. Moghaddam},
  title = {Curvelet-based seismic data processing: a multiscale and nonlinear approach},
  journal = {Geophysics},
  year = {2008},
  volume = {73},
  pages = {A1-A5},
  number = {1},
  month = {03},
  abstract = {Mitigating missing data, multiples, and erroneous
                  migration amplitudes are key factors that determine
                  image quality. Curvelets, little
                  {\textquoteleft}{\textquoteleft}plane
                  waves,{\textquoteright}{\textquoteright} complete
                  with oscillations in one direction and smoothness in
                  the other directions, sparsify a property we
                  leverage explicitly with sparsity promotion. With
                  this principle, we recover seismic data with high
                  fidelity from a small subset (20\%) of randomly
                  selected traces. Similarly, sparsity leads to a
                  natural decorrelation and hence to a robust
                  curvelet-domain primary-multiple separation for
                  North Sea data. Finally, sparsity helps to recover
                  migration amplitudes from noisy data. With these
                  examples, we show that exploiting the
                  curvelet{\textquoteright}s ability to sparsify
                  wavefrontlike features is powerful, and our results
                  are a clear indication of the broad applicability of
                  this transform to exploration
                  seismology. {\copyright}2008 Society of Exploration
                  Geophysicists},
  keywords = {curvelet transform, SLIM, acquisition, processing},
  doi = {10.1190/1.2799517},
  publisher = {SEG},
  url = { https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2008/herrmann08GEOcbs/herrmann08GEOcbs.pdf }
}

@ARTICLE{tang2008CITEocs,
  author = {Wen Tang and Jianwei Ma and Felix J. Herrmann},
  title = {Optimized Compressed Sensing for Curvelet-based Seismic Data Reconstruction},
  journal = {CiteSeer},
  year = {2008},
  abstract = {Compressed sensing (CS) or compressive sampling provides a new sampling theory to reduce data acquisition, which says that compressible signals can be exactly reconstructed from highly incomplete sets of measurements. Very recently, the CS has been applied for seismic exploration and started to compact the traditional data acquisition. In this paper, we present an optimized sampling strategy for the CS data acquisition, which leads to better performance by the curvelet sparsity-promoting inversion in comparison with random sampling and jittered sampling scheme. One of motivation is to reduce the mutual coherence between measurement sampling schemes and curvelet sparse transform in the CS framework. The basic idea of our optimization is to directly redistribute the energy in frequency domain making original spectrum easily discriminated from the random noise induced by random undersampling, while offering control on the maximum gap size. Numerical experiments on synthetic and real seismic data show good performances of the proposed optimized CS for seismic data reconstruction.},
  keywords = {Optimized Compressive Sensing, curvelet transform},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.154.7994},
  url2 = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.154.7994&rep=rep1&type=pdf}
}

@ARTICLE{herrmann2008GEOPacd,
  author = {Felix J. Herrmann and Deli Wang and D. J. Verschuur},
  title = {Adaptive curvelet-domain primary-multiple separation},
  journal = {Geophysics},
  year = {2008},
  volume = {73},
  pages = {A17-A21},
  number = {3},
  month = {08},
  abstract = {In many exploration areas, successful separation of
                  primaries and multiples greatly determines the
                  quality of seismic imaging. Despite major advances
                  made by surface-related multiple elimination (SRME),
                  amplitude errors in the predicted multiples remain a
                  problem. When these errors vary for each type of
                  multiple in different ways (as a function of offset,
                  time, and dip), they pose a serious challenge for
                  conventional least-squares matching and for the
                  recently introduced separation by curvelet-domain
                  thresholding. We propose a data-adaptive method that
                  corrects amplitude errors, which vary smoothly as a
                  function of location, scale (frequency band), and
                  angle. With this method, the amplitudes can be
                  corrected by an elementwise curvelet-domain scaling
                  of the predicted multiples. We show that this
                  scaling leads to successful estimation of primaries,
                  despite amplitude, sign, timing, and phase errors in
                  the predicted multiples. Our results on synthetic
                  and real data show distinct improvements over
                  conventional least-squares matching in terms of
                  better suppression of multiple energy and
                  high-frequency clutter and better recovery of
                  estimated primaries. {\copyright}2008 Society of
                  Exploration Geophysicists},
  keywords = {Geophysics, SLIM, processing},
  doi = {10.1190/1.2904986},
  publisher = {SEG},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2008/herrmann08GEOacd/herrmann08GEOacd.pdf }
}


@ARTICLE{herrmann2011RECORDERcsse2,
  author = {Felix J. Herrmann and Haneet Wason and Tim T.Y. Lin},
  title = {Compressive sensing in seismic exploration: an outlook on a new paradigm},
  journal = {CSEG Recorder},
  year = {2011},
  volume = {36},
  pages = {34-39},
  number = {6},
  month = {06},
  abstract = {Many seismic exploration techniques rely on the
                  collection of massive data volumes that are
                  subsequently mined for information during
                  processing. While this approach has been extremely
                  successful in the past, current efforts toward
                  higher resolution images in increasingly complicated
                  regions of the Earth continue to reveal fundamental
                  shortcomings in our workflows. Chiefly amongst these
                  is the so-called "curse of dimensionality"
                  exemplified by Nyquist's sampling criterion, which
                  disproportionately strains current acquisition and
                  processing systems as the size and desired
                  resolution of our survey areas continues to
                  increase. We offer an alternative sampling method
                  leveraging recent insights from compressive sensing
                  towards seismic acquisition and processing for data
                  that, from a traditional point of view, are
                  considered to be undersampled. The main outcome of
                  this approach is a new technology where acquisition
                  and processing related costs are decoupled the
                  stringent Nyquist sampling criterion. At the heart
                  of our approach lies randomized incoherent sampling
                  that breaks subsampling-related interferences by
                  turning them into harmless noise, which we
                  subsequently remove by promoting sparsity in a
                  transform-domain. Acquisition schemes designed to
                  fit into this regime no longer grow significantly in
                  cost with increasing resolution and dimensionality
                  of the survey area, but instead its cost ideally
                  only depends on transform-domain sparsity of the
                  expected data. Our contribution is twofold. First,
                  we demonstrate by means of carefully designed
                  numerical experiments that ideas from compressive
                  sensing can be adapted to seismic
                  acquisition. Second, we leverage the property that
                  seismic data volumes are well approximated by a
                  small percentage of curvelet coefficients. Thus
                  curvelet-domain sparsity allows us to recover
                  conventionally-sampled seismic data volumes from
                  compressively-sampled data volumes whose size
                  exceeds this percentage by only a small
                  factor. Because compressive sensing combines
                  transformation and encoding by a single linear
                  encoding step, this technology is directly
                  applicable to seismic acquisition and therefore
                  constitutes a new paradigm where acquisitions costs
                  scale with transform-domain sparsity instead of the
                  gridsize. We illustrate this principle by showcasing
                  recovery of a real seismic line from simulated
                  compressively sampled acquisitions.},
  note = {Part 1 was published in April and Part 2 was published in June},
  url = {http://csegrecorder.com/articles/view/compressive-sensing-in-seismic-exploration-an-outlook-on-a-new-paradigm},
  url2 = {https://slim.gatech.edu/Publications/Public/Journals/CSEGRecorder/2011/herrmann2011RECORDERcsse/herrmann2011RECORDERcsse.pdf}
}


@ARTICLE{herrmann2011RECORDERcsse1,
  author = {Felix J. Herrmann and Haneet Wason and Tim T.Y. Lin},
  title = {Compressive sensing in seismic exploration: an outlook on a new paradigm},
  journal = {CSEG Recorder},
  year = {2011},
  volume = {36},
  pages = {19-33},
  number = {4},
  month = {04},
  abstract = {Many seismic exploration techniques rely on the
                  collection of massive data volumes that are
                  subsequently mined for information during
                  processing. While this approach has been extremely
                  successful in the past, current efforts toward
                  higher resolution images in increasingly complicated
                  regions of the Earth continue to reveal fundamental
                  shortcomings in our workflows. Chiefly amongst these
                  is the so-called "curse of dimensionality"
                  exemplified by Nyquist's sampling criterion, which
                  disproportionately strains current acquisition and
                  processing systems as the size and desired
                  resolution of our survey areas continues to
                  increase. We offer an alternative sampling method
                  leveraging recent insights from compressive sensing
                  towards seismic acquisition and processing for data
                  that, from a traditional point of view, are
                  considered to be undersampled. The main outcome of
                  this approach is a new technology where acquisition
                  and processing related costs are decoupled the
                  stringent Nyquist sampling criterion. At the heart
                  of our approach lies randomized incoherent sampling
                  that breaks subsampling-related interferences by
                  turning them into harmless noise, which we
                  subsequently remove by promoting sparsity in a
                  transform-domain. Acquisition schemes designed to
                  fit into this regime no longer grow significantly in
                  cost with increasing resolution and dimensionality
                  of the survey area, but instead its cost ideally
                  only depends on transform-domain sparsity of the
                  expected data. Our contribution is split into two
                  part.},
  note = {Part 1 was published in April and Part 2 was published in June},
  url = {http://csegrecorder.com/articles/view/compressive-sensing-in-seismic-exploration-outlook-on-a-new-paradigm},
  url2 = {https://slim.gatech.edu/Publications/Public/Journals/CSEGRecorder/2011/herrmann2011RECORDERcsse/herrmann2011RECORDERcsse.pdf}
}


@ARTICLE{kumar2010TNPecr,
  author = {Vishal Kumar and Jounada Oueity and Ron Clowes and Felix J. Herrmann},
  title = {Enhancing crustal reflection data through curvelet denoising},
  journal = {Technophysics},
  year = {2011},
  volume = {508},
  pages = {106-116},
  number = {1-4},
  month = {07},
  abstract = {Suppression of incoherent noise, which is present in the
                  seismic signal and may often lead to ambiguous
                  interpretation, is a key step in processing
                  associated with crustal reflection data. In this
                  paper, we make use of the parsimonious
                  representation of seismic data in the curvelet
                  domain to perform the noise attenuation while
                  preserving the coherent energy and its amplitude
                  information. Curvelets are a recently developed
                  mathematical transform that has as one of its
                  properties minimal overlap between seismic signal
                  and noise in the transform domain, thereby
                  facilitating signal-noise separation. The problem is
                  cast as an inverse problem and the results are
                  obtained by updating the solution at each
                  iteration. We demonstrate the effectiveness of this
                  procedure at removing noise on both synthetic shot
                  gathers and a synthetic stacked seismic section. We
                  then apply curvelet denoising to deep crustal
                  seismic reflection data where the signal-to-noise
                  ratio is low. The reflection data were recorded
                  along Lithoprobe's SNORCLE Line 1 across
                  Paleoproterozoic-Archean domains in Canada's
                  Northwest Territories. After initial processing, we
                  apply the iterative curvelet denoising to both
                  pre-stack shot gathers and post-stack data. Ground
                  roll, random noise and much of the anomalous
                  vertical energy is removed from the pre-stack shot
                  gathers, to the extent that crustal reflections,
                  including those from the Moho, are clearly seen on
                  individual gathers. Denoised stacked data show a
                  series of dipping reflections in the lower crust
                  that extend into the Moho. The Moho itself is
                  relatively flat and characterized by a sharp, narrow
                  band of reflections. Comparing the results for the
                  stacked data with those from F-X deconvolution,
                  curvelet denoising outperforms the latter by
                  attenuating incoherent noise with minimal harm to
                  the signal. Because curvelet denoising retains
                  amplitude information, it provides opportunities for
                  further studies of seismic sections through
                  attribute analyses. Curvelet denoising provides an
                  important new tool in the processing toolbox for
                  crustal seismic reflection data.},
  keywords = {SLIM, processing},
  doi = {10.1016/j.tecto.2010.07.01},
  url = {http://www.sciencedirect.com/science/article/pii/S0040195110003227}
}


@ARTICLE{vanLeeuwen2010IJGswi,
  author = {Tristan van Leeuwen and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Seismic waveform inversion by stochastic optimization},
  journal = {International Journal of Geophysics},
  year = {2011},
  volume = {2011},
  month = {12},
  abstract = {We explore the use of stochastic optimization methods
                  for seismic waveform inversion. The basic principle
                  of such methods is to randomly draw a batch of
                  realizations of a given misfit function and goes
                  back to the 1950s. The ultimate goal of such an
                  approach is to dramatically reduce the computational
                  cost involved in evaluating the misfit. Following
                  earlier work, we introduce the stochasticity in
                  waveform inversion problem in a rigorous way via a
                  technique called randomized trace estimation. We
                  then review theoretical results that underlie recent
                  developments in the use of stochastic methods for
                  waveform inversion. We present numerical experiments
                  to illustrate the behavior of different types of
                  stochastic optimization methods and investigate the
                  sensitivity to the batch size and the noise level in
                  the data. We find that it is possible to reproduce
                  results that are qualitatively similar to the
                  solution of the full problem with modest batch
                  sizes, even on noisy data. Each iteration of the
                  corresponding stochastic methods requires an order
                  of magnitude fewer PDE solves than a comparable
                  deterministic method applied to the full problem,
                  which may lead to an order of magnitude speedup for
                  waveform inversion in practice.},
  keywords = {SLIM, FWI, optimization},
  note = {Article ID: 689041, 18pages},
  doi = {10.1155/2011/689041},
  url = {https://slim.gatech.edu/Publications/Public/Journals/InternationJournalOfGeophysics/2011/vanLeeuwen10IJGswi/vanLeeuwen10IJGswi.pdf}
}


@ARTICLE{VanLeeuwen11TRfwiwse,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {Fast waveform inversion without source encoding},
  journal = {Geophysical Prospecting},
  year = {2013},
  month = {06},
  volume = {61},
  pages = {10-19},
  abstract = {Randomized source encoding has recently been proposed as
                  a way to dramatically reduce the costs of full
                  waveform inversion. The main idea is to replace all
                  sequential sources by a small number of simultaneous
                  sources. This introduces random crosstalk in the
                  model updates and special stochastic optimization
                  strategies are required to deal with this. Two
                  problems arise with this approach: i) source
                  encoding can only be applied to fixed-spread
                  acquisition setups, and ii) stochastic optimization
                  methods tend to converge very slowly, relying on
                  averaging to get rid of the cross-talk. Although the
                  slow convergence is partly offset by the low
                  iteration cost, we show that conventional
                  optimization strategies are bound to outperform
                  stochastic methods in the long run. In this paper we
                  argue that we don¬øt need randomized source encoding
                  to reap the benefits of stochastic optimization and
                  we review an optimization strategy that combines the
                  benefits of both conventional and stochastic
                  optimization. The method uses a gradually increasing
                  batch of sources. Thus, iterations are very cheap
                  initially and this allows the method to make fast
                  progress in the beginning. As the batch size grows,
                  the method behaves like conventional optimization,
                  allowing for fast convergence. Numerical examples
                  suggest that the stochastic and hybrid method
                  perform equally well with and without source
                  encoding and that the hybrid method outperforms both
                  conventional and stochastic optimization. The method
                  does not rely on source encoding techniques and can
                  thus be applied to non fixed-spread data.},
  keywords = {SLIM, FWI, optimization},
  doi = {10.1111/j.1365-2478.2012.01096.x},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2478.2012.01096.x/abstract},
  url2 = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalProspecting/2013/VanLeeuwen11TRfwiwse/VanLeeuwen11TRfwiwse.pdf},
  note = {Article first published online: 10 JULY 2012}
}


@ARTICLE{Li11TRfrfwi,
  author = {Xiang Li and Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Fast randomized full-waveform inversion with compressive sensing},
  journal = {Geophysics},
  year = {2012},
  volume = {77},
  pages = {A13-A17},
  number = {3},
  month = {05},
  abstract = {Wave-equation based seismic inversion can be formulated
                  as a nonlinear inverse problem where the medium
                  properties are obtained via minimization of a least-
                  squares misfit functional. The demand for higher
                  resolution models in more geologically complex areas
                  drives the need to develop techniques that explore
                  the special structure of full-waveform inversion to
                  reduce the computational burden and to regularize
                  the inverse problem. We meet these goals by using
                  ideas from compressive sensing and stochastic
                  optimization to design a novel Gauss-Newton method,
                  where the updates are computed from random subsets
                  of the data via curvelet-domain sparsity
                  promotion. Application of this idea to a realistic
                  synthetic shows improved results compared to
                  quasi-Newton methods, which require passes through
                  all data. Two different subset sampling strategies
                  are considered: randomized source encoding, and
                  drawing sequential shots firing at random source
                  locations from marine data with missing near and far
                  offsets. In both cases, we obtain excellent
                  inversion results compared to conventional methods
                  at reduced computational costs. },
  keywords = {SLIM, FWI, compressive sensing, optimization},
  doi = {10.1190/geo2011-0410.1},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2012/Li11TRfrfwi/Li11TRfrfwi.pdf}
}


@ARTICLE{lin2007GEOPcwe,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Compressed wavefield extrapolation},
  journal = {Geophysics},
  year = {2007},
  volume = {72},
  pages = {SM77-SM93},
  number = {5},
  month = {08},
  abstract = {An explicit algorithm for the extrapolation of one-way
                  wavefields is proposed that combines recent
                  developments in information theory and theoretical
                  signal processing with the physics of wave
                  propagation. Because of excessive memory
                  requirements, explicit formulations for wave
                  propagation have proven to be a challenge in 3D. By
                  using ideas from compressed sensing, we are able to
                  formulate the (inverse) wavefield extrapolation
                  problem on small subsets of the data volume, thereby
                  reducing the size of the operators. Compressed
                  sensing entails a new paradigm for signal recovery
                  that provides conditions under which signals can be
                  recovered from incomplete samplings by nonlinear
                  recovery methods that promote sparsity of the
                  to-be-recovered signal. According to this theory,
                  signals can be successfully recovered when the
                  measurement basis is incoherent with the
                  representa-tion in which the wavefield is sparse. In
                  this new approach, the eigenfunctions of the
                  Helmholtz operator are recognized as a basis that is
                  incoherent with curvelets that are known to compress
                  seismic wavefields. By casting the wavefield
                  extrapolation problem in this framework, wavefields
                  can be successfully extrapolated in the modal
                  domain, despite evanescent wave modes.  The degree
                  to which the wavefield can be recovered depends on
                  the number of missing (evanescent) wavemodes and on
                  the complexity of the wavefield. A proof of
                  principle for the compressed sensing method is given
                  for inverse wavefield extrapolation in 2D, together
                  with a pathway to 3D during which the multiscale and
                  multiangular properties of curvelets, in relation to
                  the Helmholz operator, are exploited. The results
                  show that our method is stable, has reduced dip
                  limitations, and handles evanescent waves in inverse
                  extrapolation. {\copyright}2007 Society of
                  Exploration Geophysicists},
  keywords = {SLIM, wave propagation, modelling},
  doi = {10.1190/1.2750716},
  publisher = {SEG},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2007/lin07cwe/lin07cwe.pdf}
}


@ARTICLE{Mansour11TRssma,
  author = {Hassan Mansour and Haneet Wason and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Randomized marine acquisition with compressive sampling matrices},
  journal = {Geophysical Prospecting},
  year = {2012},
  volume = {60},
  pages = {648-662},
  number = {4},
  month = {07},
  abstract = {Seismic data acquisition in marine environments is a
                  costly process that calls for the adoption of
                  simultaneous-source or randomized acquisition - an
                  emerging technology that is stimulating both
                  geophysical research and commercial
                  efforts. Simultaneous marine acquisition calls for
                  the development of a new set of design principles
                  and post-processing tools. In this paper, we discuss
                  the properties of a specific class of randomized
                  simultaneous acquisition matrices and demonstrate
                  that sparsity-promoting recovery improves the
                  quality of reconstructed seismic data volumes. We
                  propose a practical randomized marine acquisition
                  scheme where the sequential sources fire airguns at
                  only randomly time-dithered instances. We
                  demonstrate that the recovery using sparse
                  approximation from random time-dithering with a
                  single source approaches the recovery from
                  simultaneous-source acquisition with multiple
                  sources. Established findings from the field of
                  compressive sensing indicate that the choice of the
                  sparsifying transform that is incoherent with the
                  compressive sampling matrix can significantly impact
                  the reconstruction quality. Leveraging these
                  findings, we then demonstrate that the compressive
                  sampling matrix resulting from our proposed sampling
                  scheme is incoherent with the curvelet
                  transform. The combined measurement matrix exhibits
                  better isometry properties than other transform
                  bases such as a non-localized multidimensional
                  Fourier transform. We illustrate our results with
                  simulations of "ideal" simultaneous-source marine
                  acquisition, which dithers both in time and space,
                  compared with periodic and randomized
                  time-dithering.},
  keywords = {curvelet transform, Fourier, marine acquisition},
  doi = {10.1111/j.1365-2478.2012.01075.x},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2478.2012.01075.x/abstract},
  url2 = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalProspecting/2012/Mansour11TRssma/Mansour11TRssma.pdf}
}


@ARTICLE{saab2008ACHAsrb,
  author = {Rayan Saab and Ozgur Yilmaz},
  title = {Sparse recovery by non-convex optimization - instance optimality},
  journal = {Applied and Computational Harmonic Analysis},
  year = {2010},
  volume = {29},
  pages = {30-48},
  number = {1},
  month = {07},
  abstract = {In this note, we address the theoretical properties of
                  $\Delta_p$, a class of compressed sensing decoders
                  that rely on $l^p$ minimization with $p \in (0,1)$
                  to recover estimates of sparse and compressible
                  signals from incomplete and inaccurate
                  measurements. In particular, we extend the results
                  of Cand{\textquoteleft}es, Romberg and Tao [3] and
                  Wojtaszczyk [30] regarding the decoder $\Delta_1$,
                  based on $\ell^1$ minimization, to $\Delta p$ with
                  $p \in (0,1)$. Our results are two-fold. First, we
                  show that under certain sufficient conditions that
                  are weaker than the analogous sufficient conditions
                  for $\Delta_1$ the decoders $\Delta_p$ are robust to
                  noise and stable in the sense that they are $(2,p)$
                  instance optimal. Second, we extend the results of
                  Wojtaszczyk to show that, like $\Delta_1$, the
                  decoders $\Delta_p$ are (2,2) instance optimal in
                  probability provided the measurement matrix is drawn
                  from an appropriate distribution. While the
                  extension of the results of [3] to the setting where
                  $p \in (0,1)$ is straightforward, the extension of
                  the instance optimality in probability result of
                  [30] is non-trivial. In particular, we need to prove
                  that the $LQ_1$ property, introduced in [30], and
                  shown to hold for Gaussian matrices and matrices
                  whose columns are drawn uniformly from the sphere,
                  generalizes to an $LQ_p$ property for the same
                  classes of matrices. Our proof is based on a result
                  by Gordon and Kalton [18] about the Banach-Mazur
                  distances of p-convex bodies to their convex hulls.},
  keywords = {non-convex, compressive sensing},
  doi = {10.1016/j.acha.2009.08.002},
  url = {http://www.sciencedirect.com/science/article/pii/S1063520309000864},
  url2 = {https://slim.gatech.edu/Publications/Public/Journals/ACHA/2010/saab2008ACHAsrb/saab2008ACHAsrb.pdf}
}


@ARTICLE{wang2008GEOPbws,
  author = {Deli Wang and Rayan Saab and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Bayesian wavefield separation by transform-domain sparsity promotion},
  journal = {Geophysics},
  year = {2008},
  volume = {73},
  pages = {1-6},
  number = {5},
  month = {07},
  abstract = {Successful removal of coherent noise sources greatly
                  determines the quality of seismic imaging. Major
                  advances were made in this direction, e.g.,
                  Surface-Related Multiple Elimination (SRME) and
                  interferometric ground-roll removal. Still, moderate
                  phase, timing, amplitude errors and clutter in the
                  predicted signal components can be detrimental.
                  Adopting a Bayesian approach along with the
                  assumption of approximate curvelet-domain
                  independence of the to-be-separated signal
                  components, we construct an iterative algorithm that
                  takes the predictions produced by for example SRME
                  as input and separates these components in a robust
                  fashion. In addition, the proposed algorithm
                  controls the energy mismatch between the separated
                  and predicted components. Such a control, which was
                  lacking in earlier curvelet-domain formulations,
                  produces improved results for primary-multiple
                  separation on both synthetic and real data.},
  keywords = {curvelet transform, SLIM, Geophysics, processing, optimization},
  doi = {10.1190/1.2952571},
  html_version = {https://slim.gatech.edu/Publications/Public/Journals/2008/wang08TRbss/paper_html/paper.html},
  url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2008/wang08GEObws/wang08GEObws.pdf}
}
% This file was created with JabRef 2.9.
% Encoding: MacRoman

@MANUAL{hennenfent08MNrap,
  title = {Repro: a {Python} package for automating reproducible research in scientific computing},
  author = {Gilles Hennenfent and Sean Ross-Ross},
  year = {2008},
  abstract = {Repro is a Python package for automating reproducible
                  research in scientific computing. Repro works in
                  combination with SCons, a next-generation build
                  tool. The package is freely available over the
                  Internet. Downloading and installation instructions
                  are provided in this gui de. The repro package is
                  documented in various ways (many comments in source
                  code, this guide{\textendash}-written using repro
                  itself!{\textendash}-and a reference guide). In this
                  user{\textquoteright}s guide, we present a few
                  pedagogical examples that uses Matlab, Python,
                  Seismic Unix (SU), and Madagascar. We also include
                  demo pa pers. These papers are written in LaTeX and
                  compiled using repro. The figures they contain are
                  automatically generated from the source codes prov
                  ided. In that sense, the demo papers are a model of
                  self-contained documents that are fully
                  reproducible. The repro package is largely inspired
                  by some parts of Madagascar, a geophysical software
                  package for reproducible research. However, the
                  repro package is intended for a broad audience co
                  ming from a wide spectrum of interest areas.},
  keywords = {SLIM},
  month = {08},
  url = {http://repro.sourceforge.net/Site/Home.html}
}


@MANUAL{rossross07MNsda,
  title = {{SLIMpy} development and programming interface for seismic processing},
  author = {Sean Ross-Ross and Henryk Modzelewski and Cody R. Brown and Felix J. Herrmann},
  year = {2007},
  abstract = {Inverse problems in (exploration) seismology are known
                  for their large to very large scale. For instance,
                  certain sparsity-promoting inversion techniques
                  involve vectors that easily exceed unknowns while
                  seismic imaging involves the construction and
                  application of matrix-free discretized operators
                  where single matrix-vector evaluations may require
                  hours, days or even weeks on large compute
                  clusters. For these reasons, software development in
                  this field has remained the domain of highly
                  technical codes programmed in low-level languages
                  with little eye for easy development, code reuse and
                  integration with (nonlinear) programs that solve
                  inverse problems.Following ideas from the
                  Symes{\textquoteright} Rice Vector Library and
                  Bartlett{\textquoteright}s C++ object-oriented
                  interface, Thyra, and Reduction/Transformation
                  operators (both part of the Trilinos software
                  package), we developed a software-development
                  environment based on overloading. This environment
                  provides a pathway from in-core prototype
                  development to out-of-core and MPI
                  {\textquoteright}production{\textquoteright} code
                  with a high level of code reuse. This code reuse is
                  accomplished by integrating the out-of-core and MPI
                  functionality into the dynamic object-oriented
                  programming language Python. This integration is
                  implemented through operator overloading and allows
                  for the development of a coordinate-free solver
                  framework that (i) promotes code reuse; (ii)
                  analyses the statements in an abstract syntax tree
                  and (iii) generates executable statements. In the
                  current implementation, we developed an interface to
                  generate executable statements for the out-of-core
                  unix-pipe based (seismic) processing package
                  RSF-Madagascar (rsf.sf.net). The modular design
                  allows for interfaces to other seismic processing
                  packages and to in-core Python packages such as
                  numpy. So far, the implementation overloads linear
                  operators and element-wise reduction/transformation
                  operators. We are planning extensions towards
                  nonlinear operators and integration with existing
                  (parallel) solver frameworks such as Trilinos.},
  keywords = {SLIM, software},
  url = {https://slim.gatech.edu/Software/SLIM/SLIMpy/}
}


@MANUAL{rossross08MNsai,
  title = {{SLIMPy}: a python interface for unix-pipe based coordinate-free scientific computing},
  author = {Sean Ross-Ross and Henryk Modzelewski and Felix J. Herrmann},
  year = {2008},
  abstract = {SLIMpy is a Python interface that exposes the
                  functionality of seismic data processing packages,
                  such as MADAGASCAR, through oper ator overloading.
                  SLIMpy provides a concrete coordinate-free
                  implementation of classes for out-of-core linear
                  (implicit matrix-vector), and element-wise
                  operations, including calculation of norms and other
                  basic vector operations. The library is intended to
                  provide the user with an abstract sc ripting
                  language to program iterative algorithms from
                  numerical linear algebra. These algorithms require
                  repeated evaluation of operators that were initially
                  designed to be run as part of batch-oriented
                  processing flows. The current implementation
                  supports a plugin for Madagascar{\textquoteright}s
                  out-of-core UNIX pipe-based applications and is
                  extenable to pipe-based collections of programs such
                  as Seismic Unix, SEPLib, and FreeUSP. To optimize
                  perform ance, SLIMpy uses an Abstract Syntax Tree
                  that parses the algorithm and optimizes the pipes.},
  month = {07},
  url = {https://slim.gatech.edu/Software/SLIM/SLIMpy/}
}


@MANUAL{vandenberg07MNsat,
  title = {{SPARCO}: a toolbox for testing sparse reconstruction algorithms},
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  year = {2007},
  abstract = {Sparco is a suite of problems for testing and
                  benchmarking algorithms for sparse signal
                  reconstruction. It is also an environment for
                  creating new test problems, and a suite of standard
                  linear operators is provided from which new problems
                  can be assembled. Sparco is implement ed entirely in
                  Matlab and is self contained. (A few optional test
                  problems are based on the CurveLab toolbox, which
                  can be installed separately.) At the core of the
                  sparse recovery problem is the linear system
                  $Ax+r=b$, where $A$ is an $m$-by-$n$ linear operator
                  and the $m$-vector $b$ is the observed signal. The
                  goal is to find a sparse $n$-vector $x$ such that
                  $r$ is small in norm.},
  keywords = {SLIM},
  month = {10},
  url = {http://www.cs.ubc.ca/labs/scl/sparco/}
}

% This file was created with JabRef 2.9.
% Encoding: ISO8859_1

%-----2020-----%

@PHDTHESIS{sharan2020THlsh,
  author = {Shashin Sharan},
  title = {Large scale high-frequency seismic wavefield reconstruction, acquisition via rank minimization and sparsity-promoting source estimation},
  school = {Georgia Institute of Technology},
  year = {2020},
  month = {11},
  address = {Atlanta},
  abstract = {Seismic data reconstruction on a dense periodic grid from seismic
data acquired on a coarse grid is a common approach followed by most of the
oil & gas companies. This approach allows them to save on operationally
challenging and expensive dense seismic data acquisition. Dense seismic data
is one of the key requirements for generating high-resolution images of
earth's subsurface for exploration and production decisions. Based on the
Compressive Sensing (CS) paradigm, low-rank matrix factorization based
seismic data reconstruction methods are computationally cheaper and scalable
to large datasets in comparison to sparsity-promotion based methods. The
sparsity-promotion based methods are based on transformation in certain
transform domains that can be computationally expensive for large datasets.
Although, low-rank matrix factorization based methods perform well at lower
frequencies, their performance degrades at higher frequencies due to increase
in rank of approximating matrix. One of the contributions of this thesis is a
recursively weighted matrix factorization approach to improve the quality of
reconstructed data at higher frequencies. This recursively weighted approach
exploits the similarity between adjacent frequency slices. Although,
recursively weighted method improves the data reconstruction quality at
higher frequencies, it can be computationally expensive for large scale
seismic datasets. This is because of the interdependence of frequencies
preventing simultaneous reconstruction of frequencies. Another contribution
of this thesis is a computationally efficient recursively weighted framework
for large scale dataset by parallelizing data reconstruction over rows of
low-rank factors of each frequency slices. To reduce the cost and turnaround
time of seismic data acquisition simultaneous source acquisition is adapted
by the oil and gas industry in last few years. Another contribution of this
thesis is a low-rank based method for simultaneous separation and
reconstruction of seismic data on a dense periodic grid from large scale
seismic data acquired with simultaneous source acquisition. Next part of this
thesis focuses on accurate detection of fractures created by hydraulic
fracturing in unconventional reservoirs for economical production of oil and
gas. Fracturing of rocks during hydraulic fracturing gives rise to
microseismic events, which are localized along these fractures. In this work,
a sparsity-promoting microseismic source estimation framework is proposed to
detect closely spaced microseismic sources along with estimation of their
associated source-time functions from noisy microseismic data recorded by
receivers along the earth's surface or along monitor wells. Detecting closely
spaced microseismic events helps in delineating fractures and estimation of
source-time function is useful in estimating fracture's origin in time. Also,
source-time functions can be potentially useful for estimating the
source-mechanism. Also, this method does not make any prior assumption on
number of microseismic sources or shape of their source-time functions.
Therefore, this method is useful for detecting microseismic sources with
different source signatures and frequency content. Last part of this thesis
focuses on sparsity-promoting photoacoustic imaging to detect photoabsorbers
along with estimating the associated source-time functions. Traditional
photoacoustic imaging can only estimate the locations of photoacoustic
absorbers. Also, traditional methods require dense transducer coverage
whereas sparsity-promotion based method can work with reduced transducer
sampling reducing the overall data storage cost.},
  keywords = {PhD, Sparsity-promoting, Low-Rank, Wavefield Reconstruction, Compressed Sensing},
  note = {(PhD)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2020/sharan2020THlsh/sharan2020THlsh.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Thesis/2020/sharan2020THlsh/sharan2020THlsh_pres.pdf}
}

@PHDTHESIS{louboutin2020THmfi,
  author = {Mathias Louboutin},
  title = {Modeling for inversion in exploration geophysics},
  school = {Georgia Institute of Technology},
  year = {2020},
  month = {03},
  address = {Atlanta},
  abstract = {Seismic inversion, and more generally geophysical exploration, aims at better
understanding the earth's subsurface, which is one of today's most important
challenges. Firstly, it contains natural resources that are critical to our
technologies such as water, minerals and oil and gas. Secondly, monitoring
the subsurface in the context of CO2 sequestration, earthquake detection and
global seismology are of major interests with regard to safety and the
environment hazards. However, the technologies to monitor the subsurface or
find resources are scientifically extremely challenging. Seismic inversion
can be formulated as a mathematical optimization problem that minimizes the
difference between field recorded data and numerically modeled synthetic
data. The process of solving this optimization problem then requires to
numerically model, thousands of times, wave-propagation in large
three-dimensional representations of part of the earth subsurface. The
mathematical and computational complexity of this problem, therefore, calls
for software design that abstracts these requirements and facilitates
algorithm and software development.

My thesis addresses some of the challenges that arise from these problems;
mainly the computational cost and access to the right software for research
and development. In the first part, I will discuss a performance metric that
improves the current runtime-only benchmarks in exploration geophysics. This
metric, the roofline model, first provides insight at the hardware level of
the performance of a given implementation relative to the maximum achievable
performance. Second, this study demonstrates that the choice of numerical
discretization has a major impact on the achievable performance depending on
the hardware at hand and shows that a flexible framework with respect to the
discretization parameters is necessary. In the second part, I will introduce
and describe Devito, a symbolic finite-difference DSL that provides a
high-level interface to the definition of partial differential equations
(PDE) such as the wave equation. Devito, from the symbolic definition of
PDEs, then generates and compiles highly optimized C code on-the-fly to
compute the solution of the PDE. The combination of the high-level
abstractions and the just-in-time compiler enable research for geophysical
exploration and PDE-constrainted optimization based on the paradigm of
separation of concerns. This allows researchers to concentrate on their
respective field of study while having access to computationally performant
solvers with a flexible and easy to use interface to successfully implement
complex representations of the physics. The second part of my thesis will be
split into two sub-parts; first describing the symbolic application
programming interface (API), before describing and benchmarking the
just-in-time compiler. I will end my thesis with concluding remarks, the
latest developments and a brief description of projects that were enabled by
Devito.},
  keywords = {PhD, Finite-differences, HPC, performance, Imaging, Modeling, Inversion, FWI, RTM},
  note = {(PhD)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2020/louboutin2020THmfi/louboutin2020THmfi.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Thesis/2020/louboutin2020THmfi/louboutin2020THmfi_pres.pdf}
}

@PHDTHESIS{witte2020THsal,
  author = {Philipp A. Witte},
  title = {Software and algorithms for large-scale seismic inverse problems},
  school = {Georgia Institute of Technology},
  year = {2020},
  month = {02},
  address = {Atlanta},
  abstract = {Seismic imaging and parameter estimation are an import class of inverse
problems with practical relevance in resource exploration, carbon control and
monitoring systems for geohazards. The goal of seismic inverse problems is to
image subsurface geological structures and estimate physical rock properties
such as wave speed or density. Mathematically, this can be achieved by
solving an optimization problem in which we minimize the mismatch between
numerically modeled data and observed data from a seismic survey. As wave
propagation through a medium is described by wave equations, seismic inverse
problems involve solving a large number of partial differential equations
(PDEs) during numerical optimization using finite difference modeling, making
them computationally expensive. Additionally, seismic inverse problems are
typically ill-posed, non-convex or ill-conditioned, thus making them
challenging from a mathematical standpoint as well. Similar to the field of
deep learning, this calls for software that is not only optimized for
performance, but also enables geophysical domain specialists to experiment
with algorithms in high-level programming languages and using different
computing environments, such as high-performance computing (HPC) clusters or
the cloud. Furthermore, they call for the adaption of dimensionality
reduction techniques and stochastic algorithms to address computational cost
from the algorithmic side.

This thesis makes three distinct contributions to address computational
challenges encountered in seismic inverse problems and to facilitate
algorithmic development in this field. Part one introduces a large-scale
framework for seismic modeling and inversion based on the paradigm of
separation of concerns, which combines a user interface based on domain
specific abstractions with a Python package for automatic code generation to
solve the underlying PDEs. The modular code structure makes it possible to
manage the complexity of a seismic inversion code, while matrix-free linear
operators and data containers enable the implementation of algorithms in a
fashion that closely resembles the underlying mathematical notation. The
second contribution of this thesis is an algorithm for seismic imaging, that
addresses its high computational cost and large memory imprint through a
combination of on-the-fly Fourier transforms, stochastic sampling techniques
and sparsity-promoting optimization. The algorithm combines the best of both
time- and frequency-domain inversion, as the memory imprint is independent of
the number of modeled time steps, while time-to-frequency conversions avoid
the need to solve Helmholtz equations, which involve inverting
ill-conditioned matrices. Part three of this thesis introduces a novel
approach for adapting the cloud for high-performance computing applications
like seismic imaging, which does not rely on a fixed cluster of permanently
running virtual machines. Instead, computational resources are automatically
started and terminated by the cloud environment during runtime and the
workflow takes advantage of cloud-native technologies such as event-driven
computations and containerized batch processing. The performance and cost
analysis shows that this approach is able to address current shortcomings of
the cloud such as inferior resilience, while at the same time reducing
operating cost up to an order of magnitude. As such, the workflow provides a
strategy for cost effectively running large-scale seismic imaging problems in
the cloud and is a viable alternative to conventional HPC clusters.},
  keywords = {PhD, seismic, imaging, RTM, FWI, AWS, software, algorithms},
  note = {(PhD)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2020/witte2020THsal/witte2020THsal.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Thesis/2020/witte2020THsal/witte2020THsal_pres.pdf}
}

@PHDTHESIS{yang2020THsiw,
  author = {Mengmeng Yang},
  title = {Seismic imaging with extended image volumes and source estimation},
  school = {Georgia Institute of Technology},
  year = {2020},
  month = {03},
  address = {Atlanta},
  abstract = {Seismic imaging is an important tool for the exploration and production of
oil & gas, carbon sequestration, and the mitigation of geohazards. Through
the process of seismic migration, images of subsurface geological structures
are created from data collected at the surface. These images reflect changes
in the physical rock properties such as wave speed and density. While
significant progress has been made in the development of 3D imaging
technology for complex geological areas, several challenges remain, some of
which are addressed in this thesis. The first main contribution of this
thesis is in the area of creating so-called subsurface-offset gathers, which
play an increasingly important role in seismic imaging because they provide a
multitude of information ranging from the reflection mechanism itself to
information of the dips of specific reflectors and the accuracy of the
background velocity model. Unfortunately, the formation and manipulation of
these gathers
come with exceedingly high computational and storage costs because extended
image volumes are quadratic in the image size. These high costs are avoided
by using techniques from modern randomized linear algebra that allow for
compression of extended image volumes
into low-rank factorized form—i.e., the image volume is approximately
written as an outer product of a tall and a wide matrix. It is demonstrated
that this factorization provides access to different types of sub-surface
offset gathers, including common-image (point)
gathers, without the need to explicitly form this outer product. As a result,
challenging steep dip imaging situations, where conventional horizontal
offset gathers no longer focus, can be handled. Moreover, extended image
volumes for one background velocity model can directly be mapped to those of
another background velocity model. As a result, factorization costs are
incurred only once when examining imaging scenarios for different background
velocity models. The second main contribution of this thesis is on the
development of computationally efficient sparsity-promoting imaging
techniques and on-the-fly source estimation. In this work, an adaptive
technique is proposed where the unknown time signature of the sources is
estimated during imaging. Without accurate knowledge of these source
signatures, seismic images can be wrongly positioned and can have the wrong
amplitudes hampering subsequent geophysical and geological interpretations.
With the presented technique, this problem is mitigated. Finally, a
contribution is made to address the detrimental effects of surface-related
multiples. If not handled correctly, these multiples give rise to unwanted
artifacts in the image. A new technique is introduced to address this issue
in realistic settings where there is a strong density contrast at the ocean
bottom. As a result, the surface-related multiples are mapped to the
reflectors. Because bounce points at the surface can be considered as
sources, this mapping of the multiples rather than removal increases the
subsurface illumination.},
  keywords = {PhD, Extended image volumes, low rank, randomized linear algebra,
sparsity-promoting inversion, multiples, source estimation},
  note = {(PhD)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2020/yang2020THsiw/yang2020THsiw.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Thesis/2020/yang2020THsiw/yang2020THsiw_pres.pdf}
}


%-----2019-----%

@PHDTHESIS{peters2019THiso,
  author = {Bas Peters},
  title = {Intersections and sums of sets for the regularization of inverse problems},
  school = {The University of British Columbia},
  year = {2019},
  month = {05},
  address = {Vancouver},
  abstract = {Inverse problems in the imaging sciences encompass a variety of
applications. The primary problem of interest is the identification of
physical parameters from observed data that come from experiments
governed by partial-differential-equations. The secondary type of
imaging problems attempts to reconstruct images and video that are
corrupted by, for example, noise, subsampling, blur, or saturation.

The quality of the solution of an inverse problem is sensitive to issues such
as
noise and missing entries in the data. The non-convex seismic full-waveform
inversion problem suffers from parasitic local minima that lead to wrong
solutions that may look realistic even for noiseless data. To meet some of
these
challenges, I propose solution strategies that constrain the model parameters
at
every iteration to help guide the inversion.

To arrive at this goal, I present new practical workflows, algorithms, and
software, that avoid manual tuning-parameters and that allow us to
incorporate
multiple pieces of prior knowledge. Opposed to penalty methods, I avoid
balancing the influence of multiple pieces of prior knowledge by working with
intersections of constraint sets. I explore and present advantages of
constraints for imaging. Because the resulting problems are often non-trivial
to
solve, especially on large 3D grids, I introduce faster algorithms, dedicated
to
computing projections onto intersections of multiple sets.

To connect prior knowledge more directly to problem formulations, I also
combine
ideas from additive models, such as cartoon-texture decomposition and robust
principal component analysis, with intersections of multiple constraint sets
for
the regularization of inverse problems. The result is an extension of the
concept of a Minkowski set.

Examples from non-unique physical parameter estimation problems show that
constraints in combination with projection methods provide control over the
model properties at every iteration. This can lead to improved results when
the
constraints are carefully relaxed.},
  keywords = {PhD, sets, regularization, FWI, optimization},
  note = {(PhD)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2019/peters2019THiso/peters2019THiso.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Thesis/2019/peters2019THiso/peters2019THiso_pres.pdf}
}
%-----2018-----%

@PHDTHESIS{fang2018THsea,
  author = {Zhilong Fang},
  title = {Source estimation and uncertainty quantification for wave-equation based seismic imaging and inversion},
  school = {The University of British Columbia},
  year = {2018},
  month = {04},
  address = {Vancouver},
  abstract = {In  modern  seismic  exploration, wave-equation-based inversion and imaging approaches  are  widely employed for their potential  of  creating  high-resolution subsurface images from seismic data by using the wave equation
to describe  the  underlying  physical model of  wave  propagation. Despite
their successful practical applications, some key issues remain unsolved, including local minima, unknown sources, and the largely missing uncertainty
analyses  for  the  inversion. This  thesis  aims  to  address  the  following two
aspects:  to perform the inversion without prior knowledge of sources, and
to quantify uncertainties in the inversion. The unknown source can hinder the success of wave-equation-based ap-
proaches.  A simple time shift in the source can lead to misplaced reflectors
in  linearized  inversions  or  large  disturbances  in  nonlinear  problems. Unfortunately, accurate sources are typically unknown in real problems.  The first major contribution of this thesis is, given the fact that the wave equation linearly depends on the sources, I have proposed on-the-fly source estimation techniques for the following wave-equation-based approaches: (1)
time-domain  sparsity-promoting  least-squares  reverse-time  migration;  and
(2) wavefield-reconstruction  inversion. Considering the linear dependence
of the wave equation on the sources, I project out the sources by solving a
linear least-squares problem, which enables us to conduct successful wave-
equation-based inversions without prior knowledge of the sources.
Wave-equation-based  approaches  also  produce  uncertainties in the resulting velocity model due to the  noisy  data, which would influence the
subsequent exploration and financial decisions. The difficulties related to
practical uncertainty quantification lie in: (1) expensive computation related
to wave-equation solves, and (2) the nonlinear parameter-to-data map. The
second major contribution of this thesis is the proposal of a computationally
feasible Bayesian framework to analyze uncertainties in the resulting velocity models. Through relaxing the wave-equation constraints, I obtain a less
nonlinear parameter-to-data map and a posterior distribution that can be adequately approximated by a Gaussian distribution. I derive an implicit
formulation to construct the covariance matrix of the Gaussian distribution,
which allows us to sample the Gaussian distribution in a computationally
efficient manner. I demonstrate that the proposed Bayesian framework can
provide adequately accurate uncertainty analyses for intermediate to large-
scale problems with an acceptable computational cost.},
  keywords = {PhD, WRI, LS-RTM, source estimation, UQ, FWI},
  note = {(PhD)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2018/fang2018THsea/fang2018THsea.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Thesis/2018/fang2018THsea/fang2018THsea_pres.pdf}
}

%-----2017-----%

@PHDTHESIS{dasilva2017THlso,
  author = {Curt Da Silva},
  title = {Large-scale optimization algorithms for missing data completion and inverse problems},
  school = {The University of British Columbia},
  year = {2017},
  month = {09},
  address = {Vancouver},
  abstract = {Inverse problems are an important class of problems
                  found in many areas of science and engineering. In
                  these problems, one aims to estimate unknown
                  parameters of a physical system through indirect
                  multi-experiment measurements. Inverse problems
                  arise in a number of fields including seismology,
                  medical imaging, and astronomy, among others. An
                  important aspect of inverse problems is the quality
                  of the acquired data itself. Real-world data
                  acquisition restrictions, such as time and budget
                  constraints, often results in measured data with
                  missing entries. Many inversion algorithms assume
                  that the input data is fully sampled and relatively
                  noise free and produce poor results when these
                  assumptions are violated. Given the multidimensional
                  nature of real-world data, we propose a new low-rank
                  optimization method on the smooth manifold of
                  Hierarchical Tucker tensors. Tensors that exhibit
                  this low-rank structure can be recovered from
                  solving this non-convex program in an efficient
                  manner. We successfully interpolate realistically
                  sized seismic data volumes using this approach. If
                  our low-rank tensor is corrupted with non-Gaussian
                  noise, the resulting optimization program can be
                  formulated as a convex-composite problem. This class
                  of problems involves minimizing a non-smooth but
                  convex objective composed with a nonlinear smooth
                  mapping. In this thesis, we develop a level set
                  method for solving composite-convex problems and
                  prove that the resulting subproblems converge
                  linearly. We demonstrate that this method is
                  competitive when applied to examples in noisy tensor
                  completion, analysis-based compressed sensing, audio
                  declipping, total-variation deblurring and
                  denoising, and one-bit compressed sensing. With
                  respect to solving the inverse problem itself, we
                  introduce a new software design framework that
                  manages the cognitive complexity of the various
                  components involved. Our framework is modular by
                  design, which enables us to easily integrate and
                  replace components such as linear solvers, finite
                  difference stencils, preconditioners, and
                  parallelization schemes. As a result, a researcher
                  using this framework can formulate her algorithms
                  with respect to high-level components such as
                  objective functions and hessian operators. We
                  showcase the ease with which one can prototype such
                  algorithms in a 2D test problem and, with little
                  code modification, apply the same method to
                  large-scale 3D problems.},
  keywords = {PhD, optimization, convex composite, tensor completion, low-rank, tensor, interpolation, software design},
  note = {(PhD)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2017/dasilva2017THlso/dasilva2017THlso.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Thesis/2017/dasilva2017THlso/dasilva2017THlso_pres.pdf}
}


@PHDTHESIS{kumar2017THels,
  author = {Rajiv Kumar},
  title = {Enabling large-scale seismic data acquisition, processing and waveform-inversion via rank-minimization},
  school = {The University of British Columbia},
  year = {2017},
  month = {08},
  address = {Vancouver},
  abstract = {In this thesis, I adapt ideas from the field of
                  compressed sensing to mitigate the computational and
                  memory bottleneck of seismic processing workflows
                  such as missing-trace interpolation, source
                  separation and wave-equation based inversion for
                  large-scale 3- and 5-D seismic data. For
                  interpolation and source separation using
                  rank-minimization, I propose three main ingredients,
                  namely a rank-revealing transform domain, a
                  subsampling scheme that increases the rank in the
                  transform domain, and a practical large-scale
                  data-consistent rank-minimization framework, which
                  avoids the need for expensive computation of
                  singular value decompositions. We also devise a
                  wave-equation based factorization approach that
                  removes computational bottlenecks and provides
                  access to the kinematics and amplitudes of
                  full-subsurface offset extended images via actions
                  of full extended image volumes on probing vectors,
                  which I use to perform the amplitude-versus- angle
                  analyses and automatic wave-equation migration
                  velocity analyses on complex geological
                  environments. After a brief overview of matrix
                  completion techniques in Chapter 1, we propose a
                  singular value decomposition (SVD)-free
                  factorization based rank-minimization approach for
                  large-scale matrix completion problems. Then, I
                  extend this framework to deal with large-scale
                  seismic data interpolation problems, where I show
                  that the standard approach of partitioning the
                  seismic data into windows is not required, which use
                  the fact that events tend to become linear in these
                  windows, while exploiting the low-rank structure of
                  seismic data. Carefully selected synthetic and
                  realistic seismic data examples validate the
                  efficacy of the interpolation framework. Next, I
                  extend the SVD-free rank-minimization approach to
                  remove the seismic cross-talk in simultaneous source
                  acquisition. Experimental results verify that source
                  separation using the SVD-free rank-minimization
                  approaches are comparable to the sparsity-promotion
                  based techniques; however, separation via
                  rank-minimization is significantly faster and memory
                  efficient. We further introduce a matrix-vector
                  formulation to form full-subsurface extended image
                  volumes, which removes the storage and computational
                  bottleneck found in the convention methods. I
                  demonstrate that the proposed matrix-vector
                  formulation is used to form different image gathers
                  with which amplitude-versus-angle and wave-equation
                  migration velocity analyses is performed, without
                  requiring prior information on the geologic
                  dips. Finally, I conclude the thesis by outlining
                  potential future research directions and extensions
                  of the thesis work.},
  keywords = {PhD, acquisition, processing, waveform inversion, rank minimization, extended image volumes, migration velocity analysis},
  note = {(PhD)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2017/kumar2017THels/kumar2017THels.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Thesis/2017/kumar2017THels/kumar2017THels_pres.pdf}
}


@PHDTHESIS{wason2017THsss,
  author = {Haneet Wason},
  title = {Simultaneous-source seismic data acquisition and processing with compressive sensing},
  school = {The University of British Columbia},
  year = {2017},
  month = {08},
  address = {Vancouver},
  abstract = {The work in this thesis adapts ideas from the field of
                  compressive sensing (CS) that lead to new insights
                  into acquiring and processing seismic data, where we
                  can fundamentally rethink how we design seismic
                  acquisition surveys and process acquired data to
                  minimize acquisition- and processing-related
                  costs. Current efforts towards dense source/receiver
                  sampling and full azimuthal coverage to produce
                  high-resolution images of the subsurface have led to
                  the deployment of multiple sources across survey
                  areas. A step ahead from multisource acquisition is
                  simultaneous-source acquisition, where multiple
                  sources fire shots at near-simultaneous/random times
                  resulting in overlapping shot records, in comparison
                  to no overlaps during conventional sequential-source
                  acquisition. Adoption of simultaneous-source
                  techniques has helped to improve survey efficiency
                  and data density. The engine that drives
                  simultaneous-source technology is
                  simultaneous-source separation --- a methodology
                  that aims to recover conventional sequential-source
                  data from simultaneous-source data. This is
                  essential because many seismic processing techniques
                  rely on dense and periodic (or regular)
                  source/receiver sampling. We address the challenge
                  of source separation through a combination of
                  tailored simultaneous-source acquisition design and
                  sparsity-promoting recovery via convex optimization
                  using l1 objectives. We use CS metrics to
                  investigate the relationship between marine
                  simultaneous-source acquisition design and data
                  reconstruction fidelity, and consequently assert the
                  importance of randomness in the acquisition system
                  in combination with an appropriate choice for a
                  sparsifying transform (i.e., curvelet transform) in
                  the reconstruction algorithm. We also address the
                  challenge of minimizing the cost of expensive,
                  dense, periodically-sampled and replicated
                  time-lapse surveying and data processing by adapting
                  ideas from distributed compressive sensing. We show
                  that compressive randomized time-lapse surveys need
                  not be replicated to attain acceptable levels of
                  data repeatability, as long as we know the shot
                  positions (post acquisition) to a sufficient degree
                  of accuracy. We conclude by comparing
                  sparsity-promoting and rank-minimization recovery
                  techniques for marine simultaneous-source
                  separation, and demonstrate that recoveries are
                  comparable; however, the latter approach readily
                  scales to large-scale seismic data and is
                  computationally faster.},
  keywords = {PhD, acquisition, marine, simultaneous source, source separation, compressive sensing, optimization},
  note = {(PhD)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2017/wason2017THsss/wason2017THsss.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Thesis/2017/wason2017THsss/wason2017THsss_pres.pdf}
}


@PHDTHESIS{oghenekohwo2017THetl,
  author = {Felix Oghenekohwo},
  title = {Economic time-lapse seismic acquisition and imaging---{Reaping} the benefits of randomized sampling with distributed compressive sensing},
  school = {The University of British Columbia},
  year = {2017},
  month = {08},
  address = {Vancouver},
  abstract = {This thesis presents a novel viewpoint on the implicit
                  opportunities randomized surveys bring to time-lapse
                  seismic - which is a proven surveillance tool for
                  hydrocarbon reservoir monitoring. Time-lapse (4D)
                  seismic combines acquisition and processing of at
                  least two seismic datasets (or vintages) in order to
                  extract information related to changes in a
                  reservoir within a specified time interval. The
                  current paradigm places stringent requirements on
                  replicating the 4D surveys, which is an expensive
                  task often requiring uneconomical dense sampling of
                  seismic wavefields. To mitigate the challenges of
                  dense sampling, several advances in seismic
                  acquisition have been made in recent years including
                  the use of multiple sources firing at near
                  simultaneous random times, and the adaptation of
                  Compressive Sensing (CS) principles to design
                  practical acquisition engines that improve sampling
                  efficiency for seismic data acquisition. However,
                  little is known regarding the implications of these
                  developments for time-lapse studies. By conducting
                  multiple experiments modelling surveys adhering to
                  the principles of CS for 4D seismic, I propose a
                  model that demonstrates the feasibility of
                  randomized acquisitions for time-lapse seismic. The
                  proposed joint recovery model (JRM), which derives
                  from distributed CS, exploits the common information
                  in time-lapse data during recovery of dense
                  wavefields from measured subsampled data, providing
                  highly repeatable and high-fidelity vintages. I show
                  that we obtain better vintages when randomized
                  surveys are not replicated, in contrast to standard
                  practice, paving the way for an opportunity to relax
                  the rigorous requirement to replicate surveys
                  precisely. We assert that the vintages obtained
                  using our proposed model are of sufficient quality
                  to serve as inputs to processes that extract
                  time-lapse attributes from which subsurface changes
                  are deduced. Additionally, I show that recovery with
                  the JRM is robust with respect to errors due to
                  differences between actual and recorded postplot
                  information. Finally, I present an opportunity to
                  adapt our model to problems related to time-lapse
                  seismic imaging where the main finding is that we
                  can better delineate time-lapse changes by adapting
                  the joint recovery model to wave-equation based
                  inversion methods.},
  keywords = {PhD, time lapse, acquisition, joint recovery, thesis, compressive sensing, distributed compressive sensing},
  note = {(PhD)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2017/oghenekohwo2017THetl/oghenekohwo2017THetl.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Thesis/2017/oghenekohwo2017THetl/oghenekohwo2017THetl_pres.pdf}
}


%-----2015-----%

@PHDTHESIS{lin2015THpes,
  author = {Tim T.Y. Lin},
  title = {Primary estimation with sparsity-promoting bi-convex optimization},
  school = {The University of British Columbia},
  year = {2015},
  month = {10},
  address = {Vancouver},
  abstract = {This thesis establishes a novel inversion methodology
                  for the surface-related primaries from a given
                  recorded seismic wavefield, called the Robust
                  Estimation of Primaries by Sparse Inversion (Robust
                  EPSI, or REPSI). Surface-related multiples are a
                  major source of coherent noise in seismic data, and
                  inferring fine geological structures from
                  active-source seismic recordings typically first
                  necessitates its removal or mitigation. For this
                  task, current practice calls for data-driven
                  approaches which produce only approximate multiple
                  models that must be non-linearly subtracted from the
                  data, often distorting weak primary events in the
                  process. A recently proposed method called
                  Estimation of Primaries by Sparse Inversion (EPSI)
                  avoids this adaptive subtraction by directly
                  inverting for a discrete representation of the
                  underlying multiple-free subsurface impulse response
                  as a set of band-limited spikes. However, in its
                  original form, the EPSI algorithm exhibits a few
                  notable shortcomings that impede adoption. Although
                  it was shown that the correct impulse response can
                  be obtained through a sparsest solution criteria,
                  the current EPSI algorithm is not designed to take
                  advantage of this finding, but instead approximates
                  a sparse solution in an ad-hoc manner that requires
                  practitioners to decide on a multitude of inversion
                  parameters. The Robust EPSI method introduced in
                  this thesis reformulates the original EPSI problem
                  as a formal bi-convex optimization problem that
                  makes obtaining the sparsest solution an explicit
                  goal, while also reliably admit satisfactory
                  solutions using contemporary self-tuning gradient
                  methods commonly seen in large-scale machine
                  learning communities. I show that the Robust EPSI
                  algorithm is able to operate successfully on a
                  variety of datasets with minimal user input, while
                  also producing a more accurate model of the
                  subsurface impulse response when compared to the
                  original algorithm. Furthermore, this thesis makes
                  several contributions that improves the capability
                  and practicality of EPSI: a novel scattering-based
                  multiple prediction model that allows Robust EPSI to
                  deal with wider near-offset receiver gaps than
                  previously demonstrated for EPSI, as well as a
                  multigrid-inspired continuation strategy that
                  significantly reduces the computation time needed to
                  solve EPSI-type problems. These additions are
                  enabled by and built upon the formalism of the
                  Robust EPSI as developed in this thesis.},
  keywords = {PhD, inversion, EPSI, biconvex, multiples, sparsity, optimization},
  note = {(PhD)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2015/lin2015THpes/lin2015THpes.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Thesis/2015/lin2015THpes/lin2015THpes_pres.pdf}
}


@PHDTHESIS{tu2015THfis,
  author = {Ning Tu},
  title = {Fast imaging with surface-related multiples},
  school = {The University of British Columbia},
  year = {2015},
  month = {08},
  address = {Vancouver},
  abstract = {Surface-related multiples, which are waves that bounce
                  more than once between the water surface and the
                  subsurface reflectors, constitute a significant part
                  of the data acquired in marine seismic surveys. If
                  left untreated, they can lead to misplaced phantom
                  reflectors in the image, and result in erroneous
                  interpretations of the subsurface structure. As a
                  result, these multiples are removed before the
                  imaging procedure in conventional seismic data
                  processing. However, because they interact more with
                  the subsurface medium, they may carry extra
                  information that is not present in the primaries.
                  Therefore instead of removing these multiples, a
                  more desirable alternative is to make active use of
                  them. We derive from the well-established
                  "Surface-Related Multiple Elimination" relation, and
                  arrive at a linearized expression of the
                  wave-equation based modelling that incorporates the
                  surface- related multiples.  We then present a
                  computationally efficient approach to iteratively
                  invert this expression to obtain an image of the
                  subsurface from data that contain multiples. We
                  achieve the computational efficiency inside each
                  iteration by (i) using the wave-equation solver to
                  implicitly carry out the expensive multiple
                  prediction; and (ii) reducing the number of
                  wave-equation solves during data simulation by
                  subsampling the monochromatic source experiments. We
                  show that, compared with directly applying the
                  cross-correlation/deconvolutional imaging
                  conditions, the presented approach can suppress the
                  coherent imaging artifacts from multiples more
                  effectively. We also show that, by curvelet-domain
                  sparsity promoting and occasionally drawing new data
                  samples during the inversion, the proposed inversion
                  method gains improved robustness to velocity errors
                  in the background model, as well as modelling errors
                  incurred during linearization of the
                  wave-equation. To combine the information encoded in
                  both the primaries and the multiples, we then
                  propose a highly accurate source estimation method
                  to jointly invert the total upgoing wavefield. We
                  show with field data examples that we can reap
                  benefits from both the relative noise-free primaries
                  and the extra illumination coverage of the
                  multiples. We also demonstrate that the inclusion of
                  multiples help mitigate the amplitude ambiguity
                  during source estimation. We conclude the thesis
                  with an outlook for future research directions, as
                  well as potential extensions of the proposed work.},
  keywords = {inversion, seismic imaging, multiples, least-squares},
  note = {(PhD)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2015/tu2015THfis/tu2015THfis.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Thesis/2015/tu2015THfis/tu2015THfis_pres.pdf}
}


@PHDTHESIS{li2015THsps,
  author = {Xiang Li},
  title = {Sparsity promoting seismic imaging and full-waveform inversion},
  school = {The University of British Columbia},
  year = {2015},
  month = {07},
  address = {Vancouver},
  abstract = {This thesis will address the large computational costs
                  of solving least-squares migration and full-waveform
                  inversion problems. Least-squares seismic imaging
                  and full-waveform inversion are seismic inversion
                  techniques that require iterative minimizations of
                  large least-squares misfit functions. Each iteration
                  requires an evaluation of the Jacobian operator and
                  its adjoint, both of which require two wave-equation
                  solves for all sources, creating prohibitive
                  computational costs. In order to reduce costs, we
                  utilize randomized dimensionality reduction
                  techniques, reducing the number of sources used
                  during inversion. The randomized dimensionality
                  reduction techniques create subsampling related
                  artifacts, which we mitigate by using
                  curvelet-domain sparsity-promoting inversion
                  techniques. Our method conducts least-squares
                  imaging at the approximate cost of one reverse-time
                  migration with all sources, and computes the
                  Gauss-Newton full-waveform inversion update at
                  roughly the cost of one gradient update with all
                  sources. Finally, during our research of the
                  full-waveform inversion problem, we discovered that
                  we can utilize our method as an alternative approach
                  to add sparse constraints on the entire velocity
                  model by imposing sparsity constraints on each model
                  update separately, rather than regularizing the
                  total velocity model as typically practiced. We also
                  observed this alternative approach yields a faster
                  decay of the residual and model error as a function
                  of iterations. We provided empirical arguments why
                  and when imposing sparsity on the updates can lead
                  to improved full-waveform inversion results.},
  keywords = {full-waveform inversion, Gauss-Newton method, sparsity promoting, least-squares imaging, seismic imaging},
  note = {(PhD)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2015/li2015THsps/li2015THsps.pdf},
  presentation = {https://slim.gatech.edu/Publications/Public/Thesis/2015/li2015THsps/li2015THsps_pres.pdf}
}


%-----2010-----%

@PHDTHESIS{moghaddam10phd,
  author = {Peyman P. Moghaddam},
  title = {Curvelet-based migration amplitude recovery},
  school = {The University of British Columbia},
  year = {2010},
  month = {05},
  address = {Vancouver},
  abstract = {Migration can accurately locate reflectors in the earth
                  but in most cases fails to correctly resolve their
                  amplitude. This might lead to mis-interpretation of
                  the nature of reflector. In this thesis, I
                  introduced a method to accurately recover the
                  amplitude of the seismic reflector. This method
                  relies on a new transform-based recovery that
                  exploits the expression of seismic images by the
                  recently developed curvelet transform. The elements
                  of this transform, called curvelets, are
                  multi-dimensional, multi-scale, and
                  multi-directional. They also remain approximately
                  invariant under the imaging operator. I exploit
                  these properties of the curvelets to introduce a
                  method called Curvelet Match Filtering (CMF) for
                  recovering the seismic amplitude in presence of
                  noise in both migrated image and data. I detail the
                  method and illustrate its performance on synthetic
                  dataset. I also extend CMF formulation to other
                  geophysical applications and present results on
                  multiple removal. In addition of that, I investigate
                  preconditioning of the migration which results to
                  rapid convergence rate of the iterative method using
                  migration.},
  note = {(PhD)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2010/moghaddam10phd.pdf}
}


%-----2008-----%

@PHDTHESIS{hennenfent08phd,
  author = {Gilles Hennenfent},
  title = {Sampling and reconstruction of seismic wavefields in the curvelet domain},
  school = {The University of British Columbia},
  year = {2008},
  month = {05},
  address = {Vancouver},
  abstract = {Wavefield reconstruction is a crucial step in the
                  seismic processing flow. For instance, unsuccessful
                  interpolation leads to erroneous multiple
                  predictions that adversely affect the performance of
                  multiple elimination, and to imaging artifacts. We
                  present a new non-parametric transform-based
                  reconstruction method that exploits the compression
                  of seismic data by the recently developed curvelet
                  transform. The elements of this transform, called
                  curvelets, are multi-dimensional, multi-scale, and
                  multi-directional. They locally resemble wavefronts
                  present in the data, which leads to a compressible
                  representation for seismic data. This compression
                  enables us to formulate a new curvelet-based seismic
                  data recovery algorithm through sparsity-promoting
                  inversion (CRSI). The concept of sparsity-promoting
                  inversion is in itself not new to
                  geophysics. However, the recent insights from the
                  field of {\textquoteleft}{\textquoteleft}compressed
                  sensing{\textquoteright}{\textquoteright} are new
                  since they clearly identify the three main
                  ingredients that go into a successful formulation of
                  a reconstruction problem, namely a sparsifying
                  transform, a sub-Nyquist sampling strategy that
                  subdues coherent aliases in the sparsifying domain,
                  and a data-consistent sparsity-promoting
                  program. After a brief overview of the curvelet
                  transform and our seismic-oriented extension to the
                  fast discrete curvelet transform, we detail the CRSI
                  formulation and illustrate its performance on
                  synthetic and real datasets. Then, we introduce a
                  sub-Nyquist sampling scheme, termed jittered
                  undersampling, and show that, for the same amount of
                  data acquired, jittered data are best interpolated
                  using CRSI compared to regular or random
                  undersampled data. We also discuss the large-scale
                  one-norm solver involved in CRSI. Finally, we extend
                  CRSI formulation to other geophysical applications
                  and present results on multiple removal and
                  migration-amplitude recovery.},
  keywords = {curvelet transform, reconstruction, SLIM},
  note = {(PhD)},
  url = {https://slim.gatech.edu/Publications/Public/Thesis/2008/hennenfent08phd.pdf}
}

%----- 2020 -----%

@MISC{siahkoohi2020SEGCHAPTERGTdlbuq,
  title = {A deep-learning based Bayesian approach to seismic imaging and uncertainty quantification},
  booktitle = {GT SEG Student Chapter},  
  year = {2020},
  abstract = {Uncertainty quantification is essential when dealing with
ill-conditioned inverse problems due to the inherent nonuniqueness of the
solution. Bayesian approaches allow us to determine how likely an estimation
of the unknown parameters is via formulating the posterior distribution.
Unfortunately, it is often not possible to formulate a prior distribution
that precisely encodes our prior knowledge about the unknown. Furthermore,
adherence to handcrafted priors may greatly bias the outcome of the Bayesian
analysis. To address this issue, we propose to use the functional form of a
randomly initialized convolutional neural network as an implicit structured
prior, which is shown to promote natural images and excludes images with
unnatural noise. In order to incorporate the model uncertainty into the final
estimate, we sample the posterior distribution using stochastic gradient
Langevin dynamics and perform Bayesian model averaging on the obtained
samples. Our synthetic numerical experiment verifies that deep priors
combined with Bayesian model averaging are able to partially circumvent
imaging artifacts and reduce the risk of overfitting in the presence of
extreme noise. Finally, we present pointwise variance of the estimates as a
measure of uncertainty, which coincides with regions that are more difficult
to image.},
  keywords = {deep learning, seismic imaging, stochastic gradient Langevin dynamics, uncertainty quantification},
  note = {(SEG Student Chapter)},
  url = {https://slim.gatech.edu/Publications/Public/Lectures/SEGCHAPTERGT/2020/siahkoohi2020SEGCHAPTERGTdlbuq/siahkoohi2020SEGCHAPTERGTdlbuq.pdf},
  author = {Ali Siahkoohi and Gabrio Rizzuti and Felix J. Herrmann}
}

%----- 2019 -----%

@MISC{Herrmann2019SEGDL,
  title = {Sometimes it pays to be cheap – Compressive time-lapse seismic data acquisition},
  booktitle = {SEG Distinguished Lecture},  
  organization = {Society of Exploration Geophysicists},
  year = {2019},
  abstract = {During these times of sustained low oil prices, it is essential to look for new innovative ways to collect (time-lapse) seismic data at reduced costs and preferably also at reduced environmental impact. By now, there is an increasing body of corroborating evidence — whether these are simulated case studies or actual acquisitions on land and marine — that seismic acquisition based on the principles of compressive sensing delivers on this premise by removing the need to acquire replicated dense surveys. Up to ten-fold increases in acquisition efficiency have been reported by industry while there are indications that this breakthrough is only the beginning of a paradigm shift where full-azimuth time-lapse processing will become a reality. To familiarize the audience with this new technology, I will first describe the basics of compressive sensing, how it relates to missing-trace interpolation and simultaneous source acquisition, followed by how this technology is driving innovations in full-azimuth (time-lapse) acquisition, yielding high-fidelity data with a high degree of repeatability and at a fraction of the costs.},
  keywords = {presentation, Compressive Sensing, Time-lapse Marine Acquisition},
  note = {(SEG Distinguished Lecture)},
  url = {https://slim.gatech.edu/Publications/Public/Lectures/SEG-DL/2019/Herrmann2019SEGDL/Herrmann2019SEGDL/},
  url2 = {https://slim.gatech.edu/Publications/Public/Lectures/SEG-DL/2019/Herrmann2019SEGDL/Herrmann2019SEGDL.pdf},
  author = {Felix J. Herrmann}
}

@MISC{witte2019HOTCSEdsagip,
  title = {Domain-specific abstractions for large-scale geophysical inverse problems},
  booktitle = {HotCSE Seminar},  
  year = {2019},
  abstract = {During these times of sustained low oil prices, it is essential to look for new innovative ways to collect (time-lapse) seismic data at reduced costs and preferably also at reduced environmental impact. By now, there is an increasing body of corroborating evidence — whether these are simulated case studies or actual acquisitions on land and marine — that seismic acquisition based on the principles of compressive sensing delivers on this premise by removing the need to acquire replicated dense surveys. Up to ten-fold increases in acquisition efficiency have been reported by industry while there are indications that this breakthrough is only the beginning of a paradigm shift where full-azimuth time-lapse processing will become a reality. To familiarize the audience with this new technology, I will first describe the basics of compressive sensing, how it relates to missing-trace interpolation and simultaneous source acquisition, followed by how this technology is driving innovations in full-azimuth (time-lapse) acquisition, yielding high-fidelity data with a high degree of repeatability and at a fraction of the costs.},
  keywords = {software, julia, large-scale, inversion, full-waveform-inversion, imaging},
  note = {(HotCSE)},
  url = {https://slim.gatech.edu/Publications/Public/Lectures/HotCSE/2019/witte2019HOTCSEdsagip/witte2019HOTCSEdsagip.pdf},
  author = {Philipp A. Witte and Mathias Louboutin and Felix J. Herrmann}
}

@MISC{herrmann2019HOTCSEliwcuq,
  title = {Learned imaging with constraints and uncertainty quantification},
  booktitle = {HotCSE Seminar},  
  year = {2019},
  abstract = {We outline new approaches to incorporate ideas from convolutional networks into wave-based least-squares imaging. The aim is to combine hand-crafted constraints with deep convolutional networks allowing us to directly train a network capable of generating samples from the posterior. The main contributions include combination of weak deep priors with hard handcrafted constraints and a possible new way to sample the posterior.},
  keywords = {HotCSE, Uncertainty quantification, Deep Learning, Imaging, Expectation Maximization},
  note = {(HotCSE)},
  url = {https://slim.gatech.edu/Publications/Public/Lectures/HotCSE/2019/herrmann2019HOTCSEliwcuq/herrmann2019HOTCSEliwcuq.pdf},
  author = {Felix J. Herrmann and Ali Siahkoohi and Gabrio Rizzuti}
}
% This file was created with JabRef 2.6.
% Encoding: MacRoman

@CONFERENCE{erlangga08SINBADimf,
  author = {Yogi A. Erlangga and K. Vuik and K. Oosterlee and D. Riyanti and R. Nabben},
  title = {Iterative methods for 2{D}/3{D} {Helmholtz} operator},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We present an iterative method for solving the 2D/3D
                  Helmholtz equation. The method is mainly based on a
                  Krylov method, preconditioned by a special operator
                  which represents a damped Helmholtz operator. The
                  discretization of the preconditioning operator is
                  then solved by one multigrid sweep. It can be shown
                  that while the spectrum is bounded above by one, the
                  smallest eigenvalue of the preconditioned system is
                  of order $k^{-1}$. In this situation, the
                  convergence of a Krylov method will be proportional
                  to the frequency of the problem. Further convergence
                  acceleration can be achieved if eigenvalues of order
                  $k^{-1}$ are projected from the spectrum. This can
                  be done by a projection operator, similar to but
                  more stable than deflation. This projection operator
                  has been the core of a new multilevel method, called
                  multilevel Krylov method, proposed by Erlangga and
                  Nabben only recently. Putting the preconditioned
                  Helmholtz operator in this setting, a convergence
                  which is independent of frequency can be obtained.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.gatech.edu/SINBAD2008/Program_files/SINBAD2008_Erlangga_Ite.pdf}
}


@ARTICLE{berkhout97eom,
  author = {A. J. Berkhout and D. J. Verschuur},
  title = {Estimation of multiple scattering by iterative inversion, {Part} {I}: {Theoretical} considerations},
  journal = {Geophysics},
  year = {1997},
  volume = {62},
  pages = {1586-1595},
  number = {5},
  abstract = {A review has been given of the surface-related multiple
                  problem by making use of the so-called feedback
                  model. From the resulting equations it has been
                  concluded that the proposed solution does not
                  require any properties of the subsurface. However,
                  source-detector and reflectivity properties of the
                  surface need be specified. Those properties have
                  been quantified in a surface operator and this
                  operator is estimated as part of the multiple
                  removal problem. The surface-related multiple
                  removal algorithm has been formulated in terms of a
                  Neumann series and in terms of an iterative
                  equation. The Neumann formulation requires a
                  nonlinear optimization process for the surface
                  operator; while the iterative formulation needs a
                  number of linear optimizations. The iterative
                  formulation also has the advantage that it can be
                  integrated easily with another multiple removal
                  method. An algorithm for the removal of internal
                  multiples has been proposed as well. This algorithm
                  is an extension of the surface-related
                  method. Removal of internal multiples requires
                  knowledge of the macro velocity model between the
                  surface and the upper boundary of the multiple
                  generating layer. In part II (also published in this
                  issue) the success of the proposed algorithms has
                  been demonstrated on numerical experiments and field
                  data examples. {\copyright}1997 Society of
                  Exploration Geophysicists},
  bdsk-url-1 = {http://library.seg.org/doi/abs/10.1190/1.1444261},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.1444261},
  date-added = {2008-05-07 18:38:50 -0700},
  date-modified = {2008-08-14 14:46:15 -0700},
  doi = {10.1190/1.1444261},
  issue = {5},
  keywords = {SRME},
  publisher = {SEG},
  url = {http://library.seg.org/doi/abs/10.1190/1.1444261}
}


@BOOK{biondo063ds,
  author = {B. L. Biondi},
  title = {3-{D} seismic imaging},
  publisher = {SEG},
  year = {2006},
  number = {14},
  series = {Investigations in Geophysics},
  date-added = {2008-05-08 15:25:18 -0700},
  date-modified = {2008-05-20 19:45:00 -0700},
  issue = {14},
  keywords = {imaging}
}


@ARTICLE{cordoba78wpa,
  author = {A. C\'ordoba and C. Fefferman},
  title = {Wave packets and {Fourier} integral operators},
  journal = {Communications in Partial Differential Equations},
  year = {1978},
  volume = {3},
  pages = {979-1005},
  number = {11},
  bdsk-url-1 = {http://dx.doi.org/10.1080/03605307808820083},
  date-added = {2008-05-07 11:53:23 -0700},
  date-modified = {2008-05-20 11:48:08 -0700},
  doi = {10.1080/03605307808820083},
  issue = {11},
  keywords = {wave packets, FIO},
  publisher = {Taylor \& Francis}
}


@PHDTHESIS{candes98rta,
  author = {E. J. Cand\`es},
  title = {Ridgelets: theory and applications},
  school = {Stanford University},
  year = {1998},
  address = {Stanford, CA},
  bdsk-url-1 = {http://www-stat.stanford.edu/%7Ecandes/papers/Thesis.ps.gz},
  date-added = {2008-05-27 18:24:11 -0700},
  date-modified = {2008-05-27 18:26:14 -0700},
  keywords = {ridgelet transform}
}


@ARTICLE{candes05tcr,
  author = {E. J. Cand\`es and L. Demanet},
  title = {The curvelet representation of wave propagators is optimally sparse},
  journal = {Communications on Pure and Applied Mathematics},
  year = {2005},
  volume = {58},
  pages = {1472-1528},
  number = {11},
  abstract = {This paper argues that curvelets provide a powerful tool
                  for representing very general linear symmetric
                  systems of hyperbolic differential
                  equations. Curvelets are a recently developed
                  multiscale system [10, 7] in which the elements are
                  highly anisotropic at fine scales, with effective
                  support shaped according to the parabolic scaling
                  principle width ≈ length^2 at fine scales. We prove
                  that for a wide class of linear hyperbolic
                  differential equations, the curvelet representation
                  of the solution operator is both optimally sparse
                  and well organized. * It is sparse in the sense that
                  the matrix entries decay nearly exponentially fast
                  (i.e. faster than any negative polynomial), * and
                  well-organized in the sense that the very few
                  nonnegligible entries occur near a few shifted
                  diagonals. Indeed, we show that the wave group maps
                  each curvelet onto a sum of curvelet-like waveforms
                  whose locations and orientations are obtained by
                  following the different Hamiltonian flows---hence
                  the diagonal shifts in the curvelet representation.
                  A physical interpretation of this result is that
                  curvelets may be viewed as coherent waveforms with
                  enough frequency localization so that they behave
                  like waves but at the same time, with enough spatial
                  localization so that they simultaneously behave like
                  particles.},
  bdsk-url-1 = {http://www-stat.stanford.edu/%7Ecandes/papers/CurveletsWaves.pdf},
  date-added = {2008-05-07 11:10:43 -0700},
  date-modified = {2008-08-14 14:57:23 -0700},
  doi = {10.1002/cpa.20078},
  issue = {11},
  keywords = {curvelet transform, FIO},
  pdf = {http://www-stat.stanford.edu/%7Ecandes/papers/CurveletsWaves.pdf}
}


@ARTICLE{candes06fdc,
  author = {E. J. Cand\`es and L. Demanet and D. L. Donoho and L. Ying},
  title = {Fast discrete curvelet transforms},
  journal = {Multiscale Modeling and Simulation},
  year = {2006},
  volume = {5},
  pages = {861-899},
  number = {3},
  abstract = {This paper describes two digital implementations of a
                  new mathematical transform, namely, the second
                  generation curvelet transform [12, 10] in two and
                  three dimensions. The first digital transformation
                  is based on unequally-spaced fast Fourier transforms
                  (USFFT) while the second is based on the wrapping of
                  specially selected Fourier samples. The two
                  implementations essentially differ by the choice of
                  spatial grid used to translate curvelets at each
                  scale and angle. Both digital transformations return
                  a table of digital curvelet coefficients indexed by
                  a scale parameter, an orientation parameter, and a
                  spatial location parameter. And both implementations
                  are fast in the sense that they run in O(n^2 log n)
                  flops for n by n Cartesian arrays; in addition, they
                  are also invertible, with rapid inversion algorithms
                  of about the same complexity. Our digital
                  transformations improve upon earlier
                  implementations---based upon the first generation of
                  curvelets---in the sense that they are conceptually
                  simpler, faster and far less redundant. The software
                  CurveLab, which implements both transforms presented
                  in this paper, is available at
                  http://www.curvelet.org.},
  bdsk-url-1 = {http://dx.doi.org/10.1137/05064182X},
  bdsk-url-2 = {http://www-stat.stanford.edu/%7Ecandes/papers/FDCT.pdf},
  date-added = {2008-05-06 19:34:41 -0700},
  date-modified = {2008-08-14 14:58:30 -0700},
  doi = {10.1137/05064182X},
  issue = {3},
  keywords = {curvelet transform},
  pdf = {http://www-stat.stanford.edu/%7Ecandes/papers/FDCT.pdf},
  publisher = {SIAM}
}


@INCOLLECTION{candes00cas,
  author = {E. J. Cand\`es and D. L. Donoho},
  title = {Curvelets: a surprisingly effective nonadaptive representation of objects with edges},
  booktitle = {Curve and surface fitting},
  publisher = {Vanderbilt University Press},
  year = {2000},
  editor = {A. Cohen, C. Rahut, and L. L. Schumaker},
  pages = {105-120},
  address = {Nashville, TN},
  abstract = {It is widely believed that to efficiently represent an
                  otherwise smooth ob ject with discontinuities along
                  edges, one must use an adaptive representation that
                  in some sense `tracks' the shape of the
                  discontinuity set. This folk-belief --- some would
                  say folk-theorem --- is incorrect. At the very
                  least, the possible quantitative advantage of such
                  adaptation is vastly smaller than commonly
                  believed. We have recently constructed a tight frame
                  of curvelets which provides stable, efficient, and
                  near-optimal representation of otherwise smooth ob
                  jects having discontinuities along smooth curves. By
                  applying naive thresholding to the curvelet
                  transform of such an ob ject, one can form m-term
                  approximations with rate of L2 approximation
                  rivaling the rate obtainable by complex adaptive
                  schemes which attempt to `track' the discontinuity
                  set. In this article we explain the basic issues of
                  efficient m-term approximation, the construction of
                  efficient adaptive representation, the construction
                  of the curvelet frame, and a crude analysis of the
                  performance of curvelet schemes.},
  bdsk-url-1 = {http://www-stat.stanford.edu/%7Ecandes/papers/Curvelet-SMStyle.pdf},
  date-added = {2008-05-26 17:48:55 -0700},
  date-modified = {2008-08-14 15:26:58 -0700},
  keywords = {curvelet transform}
}


@ARTICLE{candes05cct,
  author = {E. J. Cand\`es and D. L. Donoho},
  title = {Continuous curvelet transform: {I.} {Resolution} of the wavefront set},
  journal = {Applied and Computational Harmonic Analysis},
  year = {2005},
  volume = {19},
  pages = {162-197},
  number = {2},
  month = {09},
  bdsk-url-1 = {http://dx.doi.org/10.1016/j.acha.2005.02.003},
  date-added = {2008-05-26 18:21:22 -0700},
  date-modified = {2008-05-26 18:23:57 -0700},
  issue = {2},
  keywords = {curvelet transform}
}


@ARTICLE{candes05cct1,
  author = {E. J. Cand\`es and D. L. Donoho},
  title = {Continuous curvelet transform: {II.} {Discretization} and frames},
  journal = {Applied and Computational Harmonic Analysis},
  year = {2005},
  volume = {19},
  pages = {198-222},
  number = {2},
  month = {09},
  bdsk-url-1 = {http://dx.doi.org/10.1016/j.acha.2005.02.004},
  date-added = {2008-05-26 18:23:17 -0700},
  date-modified = {2008-05-26 18:24:36 -0700},
  issue = {2},
  keywords = {curvelet transform}
}


@ARTICLE{candes04ntf,
  author = {E. J. Cand\`es and D. L. Donoho},
  title = {New tight frames of curvelets and optimal representations of objects with piecewise-{C}$^2$ singularities},
  journal = {Communications on Pure and Applied Mathematics},
  year = {2004},
  volume = {57},
  pages = {219-266},
  number = {2},
  bdsk-url-1 = {http://dx.doi.org/10.1002/cpa.10116},
  bdsk-url-2 = {http://www-stat.stanford.edu/%7Ecandes/papers/CurveEdges.pdf},
  date-added = {2008-05-07 11:47:59 -0700},
  date-modified = {2008-08-14 14:46:59 -0700},
  doi = {10.1002/cpa.10116},
  issue = {2},
  keywords = {curvelet transform},
  pdf = {http://www-stat.stanford.edu/%7Ecandes/papers/CurveEdges.pdf}
}


@ARTICLE{chauris08sdm,
  author = {H. Chauris and T. Nguyen},
  title = {Seismic demigration/migration in the curvelet domain},
  journal = {Geophysics},
  year = {2008},
  volume = {73},
  pages = {S35-S46},
  number = {2},
  abstract = {Curvelets can represent local plane waves. They
                  efficiently decompose seismic images and possibly
                  imaging operators. We study how curvelets are
                  distorted after demigration followed by migration in
                  a different velocity model. We show that for small
                  local velocity perturbations, the
                  demigration/migration is reduced to a simple
                  morphing of the initial curvelet. The derivation of
                  the expected curvature of the curvelets shows that
                  it is easier to sparsify the demigration/migration
                  operator than the migration operator. An application
                  on a 2D synthetic data set, generated in a smooth
                  heterogeneous velocity model and with a complex
                  reflectivity, demonstrates the usefulness of
                  curvelets to predict what a migrated image would
                  become in a locally different velocity model without
                  the need for remigrating the full input data
                  set. Curvelets are thus well suited to study the
                  sensitivity of a prestack depth-migrated image with
                  respect to the heterogeneous velocity model used for
                  migration. {\copyright}2008 Society of Exploration
                  Geophysicists},
  bdsk-url-1 = {http://library.seg.org/doi/abs/10.1190/1.2831933},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.2831933},
  date-added = {2008-05-07 14:48:33 -0700},
  date-modified = {2008-08-14 14:59:04 -0700},
  doi = {10.1190/1.2831933},
  issue = {2},
  keywords = {curvelet transform, imaging},
  pdf = {http://library.seg.org/doi/abs/10.1190/1.2831933},
  publisher = {SEG}
}


@BOOK{claerbout92esa,
  author = {J. F. Claerbout},
  title = {Earth soundings analysis: processing versus inversion},
  publisher = {Blackwell Scientific Publications},
  year = {1992},
  address = {Boston},
  bdsk-url-1 = {http://sepwww.stanford.edu/sep/prof/pvi.pdf},
  date-added = {2008-05-06 19:27:28 -0700},
  date-modified = {2008-05-07 11:44:19 -0700},
  keywords = {PEF},
  pdf = {http://sepwww.stanford.edu/sep/prof/pvi.pdf}
}


@ARTICLE{claerbout71tau,
  author = {J. F. Claerbout},
  title = {Toward a unified theory of reflector mapping},
  journal = {Geophysics},
  year = {1971},
  volume = {36},
  pages = {467-481},
  number = {3},
  abstract = {Schemes for seismic mapping of reflectors in the
                  presence of an arbitrary velocity model, dipping and
                  curved reflectors, diffractions, ghosts, surface
                  elevation variations, and multiple reflections are
                  reviewed and reduced to a single formula involving
                  up and downgoing waves. The mapping formula may be
                  implemented without undue complexity by means of
                  difference approximations to the relativistic
                  Schroedinger equation. {\copyright}1971 Society of
                  Exploration Geophysicists},
  bdsk-url-1 = {http://library.seg.org/doi/abs/10.1190/1.1440185},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.1440185},
  date-added = {2008-05-08 14:59:36 -0700},
  date-modified = {2008-08-14 14:59:35 -0700},
  doi = {10.1190/1.1440185},
  issue = {3},
  keywords = {WEM, imaging},
  pdf = {http://library.seg.org/doi/abs/10.1190/1.1440185},
  publisher = {SEG}
}


@ARTICLE{daubechies04ait,
  author = {I. Daubechies and M. Defrise and C. {De Mol}},
  title = {An iterative thresholding algorithm for linear inverse problems with a sparsity constraint},
  journal = {Communications on Pure and Applied Mathematics},
  year = {2004},
  volume = {57},
  pages = {1413-1457},
  number = {11},
  abstract = {We consider linear inverse problems where the solution
                  is assumed to have a sparse expansion on an
                  arbitrary preassigned orthonormal basis. We prove
                  that replacing the usual quadratic regularizing
                  penalties by weighted p-penalties on the
                  coefficients of such expansions, with 1 p 2, still
                  regularizes the problem. Use of such p-penalized
                  problems with p < 2 is often advocated when one
                  expects the underlying ideal noiseless solution to
                  have a sparse expansion with respect to the basis
                  under consideration. To compute the corresponding
                  regularized solutions, we analyze an iterative
                  algorithm that amounts to a Landweber iteration with
                  thresholding (or nonlinear shrinkage) applied at
                  each iteration step. We prove that this algorithm
                  converges in norm. {\copyright} 2004 Wiley
                  Periodicals, Inc.},
  bdsk-url-1 = {http://dx.doi.org/10.1002/cpa.20042},
  date-added = {2008-05-20 13:58:17 -0700},
  date-modified = {2008-08-14 15:01:17 -0700},
  issue = {11},
  pdf = {http://dx.doi.org/10.1002/cpa.20042},
  refer1 = {10.1002/cpa.20042}
}


@ARTICLE{do2002can,
  author = {M. N. Do and M. Vetterli},
  title = {Contourlets: a new directional multiresolution image representation},
  journal = {Proceedings. 2002 International Conference on Image Processing.},
  year = {2002},
  volume = {1},
  abstract = {We propose a new scheme, named contourlet, that provides
                  a flexible multiresolution, local and directional
                  image expansion. The contourlet transform is
                  realized efficiently via a double iterated filter
                  bank structure. Furthermore, it can be designed to
                  satisfy the anisotropy scaling relation for curves,
                  and thus offers a fast and structured curvelet-like
                  decomposition. As a result, the contourlet transform
                  provides a sparse representation for two-dimensional
                  piecewise smooth signals resembling images. Finally,
                  we show some numerical experiments demonstrating the
                  potential of contourlets in several image processing
                  tasks.},
  bdsk-url-1 = {http://dx.doi.org/10.1109/ICIP.2002.1038034},
  date-added = {2008-05-07 11:58:00 -0700},
  date-modified = {2008-08-14 15:01:55 -0700},
  doi = {10.1109/ICIP.2002.1038034},
  keywords = {contourlet transform}
}


@TECHREPORT{donoho99dct,
  author = {D. L. Donoho and M. R. Duncan},
  title = {Digital curvelet transform: strategy, implementation, and experiments},
  institution = {Stanford Statistics Department},
  year = {1999},
  month = {11},
  bdsk-url-1 = {http://citeseer.ist.psu.edu/rd/44392127,300178,1,0.25,Download/http://citeseer.ist.psu.edu/cache/papers/cs/15527/http:zSzzSzwww-stat.stanford.eduzSz~donohozSzReportszSz1999zSzDCvT.pdf/donoho99digital.pdf},
  date-added = {2008-05-26 17:33:51 -0700},
  date-modified = {2008-05-26 17:35:32 -0700},
  keywords = {curvelet transform}
}


@ARTICLE{douma07los,
  author = {H. Douma and M. V. de Hoop},
  title = {Leading-order seismic imaging using curvelets},
  journal = {Geophysics},
  year = {2007},
  volume = {72},
  pages = {S231-S248},
  number = {6},
  abstract = {Curvelets are plausible candidates for simultaneous
                  compression of seismic data, their images, and the
                  imaging operator itself. We show that with
                  curvelets, the leading-order approximation (in
                  angular frequency, horizontal wavenumber, and
                  migrated location) to common-offset (CO) Kirchhoff
                  depth migration becomes a simple transformation of
                  coordinates of curvelets in the data, combined with
                  amplitude scaling. This transformation is calculated
                  using map migration, which employs the local slopes
                  from the curvelet decomposition of the data. Because
                  the data can be compressed using curvelets, the
                  transformation needs to be calculated for relatively
                  few curvelets only. Numerical examples for
                  homogeneous media show that using the leading-order
                  approximation only provides a good approximation to
                  CO migration for moderate propagation times. As the
                  traveltime increases and rays diverge beyond the
                  spatial support of a curvelet; however, the
                  leading-order approximation is no longer accurate
                  enough. This shows the need for correction beyond
                  leading order, even for homogeneous
                  media. {\copyright}2007 Society of Exploration
                  Geophysicists},
  bdsk-url-1 = {http://library.seg.org/doi/abs/10.1190/1.2785047},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.2785047},
  date-added = {2008-05-07 14:35:47 -0700},
  date-modified = {2008-08-14 15:02:25 -0700},
  doi = {10.1190/1.2785047},
  issue = {6},
  keywords = {curvelet transform, imaging},
  pdf = {http://library.seg.org/doi/abs/10.1190/1.2785047},
  publisher = {SEG}
}


@INCOLLECTION{feichtinger94tap,
  author = {H. G. Feichtinger and K. Grochenig},
  title = {Theory and practice of irregular sampling},
  booktitle = {Wavelets: mathematics and applications},
  publisher = {CRC Press},
  year = {1994},
  editor = {J. J. Benedetto and M. Frazier},
  series = {Studies in Advanced Mathematics},
  chapter = {8},
  pages = {305-363},
  address = {Boca Raton, FL},
  bdsk-url-1 = {http://www.univie.ac.at/nuhag-php/bibtex/open_files/fegr94_fgthpra.pdf},
  date-added = {2008-05-20 17:10:18 -0700},
  date-modified = {2008-05-20 17:24:38 -0700},
  keywords = {sampling},
  pdf = {http://www.univie.ac.at/nuhag-php/bibtex/open_files/fegr94_fgthpra.pdf}
}


@MISC{fomel07mos,
  author = {S. Fomel and P. Sava},
  title = {{MADAGASCAR}: open-source software package for geophysical data processing and reproducible numerical experiments},
  year = {2007},
  abstract = {Madagascar is an open-source software package for
                  geophysical data analysis and reproducible numerical
                  experiments. Its mission is to provide -a convenient
                  and powerful environment -a convenient technology
                  transfer tool for researchers working with digital
                  image and data processing.  The technology developed
                  using the Madagascar project management system is
                  transferred in the form of recorded processing
                  histories, which become "computational recipes" to
                  be verified, exchanged, and modified by users of the
                  system.},
  bdsk-url-1 = {http://rsf.sf.net},
  date-added = {2008-06-26 15:31:10 -0700},
  date-modified = {2008-08-14 15:31:44 -0700},
  keywords = {software},
  url = {http://rsf.sf.net}
}


@ARTICLE{guo07osm,
  author = {K. Guo and D. Labate},
  title = {Optimally sparse multidimensional representation using shearlets},
  journal = {Journal of Mathematical Analysis},
  year = {2007},
  volume = {39},
  pages = {298-318},
  number = {1},
  bdsk-url-1 = {http://www4.ncsu.edu/~dlabate/shear_GL.pdf},
  bdsk-url-2 = {http://dx.doi.org/10.1137/060649781},
  date-added = {2008-05-07 12:03:03 -0700},
  date-modified = {2008-05-08 10:28:30 -0700},
  doi = {10.1137/060649781},
  issue = {1},
  keywords = {shearlet transform},
  pdf = {http://www4.ncsu.edu/~dlabate/shear_GL.pdf},
  publisher = {SIAM}
}


@ARTICLE{hampson86ivs,
  author = {D. Hampson},
  title = {Inverse velocity stacking for multiple elimination},
  journal = {Journal of the Canadian Society of Exploration Geophysicists},
  year = {1986},
  volume = {22},
  pages = {44-45},
  number = {1},
  bdsk-url-1 = {http://209.91.124.56/publications/journal/1986_12/1986_Hampson_D_inverse_velocity_stacking.pdf},
  date-added = {2008-05-06 19:09:45 -0700},
  date-modified = {2008-05-07 11:44:52 -0700},
  issue = {1},
  keywords = {Radon transform},
  pdf = {http://209.91.124.56/publications/journal/1986_12/1986_Hampson_D_inverse_velocity_stacking.pdf},
  publisher = {CSEG}
}


@ARTICLE{hindriks00ro3,
  author = {K. Hindriks and A. J. W. Duijndam},
  title = {Reconstruction of {3-D} seismic signals irregularly sampled along two spatial coordinates},
  journal = {Geophysics},
  year = {2000},
  volume = {65},
  pages = {253-263},
  number = {1},
  abstract = {Seismic signals are often irregularly sampled along
                  spatial coordinates, leading to suboptimal
                  processing and imaging results. Least-squares
                  estimation of Fourier components is used for the
                  reconstruction of band-limited seismic signals that
                  are irregularly sampled along two spatial
                  coordinates. A simple and efficient diagonal
                  weighting scheme, based on the areas surrounding the
                  spatial samples, takes the properties of the noise
                  (signal outside the bandwidth) into account in an
                  approximate sense. Diagonal stabilization based on
                  the energies of the signal and the noise ensures
                  robust estimation. Reconstruction by temporal
                  frequency component allows the specification of
                  varying bandwidth in two dimensions, depending on
                  the minimum apparent velocity. This parameterization
                  improves the reconstruction capability for lower
                  temporal frequencies. The shape of the spatial
                  aperture affects the method of sampling the Fourier
                  domain. Taking into account this property, a larger
                  bandwidth can be recovered. The properties of the
                  least-squares estimator allow a very efficient
                  implementation which, when using a conjugate
                  gradient algorithm, requires a modest number of 2-D
                  fast Fourier transforms per temporal frequency. The
                  method shows signicant improvement over the
                  conventionally used binning and stacking method on
                  both synthetic and real data. The method can be
                  applied to any subset of seismic data with two
                  varying spatial coordinates. {\copyright}2000
                  Society of Exploration Geophysicists},
  bdsk-url-1 = {http://link.aip.org/link/?GPY/65/253/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.1444716},
  date-added = {2008-05-20 16:12:37 -0700},
  date-modified = {2008-08-14 15:05:01 -0700},
  doi = {10.1190/1.1444716},
  issue = {1},
  keywords = {reconstruction},
  pdf = {http://link.aip.org/link/?GPY/65/253/1},
  publisher = {SEG}
}


@PHDTHESIS{kunis06nff,
  author = {S. Kunis},
  title = {Nonequispaced {FFT}: generalisation and inversion},
  school = {L\"ubeck university},
  year = {2006},
  bdsk-url-1 = {http://www.analysis.uni-osnabrueck.de/kunis/paper/KunisDiss.pdf},
  date-added = {2008-05-07 18:51:16 -0700},
  date-modified = {2008-05-20 11:49:04 -0700},
  keywords = {NFFT},
  pdf = {http://www.analysis.uni-osnabrueck.de/kunis/paper/KunisDiss.pdf}
}


@ARTICLE{lu07mdf,
  author = {Y. M. Lu and M. N. Do},
  title = {Multidimensional directional filter banks and surfacelets},
  journal = {IEEE Transactions on Image Processing},
  year = {2007},
  volume = {16},
  pages = {918-931},
  number = {4},
  month = {04},
  abstract = {In 1992, Bamberger and Smith proposed the directional
                  filter bank (DFB) for an efficient directional
                  decomposition of 2-D signals. Due to the
                  nonseparable nature of the system, extending the DFB
                  to higher dimensions while still retaining its
                  attractive features is a challenging and previously
                  unsolved problem. We propose a new family of filter
                  banks, named NDFB, that can achieve the directional
                  decomposition of arbitrary N-dimensional (Nges2)
                  signals with a simple and efficient tree-structured
                  construction. In 3-D, the ideal passbands of the
                  proposed NDFB are rectangular-based pyramids
                  radiating out from the origin at different
                  orientations and tiling the entire frequency
                  space. The proposed NDFB achieves perfect
                  reconstruction via an iterated filter bank with a
                  redundancy factor of N in N-D. The angular
                  resolution of the proposed NDFB can be iteratively
                  refined by invoking more levels of decomposition
                  through a simple expansion rule. By combining the
                  NDFB with a new multiscale pyramid, we propose the
                  surfacelet transform, which can be used to
                  efficiently capture and represent surface-like
                  singularities in multidimensional data},
  bdsk-url-1 = {http://dx.doi.org/10.1109/TIP.2007.891785},
  date-added = {2008-05-07 12:19:48 -0700},
  date-modified = {2008-08-14 15:05:31 -0700},
  doi = {10.1109/TIP.2007.891785},
  issn = {1057-7149},
  issue = {4},
  keywords = {surfacelet transform},
  publisher = {IEEE}
}


@BOOK{mallat99awt,
  title = {A wavelet tour of signal processing, second edition},
  publisher = {Academic Press},
  year = {1999},
  author = {S. Mallat},
  month = {09},
  date-added = {2008-05-22 16:32:31 -0700},
  date-modified = {2008-05-22 16:33:57 -0700},
  howpublished = {Hardcover},
  isbn = {012466606X},
  keywords = {wavelet transform}
}


@CONFERENCE{morton98fsr,
  author = {S. A. Morton and C. C. Ober},
  title = {Faster shot-record depth migrations using phase encoding},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {1998},
  volume = {17},
  number = {1},
  pages = {1131-1134},
  publisher = {SEG},
  bdsk-url-1 = {http://link.aip.org/link/?SGA/17/1131/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.1820088},
  date-added = {2008-05-27 16:44:01 -0700},
  date-modified = {2008-05-27 16:45:21 -0700},
  doi = {10.1190/1.1820088},
  issue = {1},
  pdf = {http://link.aip.org/link/?SGA/17/1131/1}
}


@ARTICLE{paige82lsq,
  author = {C. C. Paige and M. A. Saunders},
  title = {{LSQR}: an algorithm for sparse linear equations and sparse least squares},
  journal = {Transactions on Mathematical Software},
  year = {1982},
  volume = {8},
  pages = {43-71},
  number = {1},
  address = {New York, NY, USA},
  bdsk-url-1 = {http://doi.acm.org/10.1145/355984.355989},
  date-added = {2008-05-20 14:00:44 -0700},
  date-modified = {2008-05-20 19:47:37 -0700},
  doi = {http://doi.acm.org/10.1145/355984.355989},
  issn = {0098-3500},
  issue = {1},
  keywords = {LSQR},
  publisher = {ACM}
}


@INCOLLECTION{potts01mst,
  author = {D. Potts and G. Steidl and M. Tasche},
  title = {Fast {Fourier} transforms for nonequispaced data: a tutorial},
  booktitle = {Modern sampling theory: mathematics and applications},
  publisher = {Birkhauser},
  year = {2001},
  editor = {J. J. Benedetto and P. Ferreira},
  chapter = {12},
  pages = {249-274},
  abstract = {In this section, we consider approximate methods for the
                  fast computiation of multivariate discrete Fourier
                  transforms for nonequispaced data (NDFT) in the time
                  domain and in the frequency domain. In particular,
                  we are interested in the approximation error as
                  function of arithmetic complexity of the
                  algorithm. We discuss the robustness of
                  NDFT-algorithms with respect to roundoff errors and
                  apply NDFT-algorithms for the fast computation of
                  Bessel transforms.},
  bdsk-url-1 = {http://www.tu-chemnitz.de/~potts/paper/ndft.pdf},
  date-added = {2008-05-07 18:44:29 -0700},
  date-modified = {2008-08-14 15:28:37 -0700},
  keywords = {NFFT},
  pdf = {http://www.tu-chemnitz.de/~potts/paper/ndft.pdf}
}


@ARTICLE{romero00peo,
  author = {L. A. Romero and D. C. Ghiglia and C. C. Ober and S. A. Morton},
  title = {Phase encoding of shot records in prestack migration},
  journal = {Geophysics},
  year = {2000},
  volume = {65},
  pages = {426-436},
  number = {2},
  abstract = {Frequency-domain shot-record migration can produce
                  higher quality images than Kirchhoff migration but
                  typically at a greater cost. The computing cost of
                  shot-record migration is the product of the number
                  of shots in the survey and the expense of each
                  individual migration. Many attempts to reduce this
                  cost have focused on the speed of the individual
                  migrations, trying to achieve a better trade-off
                  between accuracy and speed. Another approach is to
                  reduce the number of migrations. We investigate the
                  simultaneous migration of shot records using
                  frequency-domain shot-record migration algorithms.
                  The difficulty with this approach is the production
                  of so-called crossterms between unrelated shot and
                  receiver wavefields, which generate unwanted
                  artifacts or noise in the final image. To reduce
                  these artifacts and obtain an image comparable in
                  quality to the single-shot-per-migration result, we
                  have introduced a process called phase encoding,
                  which shifts or disperses these crossterms. The
                  process of phase encoding thus allows one to trade
                  S/N ratio for the speed of migrating the entire
                  survey. Several encoding functions and two
                  application strategies have been tested. The first
                  strategy, combining multiple shots per migration and
                  using each shot only once, reduces computation in
                  direct relation to the number of shots combined. The
                  second strategy, performing multiple migrations of
                  all the shots in the survey, provides a means to
                  reduce the crossterm noise by stacking the resulting
                  images. The additional noise in both strategies may
                  be tolerated if it is no stronger than the inherent
                  seismic noise in the migrated image and if the final
                  image is achieved with less cost. {\copyright}2000
                  Society of Exploration Geophysicists},
  bdsk-url-1 = {http://library.seg.org/doi/abs/10.1190/1.1444737},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.1444737},
  date-added = {2008-05-27 16:42:50 -0700},
  date-modified = {2008-08-14 15:07:08 -0700},
  doi = {10.1190/1.1444737},
  issue = {2},
  pdf = {http://library.seg.org/doi/abs/10.1190/1.1444737},
  publisher = {SEG}
}


@ARTICLE{sacchi98iae,
  author = {M. D. Sacchi and T. J. Ulrych and C. J. Walker},
  title = {Interpolation and extrapolation using a high-resolution discrete {Fourier} transform},
  journal = {IEEE Transactions on Signal Processing},
  year = {1998},
  volume = {46},
  pages = {31-38},
  number = {1},
  abstract = {We present an iterative nonparametric approach to
                  spectral estimation that is particularly suitable
                  for estimation of line spectra. This approach
                  minimizes a cost function derived from Bayes'
                  theorem. The method is suitable for line spectra
                  since a ``long tailed'' distribution is used to
                  model the prior distribution of spectral
                  amplitudes. An important aspect of this method is
                  that since the data themselves are used as
                  constraints, phase information can also be recovered
                  and used to extend the data outside the original
                  window. The objective function is formulated in
                  terms of hyperpa- rameters that control the degree
                  of fit and spectral resolution. Noise rejection can
                  also be achieved by truncating the number of
                  iterations. Spectral resolution and extrapolation
                  length are controlled by a single parameter. When
                  this parameter is large compared with the spectral
                  powers, the algorithm leads to zero extrapolation of
                  the data, and the estimated Fourier transform yields
                  the periodogram. When the data are sampled at a
                  constant rate, the algorithm uses one Levinson
                  recursion per iteration. For irregular sampling
                  (unevenly sampled and/or gapped data), the algorithm
                  uses one Cholesky decomposition per iteration. The
                  performance of the algorithm is illustrated with
                  three different problems that frequently arise in
                  geophysical data processing: 1) harmonic retrieval
                  from a time series contaminated with noise; 2)
                  linear event detection from a finite aperture array
                  of receivers [which, in fact, is an extension of
                  1)], 3) interpolation/extrapolation of gapped data.
                  The performance of the algorithm as a spectral
                  estimator is tested with the Kay and Marple data
                  set. It is shown that the achieved resolution is
                  comparable with parametric methods but with more
                  accurate representation of the relative power in the
                  spectral lines.},
  bdsk-url-1 = {http://saig.physics.ualberta.ca/s/sites/default/files/upload/articles/Sacchi_Ulrych_Walker_IEEE_98.pdf},
  date-added = {2008-05-06 19:18:50 -0700},
  date-modified = {2008-08-14 15:08:37 -0700},
  doi = {10.1109/78.651165},
  issue = {1},
  keywords = {Fourier transform, reconstruction},
  pdf = {http://saig.physics.ualberta.ca/s/sites/default/files/upload/articles/Sacchi_Ulrych_Walker_IEEE_98.pdf},
  publisher = {IEEE}
}


@PHDTHESIS{schonewille00phd,
  author = {M. A. Schonewille},
  title = {Fourier reconstruction of irregularly sampled seismic data},
  school = {Delft University of Technology},
  year = {2000},
  address = {Delft, The Netherlands},
  month = {11},
  date-added = {2008-05-06 19:03:35 -0700},
  date-modified = {2008-05-09 14:43:57 -0700},
  keywords = {Fourier transform, reconstruction},
  rating = {0},
  read = {Yes}
}


@ARTICLE{smith98ahs,
  author = {H. Smith},
  title = {A {Hardy} space for {Fourier} integral operators},
  journal = {Journal of Geometric Analysis},
  year = {1998},
  volume = {8},
  pages = {629-653},
  number = {4},
  date-added = {2008-05-07 12:25:03 -0700},
  date-modified = {2008-08-14 15:09:47 -0700},
  issue = {4},
  keywords = {FIO}
}


@BOOK{snieder93giu,
  author = {R. Snieder},
  title = {Global inversions using normal mode and long-period surface waves},
  publisher = {Chapman and Hall},
  year = {1993},
  date-added = {2008-05-20 17:16:42 -0700},
  date-modified = {2008-05-20 17:19:44 -0700},
  keywords = {sampling}
}


@ARTICLE{spitz91sti,
  author = {S. Spitz},
  title = {Seismic trace interpolation in the {FX} domain},
  journal = {Geophysics},
  year = {1991},
  volume = {56},
  pages = {785-794},
  number = {6},
  abstract = {Interpolation of seismic traces is an effective means of
                  improving migration when the data set exhibits
                  spatial aliasing. A major difficulty of standard
                  interpolation methods is that they depend on the
                  degree of reliability with which the various
                  geological events can be separated. In this respect,
                  a multichannel interpolation method is described
                  which requires neither a priori knowledge of the
                  directions of lateral coherence of the events, nor
                  estimation of these directions. The method is based
                  on the fact that linear events present in a section
                  made of equally spaced traces may be interpolated
                  exactly, regardless of the original spatial
                  interval, without any attempt to determine their
                  true dips. The predictability of linear events in
                  the f-x domain allows the missing traces to be
                  expressed as the output of a linear system, the
                  input of which consists of the recorded traces. The
                  interpolation operator is obtained by solving a set
                  of linear equations whose coefficients depend only
                  on the spectrum of the spatial prediction filter
                  defined by the recorded traces. Synthetic examples
                  show that this method is insensitive to random noise
                  and that it correctly handles curvatures and lateral
                  amplitude variations. Assessment of the method with
                  a real data set shows that the interpolation yields
                  an improved migrated section. {\copyright}1991
                  Society of Exploration Geophysicists},
  bdsk-url-1 = {http://dx.doi.org/10.1190/1.1443096},
  date-added = {2008-05-06 19:29:12 -0700},
  date-modified = {2008-08-14 15:18:16 -0700},
  doi = {10.1190/1.1443096},
  issue = {6},
  keywords = {PEF},
  publisher = {SEG}
}


@ARTICLE{starck02tct,
  author = {J.-L. Starck and E. J. Cand\`es and D. L. Donoho},
  title = {The curvelet transform for image denoising},
  journal = {IEEE Transactions on Image Processing},
  year = {2002},
  volume = {11},
  pages = {670-684},
  number = {6},
  month = {06},
  abstract = {We describe approximate digital implementations of two
                  new mathematical transforms, namely, the ridgelet
                  transform and the curvelet transform. Our
                  implementations offer exact reconstruction,
                  stability against perturbations, ease of
                  implementation, and low computational complexity. A
                  central tool is Fourier-domain computation of an
                  approximate digital Radon transform. We introduce a
                  very simple interpolation in the Fourier space which
                  takes Cartesian samples and yields samples on a
                  rectopolar grid, which is a pseudo-polar sampling
                  set based on a concentric squares geometry. Despite
                  the crudeness of our interpolation, the visual
                  performance is surprisingly good. Our ridgelet
                  transform applies to the Radon transform a special
                  overcomplete wavelet pyramid whose wavelets have
                  compact support in the frequency domain. Our
                  curvelet transform uses our ridgelet transform as a
                  component step, and implements curvelet subbands
                  using a filter bank of a&grave; trous wavelet
                  filters. Our philosophy throughout is that
                  transforms should be overcomplete, rather than
                  critically sampled. We apply these digital
                  transforms to the denoising of some standard images
                  embedded in white noise. In the tests reported here,
                  simple thresholding of the curvelet coefficients is
                  very competitive with "state of the art" techniques
                  based on wavelets, including thresholding of
                  decimated or undecimated wavelet transforms and also
                  including tree-based Bayesian posterior mean
                  methods. Moreover, the curvelet reconstructions
                  exhibit higher perceptual quality than wavelet-based
                  reconstructions, offering visually sharper images
                  and, in particular, higher quality recovery of edges
                  and of faint linear and curvilinear
                  features. Existing theory for curvelet and ridgelet
                  transforms suggests that these new approaches can
                  outperform wavelet methods in certain image
                  reconstruction problems. The empirical results
                  reported here are in encouraging agreement},
  bdsk-url-1 = {http://dx.doi.org/10.1109/TIP.2002.1014998},
  bdsk-url-2 = {http://ieeexplore.ieee.org/iel5/83/21845/01014998.pdf},
  date-added = {2008-05-26 17:38:14 -0700},
  date-modified = {2008-08-14 15:19:16 -0700},
  doi = {10.1109/TIP.2002.1014998},
  issn = {1057-7149},
  issue = {6},
  keywords = {curvelet transform},
  publisher = {IEEE}
}


@ARTICLE{symes07rtm,
  author = {W. W. Symes},
  title = {Reverse time migration with optimal checkpointing},
  journal = {Geophysics},
  year = {2007},
  volume = {72},
  pages = {SM213-SM221},
  number = {5},
  abstract = {Reverse time migration (RTM) requires that fields
                  computed in forward time be accessed in reverse
                  order. Such out-of-order access, to recursively
                  computed fields, requires that some part of the
                  recursion history be stored (checkpointed), with the
                  remainder computed by repeating parts of the forward
                  computation. Optimal checkpointing algorithms choose
                  checkpoints in such a way that the total storage is
                  minimized for a prescribed level of excess
                  computation, or vice versa. Optimal checkpointing
                  dramatically reduces the storage required by RTM,
                  compared to that needed for nonoptimal
                  implementations, at the price of a small increase in
                  computation. This paper describes optimal
                  checkpointing in a form which applies both to RTM
                  and other applications of the adjoint state method,
                  such as construction of velocity updates from
                  prestack wave equation migration. {\copyright}2007
                  Society of Exploration Geophysicists},
  bdsk-url-1 = {http://link.aip.org/link/?GPY/72/SM213/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.2742686},
  date-added = {2008-05-08 14:42:11 -0700},
  date-modified = {2008-08-14 15:19:43 -0700},
  doi = {10.1190/1.2742686},
  issue = {5},
  keywords = {RTM, imaging},
  pdf = {http://link.aip.org/link/?GPY/72/SM213/1},
  publisher = {SEG}
}


@ARTICLE{thorson85vsa,
  author = {J. R. Thorson and J. F. Claerbout},
  title = {Velocity-stack and slant-stack stochastic inversion},
  journal = {Geophysics},
  year = {1985},
  volume = {50},
  pages = {2727-2741},
  number = {12},
  abstract = {Normal moveout (NMO) and stacking, an important step in
                  analysis of reflection seismic data, involves
                  summation of seismic data over paths represented by
                  a family of hyperbolic curves. This summation
                  process is a linear transformation and maps the data
                  into what might be called a velocity space: a
                  two-dimensional set of points indexed by time and
                  velocity. Examination of data in velocity space is
                  used for analysis of subsurface velocities and
                  filtering of undesired coherent events (e.g.,
                  multiples), but the filtering step is useful only if
                  an approximate inverse to the NMO and stack
                  operation is available. One way to effect velocity
                  filtering is to use the operator LT (defined as NMO
                  and stacking) and its adjoint L as a transform pair,
                  but this leads to unacceptable filtered
                  output. Designing a better estimated inverse to L
                  than LT is a generalization of the inversion problem
                  of computerized tomography: deconvolving out the
                  point-spread function after back projection. The
                  inversion process is complicated by missing data,
                  because surface seismic data are recorded only
                  within a finite spatial aperture on the Earth's
                  surface.  Our approach to solving the problem of an
                  ill-conditioned or nonunique inverse L--1, brought
                  on by missing data, is to design a stochastic
                  inverse to L. Starting from a maximum a posteriori
                  (MAP) estimator, a system of equations can be set up
                  in which a priori information is incorporated into a
                  sparseness measure: the output of the stochastic
                  inverse is forced to be locally focused, in order to
                  obtain the best possible resolution in velocity
                  space. The size of the resulting nonlinear system of
                  equations is immense, but using a few iterations
                  with a gradient descent algorithm is adequate to
                  obtain a reasonable solution. This theory may also
                  be applied to other large, sparse linear
                  operators. The stochastic inverse of the slant-stack
                  operator (a particular form of the Radon transform),
                  can be developed in a parallel manner, and will
                  yield an accurate slant-stack inverse pair.
                  {\copyright}1985 Society of Exploration
                  Geophysicists},
  bdsk-url-1 = {http://dx.doi.org/10.1190/1.1441893},
  date-added = {2008-05-06 19:06:15 -0700},
  date-modified = {2008-08-14 15:20:19 -0700},
  doi = {10.1190/1.1441893},
  issue = {12},
  keywords = {Radon transform},
  publisher = {SEG}
}


@ARTICLE{trad03lvo,
  author = {D. Trad and T. J. Ulrych and M. D. Sacchi},
  title = {Latest views of the sparse {Radon} transform},
  journal = {Geophysics},
  year = {2003},
  volume = {68},
  pages = {386-399},
  number = {1},
  abstract = {The Radon transform (RT) suffers from the typical
                  problems of loss of resolution and aliasing that
                  arise as a consequence of incomplete information,
                  including limited aperture and
                  discretization. Sparseness in the Radon domain is a
                  valid and useful criterion for supplying this
                  missing information, equivalent somehow to assuming
                  smooth amplitude variation in the transition between
                  known and unknown (missing) data. Applying this
                  constraint while honoring the data can become a
                  serious challenge for routine seismic processing
                  because of the very limited processing time
                  available, in general, per common midpoint. To
                  develop methods that are robust, easy to use and
                  flexible to adapt to different problems we have to
                  pay attention to a variety of algorithms, operator
                  design, and estimation of the hyperparameters that
                  are responsible for the regularization of the
                  solution.In this paper, we discuss fast
                  implementations for several varieties of RT in the
                  time and frequency domains. An iterative conjugate
                  gradient algorithm with fast Fourier transform
                  multiplication is used in all cases. To preserve the
                  important property of iterative subspace methods of
                  regularizing the solution by the number of
                  iterations, the model weights are incorporated into
                  the operators. This turns out to be of particular
                  importance, and it can be understood in terms of the
                  singular vectors of the weighted transform. The
                  iterative algorithm is stopped according to a
                  general cross validation criterion for subspaces. We
                  apply this idea to several known implementations and
                  compare results in order to better understand
                  differences between, and merits of, these
                  algorithms. {\copyright}2003 Society of Exploration
                  Geophysicists},
  bdsk-url-1 = {http://link.aip.org/link/?GPY/68/386/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.1543224},
  date-added = {2008-05-07 19:03:39 -0700},
  date-modified = {2008-08-14 15:20:56 -0700},
  doi = {10.1190/1.1543224},
  issue = {1},
  keywords = {Radon transform},
  pdf = {http://link.aip.org/link/?GPY/68/386/1},
  publisher = {SEG}
}


@ARTICLE{verschuur97eom,
  author = {D. J. Verschuur and A. J. Berkhout},
  title = {Estimation of multiple scattering by iterative inversion, {Part} {II}: {Practical} aspects and examples},
  journal = {Geophysics},
  year = {1997},
  volume = {62},
  pages = {1596-1611},
  number = {5},
  abstract = {A surface-related multiple-elimination method can be
                  formulated as an iterative procedure: the output of
                  one iteration step is used as input for the next
                  iteration step (part I of this paper). In this paper
                  (part II) it is shown that the procedure can be made
                  very efficient if a good initial estimate of the
                  multiple-free data set can be provided in the first
                  iteration, and in many situations, the Radon-based
                  multiple-elimination method may provide such an
                  estimate. It is also shown that for each iteration,
                  the inverse source wavelet can be accurately
                  estimated by a linear (least-squares) inversion
                  process. Optionally, source and detector variations
                  and directivity effects can be included, although
                  the examples are given without these options. The
                  iterative multiple elimination process, together
                  with the source wavelet estimation, are illustrated
                  with numerical experiments as well as with field
                  data examples. The results show that the
                  surface-related multiple-elimination process is very
                  effective in time gates where the moveout properties
                  of primaries and multiples are very similar
                  (generally deep data), as well as for situations
                  with a complex multiple-generating system.
                  {\copyright}1997 Society of Exploration
                  Geophysicists},
  bdsk-url-1 = {http://link.aip.org/link/?GPY/62/1596/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.1444262},
  date-added = {2008-05-07 18:40:45 -0700},
  date-modified = {2008-08-14 15:21:18 -0700},
  doi = {10.1190/1.1444262},
  issue = {5},
  keywords = {SRME},
  pdf = {http://link.aip.org/link/?GPY/62/1596/1},
  publisher = {SEG}
}


@ARTICLE{xu05aft,
  author = {S. Xu and Y. Zhang and D. Pham and G. Lambar\'{e}},
  title = {Antileakage {Fourier} transform for seismic data regularization},
  journal = {Geophysics},
  year = {2005},
  volume = {70},
  pages = {V87-V95},
  number = {4},
  abstract = {Seismic data regularization, which spatially transforms
                  irregularly sampled acquired data to regularly
                  sampled data, is a long-standing problem in seismic
                  data processing. Data regularization can be
                  implemented using Fourier theory by using a method
                  that estimates the spatial frequency content on an
                  irregularly sampled grid. The data can then be
                  reconstructed on any desired grid. Difficulties
                  arise from the nonorthogonality of the global
                  Fourier basis functions on an irregular grid, which
                  results in the problem of "spectral leakage": energy
                  from one Fourier coefficient leaks onto others. We
                  investigate the nonorthogonality of the Fourier
                  basis on an irregularly sampled grid and propose a
                  technique called "antileakage Fourier transform" to
                  overcome the spectral leakage. In the antileakage
                  Fourier transform, we first solve for the most
                  energetic Fourier coefficient, assuming that it
                  causes the most severe leakage. To attenuate all
                  aliases and the leakage of this component onto other
                  Fourier coefficients, the data component
                  corresponding to this most energetic Fourier
                  coefficient is subtracted from the original input on
                  the irregular grid. We then use this new input to
                  solve for the next Fourier coefficient, repeating
                  the procedure until all Fourier coefficients are
                  estimated. This procedure is equivalent to
                  "reorthogonalizing" the global Fourier basis on an
                  irregularly sampled grid. We demonstrate the
                  robustness and effectiveness of this technique with
                  successful applications to both synthetic and real
                  data examples. {\copyright}2005 Society of
                  Exploration Geophysicists},
  bdsk-url-1 = {http://link.aip.org/link/?GPY/70/V87/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.1993713},
  date-added = {2008-05-09 17:43:47 -0700},
  date-modified = {2008-08-14 15:21:45 -0700},
  doi = {10.1190/1.1993713},
  issue = {4},
  keywords = {Fourier transform, reconstruction},
  pdf = {http://link.aip.org/link/?GPY/70/V87/1},
  publisher = {SEG}
}


@ARTICLE{ying053dd,
  author = {L. Ying and L. Demanet and E. J. Cand\`es},
  title = {3-{D} discrete curvelet transform},
  journal = {Proceedings SPIE wavelets XI, San Diego},
  year = {2005},
  volume = {5914},
  pages = {344-354},
  month = {01},
  abstract = {In this paper, we present the first 3D discrete curvelet
                  transform. This transform is an extension to the 2D
                  transform described in Candes et al..1 The resulting
                  curvelet frame preserves the important properties,
                  such as parabolic scaling, tightness and sparse
                  representation for singularities of codimension
                  one. We describe three different implementations:
                  in-core, out-of-core and MPI-based parallel
                  implementations. Numerical results verify the
                  desired properties of the 3D curvelets and
                  demonstrate the efficiency of our implementations.},
  bdsk-url-1 = {http://dx.doi.org/10.1117/12.616205},
  date-added = {2008-05-07 14:14:59 -0700},
  date-modified = {2008-08-14 15:21:59 -0700},
  doi = {10.1117/12.616205},
  keywords = {curvelet transform}
}


@PHDTHESIS{zwartjes05phd,
  author = {P. M. Zwartjes},
  title = {Fourier reconstruction with sparse inversion},
  school = {Delft University of Technology},
  year = {2005},
  address = {Delft, The Netherlands},
  month = {12},
  date-added = {2008-05-06 18:58:35 -0700},
  date-modified = {2008-05-09 14:44:04 -0700},
  keywords = {Fourier transform, reconstruction},
  rating = {0},
  read = {Yes}
}

