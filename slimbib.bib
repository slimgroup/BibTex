%-----------------------------------2012------------------------



%----------------------------------------2011-------------------------------


@conference{Herrmann11SPIEmspgn,
author = {Felix J. Herrmann and Xiang Li and Aleksandr Y. Aravkin and Tristan van Leeuwen},
title = {A modified, sparsity promoting, {G}auss-{N}ewton algorithm for seismic waveform inversion},
year = {2011},
journal = {Proc. SPIE},
volume = {2011},
number = {81380V},
notes = {TR-2011-05},
month= {08/2011},
issn = {1},
url={http://slim.eos.ubc.ca/Publications/public/Journals/SPIEreport.pdf},
url2 ={http://dx.doi.org/10.1117/12.893861},
doi = {doi:10.1117/12.893861},
abstract={ Images obtained from seismic data are used by the oil and gas industry for geophysical exploration. Cutting-edge methods for transforming the data into interpretable images are moving away from linear approximations and high-frequency asymptotics towards Full Waveform Inversion (FWI), a nonlinear data-fitting procedure based on full data modeling using the wave-equation. The size of the problem, the nonlinearity of the for- ward model, and ill-posedness of the formulation all contribute to a pressing need for fast algorithms and novel regularization techniques to speed up and improve inversion results. In this paper, we design a modified Gauss-Newton algorithm to solve the PDE- constrained optimization problem using ideas from stochastic optimization and com- pressive sensing. More specifically, we replace the Gauss-Newton subproblems by ran- domly subsampled, -$\ell_1$ regularized subproblems. This allows us us significantly reduce the computational cost of calculating the updates and exploit the compressibility of wavefields in Curvelets. We explain the relationships and connections between the new method and stochastic optimization and compressive sensing (CS), and demonstrate the efficacy of the new method on a large-scale synthetic seismic example. }
}



@CONFERENCE{aravkin11SIAMfwi,
  author = {Aleksandr Y. Aravkin and Felix J. Herrmann and Tristan van Leeuwen
	and James V. Burke and Xiang Li},
  title = {Full Waveform Inversion with Compressive Updates},
  year = {2011},
  organization = {SIAM CS\&E 2011},
  publisher = {SIAM CS\&E 2011},
  abstract = {Full-waveform inversion relies on large multi-experiment data volumes.
	While improvements in acquisition and inversion have been extremely
	successful, the current push for higher quality models reveals fundamental
	shortcomings handling increasing problem sizes numerically. To address
	this fundamental issue, we propose a randomized dimensionality-reduction
	strategy motivated by recent developments in stochastic optimization
	and compressive sensing. In this formulation conventional Gauss-Newton
	iterations are replaced by dimensionality-reduced sparse recovery
	problems with source encodings.},
  presentation = {http://slim/Publications/Public/Presentations/2011/Aravkin2.28.2011.pdf}
}

@CONFERENCE{aravkin11ICIAMspfa,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and James Burke and
	Felix J. Herrmann},
  title = {Sparsity promoting formulations and algorithms for FWI. Presented
	at AMP Medical and Seismic Imaging, 2011, Vancouver BC.},
  year = {2011},
  organization = {ICIAM 2011},
  abstract = {Full Waveform Inversion (FWI) is a computational procedure to extract
	medium parameters from seismic data. FWI is typically formulated
	as a nonlinear least squares optimization problem, and various regularization
	techniques are used to guide the optimization because the problem
	is ill-posed. We propose a novel sparse regularization which exploits
	the ability of curvelets to efficiently represent geophysical images.
	We then formulate a corresponding sparsity promoting constrained
	optimization problem, which we solve using an open source algorithm.
	The techniques are applicable to any inverse problem where sparsity
	modeling is appropriate.
	
	We demonstrate the efficacy of the formulation on a toy example (stylized
	cross-well experiment) and on a realistic Seismic example (partial
	Marmoussi model). We also discuss the tradeoff between model fit
	and sparsity promotion, with a view to extend existing techniques
	for linear inverse problems to the case where the forward model is
	nonlinear. },
month={07/2011},
  date-added = {2011-07-15},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/aravkin2011AMP.pdf},
  url = {http://slim/Publications/Public/Presentations/2011/aravkin2011AMP.pdf}
}

@CONFERENCE{aravkin11ICIAMrfwis,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Robust FWI using Student's t-distribution. Presented at Waves 2011,
	Vancouver BC.Presented at Waves 2011, Vancouver BC.},
  year = {2011},
  organization = {ICIAM 2011},
  abstract = {Iterative inversion algorithms require repeated simulation of 3D time-dependent
	acoustic, elastic, or electromagnetic wave fields, extending hundreds
	of wavelengths and hundreds of periods. Also, seismic data is rich
	in information at every representable scale. Thus simulation-driven
	optimization approaches to inversion impose great demands on simulator
	efficiency and accuracy. While computer hardware advances have been
	of critical importance in bringing inversion closer to practical
	application, algorithmic advances in simulator methodology have been
	equally important. Speakers in this two-part session will address
	a variety of numerical issues arising in the wave simulation, and
	in its application to inversion. },
  date-added = {2011-07-20},
month={07/2011},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/aravkin2011WAVES.pdf},
  url = {http://slim/Publications/Public/Presentations/2011/aravkin2011WAVES.pdf}

}

@CONFERENCE{aravkin11EAGEnspf,
  author = {Aleksandr Y. Aravkin and James V. Burke and Felix J. Herrmann and
	Tristan van Leeuwen},
  title = {A Nonlinear Sparsity Promoting Formulation and Algorithm for Full
	Waveform Inversion},
  year = {2011},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {Full Waveform Inversion (FWI) is a computational procedure to extract
	medium parameters from seismic data. FWI is typically formulated
	as a nonlinear least squares optimization problem, and various regularization
	techniques are used to guide the optimization because the problem
	is illposed. In this paper, we propose a novel sparse regularization
	which exploits the ability of curvelets to efficiently represent
	geophysical images. We then formulate a corresponding sparsity promoting
	constrained optimization problem, which we call Nonlinear Basis Pursuit
	Denoise (NBPDN) and present an algorithm to solve this problem to
	recover medium parameters. The utility of the NBPDN formulation and
	efficacy of the algorithm are demonstrated on a stylized cross-well
	exper- iment, where a sparse velocity perturbation is recovered with
	higher quality than the standard FWI formulation (solved with LBFGS).
	The NBPDN formulation and algorithm can recover the sparse perturbation
	even when the data volume is compressed to 5 percent of the original
	size using random superposition.},
  keywords = {EAGE},
month= {01/2011},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage11/aravkin2011eage.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/aravkin11EAGEnspf/aravkin11EAGEnspf.pdf }
}

@CONFERENCE{aravkin11SEGrobust,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Robust full-waveform inversion using the Student's t-distribution},
  year = {2011},
  organization = {SEG},
  publisher = {SEG},
  abstract = {Full-waveform inversion (FWI) is a computational procedure to extract
	medium parameters from seismic data. Robust meth- ods for FWI are
	needed to overcome sensitivity to noise and in cases where modeling
	is particularly poor or far from the real data generating process.
	We survey previous robust methods from a statistical perspective,
	and use this perspective to derive a new robust method by assuming
	the random errors in our model arise from the Student's t-distribution.
	We show that in contrast to previous robust methods, the new method
	progres- sively down-weighs large outliers, effectively ignoring
	them once they are large enough. This suggests that the new method
	is more robust and suitable for situations with very poor data quality
	or modeling. Experiments show that the new method recovers as well
	or better than previous robust methods, and can recover models with
	quality comparable to standard meth- ods on noise-free data when
	some of the data is completely corrupted, and even when a marine
	acquisition mask is entirely ignored in the modeling. The ability
	to ignore a marine acqui- sition mask via robust FWI methods offers
	an opportunity for stochastic optimization methods in marine acquisition.},
  keywords = {SEG},
month={04/2011},
  timestamp = {2011-04-06 15:00:00 -0700},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/aravkin11SEGrobust/aravkin11SEGrobust.pdf }

}

@CONFERENCE{herrmann11ICIAMconvexcompfwi,
  author = {Felix J. Herrmann and Aleksandr Y. Aravkin and Tristan van Leeuwen
	and Xiang Li},
  title = {FWI with sparse recovery: a convex-composite approach},
  year = {2011},
  organization = {ICIAM 2011},
  abstract = {Iterative inversion algorithms require repeated simulation of 3D time-dependent
	acoustic, elastic, or electromagnetic wave fields, extending hundreds
	of wavelengths and hundreds of periods. Also, seismic data is rich
	in information at every representable scale. Thus simulation-driven
	optimization approaches to inversion impose great demands on simulator
	efficiency and accuracy. While computer hardware advances have been
	of critical importance in bringing inversion closer to practical
	application, algorithmic advances in simulator methodology have been
	equally important. Speakers in this two-part session will address
	a variety of numerical issues arising in the wave simulation, and
	in its application to inversion. },
  date-added = {2011-07-20},
month={07/2011},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/herrmann2011ICIAM.pdf},
  url = {http://slim/Publications/Public/Presentations/2011/herrmann2011ICIAM.pdf}
}

@CONFERENCE{Herrmann11fwi,
  author = {Felix J. Herrmann and Aleksandr Y. Aravkin and Xiang Li and Tristan
	van Leeuwen},
  title = {Full Waveform Inversion with Compressive Updates},
  year = {2011},
  organization = {Sparse and Low Rank Approximation 2011},
  publisher = {Sparse and Low Rank Approximation 2011},
  abstract = {Full-waveform inversion relies on large multi-experiment data volumes.
	While improvements in acquisition and inversion have been extremely
	successful, the current push for higher quality models reveals fundamental
	shortcomings handling increasing problem sizes numerically. To address
	this fundamental issue, we propose a randomized dimensionality-reduction
	strategy motivated by recent developments in stochastic optimization
	and compressive sensing. In this formulation conventional Gauss-Newton
	iterations are replaced by dimensionality-reduced sparse recovery
	problems with source encodings.},

  presentation = {http://slim/Publications/Public/Presentations/2011/Herrmann2011css.pdf}

}

@CONFERENCE{herrmann11EAGEefmsp,
  author = {Felix J. Herrmann and Xiang Li},
  title = {Efficient least-squares migration with sparsity promotion},
  year = {2011},
  organization = {EAGE},
  publisher = {EAGE Technical Program Expanded Abstracts},
  abstract = {Seismic imaging relies on the collection of multi-experimental data
	volumes in combination with a sophisticated back-end to create high-fidelity
	inversion results. While significant improve- ments have been made
	in linearized inversion, the current trend of incessantly pushing
	for higher quality models in increasingly complicated regions reveals
	fundamental shortcomings in handling increasing problem sizes numerically.
	The so-called “curse of dimensionality” is the main culprit because
	it leads to an exponential growth in the number of sources and the
	corresponding number of wavefield simulations required by ‘wave-equation’
	migration. We address this issue by reducing the number of sources
	by a randomized dimensionality reduction technique that combines
	recent developments in stochastic optimization and compressive sensing.
	As a result, we replace the cur- rent formulations of imaging that
	rely on all data by a sequence of smaller imaging problems that use
	the output of the previous inversion as input for the next. Empirically,
	we find speedups of at least one order-of-magnitude when each reduced
	experiment is considered theoretically as a separate compressive-sensing
	experiment.},
  keywords = {EAGE},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage11/herrmann2011eage.pdf},
month= {01/2011},
  timestamp = {2011-01-14},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/herrmann11EAGEefmsp/herrmann11EAGEefmsp.pdf }
}


@CONFERENCE{herrmann11SLIMsummer1,
  author = {Felix J. Herrmann},
  title = {Gene Golub SIAM Summer School July 4 - 15, 2011},
  year = {2011},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/HerrmannLecture1.pdf},
  timestamp = {2011.08.05},
month={08/2011}
}

@CONFERENCE{herrmann11SLIMsummer2,
  author = {Felix J. Herrmann},
  title = {Lecture 2. Gene Golub SIAM Summer School July 4 - 15, 2011},
  year = {2011},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/HerrmannLecture2.pdf},
  timestamp = {2011.08.05},
month= {08/2011}
}


@CONFERENCE{Jumah11SEGdrepsi,
  author = {Bander Jumah and Felix J. Herrmann},
  title = {Dimensionality-reduced estimation of primaries by sparse inversion},
  year = {2011},
  organization = {SEG},
  publisher = {SEG Technical Program Expanded Abstracts},
  abstract = {Data-driven methods—such as the estimation of primaries by sparse
	inversion—suffer from the ’curse of dimensionality’, which leads
	to disproportional growth in computational and storage demands when
	moving to realistic 3-D field data. To re- move this fundamental
	impediment, we propose a dimensional- ity reduction technique where
	the ’data matrix’ is approximated adaptively by a randomized low-rank
	approximation. Com- pared to conventional methods, our approach has
	the advantage that the cost of the low-rank approximation is reduced
	signif- icantly, which may lead to considerable reductions in storage
	and computational costs of the sparse inversion. Application of the
	proposed formalism to synthetic data shows that significant improvements
	are achievable at low computational overhead required to compute
	the low-rank approximations.},
  date-added = {2011-04-06 15:00:00 -0700},
month= {04/2011},
  keywords = {SEG},
url= { https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/Jumah11SEGdrepsi/Jumah11SEGdrepsi.pdf }

}


@CONFERENCE{li11EAGEfwirr,
  author = {Xiang Li and Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix
	J. Herrmann},
  title = {Full-waveform inversion with randomized L1 recovery for the model
	updates},
  year = {2011},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {Full-waveform inversion (FWI) is a data fitting procedure that relies
	on the collection of seis- mic data volumes and sophisticated computing
	to create high-resolution results. With the advent of FWI, the improvements
	in acquisition and inversion have been substantial, but these improvements
	come at a high cost because FWI involves extremely large multi-experiment
	data volumes. The main obstacle is the ‘curse of dimensionality’
	exemplified by Nyquist’s sampling criterion, which puts a disproportionate
	strain on current acquisition and processing systems as the size
	and desired resolution increases. In this paper, we address the ‘curse
	of dimensionality’ by randomized dimen- sionality reduction of the
	FWI problem adapted from the field of CS. We invert for model updates
	by replacing the Gauss-Newton linearized subproblem for subsampled
	FWI with a sparsity promoting formulation, and solve this formulation
	using the SPGl1 algorithm. We speed up the algorithm and avoid overfitting
	the data by solving for the linearized updates only approximately.
	Our approach is successful because it reduces the size of seismic
	data volumes without loss of information. With this reduction, we
	can compute a Newton-like update with the reduced data volume at
	the cost of roughly one gradient update for the fully sampled wavefield.},
  keywords = {EAGE},
month= {01/2011},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage11/xiangli2011eage.pdf},
url={https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/li11EAGEfwirr/li11EAGEfwirr.pdf }
}


@CONFERENCE{li11SBGFmgnsu,
  author = {Xiang Li and Felix J. Herrmann and Tristan van Leeuwen and Aleksandr
	Y. Aravkin},
  title = {Modified Gauss-Newton with Sparse Updates},
  year = {2011},
  organization = {SBGF},
  publisher = {Submitted to SBGF},
  abstract = {Full-waveform inversion (FWI) is a data fitting procedure that relies
	on the collection of seismic data volumes and sophisticated computing
	to create high-resolution models.With the advent of FWI, the improvements
	in acquisition and inversion have been substantial, but these improvements
	come at a high cost because FWI involves extremely large multi-experiment
	data volumes. The main obstacle is the {\textquoteleft}curse of dimensionality{\textquoteright}
	exemplified by Nyquist{\textquoteright}s sampling criterion, which
	puts a disproportionate strain on current acquisition and processing
	systems as the size and desired resolution increases. In this paper,
	we address the {\textquoteleft}curse of dimensionality{\textquoteright}
	by using randomized dimensionality reduction of the FWI problem,
	coupled with a modified Gauss-Newton (GN) method designed to promote
	curvelet-domain sparsity of model updates. We solve for these updates
	using the spectral projected gradient method, implemented in the
	SPGÔøø1 software package. Our approach is successful because it reduces
	the size of seismic data volumes without loss of information. With
	this reduction, we can compute Gauss-Newton updates with the reduced
	data volume at the cost of roughly one gradient update for the fully
	sampled wavefield},
  keywords = {SBGF},
url= { https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/li11SBGFmgnsu/li11SBGFmgnsu.pdf }

}

@CONFERENCE{lin11EAGEepsic,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Estimating primaries by sparse inversion in a curvelet-like representation
	domain},
  year = {2011},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {We present an uplift in the fidelity and wavefront continuity of results
	obtained from the Estimation of Primaries by Sparse Inversion (EPSI)
	program by reconstructing the primary events in a hybrid wavelet-curvelet
	representation domain. EPSI is a multiple removal technique that
	belongs to the class of wavefield inversion methods, as an alternative
	to the traditional adaptive-subtraction process. The main assumption
	is that the correct primary events should be as sparsely-populated
	in time as possible. A convex reformulation of the original EPSI
	algorithm allows its convergence property to be preserved even when
	the solution wavefield is not formed in the physical domain. Since
	wavefronts and edge-type singularities are sparsely represented in
	the curvelet domain, sparse solutions formed in this domain will
	exhibit vastly improved continuity when compared to those formed
	in the physical domain, especially for the low-energy events at later
	arrival times. Further- more, a wavelet-type representation domain
	will preserve sparsity in the reflected events even if they originate
	from non-zero-order discontinuities in the subsurface, providing
	an additional level of robustness. This method does not require any
	changes in the underlying computational algorithm and does not explicitly
	impose continuity constraints on each update.},
  keywords = {EAGE},
month= {01/2011},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage11/lin2011eage.pdf},
url={https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/lin11EAGEepsic/lin11EAGEepsic.pdf }
}

@CONFERENCE{lin11SEGrssde,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Robust source signature deconvolution and the estimation of primaries
	by sparse inversion},
  booktitle = {SEG},
  year = {2011},
abstract= {The past few years had seen some concentrated interest on a particular wavefield-inversion approach to the popular SRME multiple removal technique called Estimation of Primaries by Sparse Inversion (EPSI). EPSI promises greatly improved tol- erance to noise, missing data, edge effect, and other physi- cal phenomenon generally not described by the SRME relation (van Groenestijn and Verschuur, 2009a,b). It is based on the premise that it is possible to stably invert for both the primary impulse response and the source signature despite beforehand having no (or very limited) explicit knowledge of latter. The key to successful applications of EPSI, as shown in very recent works (Savels et al., 2010), is a robust way to reconstruct very sparse primary impulse response events as part of the inver- sion process. Based on the various successful demonstrations in literature, there is a very strong sense that EPSI will also play an important role in future developments of source sig- nature deconvolution and the general recovering of wavefield spectrum. },
  organization = {Dept. of Earth and Ocean Sciences, University of British Columbia},
  publisher = {Dept. of Earth and Ocean Sciences, University of British Columbia},
  keywords = {deconvolution, SEG, sparse inversion},
url={ https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/lin11SEGrssde/lin11SEGrssde.pdf },
month= {04/2011}
}




@CONFERENCE{Mansour11SBGFcspsma,
  author = {Hassan Mansour and Haneet Wason and Tim T.Y. Lin and Felix J. Herrmann},
  title = {A compressive sensing perspective on simultaneous marine acquisition},
  year = {2011},
  organization = {SBGF},
  publisher = {SBGF},
  abstract = {The high cost of acquiring seismic data in Marine environments compels
	the adoption of simultaneous- source acquisition - an emerging technology
	that is stimulating both geophysical research and commercial efforts.
	In this paper, we discuss the properties of randomized simultaneous
	acquisition matrices and demonstrate that sparsity-promoting recovery
	improves the quality of the reconstructed seismic data volumes. Simultaneous
	Marine acquisition calls for the development of a new set of design
	principles and post-processing tools. Leveraging established findings
	from the field of compressed sensing, the recovery from simultaneous
	sources depends on a sparsifying transform that compresses seismic
	data, is fast, and reasonably incoherent with the compressive sampling
	matrix. To achieve this incoherence, we use random time dithering
	where sequential acquisition with a single airgun is replaced by
	continuous acquisition with multiple airguns firing at random times
	and at random locations. We demonstrate our results with simulations
	of simultaneous Marine acquisition using periodic and randomized
	time dithering.},
  keywords = {SBGF},

url= { https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/Mansour11SBGFcspsma/Mansour11SBGFcspsma.pdf  }
}


@CONFERENCE{vanleeuwen11SIAMGEOmawt,
  author = {Tristan van Leeuwen and Wim Mulder},
  title = {Multiscale aspects of waveform tomography},
  year = {2011},
  organization = {SIAM GeoSciences 2011},
  publisher = {SIAM GeoSciences 2011},
  abstract = {We consider the inference of medium velocity from transmitted acoustic
	waves. Typically, the measurements are done in a narrow frequency
	band. As a result the sensitivity of the data with respect to velocity
	perturbations varies dramatically with the scale of the perturbation.
	{\textquoteleft}Smooth{\textquoteright} perturbations will cause
	a phase shift, whereas perturbations that vary on the wavelength-scale
	cause amplitude variations. We investigate how to incorporate this
	scale dependent behavior in the formulation of the inverse problem.},
  presentation = {http://slim/Publications/Public/Presentations/2011/SIAMGS11_MS61_Leeuwen.pdf}
}


@CONFERENCE{tu11SEGmult,
  author = {Ning Tu and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Migration with surface-related multiples from incomplete seismic
	data},
  year = {2011},
  organization = {SEG},
  publisher = {SEG},
  abstract = {Seismic acquisition is confined by limited aperture that leads to
	finite illumination, which, together with other factors, hin- ders
	imaging of subsurface objects in complex geological set- tings such
	as salt structures. Conventional processing, includ- ing surface-related
	multiple elimination, further reduces the amount of information we
	can get from seismic data. With the growing consensus that multiples
	carry valuable informa- tion that is missing from primaries, we are
	motivated to exploit the extra illumination provided by multiples
	to image the sub- surface. In earlier research, we proposed such
	a method by combining primary estimation and sparsity-promoting migra-
	tion to invert for model perturbations directly from the total up-going
	wavefield. In this abstract, we focus on a particular case. By exploiting
	the extra illumination from surface-related multiples, we mitigate
	the effects caused by migrating from in- complete data with missing
	sources and missing near-offsets.},
  keywords = {SEG},
month= {04/2011},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/tu11SEGmult/tu11SEGmult.pdf  }
}


@CONFERENCE{tu11EAGEspmsrm,
  author = {Ning Tu and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Sparsity-promoting migration with surface-related multiples},
  year = {2011},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {Multiples, especially the surface-related multiples, form a significant
	part of the total up-going wave- field. If not properly dealt with,
	they can lead to false reflectors in the final image. So conventionally
	practitioners remove them prior to migration. Recently research has
	revealed that multiples can actually provide extra illumination so
	different methods are proposed to address the issue that how to use
	multiples in seismic imaging, but with various kinds of limitations.
	In this abstract, we combine primary estimation and sparsity-promoting
	migration into one convex-optimization process to include information
	from multiples. Synthetic examples show that multiples do make active
	contributions to seismic migration. Also by this combination, we
	can benefit from better recoveries of the Green’s function by using
	sparsity-promoting algorithms since reflectivity is sparser than
	the Green’s function.},
  keywords = {EAGE},
month= {01/2011},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage11/tu2011eage.pdf},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/tu11EAGEspmsrm/tu11EAGEspmsrm.pdf }
}


@CONFERENCE{vanleeuwen11AMPhsdmwi,
  author = {Tristan van Leeuwen and Mark Schmidt and Michael P. Friedlander and
	Felix J. Herrmann},
  title = {A hybrid stocahstic-deterministic method for waveform inversion.Presented
	at AMP Medical and Seismic Imaging, 2011, Vancouver BC.},
  year = {2011},
  organization = {WAVES 2011},
  abstract = {A lot of seismic and medical imaging problems can be written as a
	least-squares data- fitting problem. In particular, we consider the
	case of multi-experiment data, where the data consists of a large
	number of ‘independent’ measurements. Solving the inverse prob- lem
	then involves repeatedly forward modeling the data for each of these
	experiments. In case the number of experiments is large and the modeling
	kernel expensive to apply, such an approach may be prohibitively
	expensive. We review techniques from stochastic opti- mization which
	aim at dramatically reducing the number of experiments that need
	to be modeled at each iteration. This reduction is typically achieved
	by randomly subsampling the data. Special care needs to be taken
	in the optimization to deal with the stochasticity that is introduced
	in this way. },
  date-added = {2011-07-15},
month={07/2011},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/AMPleeuwen2011b.pdf},
  url = {http://slim/Publications/Public/Presentations/2011/AMPleeuwen2011b.pdf}
}



@CONFERENCE{vanleeuwen11EAGEhsdomwi,
  author = {Tristan van Leeuwen and Felix J. Herrmann and Mark Schmidt and Michael
	P. Friedlander},
  title = {A hybrid stochastic-deterministic optimization method for waveform
	inversion},
  year = {2011},
  organization = {EAGE},
  publisher = {EAGE Technical Program Expanded Abstracts},
  abstract = {Present-day high quality 3D acquisition can give us lower frequencies
	and longer offsets with which to invert. However, the computational
	costs involved in handling this data explosion are tremendous. Therefore,
	recent developments in full-waveform inversion have been geared towards
	reducing the computational costs involved. A key aspect of several
	approaches that have been proposed is a dramatic reduction in the
	number of sources used in each iteration. A reduction in the number
	of sources directly translates to less PDE-solves and hence a lower
	computational cost. Re- cent attention has been drawn towards reducing
	the sources by randomly combining the sources in to a few supershots,
	but other strategies are also possible. In all cases, the full data
	misfit, which involves all the sequential sources, is replaced by
	a reduced misfit that is much cheaper to evaluate because it involves
	only a small number of sources (batchsize). The batchsize controls
	the accuracy with which the reduced misfit approximates the full
	misfit. The optimization of such an inaccurate, or noisy, misfit
	is the topic of stochastic optimization. In this paper, we propose
	an optimization strategy that borrows ideas from the field of stochastic
	optimization. The main idea is that in the early stage of the optimization,
	far from the true model, we do not need a very accurate misfit. The
	strategy consists of gradually increasing the batchsize as the iterations
	proceed. We test the proposed strategy on a synthetic dataset. We
	achieve a very reasonable inversion result at the cost of roughly
	13 evaluations of the full misfit. We observe a speed-up of roughly
	a factor 20.},
  keywords = {EAGE},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage11/vanleeuwen2011eage.pdf},
month= {01/2011},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/vanleeuwen11EAGEhsdomwi/vanleeuwen11EAGEhsdomwi.pdf}
}

@CONFERENCE{vanleeuwen11WAVESpeiv,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {Probing the extended image volume for seismic velocity inversion},
  year = {2011},
  organization = {Waves 2011},
  abstract = {In seismic velocity inversion one aims to reconstruct a kinematically
	correct subsurface velocity model that can be used as input for further
	processing and inversion of the data. An important tool in velocity
	inversion is the prestack image volume. This image volume can be
	defined as a cross- correlation of the source and receivers wavefields
	for non-zero space and time lags. If the background velocity is kinematically
	acceptable, this image volume will have its main contributions at
	zero lag, even for complex models. Thus, it is an ideal tool for
	wave-equation migration velocity analysis in the presence of strong
	lateral heterogeneity. In particular, it allows us to pose migration
	velocity analysis as a PDE- constrained optimization problem, where
	the goal is to minimize the energy in the image volume at non-zero
	lag subject to fitting the data approximately. However, it is computationally
	infeasible to explicitly form the whole image volume. In this paper,
	we discuss several ways to reduce the computational costs involved
	in computing the image volume and evaluating the focusing criterion.
	We reduce the costs for calculating the data by randomized source
	synthesis. We also present an efficient way to subsample the image
	volume. Finally, we propose an alternative optimization criterion
	and suggest a multiscale inversion strategy for wave-equation MVA.
	},
  date-added = {2011-07-29},
month={07/2011},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/waves11leeuwen.pdf},
  url = {http://slim/Publications/Public/Presentations/2011/waves11leeuwen.pdf}
}
@CONFERENCE{vanleeuwen11SEGext,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {Probing the extended image volume},
  year = {2011},
  organization = {SEG},
  publisher = {SEG},
  abstract = {The prestack image volume can be defined as a cross- correlation of
	the source and receivers wavefields for non-zero space and time lags.
	If the background velocity is kinemati- cally acceptable, this image
	volume will have its main contri- butions at zero lag, even for complex
	models. Thus, it is an ideal tool for wave-equation migration velocity
	analysis in the presence of strong lateral heterogeneity. In particular,
	it allows us to pose migration velocity analysis as a PDE-constrained
	optimization problem, where the goal is to minimize the en- ergy
	in the image volume at non-zero lag subject to fitting the data approximately.
	However, it is computationally infeasi- ble to explicitly form the
	whole image volume. In this paper, we discuss several ways to reduce
	the computational costs in- volved in computing the image volume
	and evaluating the fo- cusing criterion. We reduce the costs for
	calculating the data by randomized source synthesis. We also present
	an efficient way to subsample the image volume. Finally, we propose
	an alternative optimization criterion and suggest a multiscale in-
	version strategy for wave-equation MVA.},
  keywords = {SEG},
month={04/2011},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/vanleeuwen11SEGext/vanleeuwen11SEGext.pdf }

}

@CONFERENCE{vanleeuwen11ICIAMcbmcwe,
  author = {Tristan van Leeuwen},
  title = {A correlation-based misfit criterion for wave-equation traveltime
	tomography.Presented at ICIAM 2011, Vancouver BC.},
  year = {2011},
  organization = {ICIAM 2011},
  abstract = {The inference of subsurface medium parameters from seismic data can
	be posed as a PDE-constrained data-fitting procedure. This approach
	is successful in reconstructing medium perturbations that are in
	the order of the wavelength. In practice, the data lack low frequency
	content and this means that one needs a good initial guess of the
	slowly varying component of the medium. For a wrong starting model
	an iterative reconstruction procedure is likely to end up in a local
	minimum. We propose to use a different measure of the misfit that
	makes the optimization problem well-posed in terms of the slowly
	varying velocity structures. This procedure can be seen as a generalization
	of ray-based traveltime tomography. We discuss the theoretical underpinnings
	of the method and give some numerical examples. },
  date-added = {2011-07-19},
month={07/2011},
  file = {:http\://slim/Publications/Public/Presentations/2011/ICIAM11leeuwen.pdf:PDF},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/ICIAM11leeuwen.pdf},
  url = {http://slim/Publications/Public/Presentations/2011/ICIAM11leeuwen.pdf}
}


@CONFERENCE{wason11SEGsprsd,
  author = {Haneet Wason and Felix J. Herrmann and Tim T.Y. Lin},
  title = {Sparsity-promoting recovery from simultaneous data: a compressive
	sensing approach},
  year = {2011},
  organization = {SEG},
  publisher = {SEG},
  abstract = {Seismic data acquisition forms one of the main bottlenecks in seismic
	imaging and inversion. The high cost of acquisition work and collection
	of massive data volumes compel the adoption of simultaneous-source
	seismic data acquisition - an emerging technology that is developing
	rapidly, stimulating both geophysical research and commercial efforts.
	Aimed at improving the performance of marine- and land-acquisition
	crews, simultaneous acquisition calls for development of a new set
	of design principles and post-processing tools. Leveraging developments
	from the field of compressive sensing the focus here is on simultaneous-acquisition
	design and sequential-source data recovery. Apart from proper compressive
	sensing sampling schemes, the recovery from simultaneous simulations
	depends on a sparsifying transform that compresses seismic data,
	is fast, and reasonably incoherent with the compressive-sampling
	matrix. Using the curvelet transform, in which seismic data can be
	represented parsimoniously, the recovery of the sequential-source
	data volumes is achieved using the sparsity-promoting program {\textemdash}
	SPGL1, a solver based on projected spectral gradients. The main outcome
	of this approach is a new technology where acquisition related costs
	are no longer determined by the stringent Nyquist sampling criteria.},
  keywords = {SEG},
month= {04/2011},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/wason11SEGsprsd/wason11SEGsprsd.pdf }

}
%-------------------------------------------2010------------------------------------------
@conference{herrmann10UPerc,
  title = {Emperical recovery conditions for seismic sampling},
  author = {Felix J. Herrmann},
  year = {2010},
  abstract = {In this paper, we offer an alternative sampling method leveraging
        recent insights from compressive sensing towards seismic acquisition
        and processing for data that are traditionally considered to be undersampled.
        The main outcome of this approach is a new technology where acquisition
        and processing related costs are no longer determined by overly stringent
        sampling criteria, such as Nyquist. At the heart of our approach
        lies randomized incoherent sampling that breaks subsampling related
        interferences by turning them into harmless noise, which we subsequently
        remove by promoting transform-domain sparsity. Now, costs no longer
        grow with resolution and dimensionality of the survey area, but instead
        depend on transform-domain sparsity only. Our contribution is twofold.
        First, we demonstrate by means of carefully designed numerical experiments
        that compressive sensing can successfully be adapted to seismic acquisition.
        Second, we show that accurate recovery can be accomplished for compressively
        sampled data volumes sizes that exceed the size of conventional transform-domain
        data volumes by only a small factor. Because compressive sensing
        combines transformation and encoding by a single linear encoding
        step, this technology is directly applicable to acquisition and to
        dimensionality reduction during processing. In either case, sampling,
        storage, and processing costs scale with transform-domain sparsity.
        We illustrate this principle by means of number of case studies.},
  keywords = {SEG},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/herrmann10erc.pdf}
}


@CONFERENCE{frijlink10EAGEcos,
  author = {M.O. Frijlink and Reza Shahidi and Felix J. Herrmann and R.G. van
	Borselen},
  title = {Comparison of Standard Adaptive Subtraction and Primary-multiple
	Separation in the Curvelet Domain},
  year = {2010},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {In recent years, data-driven multiple prediction methods and wavefield
	extrapolation methods have proven to be powerful methods to attenuate
	multiples from data acquired in complex 3-D geologic environments.
	These methods make use of a two-stage approach, where first the multiples
	(surface-related and / or internal) multiples are predicted before
	they are subtracted from the original input data in an adaptively.
	The quality of these predicted multiples often raises high expectations
	for the adaptive subtraction techniques, but for various reasons
	these expectations are not always met in practice. Standard adaptive
	subtraction methods use the well-known minimum energy criterion,
	stating that the total energy after optimal multiple attenuation
	should be minimal. When primaries and multiples interfere , the minimum
	energy criterion is no longer appropriate. Also, when multiples of
	different orders interfere, adaptive energy minimization will lead
	to a compromise between different amplitudes corrections for the
	different orders of multiples. This paper investigates the performance
	of two multiple subtraction schemes for a real data set that exhibits
	both interference problems. Results from an adaptive subtraction
	in the real curvelet domain, separating primaries and multiples,
	are compared to those obtained using a more conventional adaptive
	subtraction method in the spatial domain.},
  keywords = {EAGE},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/frijlink10EAGEcos/frijlink10EAGEcos.pdf}
}

@CONFERENCE{herrmann10MATHIAScssr,
  author = {Felix J. Herrmann},
  title = {Compressive Sensing and Sparse Recovery in Exploration Seismology.
	Presented at MATHIAS 2010 organized by Total SA. Paris.},
  year = {2010},
  abstract = {During this presentation, I will talk about how recent results from
	compressive sensing and sparse recovery can be used to solve problems
	in exploration seismology where incomplete sampling is ubiquitous.
	I will also talk about how these ideas apply to dimensionality reduction
	of full-waveform inversion by randomly phase encoded sources.},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2010/herrmann2010total.pdf}
}

@CONFERENCE{herrmann10EAGErds,
  author = {Felix J. Herrmann and Xiang Li},
  title = {Randomized dimensionality reduction for full-waveform inversion},
  year = {2010},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {Full-waveform inversion relies on the collection of large multi-experiment
	data volumes in combination with a sophisticated back-end to create
	high-fidelity inversion results. While improvements in acquisition
	and inversion have been extremely successful, the current trend of
	incessantly pushing for higher quality models in increasingly complicated
	regions of the Earth continues to reveal fundamental shortcomings
	in our ability to handle the ever increasing problem size numerically.
	Two causes can be identified as the main culprits responsible for
	this barrier. First, there is the so-called {\textquoteleft}{\textquoteleft}curse
	of dimensionality{\textquoteright}{\textquoteright} exemplified by
	Nyquist{\textquoteright}s sampling criterion, which puts disproportionate
	strain on current acquisition and processing systems as the size
	and desired resolution of our survey areas continues to increase.
	Secondly, there is the recent {\textquoteleft}{\textquoteleft}departure
	from Moore{\textquoteright}s law{\textquoteright}{\textquoteright}
	that forces us to lower our expectations to compute ourselves out
	of this. In this paper, we address this situation by randomized dimensionality
	reduction, which we adapt from the field of compressive sensing.
	In this approach, we combine deliberate randomized subsampling with
	structure-exploiting transform-domain sparsity promotion. Our approach
	is successful because it reduces the size of seismic data volumes
	without loss of information. With this reduction, we compute Newton-like
	updates at the cost of roughly one gradient update for the fully-sampled
	wavefield.},
  keywords = {EAGE},
  presentation = {http://slim.eos.ubc.ca/Publications/public/presentations/eage/eage10/herrmannEAGE2010rdr.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/herrmann10EAGErds/herrmann10EAGErds.pdf/ }
}

@CONFERENCE{herrmann10EAGErss,
  author = {Felix J. Herrmann},
  title = {Randomized sampling strategies},
  year = {2010},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {Seismic exploration relies on the collection of massive data volumes
	that are subsequently mined for information during seismic processing.
	While this approach has been extremely successful in the past, the
	current trend towards higher quality images in increasingly complicated
	regions continues to reveal fundamental shortcomings in our workflows
	for high-dimensional data volumes. Two causes can be identified..
	First, there is the so-called {\textquoteleft}{\textquoteleft}curse
	of dimensionality{\textquoteright}{\textquoteright} exemplified by
	Nyquist{\textquoteright}s sampling criterion, which puts disproportionate
	strain on current acquisition and processing systems as the size
	and desired resolution of our survey areas continues to increase.
	Secondly, there is the recent {\textquoteleft}{\textquoteleft}departure
	from Moore{\textquoteright}s law{\textquoteright}{\textquoteright}
	that forces us to lower our expectations to compute ourselves out
	of this curse of dimensionality. In this paper, we offer a way out
	of this situation by a deliberate randomized subsampling combined
	with structure-exploiting transform-domain sparsity promotion. Our
	approach is successful because it reduces the size of seismic data
	volumes without loss of information. As such we end up with a new
	technology where the costs of acquisition and processing are no longer
	dictated by the size of the acquisition but by the transform-domain
	sparsity of the end-product.},
  keywords = {EAGE},
  presentation = {http://slim.eos.ubc.ca/Publications/public/presentations/eage/eage10/herrmannEAGE2010rss.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/herrmann10EAGErss/herrmann10EAGErss.pdf }
}



@CONFERENCE{herrmann10IRISsns,
  author = {Felix J. Herrmann},
  title = {Sub-Nyquist sampling and sparsity: getting more information from
	fewer samples. Presented at the IRIS Workshop},
  year = {2010},
  abstract = {Many seismic exploration techniques rely on the collection of massive
	data volumes. While this approach has been extremely successful in
	the past, current efforts toward higher resolution images in increasingly
	complicated regions of the Earth continue to reveal fundamental shortcomings
	in our workflows. Chiefly amongst these is the so-called {\textquoteleft}{\textquoteleft}curse
	of dimensionality{\textquoteright}{\textquoteright} exemplified by
	Nyquist{\textquoteright}s sampling criterion, which disproportionately
	strains current acquisition and processing systems as the size and
	desired resolution of our survey areas continues to increase. In
	this presentation, we offer an alternative sampling method leveraging
	recent insights from compressive sensing towards seismic acquisition
	and processing of severely under-sampled data. The main outcome of
	this approach is a new technology where acquisition and processing
	related costs are no longer determined by overly stringent sampling
	criteria, such as Nyquist. At the heart of our approach lies randomized
	incoherent sampling that breaks subsampling related interferences
	by turning them into harmless noise, which we subsequently remove
	by promoting transform-domain sparsity. Now, costs no longer grow
	significantly with resolution and dimensionality of the survey area,
	but instead depend on transform-domain sparsity only. Our contribution
	is twofold. First, we demonstrate by means of carefully designed
	numerical experiments that compressive sensing can successfully be
	adapted to seismic exploration. Second, we show that accurate recovery
	can be accomplished for compressively sampled data volumes sizes
	that exceed the size of conventional transform-domain data volumes
	by only a small factor. Because compressive sensing combines transformation
	and encoding by a single linear encoding step, this technology is
	directly applicable to acquisition and to dimensionality reduction
	during processing. In either case, sampling, storage, and processing
	costs scale with transform-domain sparsity. Many seismic exploration
	techniques rely on the collection of massive data volumes. While
	this approach has been extremely successful in the past, current
	efforts toward higher resolution images in increasingly complicated
	regions of the Earth continue to reveal fundamental shortcomings
	in our workflows. Chiefly amongst these is the so-called {\textquoteleft}{\textquoteleft}curse
	of dimensionality{\textquoteright}{\textquoteright} exemplified by
	Nyquist{\textquoteright}s sampling criterion, which disproportionately
	strains current acquisition and processing systems as the size and
	desired resolution of our survey areas continues to increase. In
	this presentation, we offer an alternative sampling method leveraging
	recent insights from compressive sensing towards seismic acquisition
	and processing of severely under-sampled data. The main outcome of
	this approach is a new technology where acquisition and processing
	related costs are no longer determined by overly stringent sampling
	criteria, such as Nyquist. At the heart of our approach lies randomized
	incoherent sampling that breaks subsampling related interferences
	by turning them into harmless noise, which we subsequently remove
	by promoting transform-domain sparsity. Now, costs no longer grow
	significantly with resolution and dimensionality of the survey area,
	but instead depend on transform-domain sparsity only. Our contribution
	is twofold. First, we demonstrate by means of carefully designed
	numerical experiments that compressive sensing can successfully be
	adapted to seismic exploration. Second, we show that accurate recovery
	can be accomplished for compressively sampled data volumes sizes
	that exceed the size of conventional transform-domain data volumes
	by only a small factor. Because compressive sensing combines transformation
	and encoding by a single linear encoding step, this technology is
	directly applicable to acquisition and to dimensionality reduction
	during processing. In either case, sampling, storage, and processing
	costs scale with transform-domain sparsity.},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2010/herrmann2010iris.pdf}
}



@CONFERENCE{johnson10EAGEeop,
  author = {James Johnson and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Estimation of primaries via sparse inversion with reciprocity},
  year = {2010},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {Accurate removal of surface related multiples is a key step in seismic
	data processing. The industry standard for removing multiples is
	SRME, which involves convolving the data with itself to predict the
	multiples, followed by an adaptive subtraction procedure to recover
	the primaries (Verschuur and Berkhout, 1997). Other methods involve
	multidimensional division of the up-going and down-going wavefields
	(Amundsen, 2001). However, this approach may suffer from stability
	problems. With the introduction of the {\textquoteleft}{\textquoteleft}estimation
	of primaries by sparse inversion{\textquoteright}{\textquoteright}(EPSI),
	van Groenestijn and Verschuur (2009) recentely reformulated SRME
	to jointly estimate the surface-free impulse response and the source
	signature directly from the data. The advantage of EPSI is that it
	recovers the primary response directly, and does not require a second
	processing step for the subtraction of estimated multiples from the
	original data. However, because it estimates both the primary impulse
	response and source signature from the data EPSI must be regularized.
	Motivated by recent successful application of the curvelet transform
	in seismic data processing (Herrmann et al., 2007), we formulate
	EPSI as a bi-convex optimization problem that seeks sparsity on the
	surface-free Green{\textquoteright}s function and Fourier-domain
	smoothness on the source wavelet. Our main contribution compared
	to previous work (Lin and Herrmann, 2009), and the contribution of
	that author to the proceedings of this meeting(Lin and Herrmann,
	2010), is that we employ the physical principle of as source-receiver
	reciprocity to improve the inversion.},
  keywords = {EAGE},
  url = { https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/johnson10EAGEeop/johnson10EAGEeop.pdf }
}

@CONFERENCE{li10SEGfwi,
  author = {Xiang Li and Felix J. Herrmann},
  title = {Full-waveform inversion from compressively recovered updates},
  year = {2010},
  organization = {SEG},
  publisher = {SEG},
  abstract = {Full-waveform inversion relies on the collection of large multi-experiment
	data volumes in combination with a sophisticated back-end to create
	high-fidelity inversion results. While improvements in acquisition
	and inversion have been extremely successful, the current trend of
	incessantly pushing for higher quality models in increasingly complicated
	regions of the Earth reveals fundamental shortcomings in our ability
	to handle increasing problem size numerically. Two main culprits
	can be identified. First, there is the so-called {\textquoteleft}{\textquoteleft}curse
	of dimensionality{\textquoteright}{\textquoteright} exemplified by
	Nyquist{\textquoteright}s sampling criterion, which puts disproportionate
	strain on current acquisition and processing systems as the size
	and desired resolution increases. Secondly, there is the recent {\textquoteleft}{\textquoteleft}departure
	from Moore{\textquoteright}s law{\textquoteright}{\textquoteright}
	that forces us to lower our expectations to compute ourselves out
	of this. In this paper, we address this situation by randomized dimensionality
	reduction, which we adapt from the field of compressive sensing.
	In this approach, we combine deliberate randomized subsampling with
	structure-exploiting transform-domain sparsity promotion. Our approach
	is successful because it reduces the size of seismic data volumes
	without loss of information. With this reduction, we compute Newton-like
	updates at the cost of roughly one gradient update for the fully-sampled
	wavefield.},
  keywords = {SEG},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg10/Li2010fcr.pdf},
url= { https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/li10SEGfwi/li10SEGfwi.pdf }
}


@CONFERENCE{lin10EAGEseo,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Stabilization of estimation of primaries via sparse inversion},
  year = {2010},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {Recent works on surface-related multiple removal include a direct
	estimation method proposed by van Groenestijn and Verschuur (2009),
	where under a sparsity assumption the primary impulse response is
	determined directly from a data-driven wavefield inversion process
	called Estimation of Primaries by Sparse Inversion (EPSI). The authors
	have shown that this approach is superior to traditional estimation
	subtraction processes such as SRME on shallow bottom marine data,
	where by expanding the model to simultaneously invert for the near-offset
	traces, which are not directly available in most situation but are
	observable in the data multiples, a large improvement over Radon
	interpolation is demonstrated. One of the major roadblocks to the
	widespread adoption of EPSI is that one must have precise knowledge
	of a time-window that contains multiple-free primaries during each
	update. There is some anecdotal evidence that the inversion result
	is unstable under errors in the time-window length, a behavior that
	runs contrary to the strengths of EPSI and diminishes its effectiveness
	for shallow-bottom marine data where multiples are closely spaced.
	Moreover, due to the nuances involved in regularizing the model impulse
	response in the inverse problem, the EPSI approach has an additional
	number of inversion parameters to choose and often also does not
	often lead to a stable solution under perturbations to these parameters.
	We show that the specific sparsity constraint on the EPSI updates
	lead to an inherently intractable problem, and that the time-window
	and other inversion variables arise as additional regularizations
	on the unknown towards a meaningful solution. We furthermore suggest
	a way to remove almost all of these parameters via a L0 to L1 convexification,
	which stabilizes the inversion while preserving the crucial sparsity
	assumption in the primary impulse response model.},
  keywords = {EAGE},
presentation ={http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage10/lin10eagesoe.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/lin10EAGEseo/lin10EAGEseo.pdf }
}

@CONFERENCE{lin10SEGspm,
  author = {Tim T.Y. Lin and Ning Tu and Felix J. Herrmann},
  title = {Sparsity-promoting migration from surface-related multiples},
  year = {2010},
  organization = {SEG},
  publisher = {SEG},
  abstract = {Seismic imaging typically begins with the removal of multiple energy
	in the data, out of fear that it may introduce erroneous structure.
	However, seismic multiples have effectively seen more of the earth{\textquoteright}s
	structure, and if treated correctly can potential supply more information
	to a seismic image compared to primaries. Past approaches to accomplish
	this leave ample room for improvement; they either require extensive
	modification to standard migration techniques, rely too much on prior
	information, require extensive pre-processing, or resort to full-waveform
	inversion. We take some valuable lessons from these efforts and present
	a new approach balanced in terms of ease of implementation, robustness,
	efficiency and well-posedness, involving a sparsity-promoting inversion
	procedure using standard Born migration and a data-driven multiple
	modeling approach based on the focal transform.},
  keywords = {SEG},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg10/Tu-0945.pdf
	},
url= { https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/lin10SEGspm/lin10SEGspm.pdf }
}

@CONFERENCE{moghaddam10SEGrfw,
  author = {Peyman P. Moghaddam and Felix J. Herrmann},
  title = {Randomized full-waveform inversion: a dimenstionality-reduction approach},
  year = {2010},
  organization = {SEG},
  publisher = {SEG},
  abstract = {Full-waveform inversion relies on the collection of large multi-experiment
	data volumes in combination with a sophisticated back-end to create
	high-fidelity inversion results. While improvements in acquisition
	and inversion have been extremely successful, the current trend of
	incessantly pushing for higher quality models in increasingly complicated
	regions of the Earth reveals fundamental shortcomings in our ability
	to handle increasing problem sizes numerically. Two main culprits
	can be identified. First, there is the so-called {\textquoteleft}{\textquoteleft}curse
	of dimensionality{\textquoteright}{\textquoteright} exemplified by
	Nyquist{\textquoteright}s sampling criterion, which puts disproportionate
	strain on current acquisition and processing systems as the size
	and desired resolution increases. Secondly, there is the recent {\textquoteleft}{\textquoteleft}departure
	from Moore{\textquoteright}s law{\textquoteright}{\textquoteright}
	that forces us to develop algorithms that are amenable to parallelization.
	In this paper, we discuss different strategies that address these
	issues via randomized dimensionality reduction.},
  keywords = {SEG},
  presentation = {http://slim.eos.ubc.ca/Publications/public/presentations/seg/seg10/Herrmann10RFW.pdf
	},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/moghaddam10SEGrfw/moghaddam10SEGrfw.pdf  }
}

%------------------------------2009---------------------------------------------
@CONFERENCE{erlangga09SEGfwi,
  author = {Yogi A. Erlangga and Felix J. Herrmann},
  title = {Full-Waveform Inversion with Gauss-Newton-Krylov Method},
  booktitle = {SEG},
  year = {2009},
  abstract = {This abstract discusses an implicit implementation of the Gauss-Newton
	method, used for the frequency-domain full-waveform inversion, where
	the inverse of the Hessian for the update is never formed explicitly.
	Instead, the inverse of the Hessian is computed approximately by
	a conjugate gradient (CG) method, which only requires the action
	of the Hessian on the CG search direction. This procedure avoids
	an excessive computer storage, usually needed for storing the Hessian,
	at the expense of extra computational work in CG. An effective preconditioner
	for the Hessian is important to improve the convergence of CG, and
	hence to reduce the overall computational work.},
  keywords = {Presentation, SEG, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/public/presentations/seg/seg09/erlangga09segfwi.pdf}
}

@CONFERENCE{Erlangga09EAGEmwi,
  author = {Yogi A. Erlangga and Felix J. Herrmann},
  title = {Migration with implicit solvers for the time-harmonic Helmholtz equation},
  year = {2009},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {From the measured seismic data, the location and the amplitude of
	reflectors can be determined via a migration algorithm. Classically,
	following Claerbout{\textquoteright}s imaging principle [2], a reflector
	is located at the position where the source{\textquoteright}s forward-propagated
	wavefield correlates with the backward-propagated wavefield of the
	receiver data. Lailly and Tarantola later showed that this imaging
	principle is an instance of inverse problems, with the associated
	migration operator formulated via a least-squares functional; see
	[6, 12, 13]. Furthermore, they showed that the migrated image is
	associated with the gradient of this functional with respect to the
	image. If the solution of the least-squares functional is done iteratively,
	the correlation-based image coincides up to a constant with the first
	iteration of a gradient method. In practice, this migration is done
	either in the time domain or in the frequency domain. In the frequency-domain
	migration, the main bottleneck thus far, which renders its full implementation
	to large scale problems, is the lack of efficient solvers for computing
	wavefields. Robust direct methods easily run into excessive memory
	requirements as the size of the problem increases. On the other hand,
	iterative methods, which are less demanding in terms of memory, suffered
	from lack of convergence. During the past years, however, progress
	has been made in the development of an efficient iterative method
	[4, 3] for the frequency-domain wavefield computations. In this paper,
	we will show the significance of this method (called MKMG) in the
	context of the frequency-domain migration, where multi-shot-frequency
	wavefields (of order of 10,000 related wavefields) need to be computed.},
  keywords = {EAGE },
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/erlanggaEAGE2009.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2009/Erlangga09EAGEmwi/Erlangga09EAGEmwi.pdf}
}

@CONFERENCE{erlangga09SEGswi,
  author = {Yogi A. Erlangga and Felix J. Herrmann},
  title = {Seismic waveform inversion with Gauss-Newton-Krylov method},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  number = {1},
  pages = {2357-2361},
  organization = {SEG},
  publisher = {SEG},
  abstract = {This abstract discusses an implicit implementation of the Gauss-Newton
	method, used for the frequency-domain full-waveform inversion, where
	the inverse of the Hessian for the update is never formed explicitly.
	Instead, the inverse of the Hessian is computed approximately by
	a conjugate gradient (CG) method, which only requires the action
	of the Hessian on the CG search direction. This procedure avoids
	an excessive computer storage, usually needed for storing the Hessian,
	at the expense of extra computational work in CG. An effective preconditioner
	for the Hessian is important to improve the convergence of CG, and
	hence to reduce the overall computational work.},
  keywords = {SEG},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/erlangga09SEGswi/erlangga09SEGswi.pdf }
}

@CONFERENCE{friedlander09VIETcsgpa,
  author = {Michael P. Friedlander},
  title = {Computing sparse and group-sparse approximations},
  ogranization = {2009 High Performance Scientific Computing Conference},
  year = {2009},
  address = {Hanoi, Vietnam},
  keywords = {minimization, Presentation, SLIM}
}


@CONFERENCE{friedlander09NUalssr,
  author = {Michael P. Friedlander},
  title = {Algorithms for large-scale sparse reconstruction},
  organization = {IEMS Colloquim Speaker},
  year = {2009},
  address = {Northwestern University},
  keywords = {minimization, Presentation, SLIM}
}

@CONFERENCE{friedlander09SCAIMspot,
  author = {E. van den Berg and Michael P. Friedlander},
  title = {Spot: A linear-operator toolbox for Matlab},
  organization = {SCAIM Seminar},
  year = {2009},
  address = {University of British Columbia},
  keywords = {minimization, Presentation, SLIM},
presentation ={http://slim.eos.ubc.ca/Publications/Private/Presentations/sinbad/2010Fall/Thu-13-50-Friedlander.pdf }
}

@CONFERENCE{herrmann09SEGcib,
  author = {Felix J. Herrmann},
  title = {Compressive imaging by wavefield inversion with group sparsity},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  number = {1},
  pages = {2337-2341},
  organization = {SEG},
  publisher = {SEG},
  abstract = {Migration relies on multi-dimensional correlations between source-
	and residual wavefields. These multi-dimensional correlations are
	computationally expensive because they involve operations with explicit
	and full matrices that contain both wavefields. By leveraging recent
	insights from compressive sampling, we present an alternative method
	where linear correlation-based imaging is replaced by imaging via
	multidimensional deconvolutions of compressibly sampled wavefields.
	Even though this approach goes at the expense of having to solve
	a sparsity-promotion recovery program for the image, our wavefield
	inversion approach has the advantage of reducing the system size
	in accordance to transform-domain sparsity of the image. Because
	seismic images also exhibit a focusing of the energy towards zero
	offset, the compressive-wavefield inversion itself is carried out
	using a recent extension of one-norm solver technology towards matrix-valued
	problems. These so-called hybrid $(1,\,2)$-norm solvers allow us
	to penalize pre-stack energy away from zero offset while exploiting
	joint sparsity amongst near-offset images. Contrary to earlier work
	to reduce modeling and imaging costs through random phase-encoded
	sources, our method compressively samples wavefields in model space.
	This approach has several advantages amongst which improved system-size
	reduction, and more flexibility during subsequent inversions for
	subsurface properties.},
  keywords = {SEG},
  presentation = {http://slim.eos.ubc.ca/Publications/public/presentations/seg/seg09/herrmann09segciw.pdf
	},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09SEGcib/herrmann09SEGcib.pdf }
}

@CONFERENCE{Herrmann09EAGEcsa,
  author = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title = {Compressive sensing applied to full-waveform inversion},
  year = {2009},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {With the recent resurgence of full-waveform inversion, the computational
	cost of solving forward modeling problems has become{\textendash}-aside
	from issues with non-uniqueness{\textendash}-one of the major impediments
	withstanding successful application of this technology to industry-size
	data volumes. To overcome this impediment, we argue that further
	improvements in this area will depend on a problem formulation with
	a computational complexity that is no longer strictly determined
	by the size of the discretization but by transform-domain sparsity
	of its solution. In this new paradigm, we bring computational costs
	in par with our ability to compress seismic data and images. This
	premise is related to two recent developments. First, there is the
	new field of compressive sensing (CS in short throughout the paper,
	Cand{\textquoteleft}es et al., 2006; Donoho, 2006){\textendash}-where
	the argument is made, and rigorously proven, that compressible signals
	can be recovered from severely sub-Nyquist sampling by solving a
	sparsity promoting program. Second, there is in the seismic community
	the recent resurgence of simultaneous-source acquisition (Beasley,
	2008; Krohn and Neelamani, 2008; Herrmann et al., 2009; Berkhout,
	2008; Neelamani et al., 2008), and continuing efforts to reduce the
	cost of seismic modeling, imaging, and inversion through phase encoding
	of simultaneous sources (Morton and Ober, 1998; Romero et al., 2000;
	Krohn and Neelamani, 2008; Herrmann et al., 2009), removal of subsets
	of angular frequencies (Sirgue and Pratt, 2004; Mulder and Plessix,
	2004; Lin et al., 2008) or plane waves (Vigh and Starr, 2008). By
	using CS principles, we remove sub-sampling interferences asocciated
	with these approaches through a combination of exploiting transform-domain
	sparsity, properties of certain sub-sampling schemes, and the existence
	of sparsity promoting solvers.},
  keywords = {EAGE},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Herrmann09eagecs.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2009/Herrmann09EAGEcsa/Herrmann09EAGEcsa.pdf }
}

@CONFERENCE{Herrmann09SAMPTAcws,
  author = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title = {Compressive-wavefield simulations},
  year = {2009},
  organization = {SAMPTA},
  publisher = {SAMPTA},
  abstract = {Full-waveform inversion{\textquoteright}s high demand on computational
	resources forms, along with the non-uniqueness problem, the major
	impediment withstanding its widespread use on industrial-size datasets.
	Turning modeling and inversion into a compressive sensing problem{\textendash}-where
	simulated data are recovered from a relatively small number of independent
	simultaneous sources{\textendash}-can effectively mitigate this high-cost
	impediment. The key is in showing that we can design a sub-sampling
	operator that commutes with the time-harmonic Helmholtz system. As
	in compressive sensing, this leads to a reduction in simulation cost.
	Moreover, this reduction is commensurate with the transform-domain
	sparsity of the solution, implying that computational costs are no
	longer determined by the size of the discretization but by transform-domain
	sparsity of the solution of the CS problem which forms our data.
	The combination of this sub-sampling strategy with our recent work
	on implicit solvers for the Helmholtz equation provides a viable
	alternative to full-waveform inversion schemes based on explicit
	finite-difference methods.},
  keywords = {SAMPTA},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SAMPTA/2009/Herrmann09SAMPTAcws/Herrmann09SAMPTAcws.pdf }
}

@CONFERENCE{herrmann09EAGEbnrs,
  author = {Felix J. Herrmann and Gang Tang and Reza Shahidi and Gilles Hennenfent
	and Tim T.Y. Lin},
  title = {Beating Nyquist by randomized sampling. Presented at the EAGE (workshop),
	Amsterdam},
  year = {2009},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Herrmann09eagews.pdf}
}


@CONFERENCE{herrmann09IAPcsisa,
  author = {Felix J. Herrmann and Yogi Erlangga and Tim T.Y. Lin},
  title = {Compressive seismic imaging with simultaneous acquisition presented
	at the IAP meeting, Vienna,},
  year = {2009},
  abstract = {The shear size of seismic data volumes forms one of the major impediments
	for the inversion of seismic data. Turning forward modeling and inversion
	into a compressive sensing (CS) problem - where simulated data are
	recovered from a relatively small number of independent sources -
	can effectively mitigate this high-cost impediment. Our key contribution
	lies in the design of a sub-sampling operator that commutes with
	the time-harmonic Helmholtz system. As in compressive sensing, this
	leads to a reduction of simulation cost. This reduction is commensurate
	with the transform-domain sparsity of the solution., implying that
	computational costs are no longer determined by the size of the discretization
	but by transform-domain sparsity of the solution of the CS problem
	that recovers the data. The combination of this sub-sampling strategy
	with our recent work on preconditioned implicit solvers for the time-harmonic
	Helmholtz equation provides a viable alternative to full-waveform
	inversion schemes based on explicit time-domain finite-difference
	methods.},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Herrmann09AIP1.pdf}
}

@CONFERENCE{herrmann09SEGsns,
  author = {Felix J. Herrmann},
  title = {Sub-Nyquist sampling and sparsity: getting more information from
	fewer samples},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  number = {1},
  pages = {3410-3415},
  organization = {SEG},
  publisher = {SEG},
  abstract = {Seismic exploration relies on the collection of massive data volumes
	that are subsequently mined for information during seismic processing.
	While this approach has been extremely successful in the past, the
	current trend of incessantly pushing for higher quality images in
	increasingly complicated regions of the Earth continues to reveal
	fundamental shortcomings in our workflows to handle massive high-dimensional
	data volumes. Two causes can be identified as the main culprits responsible
	for this barrier. First, there is the so-called {\textquoteleft}{\textquoteleft}curse
	of dimensionality{\textquoteright}{\textquoteright} exemplified by
	Nyquist{\textquoteright}s sampling criterion, which puts disproportionate
	strain on current acquisition and processing systems as the size
	and desired resolution of our survey areas continues to increase.
	Secondly, there is the recent {\textquoteleft}{\textquoteleft}departure
	from Moore{\textquoteright}s law{\textquoteright}{\textquoteright}
	that forces us to lower our expectations to compute ourselves out
	of this curse of dimensionality. In this paper, we offer a way out
	of this situation by a deliberate \emph{randomized} subsampling combined
	with structure-exploiting transform-domain sparsity promotion. Our
	approach is successful because it reduces the size of seismic data
	volumes without loss of information. Because of this size reduction
	both impediments are removed and we end up with a new technology
	where the costs of acquisition and processing are no longer dictated
	by the \emph{size of the acquisition} but by the transform-domain
	\emph{sparsity} of the end-product after processing.},
  keywords = {SEG},
  presentation = { http://slim.eos.ubc.ca/Publications/public/presentations/seg/seg09/herrmann09segsns.pdf},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09SEGsns/herrmann09SEGsns.pdf }
}

@CONFERENCE{herrmann09PIMScssr3,
  author = {Felix J. Herrmann},
  title = {Compressed sensing and sparse recovery in exploration seismology.
	Lecture III presented at the PIMS Summer School on Seismic Imaging.
	Seattle.},
  year = {2009},
  abstract = {In this course, I will present how recent results from compressed
	sensing and sparse recovery apply to exploration seismology. During
	the first lecture, I will present the basic principles of compressive
	sensing; the importance of random jitter sampling and sparsifying
	transforms; and large-scale one-norm solvers. I will discuss the
	application of these techniques to missing trace interpolation. The
	second lecture will be devoted to coherent signal separation based
	on curveletdomain matched filtering and Bayesian separation with
	sparsity promotion. Applications of these techniques to the primary-multiple
	wavefield-separation problem on real data will be discussed as well.
	The third lecture will be devoted towards sparse recovery in seismic
	modeling and imaging and includes the problem of preconditioning
	the imaging operators, and the recovery from simultaneous source-acquired
	data.},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Herrmann09PIMS3.pdf}
}

@CONFERENCE{herrmann09PIMScssr2,
  author = {Felix J. Herrmann},
  title = {Compressed sensing and sparse recovery in exploration seismology.
	Lecture II presented at the PIMS Summer School on Seismic Imaging.
	Seattle.},
  year = {2009},
  abstract = {In this course, I will present how recent results from compressed
	sensing and sparse recovery apply to exploration seismology. During
	the first lecture, I will present the basic principles of compressive
	sensing; the importance of random jitter sampling and sparsifying
	transforms; and large-scale one-norm solvers. I will discuss the
	application of these techniques to missing trace interpolation. The
	second lecture will be devoted to coherent signal separation based
	on curveletdomain matched filtering and Bayesian separation with
	sparsity promotion. Applications of these techniques to the primary-multiple
	wavefield-separation problem on real data will be discussed as well.
	The third lecture will be devoted towards sparse recovery in seismic
	modeling and imaging and includes the problem of preconditioning
	the imaging operators, and the recovery from simultaneous source-acquired
	data.},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Herrmann09PIMS2.pdf}
}

@CONFERENCE{herrmann09PIMScssr1,
  author = {Felix J. Herrmann},
  title = {Compressed sensing and sparse recovery in exploration seismology.
	Lecture I presented at the PIMS Summer School on Seismic Imaging.
	Seattle.},
  year = {2009},
  abstract = {In this course, I will present how recent results from compressed
	sensing and sparse recovery apply to exploration seismology. During
	the first lecture, I will present the basic principles of compressive
	sensing; the importance of random jitter sampling and sparsifying
	transforms; and large-scale one-norm solvers. I will discuss the
	application of these techniques to missing trace interpolation. The
	second lecture will be devoted to coherent signal separation based
	on curveletdomain matched filtering and Bayesian separation with
	sparsity promotion. Applications of these techniques to the primary-multiple
	wavefield-separation problem on real data will be discussed as well.
	The third lecture will be devoted towards sparse recovery in seismic
	modeling and imaging and includes the problem of preconditioning
	the imaging operators, and the recovery from simultaneous source-acquired
	data},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Herrmann09PIMS1.pdf}
}

@CONFERENCE{herrmann09SEGrpl,
  author = {Felix J. Herrmann},
  title = {Reflector-preserved lithological upscaling},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  number = {1},
  pages = {3466-3470},
  organization = {SEG},
  publisher = {SEG},
  abstract = {By combining Percolation models with lithological smoothing, we arrive
	at method for upscaling rock elastic constants that preserves reflections.
	In this approach, the Percolation model predicts sharp onsets in
	the elastic moduli of sand-shale mixtures when the shales reach a
	critical volume fraction. At that point, the shale inclusions form
	a connected cluster, and the macroscopic rock properties change with
	the power-law growth of the cluster. This switch-like nonlinearity
	preserves singularities, and hence reflections, even if no sharp
	transition exists in the lithology or if they are smoothed out using
	standard upscaling procedures.},
  keywords = {SEG},
  presentation = {http://slim.eos.ubc.ca/Publications/public/presentations/seg/seg09/herrmann09segrpu.pdf},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09SEGrpl/herrmann09SEGrpl.pdf }
}

@CONFERENCE{kumar09SEGins,
  author = {Vishal Kumar and Felix J. Herrmann},
  title = {Incoherent noise suppression with curvelet-domain sparsity},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  number = {1},
  pages = {3356-3360},
  organization = {SEG},
  publisher = {SEG},
  abstract = {The separation of signal and noise is a key issue in seismic data
	processing. By noise we refer to the incoherent noise that is present
	in the data. We use the recently introduced multiscale and multidirectional
	curvelet transform for suppression of random noise. The curvelet
	transform decomposes data into directional plane waves that are local
	in nature. The coherent features of the data occupy the large coefficients
	in the curvelet domain, whereas the incoherent noise lives in the
	small coefficients. In other words, signal and noise have minimal
	overlap in the curvelet domain. This gives us a chance to use curvelets
	to suppress noise present in data.},
  keywords = {SEG},
  presentation = {http://slim.eos.ubc.ca/Publications/public/presentations/seg/seg09/kumar09segins.pdf
	},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/kumar09SEGins/kumar09SEGins.pdf }
}

@CONFERENCE{Lin09SEGcsf,
  author = {Tim T.Y. Lin and Yogi A. Erlangga and Felix J. Herrmann},
  title = {Compressive simultaneous full-waveform simulation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  number = {1},
  pages = {2577},
  organization = {SEG},
  publisher = {SEG},
  abstract = {The fact that the computational complexity of wavefield simulation
	is proportional to the size of the discretized model and acquisition
	geometry, and not to the complexity of the simulated wavefield, is
	a major impediment within seismic imaging. By turning simulation
	into a compressive sensing problem{\textendash}where simulated data
	is recovered from a relatively small number of independent simultaneous
	sources{\textendash}we remove this impediment by showing that compressively
	sampling a simulation is equivalent to compressively sampling the
	sources, followed by solving a reduced system. As in compressive
	sensing, this allows for a reduction in sampling rate and hence in
	simulation costs. We demonstrate this principle for the time-harmonic
	Helmholtz solver. The solution is computed by inverting the reduced
	system, followed by a recovery of the full wavefield with a sparsity
	promoting program. Depending on the wavefield{\textquoteright}s sparsity,
	this approach can lead to a significant cost reduction, in particular
	when combined with the implicit preconditioned Helmholtz solver,
	which is known to converge even for decreasing mesh sizes and increasing
	angular frequencies. These properties make our scheme a viable alternative
	to explicit time-domain finite-difference.},
  keywords = {SEG},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/Lin09SEGcsf/Lin09SEGcsf.pdf }

}

@CONFERENCE{lin09EAGEdsa,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Designing simultaneous acquisitions with compressive sensing},
  year = {2009},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {The goal of this paper is in designing a functional simultaneous acquisition
	scheme by applying the principles of compressive sensing. By framing
	the acquisition in a compressive sensing setting we immediately gain
	insight into not only how to choose the source signature and shot
	patterns, but also in how well we can hope to demultiplex the data
	when given a set amount of reduction in the number of sweeps. The
	principles of compressive sensing dictates that the quality of the
	demultiplexed data is closely related to the transform-domain sparsity
	of the solution. This means that, given an estimate in the complexity
	of the expectant data wavefield, it is possible to controllably reduce
	the number of shots that needs to be recorded in the field. We show
	a proof of concept by introducing an acquisition compatible with
	compressive sensing based on randomly phase-encoded vibroseis sweeps.},
  keywords = {EAGE},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/lin2009eagedsa.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2009/lin09EAGEdsa/lin09EAGEdsa.pdf }
}

@CONFERENCE{lin09SEGucs,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Unified compressive sensing framework for simultaneous acquisition
	with primary estimation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  number = {1},
  pages = {3113-3117},
  organization = {SEG},
  publisher = {SEG},
  abstract = {The central promise of simultaneous acquisition is a vastly improved
	crew efficiency during acquisition at the cost of additional post-processing
	to obtain conventional source-separated data volumes. Using recent
	theories from the field of compressive sensing, we present a way
	to systematically model the effects of simultaneous acquisition.
	Our formulation form a new framework in the study of acquisition
	design and naturally leads to an inversion-based approach for the
	separation of shot records. Furthermore, we show how other inversion-based
	methods, such as a recently proposed method from van Groenestijn
	and Verschuur (2009) for primary estimation, can be processed together
	with the demultiplexing problem to achieve a better result compared
	to a separate treatment of these problems.},
  keywords = {SEG},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg09/lin09segucs.pdf
	},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/lin09SEGucs/lin09SEGucs.pdf }
}

@CONFERENCE{lin09DELPHIrwi,
  author = {Tim T.Y. Lin and Felix J. Herrmann and Yogi A. Erlangga},
  title = {Randomized wavefield inversion presented at the DELPHI meeting.The
	Hague.},
  year = {2009},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Delphi2009.pdf}
}

@CONFERENCE{saab09SAMPTAnccs,
  author = {Rayan Saab and Ozgur Yilmaz},
  title = {A short note on non-convex compressed sensing},
  year = {2009},
  organization = {SAMPTA},
  publisher = {SAMPTA},
  abstract = {In this note, we summarize the results we recently proved in\cite{SY08}
	on the theoretical performance guarantees of the decoders $øÓ_p$.
	These decoders rely on $\ell^p$ minimization with $p {\i}n (0,1)$
	to recover estimates of sparse and compressible signals from incomplete
	and inaccurate measurements. Our guarantees generalize the results
	of \cite{CRT05} and \cite{Wojtaszczyk08} about decoding by $\ell_p$
	minimization with $p=1$, to the setting where $p {\i}n (0,1)$ and
	are obtained under weaker sufficient conditions. We also present
	novel extensions of our results in \cite{SY08} that follow from the
	recent work of DeVore et al. in \cite{DPW08}. Finally, we show some
	insightful numerical experiments displaying the trade-off in the
	choice of $p {\i}n (0,1]$ depending on certain properties of the
	input signal.},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SAMPTA/2009/saab09SAMPTAnccs/saab09SAMPTAnccs.pdf }
}

@CONFERENCE{shahidi09SEGcmf,
  author = {Reza Shahidi and Felix J. Herrmann},
  title = {Curvelet-domain matched filtering with frequency-domain regularization},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  number = {1},
  pages = {3645},
  organization = {SEG},
  publisher = {SEG},
  abstract = {In Herrmann et al. (2008), it is shown that zero-order pseudodifferential
	operators, which model the migration-demigration operator and the
	operator mapping the predicted multiples to the true multiples, can
	be represented by a diagonal weighting in the curvelet domain. In
	that paper, a smoothness constraint was introduced in the phase space
	of the operator in order to regularize the solution to make it unique.
	In this paper, we use recent results in Demanet and Ying (2008) on
	the discrete symbol calculus to impose a further smoothness constraint,
	this time in the frequency domain. It is found that with this additional
	constraint, faster convergence is realized. Results on a synthetic
	pseudodifferential operator as well as on an example of primary-multiple
	separation in seismic data are included, comparing the model with
	and without the new smoothness constraint, from which it is found
	that results of improved quality are also obtained.},
  keywords = {SEG},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/shahidi09SEGcmf/shahidi09segcmf.pdf }
}

@CONFERENCE{tang09SEGhdb,
  author = {Gang Tang and Reza Shahidi and Felix J. Herrmann and Jianwei Ma},
  title = {Higher dimensional blue-noise sampling schemes for curvelet-based
	seismic data recovery},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  number = {1},
  pages = {191-195},
  organization = {SEG},
  publisher = {SEG},
  abstract = {In combination with compressive sensing, a successful reconstruction
	scheme called Curvelet-based Recovery by Sparsity-promoting Inversion
	(CRSI) has been developed, and has proven to be useful for seismic
	data processing. One of the most important issues for CRSI is the
	sampling scheme, which can greatly affect the quality of reconstruction.
	Unlike usual regular undersampling, stochastic sampling can convert
	aliases to easy-to-eliminate noise. Some stochastic sampling methods
	have been developed for CRSI, e.g. jittered sampling, however most
	have only been applied to 1D sampling along a line. Seismic datasets
	are usually higher dimensional and very large, thus it is desirable
	and often necessary to develop higher dimensional sampling methods
	to deal with these data. For dimensions higher than one, few results
	have been reported, except uniform random sampling, which does not
	perform well. In the present paper, we explore 2D sampling methodologies
	for curvelet-based reconstruction, possessing sampling spectra with
	blue noise characteristics, such as Poisson Disk sampling, Farthest
	Point Sampling, and the 2D extension of jittered sampling. These
	sampling methods are shown to lead to better recovery and results
	are compared to the other more traditional sampling protocols.},
  keywords = {SEG},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/tang09SEGhdb/tang09SEGhdb.pdf }
}

@CONFERENCE{vandenberg08SLIMocf,
  author = {E. van den Berg and Mark Schmidt and Michael P. Friedlander and K.
	Murphy},
  title = {Optimizing Costly Functions with Simple Constraints: A Limited-Memory
	Projected Quasi-Newton Algorithm},
  year = {2009},
  volume = {12},
  series = {Twelfth International Conference on Artificial Intelligence and Statistics},
  month = {April},
  abstract = {An optimization algorithm for minimizing a smooth function over a
	convex set is described. Each iteration of the method computes a
	descent direction by minimizing, over the original constraints, a
	diagonal-plus low-rank quadratic approximation to the function. The
	quadratic approximation is constructed using a limited-memory quasi-Newton
	update. The method is suitable for large-scale problems where evaluation
	of the function is substan- tially more expensive than projection
	onto the constraint set. Numerical experiments on one- norm regularized
	test problems indicate that the proposed method is competitve with
	state- of-the-art methods such as bound-constrained L-BFGS and orthant-wise
	descent. We further show that the method generalizes to a wide class
	of problems, and substantially improves on state-of-the-art methods
	for problems such as learning the structure of Gaussian graphi- cal
	models (involving positive-definite matrix constraints) and Markov
	random fields (in- volving second-order cone constraints).},
  date-added = {2009-01-29 17:16:34 -0800},
  date-modified = {2009-01-29 17:16:34 -0800},
  keywords = {SLIM},
  pdf = {http://www.cs.ubc.ca/~mpf/public/group.pdf}
}

@CONFERENCE{yan09SEGgpb,
  author = {Jiupeng Yan and Felix J. Herrmann},
  title = {Groundroll prediction by interferometry and separation by curvelet-domain
	matched filtering},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  number = {1},
  pages = {3297-3301},
  organization = {SEG},
  publisher = {SEG},
  abstract = {The removal of groundroll in land based seismic data is a critical
	step for seismic imaging. In this paper, we introduce a work flow
	to predict the groundroll by interferometry and then separate the
	groundroll in the curvelet domain. Thus workflow is similar to the
	workflow of surface-related multiple elimination (SRME). By exploiting
	the adaptability and sparsity of curvelets, we are able to significantly
	improve the separation of groundroll in comparison to results yielded
	by frequency-domain adaptive subtraction methods. We provide synthetic
	data example to illustrate our claim.},
  keywords = {SEG},
  presentation = {http://slim.eos.ubc.ca/Publications/public/presentations/seg/seg09/yan09seggpi.pdf},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/yan09SEGgpb/yan09SEGgpb.pdf }
}

@CONFERENCE{yan09SEGgpb2,
  author = {Jiupeng Yan and Felix J. Herrmann},
  title = {Groundroll prediction by interferometry and separation by curvelet-domain
	filtering. Presented at the 79th SEG Meeting, Houston},
  year = {2009},
  abstract = {The removal of groundroll in land based seismic data is a critical
	step for seismic imaging. In this paper, we introduce a work flow
	to predict the groundroll by interferometry and then separate the
	groundroll in the curvelet domain. Thus workflow is similar to the
	workflow of surface-related multiple elimination (SRME). By exploiting
	the adaptability and sparsity of curvelets, we are able to significantly
	improve the separation of groundroll in comparison to results yielded
	by frequency-domain adaptive subtraction methods. We provide synthetic
	data example to illustrate our claim.},
  url = {http://slim.eos.ubc.ca/Publications/public/presentations/seg/seg09/yan09seggpi.pdf}
}

%--------------------------------------2008------------------------------
@CONFERENCE{Berg08SINBADsat,
  author = {E. van den Berg},
  title = {Sparco: A testing framework for sparse reconstruction},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {Sparco is a framework for testing and benchmarking algorithms for
	sparse reconstruction. It includes a large collection of sparse reconstruction
	problems drawn from the imaging, compressed sensing, and geophysics
	literature. Sparco is also a framework for implementing new test
	problems and can be used as a tool for reproducible research. We
	describe the software environment, and demonstrate its usefulness
	for testing and comparing solvers for sparse reconstruction.},
  date-modified = {2008-08-22 12:54:25 -0700},
  keywords = {SLIM, SINBAD, Presentation},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Ewout_Spar.pdf}
}

@CONFERENCE{erlangga08SEGaim,
  author = {Yogi A. Erlangga and Felix J. Herrmann},
  title = {An iterative multilevel method for computing wavefields in frequency-domain
	seismic inversion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  number = {1},
  pages = {1957-1960},
  month = {November},
  organization = {SEG},
  publisher = {SEG},
  abstract = {We describe an iterative multilevel method for solving linear systems
	representing forward modeling and back propagation of wavefields
	in frequency-domain seismic inversions. The workhorse of the method
	is the so-called multilevel Krylov method, applied to a multigrid-preconditioned
	linear system, and is called multigrid-multilevel Krylov (MKMG) method.
	Numerical experiments are presented for 2D Marmousi synthetic model
	for a range of frequencies. The convergence of the method is fast,
	and depends only mildly on frequency. The method can be considered
	as the first viable alternative to LU factorization, which is practically
	prohibitive for 3D seismic inversions.},
  keywords = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg08/erlangga08imm.pdf},
url= { https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/erlangga08SEGaim/erlangga08SEGaim.pdf }

}

@CONFERENCE{erlangga08SINBADimf,
  author = {Yogi A. Erlangga and K. Vuik and K. Oosterlee and D. Riyanti and
	R. Nabben},
  title = {Iterative methods for 2D/3D Helmholtz operator},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We present an iterative method for solving the 2D/3D Helmholtz equation.
	The method is mainly based on a Krylov method, preconditioned by
	a special operator which represents a damped Helmholtz operator.
	The discretization of the preconditioning operator is then solved
	by one multigrid sweep. It can be shown that while the spectrum is
	bounded above by one, the smallest eigenvalue of the preconditioned
	system is of order $k^{-1}$. In this situation, the convergence of
	a Krylov method will be proportional to the frequency of the problem.
	Further convergence acceleration can be achieved if eigenvalues of
	order $k^{-1}$ are projected from the spectrum. This can be done
	by a projection operator, similar to but more stable than deflation.
	This projection operator has been the core of a new multilevel method,
	called multilevel Krylov method, proposed by Erlangga and Nabben
	only recently. Putting the preconditioned Helmholtz operator in this
	setting, a convergence which is independent of frequency can be obtained.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Erlangga_Ite.pdf}
}

@CONFERENCE{eso08SEGira,
  author = {R. A. Eso and S. Napier and Felix J. Herrmann and D. W. Oldenburg},
  title = {Iterative reconstruction algorithm for non-linear operators},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  number = {1},
  pages = {579-583},
  month = {November},
  organization = {SEG},
  publisher = {SEG},
  abstract = {Iterative soft thresholding of a models wavelet coefficients can be
	used to obtain models that are sparse with respect to a known basis
	function. We generate sparse models for non-linear forward operators
	by applying the soft thresholding operator to the model obtained
	through a Gauss-Newton iteration and apply the technique in a synthetic
	2.5D DC resistivity crosswell tomographic example.},
  keywords = {SLIM, SEG},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/eso08SEGira/eso08SEGira.pdf }
}

@CONFERENCE{friedlander08SINBADafl,
  author = {Michael P. Friedlander},
  title = {Algorithms for Large-Scale Sparse Reconstruction},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {Many signal processing applications seek to approximate a signal as
	a linear combination of only a few elementary atoms drawn from a
	large collection. This is known as sparse reconstruction, and the
	theory of compressed sensing allows us to pose it as a structured
	convex optimization problem. I will discuss the role of duality in
	revealing some unexpected and useful properties of these problems,
	and will show how they can lead to practical, large-scale algorithms.
	I will also describe some applications of these algorithms.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Michael_Alg.pdf}
}

@CONFERENCE{friedlander08SIAMasa,
  author = {Michael P. Friedlander},
  title = {Active-set Approaches to Basis Pursuit Denoising},
  booktitle = {SIAM Optimization},
  year = {2008},
  month = {May},
  organization = {SIAM Optimization},
  publisher = {SIAM Optimization},
  file = {:http\://www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf:PDF},
  keywords = {Presentation, SLIM},
  url = {http://www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf}
}

@CONFERENCE{friedlander08WCOMasm,
  author = {Michael P. Friedlander and M. A. Saunders},
  title = {Active-set methods for basis pursuit},
  organization = {West Coast Opitmization Meeting (WCOM)},
  year = {2008},
  month = {September},
  abstract = {Many imaging and compressed sensing applications seek sparse solutions
	to large under-determined least-squares problems. The basis pursuit
	(BP) approach minimizes the 1-norm of the solution, and the BP denoising
	(BPDN) approach balances it against the least-squares fit. The duals
	of these problems are conventional linear and quadratic programs.
	We introduce a modified parameterization of the BPDN problem and
	explore the effectiveness of active-set methods for solving its dual.
	Our basic algorithm for the BP dual unifies several existing algorithms
	and is applicable to large-scale examples.},
  file = {:http\://www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf:PDF},
  url = {http://www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf}
}

@CONFERENCE{hennenfent08SINBADnii2,
  author = {Gilles Hennenfent},
  title = {New insights into one-norm solvers from the Pareto curve},
  year = {2008},
  abstract = {Several geophysical ill-posed inverse problems are successfully solved
	by promoting sparsity using one-norm regularization. The practicality
	of this approach depends on the effectiveness of the one-norm solver
	used and on its robustness under limited number of iterations. We
	propose an approach to understand the behavior and evaluate the performance
	of one-norm solvers. The technique consists of tracking on a graph
	the data misfit versus the one norm of successive iterates. By comparing
	the solution paths to the Pareto curve, we are able to assess the
	performance of the solvers and the quality of the solutions. Such
	an assessment is particularly relevant given the renewed interest
	in one-norm regularization.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Gilles_New.pdf}
}

@CONFERENCE{hennenfent08SEGonri,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {One-norm regularized inversion: learning from the Pareto curve},
  year = {2008},
  organization = {SEG},
  publisher = {UBC Earth and Ocean Sciences},
  abstract = {Geophysical inverse problems typically involve a trade off between
	data misfit and some prior. Pareto curves trace the optimal trade
	off between these two competing aims. These curves are commonly used
	in problems with two-norm priors where they are plotted on a log-log
	scale and are known as L-curves. For other priors, such as the sparsity-promoting
	one norm, Pareto curves remain rela- tively unexplored. First, we
	show how these curves provide an objective criterion to gauge how
	robust one-norm solvers are when they are limited by a maximum number
	of matrix-vector products that they can perform. Second, we use Pareto
	curves and their properties to define and compute one-norm compressibilities.
	We argue this notion is key to understand one-norm regularized inversion.
	Third, we illustrate the correlation between the one-norm compressibility
	and the perfor- mance of Fourier and curvelet reconstructions with
	sparsity promoting inversion.},
  keywords = {SEG},
url= { https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/hennenfent08SEGonri/hennenfent08SEGonri.pdf }
}



@CONFERENCE{hennenfent08SINBADsdw2,
  author = {Gilles Hennenfent},
  title = {Simply denoise: wavefield reconstruction via jittered undersampling},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We present a new discrete undersampling scheme designed to favor wavefield
	reconstruction by sparsity-promoting inversion with transform elements
	that are localized in the Fourier domain. Our work is motivated by
	empirical observations in the seismic community, corroborated by
	recent results from compressive sampling, which indicate favorable
	(wavefield) reconstructions from random as opposed to regular undersampling.
	As predicted by theory, random undersampling renders coherent aliases
	into harmless incoherent random noise, effectively turning the interpolation
	problem into a much simpler denoising problem. A practical requirement
	of wavefield reconstruction with localized sparsifying transforms
	is the control on the maximum gap size. Unfortunately, random undersampling
	does not provide such a control and the main purpose of this paper
	is to introduce a sampling scheme, coined jittered undersampling,
	that shares the benefits of random sampling, while offering control
	on the maximum gap size. Our contribution of jittered sub-Nyquist
	sampling proofs to be key in the formulation of a versatile wavefield
	sparsity-promoting recovery scheme that follows the principles of
	compressive sampling. After studying the behavior of the jittered-undersampling
	scheme in the Fourier domain, its performance is studied for curvelet
	recovery by sparsity-promoting inversion (CRSI). Our findings on
	synthetic and real seismic data indicate an improvement of several
	decibels over recovery from regularly-undersampled data for the same
	amount of data collected.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Gilles_jit.pdf}
}

@CONFERENCE{herrmann08SEGcdm,
 title = {Curvelet-domain matched filtering},
 author = {Felix J. Herrmann and Peyman P. Moghaddam and Deli Wang},
 month = {August},
 year = {2008},
 abstract = {Matching seismic wavefields and images lies at the heart
of many pre-/post-processing
	steps part of seismic imaging{\textendash}- whether one is matching
	predicted wavefield components, such as multiples, to the actual
	to-be-separated wavefield components present in the data or whether
	one is aiming to restore migration amplitudes by scaling, using an
	image-to-remigrated-image matching procedure to calculate the scaling
	coefficients. The success of these wavefield matching procedures
	depends on our ability to (i) control possible overfitting, which
	may lead to accidental removal of energy or to inaccurate image-amplitude
	corrections, (ii) handle data or images with nonunique dips, and
	(iii) apply subsequent wavefield separations or migraton amplitude
	corrections stably. In this paper, we show that the curvelet transform
	allows us to address all these issues by imposing smoothness in phase
	space, by using their capability to handle conflicting dips, and
	by leveraging their ability to represent seismic data and images
	sparsely. This latter property renders curvelet-domain sparsity promotion
	an effective prior.},
 keywords = {SLIM, SEG},
 number = {TR-2008-6},
organization={SEG},
 presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg08/herrmann08cmf.pdf},
 publisher = {UBC Earth and Ocean Sciences Department},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/herrmann08SEGcdm/herrmann08SEGcdm.pdf }
}


@CONFERENCE{herrmann08SINBADacd2,
  author = {Felix J. Herrmann},
  title = {Adaptive curvelet-domain primary-multiple separation},
  organization = {SINBAD},
  year = {2008},
  note = {SINBAD 2008},
  abstract = {In many exploration areas, successful separation of primaries and
	multiples greatly determines the quality of seismic imaging. Despite
	major advances made by Surface-Related Multiple Elimination (SRME),
	amplitude errors in the predicted multiples remain a problem. When
	these errors vary for each type of multiple differently (as a function
	of offset, time and dip), these amplitude errors pose a serious challenge
	for conventional least-squares matching and for the recently introduced
	separation by curvelet-domain thresholding. We propose a data-adaptive
	method that corrects amplitude errors, which vary smoothly as a function
	of location, scale (frequency band) and angle. In that case, the
	amplitudes can be corrected by an element-wise curvelet-domain scaling
	of the predicted multiples. We show that this scaling leads to a
	successful estimation of the primaries, despite amplitude, sign,
	timing and phase errors in the predicted multiples. Our results on
	synthetic and real data show distinct improvements over conventional
	least-squares matching, in terms of better suppression of multiple
	energy and high-frequency clutter and better recovery of the estimated
	primaries.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Felix_Ada.pdf}
}


@CONFERENCE{Herrmann08SEGcdm3,
  author = {Felix J. Herrmann},
  title = {Curvelet-domain matched filtering},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  number = {1},
  pages = {3643-3647},
  month = {November},
  organization = {SEG},
  publisher = {SEG},
  abstract = {Matching seismic wavefields lies at the heart of seismic processing
	whether one is adaptively subtracting multiples predictions or groundroll.
	In both cases, the predictions are matched to the actual to-be-separated
	wavefield components in the observed data. The success of these wavefield
	matching procedures depends on our ability to (i) control possible
	overfitting, which may lead to accidental removal of primary energy,
	(ii) handle data with nonunique dips, and (iii) apply wavefield separation
	after matching stably. In this paper, we show that the curvelet transform
	allows us to address these issues by imposing smoothness in phase
	space, by using their capability to handle conflicting dips, and
	by leveraging their ability to represent seismic data sparsely.},
  keywords = {SEG, SLIM},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/Herrmann08SEGcdm3/Herrmann08SEGcdm3.pdf }
}

@CONFERENCE{herrmann08IONcsa,
  author = {Felix J. Herrmann},
  title = {Compressive sampling: a new paradigm for seismic data acquistion
	and processing?},
  booktitle = {ION},
  year = {2008},
  abstract = {Seismic data processing and imaging are firmly rooted in the well-established
	paradigm of regular Nyquist sampling. Faced with a typical uncooperative
	environment, practitioners of seismic data acquisition make all efforts
	to comply to this theory by creating regularly-sampled seismic-data
	volumes that are suitable for Fourier-based processing flows. The
	current advent of new alternative transform domains{\textendash}-
	such as the sparsifying curvelet domain, where seismic data is decomposed
	into localized, multiscale and multidirectional plane waves{\textendash}-
	opens the possibility to change this paradigm by no longer combating
	sampling irregularity but by embracing it. During this talk, we show
	that as long as seismic data volumes permit a compressible representation{\textendash}-i.e.,
	data can be represented as a superposition of relatively few number
	of elementary waveforms{\textendash}- Nyquist sampling is unnecessary
	pessimistic. So far, nothing new, we all know from the work on Fourier-
	or other transform-based seismic-data regularization methodologies
	that wavefields can be recovered accurately from sub-Nyquist samplings
	through some sort of optimization procedure. What is new, however,
	are recent insights from the field of "compressive sampling", which
	dictate the conditions that guarantee or, at least, in practice provide
	conditions that favor sparsity-promoting recovery from sub-Nyquist
	sampling. Random sub-sampling, or to be more precise, jitter sub-sampling
	creates favorable conditions for curvelet-based recovery. We explain
	this phenomenon by arguing that this type of sampling leads to noisy
	data, hence our slogan "Simply denoise: wavefield reconstruction
	via jittered undersampling", where we bank on separating incoherent
	sub-sampling noise with curvelet-domain sparsity promotion. During
	our presentation, we introduce you to what curvelets are, why random
	jitter sampling is important and why this opens a pathway towards
	a new paradigm of curvelet-domain seismic data processing. Our claims
	will be supported by examples on synthetic and field data. This is
	joint work with Gilles Hennenfent, PhD. student at SLIM.},
  keywords = {ION, Presentation, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2008/herrmann08ion_pres.pdf}
}

@CONFERENCE{herrmann08SINBADfwr,
  author = {Felix J. Herrmann},
  title = {(De)-Focused wavefield reconstructions},
  booktitle = {SINBAD 2008},
  year = {2008},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Felix_Dfo.pdf}
}

@CONFERENCE{herrmann08SEGgbu,
  author = {Felix J. Herrmann},
  title = {Seismic noise: the good, the bad, \& the ugly},
  booktitle = {SEG},
  year = {2008},
  keywords = {Presentation, SEG, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg08/herrmann08gbu.pdf}
}

@CONFERENCE{herrmann08SINBADpsm,
  author = {Felix J. Herrmann},
  title = {Phase-space matched filtering and migration preconditioning},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {During this talk, I will report on new phase-space regularization
	functionals defined in terms of splines. This spline representation
	reduces the dimensionality of estimating our phase-space matched
	filter. We will discuss how this filter can be used in migration
	preconditioning. This is joint work with Christiaan Stolk.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Felix_Pha.pdf}
}

@CONFERENCE{herrmann08SINBADs2c,
  author = {Felix J. Herrmann},
  title = {SINBAD 2008 Consortium meeting},
  booktitle = {SINBAD 2008},
  year = {2008},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Felix_Ope.pdf}
}

@CONFERENCE{Herrmann08SIAMcsm,
  author = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title = {Compressive sampling meets seismic imaging},
  booktitle = {SIAM},
  year = {2008},
  abstract = {Compressive sensing has led to fundamental new insights in the recovery
	of compressible signals from sub-Nyquist samplings. It is shown how
	jittered subsampling can be used to create favorable recovery conditions.
	Applications include mitigation of incomplete acquisitions and wavefield
	computations. While the former is a direct adaptation of compressive
	sampling, the latter application represents a new way of compressing
	wavefield extrapolation operators. Operators are not diagonalized
	but are compressively sampled reducing the computational costs.},
  keywords = {Presentation, SIAM, SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2008/herrmann08siam08.pdf}
}

@CONFERENCE{herrmann08SINBADitc,
  author = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin and Cody
	R. Brown},
  title = {Introduction to compressive (wavefield) computation},
  booktitle = {SINBAD 2008},
  year = {2008},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Felix_Int.pdf}
}

@CONFERENCE{herrmann08SEGswi,
  author = {Felix J. Herrmann and Deli Wang},
  title = {Seismic wavefield inversion with curvelet-domain sparsity promotion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  number = {1},
  pages = {2497-2501},
  month = {November},
  organization = {SEG},
  publisher = {SEG},
  abstract = {Inverting seismic wavefields lies at the heart of seismic data processing
	and imaging{\textendash}- whether one is applying {\textquoteleft}{\textquoteleft}a
	poor man{\textquoteright}s inverse{\textquoteright}{\textquoteright}
	by correlating wavefields during imaging or whether one inverts wavefields
	as part of a focal transform interferrometric deconvolution or as
	part of computing the {\textquoteright}data verse{\textquoteright}.
	The success of these wavefield inversions depends on the stability
	of the inverse with respect to data imperfections such as finite
	aperture, bandwidth limitation, and missing data. In this paper,
	we show how curvelet domain sparsity promotion can be used as a suitable
	prior to invert seismic wavefields. Examples include, seismic data
	regularization with the focused curvelet-based recovery by sparsity-promoting
	inversion (fCRSI), which involves the inversion of the primary-wavefield
	operator, the prediction of multiples by inverting the adjoint of
	the primary operator, and finally the inversion of the data itself
	{\textendash}- the so-called {\textquoteright}data inverse{\textquoteright}.
	In all cases, curvelet-domain sparsity leads to a stable inversion.},
  keywords = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg08/herrmann08swi.pdf
	},
url={https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/herrmann08SEGswi/herrmann08SEGswi.pdf }
}

@CONFERENCE{johnson08SINBADsdi,
  author = {James Johnson and Gilles Hennenfent},
  title = {Seismic Data Interpolation with Symmetry},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {Due to the physics of reciprocity seismic data sets are symmetric
	in the source and receiver coordinates. Often seismic data sets are
	incomplete and the missing data must be interpolated. Typically,
	missing traces do not occur symmetrically. The purpose of this project
	is to extend the current formulation for solving the seismic interpolation
	problems in such a way that they enforce reciprocity. The method
	decomposes the seismic data volume into symmetric and antisymmetric
	parts. This decomposition leads to an augmented system of equations
	for the L1-solver that promotes sparsity in the curvelet domain.
	Interpolation is carried out on the entire system during which the
	asymmetric component of the volume is forced to zero, while the symmetric
	part of the data volume is matched to the measured data.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_James_Sei.pdf}
}

@CONFERENCE{kumar08SINBADcd,
  author = {Vishal Kumar},
  title = {Curvelet Denoising},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {The separation of signal and noise is an important issue in seismic
	data processing. By noise we refer to the incoherent noise which
	is present in the data. In our case, we showed curvelets concentrate
	seismic signal energy in few significant coefficients unlike noise
	energy that is spread all over the coefficients. The sparsity of
	seismic data in the curvelet domain makes curvelets an ideal choice
	for separating the noise from the seismic data. In our approach the
	denoising problem is framed as curvelet-regularized inversion problem.
	After initial processing, we applied the algorithm to the poststack
	data and compared our results with conventional wavelet denoising.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Vishal_Den.pdf}
}

@CONFERENCE{kumar08SINBADcrd,
  author = {Vishal Kumar},
  title = {Curvelet-Regularized Deconvolution},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {The removal of source signature from seismic data is an important
	step in seismic data processing. The Curvelet transform provides
	sparse representations for images that comprise smooth objects separated
	by piece-wise smooth discontinuities (e.g. seismic reflectivity).
	In this approach the sparseness of reflectivity in Curvelet domain
	is used as a prior to stabilize the inversion process. Our Curvelet-regularized
	deconvolution algorithm uses recently developed SPGL1 solver which
	does adaptive sampling of the trade-off curve. We applied the algorithm
	on a synthetic example and compared our results with that of Spiky
	deconvolution approach.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Vishal_Dec.pdf}
}

@CONFERENCE{kumar08CSEGcrs,
  author = {Vishal Kumar and Felix J. Herrmann},
  title = {Curvelet-regularized seismic deconvolution},
  booktitle = {CSEG Technical Program Expanded Abstracts},
  year = {2008},
  month = {May},
  organization = {CSEG},
  publisher = {CSEG},
  abstract = {There is an inherent continuity along reflectors of a seismic image.
	We use the recently introduced multiscale and multidirectional curvelet
	transform to exploit this continuity along reflectors for cases in
	which the assumption of spiky reflectivity may not hold. We show
	that such type of seismic reflectivity can be represented in the
	curvelet-domain by a vector whose entries decay rapidly. This curvelet-domain
	compression of reflectivity opens new perspectives towards solving
	classical problems in seismic processing including the deconvolution
	problem. In this paper, we present a formulation that seeks curvelet-domain
	sparsity for non-spiky reflectivity and we compare our results with
	those of spiky deconvolution.},
  keywords = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/cseg/kumar08crs.pdf
	},
  url = {http://slim.eos.ubc.ca/Publications/Private/Conferences/CSEG/2008/CSEG2008_Vishal.pdf}
}

@CONFERENCE{kumar08SEGdwc,
  author = {Vishal Kumar and Felix J. Herrmann},
  title = {Deconvolution with curvelet-domain sparsity},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  number = {1},
  pages = {1996-2000},
  month = {November},
  organization = {SEG},
  publisher = {SEG},
  abstract = {There is an inherent continuity along reflectors of a seismic image.
	We use the recently introduced multiscale and multidirectional curvelet
	transform to exploit this continuity along reflectors for cases in
	which the assumption of spiky reflectivity may not hold. We show
	that such type of seismic reflectivity can be represented in the
	curvelet-domain by a vector whose entries decay rapidly. This curvelet-domain
	compression of reflectivity opens new perspectives towards solving
	classical problems in seismic processing including the deconvolution
	problem. In this paper, we present a formulation that seeks curvelet-domain
	sparsity for non-spiky reflectivity and we compare our results with
	those of spiky deconvolution.},
  keywords = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg08/kumar08dcs.pdf
	},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/kumar08SEGdwc/kumar08SEGdwc.pdf }
}

@CONFERENCE{lebed08SINBADaoc,
  author = {Evgeniy Lebed},
  title = {Curvelet / Surfacelet comparison},
  booktitle = {SINBAD},
  year = {2008},
  abstract = {Curvelets and Surfacelets are two transforms that aim to achieve a
	multiscale and a multidirectional decomposition of arbitrary N-dimensional
	($N>=2$) signals. While both transforms are Fourier-based, their
	construction is intrinsically different. In this talk we will give
	and overview of the construction of the two transforms, and explore
	their properties such as frequency domain / spatial domain coherence,
	sparsity, redundancy and computational complexity.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Evgeniy_Curv.pdf}
}


@CONFERENCE{lebed08SEGhggt,
author= { Evgeniy Lebed and Felix J. Herrmann},
title={ A hitchhiker{\textquoteright}s guide to the galaxy of transform-domain sparsification},
year = {2008},
  organization = {SEG},
  publisher = {SEG},
  abstract = {The ability to efficiently and sparsely represent seismic data is
	becoming an increasingly important problem in geophysics. Over the
	last decade many transforms such as wavelets, curvelets, contourlets,
	surfacelets, shearlets, and many other types of {\textquoteright}x-lets{\textquoteright} have been
	developed to try to resolve this issue. In this abstract we compare
	the properties of four of these commonly used transforms, namely
	the shift-invariant wavelets, complex wavelets, curvelets and surfacelets.
	We also briefly explore the performance of these transforms for the
	problem of recovering seismic wavefields from incomplete measurements.},
  keywords = {SEG},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/lebed08SEGhggt/lebed08SEGhggt.pdf}
}


@CONFERENCE{lebed08SINBADaoc1,
  author = {Evgeniy Lebed},
  title = {Applications of Curvelets/Surfacelets to seismic data processing},
  booktitle = {SINBAD},
  year = {2008},
  abstract = {In this talk we explore several applications of the curvelet and surfacelet
	transforms to seismic data processing. The first application is stable
	signal recovery in the physical domain - seismic data acquisition
	is often limited by physical and economic constraints, and the goal
	is to interpolate the data from a given subset of seismic traces.
	The second application is signal recovery in a transform domain -
	we assume that our data comes in a form of a random subset of temporal
	frequencies and the goal is to recover the missing frequencies from
	this data. Since seismic signals are generally not bandwidth limited,
	this in fact becomes an anti-aliasing problem. In both these problems
	the recovery is resolved via a robust l_1 solver that exploits the
	sparsity of the signals in curvelet/surfacelet domains. In the last
	application we explore the problem of primary-multiple separation
	by simple thresholding.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Evgeniy_App.pdf}
}

@CONFERENCE{lin08SINBADcwe,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Compressed wavefield extrapolation},
  booktitle = {SINBAD},
  year = {2008},
  keywords = {Presentation, SINBAD, SLIM},
presentation ={ https://www.slim.eos.ubc.ca/sites/data/Papers/lin08cwe.pdf}
}

@CONFERENCE{lin08SEGiso,
  author = {Tim T.Y. Lin and Evgeniy Lebed and Yogi A. Erlangga and Felix J.
	Herrmann},
  title = {Interpolating solutions of the Helmholtz equation with compressed
	sensing},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  number = {1},
  pages = {2122-2126},
  month = {November},
  organization = {SEG},
  publisher = {SEG},
  abstract = {We present an algorithm which allows us to model wavefields with frequency-domain
	methods using a much smaller number of frequencies than that typically
	required by the classical sampling theory in order to obtain an alias-free
	result. The foundation of the algorithm is the recent results on
	the compressed sensing, which state that data can be successfully
	recovered from an incomplete measurement if the data is sufficiently
	sparse. Results from numerical experiment show that only 30\% of
	the total frequency spectrum is need to capture the full wavefield
	information when working in the hard 2D synthetic Marmousi model.},
  keywords = {SLIM, SEG},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg08/lin08ish.pdf
	},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/lin08SEGiso/lin08SEGiso.pdf } 
}

@CONFERENCE{maysami08SEGlcf,
  author = {Mohammad Maysami and Felix J. Herrmann},
  title = {Lithological constraints from seismic waveforms: application to opal-A
	to opal-CT transition},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  number = {1},
  pages = {2011-2015},
  month = {November},
  organization = {SEG},
  publisher = {SEG},
  abstract = {n this paper, we present a new method for seismic waveform characterization
	whose aim is threefold, namely (i) extraction of detailed information
	on the sharpness of transitions in the subsurface from seismic waveforms,
	(ii) reflector modeling, based on binary-mixture and percolation
	theory, and (iii) establishment of well-seismic ties, through parameterizations
	of our waveform and critical reflector model. We test this methodology
	on the opal-A (Amorphous) to opal-CT (Cristobalite/Tridymite) transition
	imaged in a migrated section of North Sea field data West of the
	Shetlands.},
  keywords = {SEG, SLIM},
url={https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/maysami08SEGlcf/maysami08SEGlcf.pdf }
}

@CONFERENCE{modzelewski08SINBADdas,
  author = {Henryk Modzelewski},
  title = {Design and specifications for SLIM{\textquoteright}s software framework},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {The SLIM group is actively developing software for seismic imaging.
	This talk will give a general overview of the software development
	during SINBAD project with focus on the final release in February
	2008. The covered topics will include: 1) adopting Python for object-oriented
	programming, 2) including parallelism into the algorithms used in
	seismic imaging/modeling, 3) in-house algorithms for seismic imaging,
	and 4) contributions to Madagascar (RSF). The talk will serve as
	an introduction to the other presentations in the session "SINBAD
	Software releases".},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Henryk_Des.pdf}
}

@CONFERENCE{moghaddam08SINBADrtm,
  author = {Peyman P. Moghaddam},
  title = {Reverse-time Migration Amplitude Recovery with Curvelets},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We recover the amplitude of a seismic image by approximating the normal
	(demigration-migration) operator. In this approximation, we make
	use of the property that curvelets remain invariant under the action
	of the normal operator. We propose a seismic amplitude recovery method
	that employs an eigenvalue like decomposition for the normal operator
	using curvelets as eigenvectors. Subsequently, we propose an approximate
	nonlinear singularity-preserving solution to the least-squares seismic
	imaging problem with sparseness in the curvelet domain and spatial
	continuity constraints. Our method is tested with a reverse-time
	{\textquoteright}wave-equation{\textquoteright} migration code simulating
	the acoustic wave equation.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Peyman_Mig.pdf}
}

@CONFERENCE{moghaddam08SEGcbm,
  author = {Peyman P. Moghaddam and Cody R. Brown and Felix J. Herrmann},
  title = {Curvelet-based migration preconditioning},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  number = {1},
  pages = {2211-2215},
  month = {November},
  organization = {SEG},
  publisher = {SEG},
  abstract = {In this paper, we introduce a preconditioner for seismic imaging{\textendash}-i.e.,
	the inversion of the linearized Born scattering operator. This preconditioner
	approximately corrects for the {\textquoteleft}{\textquoteleft}square
	root{\textquoteright}{\textquoteright} of the normal{\textendash}-i.e.,
	the demigration-migration operator. This approach consists of three
	parts, namely (i) a left preconditoner, defined by a fractional time
	integration designed to make the migration operator zero order, and
	two right preconditioners that apply (ii) a scaling in the physical
	domain accounting for a spherical spreading, and (iii) a curvelet-domain
	scaling that corrects for spatial and reflector-dip dependent amplitude
	errors. We show that a combination of these preconditioners lead
	to a significant improvement of the convergence for iterative least-squares
	solutions to the seismic imaging problem based on reverse-time migration
	operators.},
  keywords = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg08/brown08cmp.pdf
	},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/moghaddam08SEGcbm/moghaddam08SEGcbm.pdf }
}

@CONFERENCE{ross08SINBADsit,
  author = {Sean Ross-Ross},
  title = {Seismic inversion through operator overloading},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {Geophysical processing is dominated by many different out of core
	memory software environments (OOCE). Such environments include Madagascar
	and SU and are designed to handle data that can not be operated on
	in memory. Each base operation is created as a main program that
	reads data from disk and writes the result to disk. The main programs
	can also be chained together on stdin/out pipes using a shell only
	writing data to disk at the end. To be efficient, the algorithm using
	an OOCE must chain together the longest pipe to avoid disk I/O, as
	a result it is very difficult to use iterative techniques. The algorithms
	are written in shell scripts can be difficult to read and understand.
	SLIMpy is a software library that contains definitions of coordinate
	free vectors and linear operators. It allows the user to design and
	run algorithms with any out of core package, in a Matlab style interface
	while maintaining optimal efficiency and speed. SLIMpy looks at each
	main program of each OOCE as a Matrix vector operation or vector
	reduction/transformation operation. It uses operator overloading
	to generate an abstract syntax tree (AST) which can be optimized
	in many ways before executing its commands. The AST also provides
	a pathway for embarrassingly parallel applications by splitting the
	tree over different nodes and processors. SLIMpy provides an interface
	to these OOCE that allows for optimal construction of commands and
	allows for iterative techniques. It smoothes the transition from
	other languages such as Matlab and allows the algorithm designer
	to write readable and reusable code. SLIMpy also adds to OOCE by
	allowing for easy parallelization.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Sean_Sli.pdf}
}

@CONFERENCE{saab08ICASSPssa,
  author = {Rayan Saab and  Rick Chartrand and Ozgur Yilmaz},
  title = {Stable sparse approximations via nonconvex optimization},
  year = {2008},
  organization = {ICASSP},
keywords= {ICASSP } , 
abstract = {We present theoretical results pertaining to the ability of lp minimization to recover sparse and compressible signals from incomplete and noisy measurements. In particular, we extend the results of Cande`s, Romberg and Tao [1] to the p < 1 case. Our results indicate that depending on the restricted isometry constants (see, e.g.,[2] and [3]) and the noise level, lp minimization with certain values of p < 1 provides better theoretical guarantees in terms of stability and robustness than l1 minimization does. This is especially true when the restricted isometry constants are relatively large.},
url= { https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICASSP/2008/saab08ICASSPssa/saab08ICASSPssa.pdf }
}



@CONFERENCE{saab08SINBADcps,
  author = {Rayan Saab},
  title = {Curvelet-Based Primary-Multiple Separation from a Bayesian Perspective},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We present a novel primary-multiple separation scheme which makes
	use of the sparsity of both primaries and multiples in a transform
	domain, such as the curvelet transform, to provide estimates of each.
	The proposed algorithm utilizes seismic data as well as the output
	of a preliminary step that provides (possibly) erroneous predictions
	of the multiples. The algorithm separates the signal components,
	i.e., the primaries and multiples, by solving an optimization problem
	that assumes noisy input data and can be derived from a Bayesian
	perspective. More precisely, the optimization problem can be arrived
	at via an assumption of a weighted Laplacian distribution for the
	primary and multiple coefficients in the transform domain and of
	white Gaussian noise contaminating both the seismic data and the
	preliminary prediction of the multiples, which both serve as input
	to the algorithm.},
  date-modified = {2008-08-22 12:45:53 -0700},
  keywords = {SLIM, SINBAD, Presentation},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Rayan_Curv.pdf}
}

@CONFERENCE{vandenberg08IAMesr,
  author = {E. van den Berg},
  title = {Exact sparse reconstruction and neighbourly polytopes},
  booktitle = {IAM},
  year = {2008},
  bdsk-url-1 = {http://slim.eos.ubc.ca/Publications/Private/Presentations/2008/vandenberg08iam_pres.pdf},
  date-added = {2008-08-26 15:44:44 -0700},
  date-modified = {2008-08-26 15:45:58 -0700},
  keywords = {SLIM, IAM, Presentation},
  presentation = {http://slim.eos.ubc.ca/Publications/Private/Presentations/2008/vandenberg08iam_pres.pdf}
}


@CONFERENCE{wang08SINBADrri,
  author = {Deli Wang and Rayan Saab and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Recent results in curvelet-based primary-multiple separation},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We present a nonlinear curvelet-based sparsity-promoting formulation
	for the primary-multiple separation problem. We show that these coherent
	signal components can be separated robustly by explicitly exploiting
	the locality of curvelets in phase space (space-spatial frequency
	plane) and their ability to compress data volumes that contain wavefronts.
	This work is an extension of earlier results and the presented algorithms
	are shown to be stable under noise and moderately erroneous multiple
	predictions.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Deli_Rec.pdf}
}

@CONFERENCE{yan08SINBADwru,
  author = {Jiupeng Yan},
  title = {Wavefield Reconstruction Using Simultaneous Denoising Interpolation
	vs. Denoising after Interpolation},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {This report represents and compares two methods of wavefield reconstruction
	from noisy seismic data with missing traces. The two methods are
	(i) First interpolate incomplete noisy data to get complete noisy
	data and then denoise, and (ii) Interpolate and denoise the incomplete
	noisy data simultaneously. A sample test of synthetic data will be
	presented. The results of tests show that denoising after interpolation
	is better than simultaneous denoising and interpolation if the parameter
	of the denoising problem is chosen appropriately.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Jiapeng_Wav.pdf}
}

@CONFERENCE{yarham08SINBADbss,
  author = {Carson Yarham},
  title = {Bayesian signal separation applied to ground-roll removal},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {Accurate and adaptive noise removal is a critical part in seismic
	processing. Recent developments in signal separation methods have
	allowed a more flexible and accurate framework in which to perform
	ground roll and reflector separation. The use of a new Bayesian separation
	scheme developed at the SLIM group that contains control parameters
	to adjust for the uniqueness of specific problems is used. The sensitivity
	and variation of the control parameters is examined and this method
	is applied to synthetic and real data and the results are compared
	to previous methods.},
  date-modified = {2008-08-22 12:42:58 -0700},
  keywords = {SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Carson_Gro.pdf}
}

@CONFERENCE{yarham08SEGbgr,
  author = {Carson Yarham and Felix J. Herrmann},
  title = {Bayesian ground-roll seperation by curvelet-domain sparsity promotion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  number = {1},
  pages = {2576-2580},
  month = {November},
  organization = {SEG},
  publisher = {SEG},
  abstract = {The removal of coherent noise generated by surface waves in land based
	seismic is a prerequisite to imaging the subsurface. These surface
	waves, termed as ground roll, overlay important reflector information
	in both the t-x and f-k domains. Standard techniques of ground-roll
	removal commonly alter reflector information. We propose the use
	of the curvelet domain as a sparsifying transform in which to preform
	signal-separation techniques that preserves reflector information
	while increasing ground-roll removal. We look at how this method
	preforms on synthetic data for which we can build quantitative results
	and a real field data set.},
  keywords = {SLIM},
presentation ={http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg08/herrmann08bgs.pdf },
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/yarham08SEGbgr/yarham08SEGbgr.pdf  }
}

@CONFERENCE{yilmaz08SINBADsse,
  author = {Ozgur Yilmaz},
  title = {Stable sparse expansions via non-convex optimization},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We present theoretical results pertaining to the ability of p-(quasi)norm
	minimization to recover sparse and compressible signals from incomplete
	and noisy measurements. In particular, we extend the results of Candes,
	Romberg and Tao for 1-norm to the p<1 case. Our results indicate
	that depending on the restricted isometry constants and the noise
	level, p-norm minimization with certain values of p<1 provides better
	theoretical guarantees in terms of stability and robustness compared
	to 1-norm minimization. This is especially true when the restricted
	isometry constants are relatively large, or equivalently, when the
	data is significantly undersampled.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Ozgur_Sta.pdf}
}

%----------------------------2007---------------------

@CONFERENCE{challa07EAGEsrf,
  author = {Challa S. Sastry and Gilles Hennenfent and Felix J. Herrmann},
  title = {Signal reconstruction from incomplete and misplaced measurements},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  month = {June},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {Constrained by practical and economical considerations, one often
	uses seismic data with missing traces. The use of such data results
	in image artifacts and poor spatial resolution. Sometimes due to
	practical limitations, measurements may be available on a perturbed
	grid, instead of on the designated grid. Due to algorithmic requirements,
	when such measurements are viewed as those on the designated grid,
	the recovery procedures may result in additional artifacts. This
	paper interpolates incomplete data onto regular grid via the Fourier
	domain, using a recently developed greedy algorithm. The basic objective
	is to study experimentally as to what could be the size of the perturbation
	in measurement coordinates that allows for the measurements on the
	perturbed grid to be considered as on the designated grid for faithful
	recovery. Our experimental work shows that for compressible signals,
	a uniformly distributed perturbation can be offset with slightly
	more number of measurements.},
  keywords = {SLIM, EAGE},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/challa07EAGEsrf/challa07EAGEsrf.pdf}
}


@CONFERENCE{fomel07ICASSPrepro,
  author = {S. Fomel and Gilles Hennenfent},
  title = {Reproducible computational experiments using scons},
  organization = {ICASSP},
year={2007 },
keywords= {ICASSP },
  abstract = {SCons (from Software Construction) is a well-known open- source program designed primarily for building software. In this paper, we describe our method of extending SCons for managing data processing flows and reproducible computational experiments. We demonstrate our usage of SCons with a simple example.},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICASSP/2007/fomel07ICASSPrepro/fomel07ICASSPrepro.pdf  }
}


@CONFERENCE{hennenfent07SINBADjdn,
  author = {Gilles Hennenfent},
  title = {Just denoise. Nonlinear recovery from randomly sampled data},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {In this talk, we turn the interpolation problem of coarsely-sampled
	data into a denoising problem. From this point of view, we illustrate
	the benefit of random sampling at sub-Nyquist rate over regular sampling
	at the same rate. We show that, using nonlinear sparsity-promoting
	optimization, coarse random sampling may actually lead to significantly
	better wavefield reconstruction than equivalent regularly sampled
	data.},
  keywords = {Presentation, SINBAD, SLIM}
}

@CONFERENCE{hennenfent07EAGEcrw,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Curvelet reconstruction with sparsity-promoting inversion: successes
	and challenges},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  month = {June},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {In this overview of the recent Curvelet Reconstruction with Sparsity-promoting
	Inver- sion (CRSI) method, we present our latest 2-D and 3-D interpolation
	results on both synthetic and real datasets. We compare these results
	to interpolated data using other ex- isting methods. Finally, we
	discuss the challenges related to sparsity-promoting solvers for
	the large-scale problems the industry faces.},
  keywords = {SLIM, EAGE},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEcrw/hennenfent07EAGEcrw.pdf },
presentation={https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEcrw/hennenfent07EAGEcrw_pres.pdf }
}

@CONFERENCE{hennenfent07EAGEisf,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Irregular sampling: from aliasing to noise},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  month = {June},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {Seismic data is often irregularly and/or sparsely sampled along spatial
	coordinates. We show that these acquisition geometries are not necessarily
	a source of adversity in order to accurately reconstruct adequately-sampled
	data. We use two examples to illustrate that it may actually be better
	than equivalent regularly subsampled data. This comment was already
	made in earlier works by other authors. We explain this behavior
	by two key observations. Firstly, a noise-free underdetermined problem
	can be seen as a noisy well-determined problem. Secondly, regularly
	subsampling creates strong coherent acquisition noise (aliasing)
	difficult to remove unlike the noise created by irregularly subsampling
	that is typically weaker and Gaussian-like.},
  keywords = {SLIM, EAGE},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEisf/hennenfent07EAGEisf.pdf },
presentation= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEisf/hennenfent07EAGEisf_pres.pdf },
url2= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEisf/hennenfent07eage_pres.pdf }
}

@CONFERENCE{hennenfent07SINBADrii,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Recent insights in $L_1$ solvers},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {During this talk, an overview is given on our work on norm-one solvers
	as part of the DNOISE project. Gilles will explain the ins and outs
	of our iterative thresholding solver based on log cooling while Felix
	will present the work of Michael Friedlander "A Newton root-finding
	algorithms for large-scale basis pursuit denoise". Both approaches
	involve the solution of the basis pursuit problem that seeks a minimum
	one-norm solution of an underdetermined least-squares problem. Basis
	pursuit denoise (BPDN) fits the least-squares problem only approximately,
	and a single parameter determines a curve that traces the trade-off
	between the least-squares fit and the one-norm of the solution. In
	the work of Friedlander, it is shown show that the function that
	describes this curve is convex and continuously differentiable over
	all points of interest. They describe an efficient procedure for
	evaluating this function and its derivatives. As a result, they can
	compute arbitrary points on this curve. Their method is suitable
	for large-scale problems. Only matrix-vector operations are required.
	This is joint work with Ewout van der Berg and Michael P. Friedlander},
  keywords = {Presentation, SINBAD, SLIM}
}

@CONFERENCE{hennenfent07SEGrsn,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Random sampling: New insights into the reconstruction of coarsely
	sampled wavefields},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  number = {1},
  pages = {2575-2579},
  organization = {SEG},
  publisher = {SEG},
  abstract = {In this paper, we turn the interpolation problem of coarsely-sampled
	data into a denoising problem. From this point of view, we illustrate
	the benefit of random sampling at sub-Nyquist rate over regular sampling
	at the same rate. We show that, using nonlinear sparsity-promoting
	optimization, coarse random sampling may actually lead to significantly
	better wavefield reconstruction than equivalent regularly sampled
	data. {\copyright}2007 Society of Exploration Geophysicists},
  doi = {10.1190/1.2793002},
  keywords = {SLIM},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/hennenfent07SEGrsn/hennenfent07SEGrsn.pdf },
presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/hennenfent07SEGrsn/hennenfent07SEGrsn_pres.pdf }
}

@CONFERENCE{herrmann07AMScsi,
  author = {Felix J. Herrmann},
  title = {Compressive seismic imaging},
  booktitle = {AMS Von Neumann},
  year = {2007},
  abstract = {Seismic imaging involves the solution of an inverse-scattering problem
	during which the energy of (extremely) large data volumes is collapsed
	onto the Earth{\textquoteright}s reflectors. We show how the ideas
	from {\textquoteright}compressive sampling{\textquoteright} can alleviate
	this task by exploiting the curvelet transform{\textquoteright}s
	{\textquoteright}wavefront-set detection{\textquoteright} capability
	and {\textquoteright}invariance{\textquoteright} property under wave
	propagation. First, a wavelet-vaguellete technique is reviewed, where
	seismic amplitudes are recovered from complete data by diagonalizing
	the Gramm matrix of the linearized scattering problem. Next, we show
	how the recovery of seismic wavefields from incomplete data can be
	cast into a compressive sampling problem, followed by a proposal
	to compress wavefield extrapolation operators via compressive sampling
	in the modal domain. During the latter approach, we explicitly exploit
	the mutual incoherence between the eigenfunctions of the Helmholtz
	operator and the curvelet frame elements that compress the extrapolated
	wavefield. This is joint work with Gilles Hennenfent, Peyman Moghaddam,
	Tim Lin, Chris Stolk and Deli Wang.},
  keywords = {AMS Von Neumann, Presentation, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07Neumann.pdf}
}

@CONFERENCE{herrmann07PIMScsm,
  author = {Felix J. Herrmann},
  title = {Compressive sampling meets seismic imaging},
  booktitle = {PIMS},
  year = {2007},
  keywords = {PIMS, Presentation, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07PIMS.pdf}
}

@CONFERENCE{herrmann07SINBADcwe,
  author = {Felix J. Herrmann},
  title = {Compressed wavefield extrapolation},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {An explicit algorithm for the extrapolation of one-way wavefields
	is proposed which combines recent developments in information theory
	and theoretical signal processing with the physics of wave propagation.
	Because of excessive memory requirements, explicit formulations for
	wave propagation have proven to be a challenge in 3-D. By using ideas
	from {\textquoteleft}{\textquoteleft}compressed sensing{\textquoteright}{\textquoteright},
	we are able to formulate the (inverse) wavefield extrapolation problem
	on small subsets of the data volume, thereby reducing the size of
	the operators. According to compressed sensing theory, signals can
	successfully be recovered from an incomplete set of measurements
	when the measurement basis is incoherent with the representation
	in which the wavefield is sparse. In this new approach, the eigenfunctions
	of the Helmholtz operator are recognized as a basis that is incoherent
	with curvelets that are known to compress seismic wavefields. By
	casting the wavefield extrapolation problem in this framework, wavefields
	can successfully be extrapolated in the modal domain via a computationally
	cheaper operation. A proof of principle for the {\textquoteleft}{\textquoteleft}compressed
	sensing{\textquoteright}{\textquoteright} method is given for wavefield
	extrapolation in 2-D. The results show that our method is stable
	and produces identical results compared to the direct application
	of the full extrapolation operator. This is joint work with Tim Lin.},
  keywords = {Presentation, SINBAD, SLIM},
url={http://slim.eos.ubc.ca/Publications/Public/Journals/CompressedExtrap.pdf}
}

@CONFERENCE{herrmann07SINBADfrw,
  author = {Felix J. Herrmann},
  title = {Focused recovery with the curvelet transform},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {Incomplete data represents a major challenge for a successful prediction
	and subsequent removal of multiples. In this paper, a new method
	will be represented that tackles this challenge in a two-step approach.
	During the first step, the recently developed curvelet-based recovery
	by sparsity-promoting inversion (CRSI) is applied to the data, followed
	by a prediction of the primaries. During the second high-resolution
	step, the estimated primaries are used to improve the frequency content
	of the recovered data by combining the focal transform, defined in
	terms of the estimated primaries, with the curvelet transform. This
	focused curvelet transform leads to an improved recovery, which can
	subsequently be used as input for a second stage of multiple prediction
	and primary-multiple separation. This is joint work with Deli Wang
	and Gilles Hennenfent.},
  keywords = {Presentation, SINBAD, SLIM},
presentation ={https://www.slim.eos.ubc.ca/sites/data/Papers/Herrmann2007SINBADfoc.pdf }
}

@CONFERENCE{herrmann07SLIMfsd,
  author = {Felix J. Herrmann},
  title = {From seismic data to the composition of rocks: an interdisciplinary
	and multiscale approach to exploration seismology},
  booktitle = {Berkhout{\textquoteright}s valedictory address: the conceptual approach
	of understanding},
  year = {2007},
  abstract = {In this essay, a nonlinear and multidisciplinary approach is presented
	that takes seismic data to the composition of rocks. The presented
	work has deep roots in the {\textquoteleft}gedachtengoed{\textquoteright}
	(philosophy) of Delphi spearheaded by Guus Berkhout. Central themes
	are multiscale, object-orientation and a multidisciplinary approach.},
  keywords = {SLIM},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/Misc/herrmann07SLIMfsd/herrmann07SLIMfsd.pdf }
}

@CONFERENCE{herrmann07COIPpti,
  author = {Felix J. Herrmann},
  title = {Phase transitions in explorations seismology: statistical mechanics
	meets information theory},
  booktitle = {COIP},
  year = {2007},
  abstract = {In this paper, two different applications of phase transitions to
	exploration seismology will be discussed. The first application concerns
	a phase diagram ruling the recovery conditions for seismic data volumes
	from incomplete and noisy data while the second phase transition
	describes the behavior of bi-compositional mixtures as a function
	of the volume fraction. In both cases, the phase transitions are
	the result of randomness in large system of equations in combination
	with nonlinearity. The seismic recovery problem from incomplete data
	involves the inversion of a rectangular matrix. Recent results from
	the field of "compressive sensing" provide the conditions for a successful
	recovery of functions that are sparse in some basis (wavelet) or
	frame (curvelet) representation, by means of a sparsity ($\ell_1$-norm)
	promoting nonlinear program. The conditions for a successful recovery
	depend on a certain randomness of the matrix and on two parameters
	that express the matrix{\textquoteright} aspect ratio and the ratio
	of the number of nonzero entries in the coefficient vector for the
	sparse signal representation over the number of measurements. It
	appears that the ensemble average for the success rate for the recovery
	of the sparse transformed data vector by a nonlinear sparsity promoting
	program, can be described by a phase transition, demarcating the
	regions for the two ratios for which recovery of the sparse entries
	is likely to be successful or likely to fail. Consistent with other
	phase transition phenomena, the larger the system the sharper the
	transition. The randomness in this example is related to the construction
	of the matrix, which for the recovery of spike trains corresponds
	to the randomly restricted Fourier matrix. It is shown, that these
	ideas can be extended to the curvelet recovery by sparsity-promoting
	inversion (CRSI) . The second application of phase transitions in
	exploration seismology concerns the upscaling problem. To counter
	the intrinsic smoothing of singularities by conventional equivalent
	medium upscaling theory, a percolation-based nonlinear switch model
	is proposed. In this model, the transport properties of bi-compositional
	mixture models for rocks undergo a sudden change in the macroscopic
	transport properties as soon as the volume fraction of the stronger
	material reaches a critical point. At this critical point, the stronger
	material forms a connected cluster, which leads to the creation of
	a cusp-like singularity in the elastic moduli, which in turn give
	rise to specular reflections. In this model, the reflectivity is
	no longer explicitly due to singularities in the rocks composition.
	Instead, singularities are created whenever the volume fraction exceeds
	the critical point. We will show that this concept can be used for
	a singularity-preserved lithological upscaling.},
  keywords = {Presentation, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07COIP.pdf}
}

@CONFERENCE{herrmann07EAGErdi,
  author = {Felix J. Herrmann},
  title = {Recent developments in curvelet-based seismic processing},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  month = {June},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {Combinations of parsimonious signal representations with nonlinear
	sparsity promoting programs hold the key to the next-generation of
	seismic data processing algorithms ... Since they allow for a formulation
	that is stable w.r.t. noise \& incomplete data do not require prior
	information on the velocity or locations and dips of the events},
  keywords = {SLIM, EAGE},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07EAGEWSDEV.pdf},
url={https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/herrmann07EAGErdi/herrmann07EAGErdi.pdf }
}

@CONFERENCE{herrmann07SINBADrdi2,
  author = {Felix J. Herrmann},
  title = {Recent developments in primary-multiple separation},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {In this talk, we present a novel primary-multiple separation scheme
	which makes use of the sparsity of both primaries and multiples in
	a transform domain, such as the curvelet transform, to provide estimates
	of each. The proposed algorithm utilizes seismic data as well as
	the output of a preliminary step that provides (possibly) erroneous
	predictions of the multiples. The algorithm separates the signal
	components, i.e., the primaries and multiples, by solving an optimization
	problem that assumes noisy input data and can be derived from a Bayesian
	perspective. More precisely, the optimization problem can be arrived
	at via an assumption of a weighted Laplacian distribution for the
	primary and multiple coefficients in the transform domain and of
	white Gaussian noise contaminating both the seismic data and the
	preliminary prediction of the multiples, which both serve as input
	to the algorithm. Time permitted, we will also briefly discuss a
	propasal for adaptive curvelet-domain matched filtering. This is
	joint work with Deli Wang, Rayan Saaba, {\o}zgur Yilmaz and Eric
	Verschuur.},
  keywords = {Presentation, SINBAD, SLIM},
presentation={http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg07/herrmann07rdi.pdf}
}

@CONFERENCE{herrmann07SINBADsia2,
  author = {Felix J. Herrmann},
  title = {Seismic image amplitude recovery},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {In this talk, we recover the amplitude of a seismic image by approximating
	the normal (demigration-migration) operator. In this approximation,
	we make use of the property that curvelets remain invariant under
	the action of the normal operator. We propose a seismic amplitude
	recovery method that employs an eigenvalue like decomposition for
	the normal operator using curvelets as eigen-vectors. Subsequently,
	we propose an approximate nonlinear singularity-preserving solution
	to the least-squares seismic imaging problem with sparseness in the
	curvelet domain and spatial continuity constraints. Our method is
	tested with a reverse-time {\textquoteright}wave-equation{\textquoteright}
	migration code simulating the acoustic wave equation on the SEG-AA
	salt model. This is joint work with Peyman Moghaddam and Chris Stolk
	(University of Twente)},
  keywords = {Presentation, SINBAD, SLIM}
}

@CONFERENCE{herrmann07AIPsit,
  author = {Felix J. Herrmann},
  title = {Seismic inversion through operator overloading},
  booktitle = {AIP},
  year = {2007},
  abstract = {Inverse problems in (exploration) seismology are known for their large
	to very large scale. For instance, certain sparsity-promoting inversion
	techniques involve vectors that easily exceed 230 unknowns while
	seismic imaging involves the construction and application of matrix-free
	discretized operators where single matrix-vector evaluations may
	require hours, days or even weeks on large compute clusters. For
	these reasons, software development in this field has remained the
	domain of highly technical codes programmed in low-level languages
	with little eye for easy development, code reuse and integration
	with (nonlinear) programs that solve inverse problems. Following
	ideas from the Symes{\textquoteright} Rice Vector Library and Bartlett{\textquoteright}s
	C++ object-oriented interface, Thyra, and Reduction/Transformation
	operators (both part of the Trilinos software package), we developed
	a software-development environment based on overloading. This environment
	provides a pathway from in-core prototype development to out-of-core
	and MPI {\textquoteright}production{\textquoteright} code with a
	high level of code reuse. This code reuse is accomplished by integrating
	the out-of-core and MPI functionality into the dynamic object-oriented
	programming language Python. This integration is implemented through
	operator overloading and allows for the development of a coordinate-free
	solver framework that (i) promotes code reuse; (ii) analyses the
	statements in an abstract syntax tree and (iii) generates executable
	statements. In the current implementation, we developed an interface
	to generate executable statements for the out-of-core unix-pipe based
	(seismic) processing package RSF-Madagascar (rsf.sf.net). The modular
	design allows for interfaces to other seismic processing packages
	and to in-core Python packages such as numpy. So far, the implementation
	overloads linear operators and elementwise reduction/transformation
	operators. We are planning extensions towards nonlinear operators
	and integration with existing (parallel) solver frameworks such as
	Trilinos.},
  keywords = {AIP, Presentation, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07AIP2.pdf}
}

@CONFERENCE{herrmann07CYBERsmc,
  author = {Felix J. Herrmann},
  title = {Seismology meets compressive sampling presented at the joint NSF-IPAM
	meeting. Los Angeles. October, 2007.},
  booktitle = {Cyber},
  year = {2007},
  abstract = {Presented at Cyber-Enabled Discovery and Innovation: Knowledge Extraction
	as a success story lecture. See for more detail https://www.ipam.ucla.edu/programs/cdi2007/},
  keywords = {Cyber, Presentation, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07Cyber.pdf}
}

@CONFERENCE{herrmann07EAGEsrm,
  author = {Felix J. Herrmann},
  title = {Surface related multiple prediction from incomplete data},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  month = {June},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {Incomplete data, unknown source-receiver signatures and free-surface
	reflectivity represent challenges for a successful prediction and
	subsequent removal of multiples. In this paper, a new method will
	be represented that tackles these challenges by combining what we
	know about wavefield (de-)focussing, by weighted convolutions/correlations,
	and recently developed curvelet-based recovery by sparsity-promoting
	inversion (CRSI). With this combination, we are able to leverage
	recent insights from wave physics to- wards a nonlinear formulation
	for the multiple-prediction problem that works for incomplete data
	and without detailed knowledge on the surface effects.},
  keywords = {SLIM, EAGE},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/herrmann07EAGEsrm/herrmann07EAGEsrm.pdf}
}

@CONFERENCE{herrmann07EAGEsrm1,
  author = {Felix J. Herrmann},
  title = {Surface related multiple prediction from incomplete data},
  booktitle = {EAGE},
  year = {2007},
  keywords = {EAGE, Presentation, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07EAGPRED.pdf}
}

@CONFERENCE{herrmann07AIPssd,
  author = {Felix J. Herrmann},
  title = {Stable seismic data recovery},
  booktitle = {AIP},
  year = {2007},
  abstract = {In this talk, directional frames, known as curvelets, are used to
	recover seismic data and images from noisy and incomplete data. Sparsity
	and invariance properties of curvelets are exploited to formulate
	the recovery by a {\textquoteleft}1-norm promoting program. It is
	shown that our data recovery approach is closely linked to the recent
	theory of {\textquoteleft}{\textquoteleft}compressive sensing{\textquoteright}{\textquoteright}
	and can be seen as a first step towards a nonlinear sampling theory
	for wavefields. The second problem that will be discussed concerns
	the recovery of the amplitudes of seismic images in clutter. There,
	the invariance of curvelets is used to approximately invert the Gramm
	operator of seismic imaging. In the high-frequency limit, this Gramm
	matrix corresponds to a pseudo-differential operator, which is near
	diagonal in the curvelet domain.In this talk, directional frames,
	known as curvelets, are used to recover seismic data and images from
	noisy and incomplete data. Sparsity and invariance properties of
	curvelets are exploited to formulate the recovery by a l1-norm promoting
	program. It is shown that our data recovery approach is closely linked
	to the recent theory of {\textquoteleft}{\textquoteleft}compressive
	sensing{\textquoteright}{\textquoteright} and can be seen as a first
	step towards a nonlinear sampling theory for wavefields. The second
	problem that will be discussed concerns the recovery of the amplitudes
	of seismic images in clutter. There, the invariance of curvelets
	is used to approximately invert the Gramm operator of seismic imaging.
	In the high-frequency limit, this Gramm matrix corresponds to a pseudo-differential
	operator, which is near diagonal in the curvelet domain.},
  keywords = {AIP, Presentation, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07AIP.pdf}
}

@CONFERENCE{herrmann07EAGEsia,
  author = {Felix J. Herrmann and Gilles Hennenfent and Peyman P. Moghaddam},
  title = {Seismic imaging and processing with curvelets},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  month = {June},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {In this paper, we present a nonlinear curvelet-based sparsity-promoting
	formulation for three problems in seismic processing and imaging
	namely, seismic data regularization from data with large percentages
	of traces missing; seismic amplitude recovery for sub-salt images
	obtained by reverse-time migration and primary-multiple separation,
	given an inaccurate multiple prediction. We argue why these nonlinear
	formulations are beneficial.},
  keywords = {SLIM, EAGE},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07EAGEIMPROC.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/herrmann07EAGEsia/herrmann07EAGEsia.pdf }
}

@CONFERENCE{herrmann07EAGEjda,
  author = {Felix J. Herrmann and Peyman P. Moghaddam},
  title = {Just diagonalize: a curvelet-based approach to seismic amplitude
	recovery},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  month = {June},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {In this presentation we present a nonlinear curvelet-based sparsity-promoting
	formulation for the recovery of seismic amplitudes. We show that
	the curvelet{\textquoteright}s wavefront detection capability and
	invariance under wave propagation lead to a formulation of this recovery
	problem that is stable under noise and missing data.},
  keywords = {SLIM, EAGE},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07EAGEWSIM.pdf},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/herrmann07EAGEjda/herrmann07EAGEjda.pdf }
}

@CONFERENCE{herrmann07SEGmpf,
  author = {Felix J. Herrmann and Deli Wang and Gilles Hennenfent},
  title = {Multiple prediction from incomplete data with the focused curvelet
	transform},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  number = {1},
  pages = {2505-2509},
  organization = {SEG},
  publisher = {SEG},
  abstract = {Incomplete data represents a major challenge for a successful prediction
	and subsequent removal of multiples. In this paper, a new method
	will be represented that tackles this challenge in a two-step approach.
	During the first step, the recenly developed curvelet-based recovery
	by sparsity-promoting inversion (CRSI) is applied to the data, followed
	by a prediction of the primaries. During the second high-resolution
	step, the estimated primaries are used to improve the frequency content
	of the recovered data by combining the focal transform, defined in
	terms of the estimated primaries, with the curvelet transform. This
	focused curvelet transform leads to an improved recovery, which can
	subsequently be used as input for a second stage of multiple prediction
	and primary-multiple separation. {\copyright}2007 Society of Exploration
	Geophysicists},
  doi = {10.1190/1.2792987},
  keywords = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07SEGPRED.pdf},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/herrmann07SEGmpf/herrmann07SEGmpf.pdf }
}

@CONFERENCE{herrmann07SEGsdp,
  author = {Felix J. Herrmann and Deli Wang and Gilles Hennenfent and Peyman
	P. Moghaddam},
  title = {Seismic data processing with curvelets: a multiscale and nonlinear
	approach},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  number = {1},
  pages = {2220-2224},
  organization = {SEG},
  publisher = {SEG},
  abstract = {In this abstract, we present a nonlinear curvelet-based sparsity-promoting
	formulation of a seismic processing flow, consisting of the following
	steps: seismic data regularization and the restoration of migration
	amplitudes. We show that the curvelet{\textquoteright}s wavefront
	detection capability and invariance under the migration-demigration
	operator lead to a formulation that is stable under noise and missing
	data. {\copyright}2007 Society of Exploration Geophysicists},
  doi = {10.1190/1.2792927},
  keywords = {SLIM, SEG},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07SEGPROC.pdf},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/herrmann07SEGsdp/herrmann07SEGsdp.pdf }
}

@CONFERENCE{herrmann07SEGsnt,
  author = {Felix J. Herrmann and D. Wilkinson},
  title = {Seismic noise: the good, the bad and the ugly},
  booktitle = {SEG Summer Research Workshop: Seismic Noise: Origins Preventions,
	Mitigation, Utilization},
  year = {2007},
  note = {Presented at SEG Summer Research Workshop: Seismic Noise: Origins,
	Prevention, Mitigation, Utilization},
  abstract = {In this paper, we present a nonlinear curvelet-based sparsity-promoting
	formulation for three problems related to seismic noise, namely the
	{\textquoteright}good{\textquoteright}, corresponding to noise generated
	by random sampling; the {\textquoteright}bad{\textquoteright}, corresponding
	to coherent noise for which (inaccurate) predictions exist and the
	{\textquoteright}ugly{\textquoteright} for which no predictions exist.
	We will show that the compressive capabilities of curvelets on seismic
	data and images can be used to tackle these three categories of noise-related
	problems.},
  keywords = {SLIM, SEG},
url={https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/herrmann07SEGsnt/herrmann07SEGsnt.pdf }
}


@CONFERENCE{lin07SEGcwe1,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Compressed wavefield extrapolation with curvelets},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  number = {1},
  pages = {1997-2001},
  organization = {SEG},
  publisher = {SEG},
  abstract = {An explicit algorithm for the extrapolation of one-way wavefields
	is proposed which combines recent developments in information theory
	and theoretical signal processing with the physics of wave propagation.
	Because of excessive memory requirements, explicit formulations for
	wave propagation have proven to be a challenge in 3-D. By using ideas
	from {\textquoteleft}{\textquoteleft}compressed sensing{\textquoteright}{\textquoteright},
	we are able to formulate the (inverse) wavefield extrapolation problem
	on small subsets of the data volume, thereby reducing the size of
	the operators. According to compressed sensing theory, signals can
	successfully be recovered from an imcomplete set of measurements
	when the measurement basis is incoherent with the representation
	in which the wavefield is sparse. In this new approach, the eigenfunctions
	of the Helmholtz operator are recognized as a basis that is incoherent
	with curvelets that are known to compress seismic wavefields. By
	casting the wavefield extrapolation problem in this framework, wavefields
	can successfully be extrapolated in the modal domain via a computationally
	cheaper operation. A proof of principle for the {\textquoteleft}{\textquoteleft}compressed
	sensing{\textquoteright}{\textquoteright} method is given for wavefield
	extrapolation in 2-D. The results show that our method is stable
	and produces identical results compared to the direct application
	of the full extrapolation operator. {\copyright}2007 Society of Exploration
	Geophysicists},
  doi = {10.1190/1.2792882},
  keywords = {SLIM, SEG},
presentation ={http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/lin07SEG.pdf },
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/lin07SEGcwe1/lin07SEGcwe1.pdf }
}


@CONFERENCE{maysami07EAGEsrc,
  author = {Mohammad Maysami and Felix J. Herrmann},
  title = {Seismic reflector characterization by a multiscale detection-estimation
	method},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  month = {June},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {Seismic transitions of the subsurface are typically considered as
	zero-order singularities (step functions). According to this model,
	the conventional deconvolution problem aims at recovering the seismic
	reflectivity as a sparse spike train. However, recent multiscale
	analysis on sedimentary records revealed the existence of accumulations
	of varying order singularities in the subsurface, which give rise
	to fractional-order discontinuities. This observation not only calls
	for a richer class of seismic reflection waveforms, but it also requires
	a different methodology to detect and characterize these reflection
	events. For instance, the assumptions underlying conventional deconvolution
	no longer hold. Because of the bandwidth limitation of seismic data,
	multiscale analysis methods based on the decay rate of wavelet coefficients
	may yield ambiguous results. We avoid this problem by formulating
	the estimation of the singularity orders by a parametric nonlinear
	inversion method.},
  keywords = {SLIM, EAGE},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/maysami07EAGEsrc/maysami07EAGEsrc.pdf },
presentation={http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/maysami07SINBADsrc.pdf }
}

@CONFERENCE{moghaddam07CSEGmar,
  author = {Peyman P. Moghaddam and Felix J. Herrmann and C. C. Stolk},
  title = {Migration amplitude recovery using curvelets},
  booktitle = {CSEG Technical Program Expanded Abstracts},
  year = {2007},
  month = {May},
  organization = {CSEG},
  publisher = {CSEG},
  abstract = {In this paper, we recover the amplitude of a seismic image by approximating
	the normal operator and subsequently inverting it. Normal operator
	(migration followed by modeling) is an example of pseudo-differential.
	curvelets are proven to be invariant under the action of pseudo-differential
	operators under certain conditions. Subsequently, curvelets are forming
	as eigen-vectors for such an operator. We propose a seismic amplitude
	recovery method that employs an eigen-value decomposition for normal
	operator using curvelets as eigen-vectors and to be estimated eigenvalues.
	A post-stack reverse-time, wave-equation migration is used for evaluation
	of the proposed method.},
  file = {:http\\\://www.cseg.ca/conventions/abstracts/2007/2007abstracts/168S0131.pdf:PDF},
  keywords = {SLIM},
  url = {http\://www.cseg.ca/conventions/abstracts/2007/2007abstracts/168S0131.pdf}
}

@CONFERENCE{moghaddam07SEGrsi,
  author = {Peyman P. Moghaddam and Felix J. Herrmann and C. C. Stolk},
  title = {Robust seismic-images amplitude recovery using curvelets},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  number = {1},
  pages = {2225-2229},
  organization = {SEG},
  publisher = {SEG},
  abstract = {In this paper, we recover the amplitude of a seismic image by approximating
	the normal (demigration-migration) operator. In this approximation,
	we make use of the property that curvelets remain invariant under
	the action of the normal operator. We propose a seismic amplitude
	recovery method that employs an eigenvalue like decomposition for
	the normal operator using curvelets as eigen-vectors. Subsequently,
	we propose an approximate non-linear singularity-preserving solution
	to the least-squares seismic imaging problem with sparseness in the
	curvelet domain and spatial continuity constraints. Our method is
	tested with a reverse-time {\textquoteleft}wave-equation{\textquoteright}
	migration code simulating the acoustic wave equation on the SEG-AA
	salt model. {\copyright}2007 Society of Exploration Geophysicists},
  doi = {10.1190/1.2792928},
  keywords = {SLIM, SEG},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/moghaddam07SEGrsi/moghaddam07SEGrsi.pdf },
presentation={http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg07/moghaddam07rsi.pdf}
}

@CONFERENCE{moghaddam07CSEGsac,
  author = {Peyman P. Moghaddam and Felix J. Herrmann and C. C. Stolk},
  title = {Sparsity and continuity enhancing seismic imaging},
  booktitle = {CSEG Technical Program Expanded Abstracts},
  year = {2007},
  month = {May},
  organization = {CSEG},
  publisher = {CSEG},
  abstract = {A non-linear singularity-preserving solution to the least-squares
	seismic imaging problem with sparseness and continuity constraints
	is proposed. The applied formalism explores curvelets as a directional
	frame that, by their sparsity on the image, and their invariance
	under the imaging operators, allows for a stable recovery of the
	amplitudes. Our method is based on the estimation of the normal operator
	in the form of an {\textquoteright}eigenvalue{\textquoteright} decompsoition
	with curvelets as the eigenvectors{\textquoteright}. Subsequently,
	we propose an inversion method that derives from estimation of the
	normal operator and is formulated as a convex optimization problem.
	Sparsity in the curvelet domain as well as continuity along the reflectors
	in the image domain are promoted as part of this optimization. Our
	method is tested with a reverse-time {\textquoteright}wave-equation{\textquoteright}
	migration code simulating the acoustic wave equation.},
  file = {:http\\\://www.cseg.ca/conventions/abstracts/2007/2007abstracts/091S0130.pdf:PDF},
  keywords = {SLIM},
  url = {http\://www.cseg.ca/conventions/abstracts/2007/2007abstracts/091S0130.pdf}
}

@CONFERENCE{moghaddam07EAGEsar,
  author = {Peyman P. Moghaddam and Felix J. Herrmann and C. C. Stolk},
  title = {Seismic amplitude recovery with curvelets},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  month = {June},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {A non-linear singularity-preserving solution to the least-squares
	seismic imaging problem with sparseness and continuity constraints
	is proposed. The applied formalism explores curvelets as a directional
	frame that, by their sparsity on the image, and their invariance
	under the imaging operators, allows for a stable recovery of the
	amplitudes. Our method is based on the estimation of the normal operator
	in the form of an {\textquoteright}eigenvalue{\textquoteright} decompsoition
	with curvelets as the {\textquoteright}eigenvectors{\textquoteright}.
	Subsequently, we propose an inversion method that derives from estimation
	of the normal operator and is formulated as a convex optimization
	problem. Sparsity in the curvelet domain as well as continuity along
	the reflectors in the image domain are promoted as part of this optimization.
	Our method is tested with a reverse-time {\textquoteright}wave-equation{\textquoteright}
	migration code simulating the acoustic wave equation.},
  keywords = {SLIM, EAGE},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/moghaddam07EAGEsar/moghaddam07EAGEsar.pdf }
}

@CONFERENCE{saab07SEGcbp,
  author = {Rayan Saab and Deli Wang and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Curvelet-based primary-multiple separation from a {B}ayesian perspective},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  number = {1},
  pages = {2510-2514},
  organization = {SEG},
  publisher = {SEG},
  abstract = {In this abstract, we present a novel primary-multiple separation scheme
	which makes use of the sparsity of both primaries and multiples in
	a transform domain, such as the curvelet transform, to provide estimates
	of each. The proposed algorithm utilizes seismic data as well as
	the output of a preliminary step that provides (possibly) erroneous
	predictions of the multiples. The algorithm separates the signal
	components, i.e., the primaries and multiples, by solving an optimization
	problem that assumes noisy input data and can be derived from a Bayesian
	perspective. More precisely, the optimization problem can be arrived
	at via an assumption of a weighted Laplacian distribution for the
	primary and multiple coefficients in the transform domain and of
	white Gaussian noise contaminating both the seismic data and the
	preliminary prediction of the multiples, which both serve as input
	to the algorithm. {\copyright}2007 Society of Exploration Geophysicists},
  doi = {10.1190/1.2792988},
  keywords = {SLIM, SEG},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg07/saab07cbp.pdf},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/saab07SEGcbp/saab07SEGcbp.pdf }
}

@CONFERENCE{sastry07SINBADnor,
  author = {Challa S. Sastry},
  title = {Norm-one recovery from irregular sampled data},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {Seismic traces are sampled irregularly and insufficiently due to practical
	and economical limitations. The use of such data in seismic imaging
	results in image artifacts and poor spatial resolution. Therefore,
	before being used, the measurements are to be interpolated onto a
	regular grid. One of the methods achieving this objective is based
	on the Fourier reconstruction, which deals with the under-determined
	system of equations. The recent pursuit techniques (namely, basis
	pursuit, matching pursuit etc) admit certain promising features such
	as faster and simpler implementation even in large scale settings.
	The presentation discusses the application of the pursuit algorithms
	to the Fourier-based interpolation problem for the signals that have
	sparse Fourier spectra. In particular, the objective of the presentation
	includes: 1). studying the performance of the algorithm if, and how
	far, the measurement coordinates can be shifted from uniform distribution
	on the continuous interval. 2). studying what could be the allowable
	misplacement in the measurement coordinates that does not alter the
	quality of the reconstruction process},
  keywords = {SLIM, SINBAD, Presentation}
}

@CONFERENCE{vandenberg07SINBADipo1,
  author = {E. van den Berg and Michael P. Friedlander},
  title = {In Pursuit of a Root},
  booktitle = {2007 Von Neumann Symposium},
  year = {2007},
  keywords = {minimization, Presentation, SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Private/Presentations/sinbad/2007/friedlander07ipo.pdf}
}

@CONFERENCE{verschuur07SEGmmp,
  author = {D. J. Verschuur and Deli Wang and Felix J. Herrmann},
  title = {Multiterm multiple prediction using separated reflections and diffractions
	combined with curvelet-based subtraction},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  number = {1},
  pages = {2535-2539},
  organization = {SEG},
  publisher = {SEG},
  abstract = {The surface-related multiple elimination (SRME) method has proven
	to be successful on a large number of data cases. Most of the applications
	are still 2D, as the full 3D implementation is still expensive and
	under development. However, the earth is a 3D medium, such that 3D
	effects are difficult to avoid. Most of the 3D effects come from
	diffractive structures, whereas the specular reflections normally
	have less of a 3D behavior. By separating the seismic data in a specular
	reflecting and a diffractive part, multiple prediction can be carried
	out with these different subsets of the input data, resulting in
	several categories of predicted multiples. Because each category
	of predicted multiples can be subtracted from the input data with
	different adaptation filters, a more flexible SRME procedure is obtained.
	Based on some initial results from a Gulf of Mexico dataset, the
	potential of this approach is investigated. {\copyright}2007 Society
	of Exploration Geophysicists},
  doi = {10.1190/1.2792993},
  keywords = {SLIM, SEG},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/verschuur07SEGmmp/verschuur07SEGmmp.pdf }
}

@CONFERENCE{wang07SEGrri,
  author = {Deli Wang and Rayan Saab and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Recent results in curvelet-based primary-multiple separation: application
	to real data},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  number = {1},
  pages = {2500-2504},
  organization = {SEG},
  publisher = {SEG},
  abstract = {In this abstract, we present a nonlinear curvelet-based sparsity-promoting
	formulation for the primary-multiple separation problem. We show
	that these coherent signal components can be separated robustly by
	explicitly exploting the locality of curvelets in phase space (space-spatial
	frequency plane) and their ability to compress data volumes that
	contain wavefronts. This work is an extension of earlier results
	and the presented algorithms are shown to be stable under noise and
	moderately erroneous multiple predictions. {\copyright}2007 Society
	of Exploration Geophysicists},
  doi = {10.1190/1.2792986},
  keywords = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg07/wang07rri.pdf},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/wang07SEGrri/wang07SEGrri.pdf }
}

@CONFERENCE{yarham07SINBADnsw,
  author = {Carson Yarham},
  title = {Nonlinear surface wave prediction and separation},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {Removal of surface waves is an integral step in seismic processing.
	There are many standard techniques for removal of this type of coherent
	noise, such as f-k filtering, but these methods are not always effective.
	One of the common problems with removal of surface waves is that
	they tend to be aliased in the frequency domain. This can make removal
	difficult and affect the frequency content of the reflector signals,
	as this signals will not be completely separated. As seen in (Hennenfent,
	G. and F. Herrmann, 2006, Application of stable signal recovery to
	seismic interpolation) interpolation can be used effectively to resample
	the seismic record thus dealiasing the surface waves. This separates
	the signals in the frequency domain allowing for a more precise and
	complete removal. The use of this technique with curvelet based surface
	wave predictions and an iterative L1 separation scheme can be used
	to remove surface waves from shot records more completely that with
	standard techniques.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Private/Conferences/EAGE/2007/yarham07eage.pdf}
}

@CONFERENCE{yarham07EAGEcai,
  author = {Carson Yarham and Gilles Hennenfent and Felix J. Herrmann},
  title = {Curvelet applications in surface wave removal},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  month = {June},
  organization = {EAGE},
  publisher = {EAGE},
  abstract = {Ground roll removal of seismic signals can be a challenging prospect.
	Dealing with undersampleing causing aliased waves amplitudes orders
	of magnitude higher than reflector signals and low frequency loss
	of information due to band ...},
  keywords = {SLIM, EAGE},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/yarham07EAGEcai/yarham07EAGEcai.pdf}
}


%---------------------------------2006--------------------------------------

@CONFERENCE{hennenfent06SINBADapo,
  author = {Gilles Hennenfent},
  title = {A primer on stable signal recovery},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {During this presentation an introduction will be given on the method
	of stable recovery from noisy and incomplete data. Strong recovery
	conditions that guarantee the recovery for arbitrary acquisition
	geometries will be reviewed and numerical recovery examples will
	be presented.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/4-Felix3.pdf}
}

@CONFERENCE{hennenfent06SINBADros,
  author = {Gilles Hennenfent},
  title = {Recovery of seismic data: practical considerations},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {We propose a method for seismic data interpolation based on 1) the
	reformulation of the problem as a stable signal recovery problem
	and 2) the fact that seismic data is sparsely represented by curvelets.
	This method does not require information on the seismic velocities.
	Most importantly, this formulation potentially leads to an explicit
	recovery condition. We also propose a large-scale problem solver
	for the l1-regularization minimization involved in the recovery and
	successfully illustrate the performance of our algorithm on 2D synthetic
	and real examples.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/5-Gilles2.pdf}
}

@CONFERENCE{hennenfent06SINBADtnf,
  author = {Gilles Hennenfent},
  title = {The Nonuniform Fast Discrete Curvelet Transform (NFDCT)},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {The authors present an extension of the fast discrete curvelet transform
	(FDCT) to nonuniformly sampled data. This extension not only restores
	curvelet compression rates for nonuniformly sampled data but also
	removes noise and maps the data to a regular grid.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/10-Gilles3.pdf}
}

@CONFERENCE{hennenfent06SEGaos,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Application of stable signal recovery to seismic data interpolation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2006},
  volume = {25},
  number = {1},
  pages = {2797-2801},
  organization = {SEG},
  publisher = {SEG},
  abstract = {We propose a method for seismic data interpolation based on 1) the
	reformulation of the problem as a stable signal recovery problem
	and 2) the fact that seismic data is sparsely represented by curvelets.
	This method does not require information on the seismic velocities.
	Most importantly, this formulation potentially leads to an explicit
	recovery condition. We also propose a large-scale problem solver
	for the 1-regularization minimization involved in the recovery and
	successfully illustrate the performance of our algorithm on 2D synthetic
	and real examples. {\copyright}2006 Society of Exploration Geophysicists},
  doi = {10.1190/1.2370105},
  keywords = {SLIM, curvelets, interpolation, seismic data, regularization minimization, iterative thresholding, amplitude, SEG, continuity, fast transform},
presentation ={ https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2006/hennenfent06SEGaos/hennenfent06SEGaos_pres.pdf },
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2006/hennenfent06SEGaos/hennenfent06SEGaos.pdf },
url2 = {http://dx.doi.org/10.1190/1.2370105 }
}

@CONFERENCE{herrmann06SINBADapo1,
  author = {Felix J. Herrmann},
  title = {A primer on sparsity transforms: curvelets and wave atoms},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {During this presentation an overview will be given on the different
	sparsity transforms that are used at SLIM. Emphasis will be on two
	directional and multiscale wavelet transforms, namely the curvelet
	and the recently introduced wave-atom transforms. The main properties
	of these transforms will be listed and their performance on seismic
	data will be discussed.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/4-Felix3.pdf}
}

@CONFERENCE{herrmann06SINBADapow,
  author = {Felix J. Herrmann},
  title = {A primer on weak conditions for stable recovery},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {During this presentation an introduction will be given on the method
	of stable recovery from noisy and incomplete data. Weak recovery
	conditions that guarantee the recovery for typical acquisition geometries
	will be reviewed and numerical recovery examples will be presented.
	The advantage of these weak conditions is that they are less pessimistic
	and {\textquoteleft}verifiable{\textquoteright} or very large-scale
	acquisition geometries.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/6-Felix4.pdf}
}

@CONFERENCE{herrmann06SINBADmpf,
  author = {Felix J. Herrmann},
  title = {Multiple prediction from incomplete data},
  booktitle = {SINBAD 2006},
  year = {2006},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/16-Felix7.pdf}
}

@CONFERENCE{herrmann06SINBADom,
  author = {Felix J. Herrmann},
  title = {Opening meeting},
  booktitle = {SINBAD 2006},
  year = {2006},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/1-Felix1.pdf}
}

@CONFERENCE{herrmann06SINBADpms,
  author = {Felix J. Herrmann and Urs Boeniger and Dirk-Jacob Verschuur},
  title = {Primary-multiple separation by curvelet frames},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {Predictive multiple suppression methods consist of two main steps:
	a prediction step, during which multiples are predicted from seismic
	data, and a primary-multiple separation step, during which the predicted
	multiples are {\textquoteright}matched{\textquoteright} with the
	true multiples in the data and subsequently removed. The last step
	is crucial in practice: an incorrect separation will cause residual
	multiple energy in the result or may lead to a distortion of the
	primaries, or both. To reduce these adverse effects, a new transformed-domain
	method is proposed where primaries and multiples are separated rather
	than matched. This separation is carried out on the basis of differences
	in the multiscale and multidirectional characteristics of these two
	signal components. Our method uses the curvelet transform, which
	maps multidimensional data volumes into almost orthogonal localized
	multidimensional prototype waveforms that vary in directional and
	spatio-temporal content. Primaries-only and multiples-only signal
	components are recovered from the total data volume by a nonlinear
	optimization scheme that is stable under noisy input data. During
	the optimization, the two signal components are separated by enhancing
	sparseness (through weighted l1-norms) in the transformed domain
	subject to fitting the observed data as the sum of the separated
	components to within a user-defined tolerance level. Whenever the
	prediction for the two signal components in the transformed domain
	correlate, the recovery is suppressed while for regions where the
	correlation is small the method seeks the sparsest set of coefficients
	that represent each signal component. Our algorithm does not seek
	a matched filter and as such it differs fundamentally from traditional
	adaptive subtraction methods. The method derives its stability from
	the sparseness obtained by a non-parametric multiscale and multidirectional
	overcomplete signal representation. This sparsity serves as prior
	information and allows for a Bayesian interpretation of our method
	during which the log-likelihood function is minimized while the two
	signal components are assumed to be given by a superposition of prototype
	waveforms, drawn independently from a probability function that is
	weighted by the predicted primaries and multiples. In this paper,
	the predictions are based on the data-driven surface-related multiple
	elimination (SRME) method. Synthetic and field data examples show
	a clean separation leading to a considerable improvement in multiple
	suppression compared to the conventional method of adaptive matched
	filtering. This improved separation translates into an improved stack.},
  keywords = {Presentation, SINBAD, SLIM},
doi={10.1111/j.1365-246X.2007.03360.x},
Volume={170},
pages={781-799},
organization={Geophysical Journal International},
  url = {http://slim.eos.ubc.ca/Publications/Public/Journals/herrmann07npm.pdf}

}

@CONFERENCE{herrmann06SINBADsac,
  author = {Felix J. Herrmann},
  title = {Sparsity- and continuity-promoting seismic image recovery with curvelets},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {A nonlinear singularity-preserving solution to seismic image recovery
	with sparseness and continuity constraints is proposed. The method
	explicitly explores the curvelet transform as a directional frame
	expansion that, by virtue of its sparsity on seismic images and its
	invariance under the Hessian of the linearized imaging problem, allows
	for a stable recovery of the migration amplitudes from noisy data.
	The method corresponds to a preconditioning that corrects the amplitudes
	during a post-processing step. The solution is formulated as a nonlinear
	optimization problem where sparsity in the curvelet domain as well
	as continuity along the imaged reflectors are jointly promoted. To
	enhance sparsity, the l1-norm on the curvelet coefficients is minimized
	while continuity is promoted by minimizing an anisotropic diffusion
	norm on the image. The performance of the recovery scheme is evaluated
	with {\textquoteright}wave-equation{\textquoteright} migration code
	on a synthetic dataset. This is joint work with Peyman Moghaddam.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/14-Felix6.pdf}
}

@CONFERENCE{herrmann06SINBADsra,
  author = {Felix J. Herrmann},
  title = {Stable recovery and separation of seismic data},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {During this presentation an overview will be given on how seismic
	data regularization and separation problems can be cast into the
	framework of stable signal recovery. It is shown that the successful
	solution of these two problems depends on the existence of signal
	expansions that are compressible. Preliminary examples will be shown.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/3-Felix2.pdf}
}

@CONFERENCE{lin06SINBADci,
  author = {Tim T.Y. Lin},
  title = {Compressed imaging},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {In 1998 Grimbergen et. al. introduced a new method for computing wavefield
	propagation which improved on the previously employed local explicit
	operator method in that it exhibited no dip limitation, accurately
	handled laterally varying background ground velocity models, and
	is unconditionally stable. These desirable properties are mainly
	attributed to bringing the propagation problem into an eigenvector
	basis that diagonalizes the propagation operators. This modal-transform
	method, however, requires at each depth-level the solution of a large-scale
	sparse eigenvalue problem to compute the square-root of the Helmholtz
	operator. By using recent results from compressed sensing, we hope
	to reduce these computational costs that typically involve the synthesizes
	of the imaging operators and the cost of matrix-vector products.
	To reduce these costs, we compress the extrapolation operators by
	using only a fraction of the positive eigenvalues and temporal frequencies.
	This reduction not only leads to smaller matrices but also to reduced
	synthesis costs. These reductions go at the expense of solving a
	recovery problem from incomplete data. During the presentation, we
	show that wavefields can accurately be extrapolated with a compressed
	operators and competitive costs.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/17-Tim1.pdf}
}


@CONFERENCE{maysami06SINBADrro,
  author = {Mohammad Maysami},
  title = {Recent results on seismic deconvolution},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {One of the important steps in seismic imaging is to provide suitable
	information about boundaries. Sharp variation of physical properties
	at a layer boundary cause reflection the wavefield. In previous work
	done by C. M. Dupuis, seismic signal characterization is divided
	into two steps: detection and estimation. In the detection phase,
	the goal is to find all singularities in a seismic section regardless
	of their order and then to categorize the data to different events
	by windowing each singularity. In the estimation step, we determine
	the order of singularity more precisely by using a rough estimate
	based on the detection phase. Traditionally, a redundant dictionary
	method is employed for the detection part. However, we attempt to
	instead use a new L1-solver developed by D.L. Donoho: the Stagewise
	Orthogonal Matching Pursuit (StOMP). It approximates the solution
	to inverse problems while promoting the sparsity in the solution
	vector. This algorithm will allow us to experimentally confirm the
	recent analysis by S. Mallat on spiky deconvolution limits, which
	imposes a required minimum distance between spikes. This required
	minimum distance between different spikes is dependent on the number
	of spikes as well as the width of the chosen source wavelet used
	in convolution with the train. These results allow for the design
	of more robust and accurate detection schemes for seismic signal
	characterization.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2006/maysami06SINBADrrs.pdf}
}


@CONFERENCE{modzelewski06SINBADdas,
  author = {Henryk Modzelewski},
  title = {Design and specifications for SLIM{\textquoteright}s software framework},
  booktitle = {SINBAD 2006},
  year = {2006},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/19-Henryk1.pdf}
}

@CONFERENCE{moghaddam06SINBADioa,
  author = {Peyman P. Moghaddam},
  title = {Imaging operator approximation using Curvelets},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {In this presentation, the normal (demigation-migration) operator is
	studied in terms of a pseudo-differential operator. The invariance
	of curvelets under this operator and their sparsity on the seismic
	images is used to precondition the migration operator. A brief overview
	will be given on some of the theory from micro-local analysis which
	proofs that curvelets remain approximately invariant under the operator.
	The proper setting for which a diagonal approximation in the curvelet
	domain is accurate is discussed together with different methods that
	estimate this diagonal from of-the-shelf migration operators. This
	is joint work with Chris Stolk.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/15-Peyman2.pdf}
}

@CONFERENCE{moghaddam06SINBADsac,
  author = {Peyman P. Moghaddam},
  title = {Sparsity- and continuity-promoting norms for seismic images},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {During this presentation, the importance of sparsity and continuity
	enhancing energy norms is emphasized for seismic imaging and inversion.
	The continuity promoting energy norm is justified by the apparent
	smoothness of reflectors in the direction along and the oscillatory
	behavior across the interfaces. This energy norm is called anisotropic
	diffusion and will be defined mathematically. Denoising examples
	will be given during which seismic images are recovered from the
	noise by a joint norm-one and continuity promoting minimization.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/13-Peyman1.pdf}
}

@CONFERENCE{sastry07SINBADrfu,
  author = {Challa S. Sastry and Gilles Hennenfent and Felix J. Herrmann},
  title = {Recovery from unstructured data},
  booktitle = {SINBAD 2006},
  year = {2006},
  keywords = {SLIM, SINBAD, Presentation},
  presentation= {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/11-Sastry1.pdf}
}

@CONFERENCE{thomson06SINBADlss,
  author = {Darren Thomson},
  title = {Large-scale seismic data recovery by the Parallel Windowed Curvelet
	Transform},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {We propose using overlapping, tapered windows to process seismic data
	in parallel. This method consists of numerically tight linear operators
	and adjoints that are suitable for use in iterative algorithms. This
	method is also highly scalable and makes parallelprocessing of large
	seismic data sets feasible. We use this scheme to define the Parallel
	Windowed Fast Discrete Curvelet Transform (PWFDCT), which we have
	applied to a seismic data interpolation algorithm. Some preliminary
	results will be shown. Henryk Modzeleweski: Design and specifications
	for SLIMPy's software framework The SLIM group is actively developing
	software for seismic imaging. This talk will give a general overview
	of the software development philosophy adopted by SLIM. The covered
	topics will include: 1) adopting Python for object-oriented programming,
	2) including parallelism into the algorithms used in seismic imaging/modeling,
	3) in-house algorithms for seismic imaging, and 4) contributions
	to Madagascar (RSF). The talk will serve as an introduction to the
	other presentations in the session ``SINBAD Software releases".},
  keywords = {SLIM, SINBAD, Presentation},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/18-Darren1.pdf}
}


@CONFERENCE{thomson06SEGpwfd,
  author = {Darren Thomson and Gilles Hennenfent and Henryk Modzelewski and Felix
	J. Herrmann},
  title = {A parallel windowed fast discrete curvelet transform applied to seismic
	processing},
  year = {2006},
  organization = {SEG},
  abstract = {We propose using overlapping, tapered windows to process seismic data
	in parallel. This method consists of numerically tight linear oper
	ators and adjoints that are suitable for use in iterative algorithms.
	This method is also highly scalable and makes parallel processing
	of large seismic data sets feasible. We use this scheme to define
	the Parallel Windowed Fast Discrete Curvelet Transform (PWFDCT),
	which we apply to a seismic data interpolation algorithm. The successful
	performance of our parallel processing scheme and algorithm on
	a two-dimensional synthetic data is shown.},
  keywords = {SEG},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2006/thomson06SEGpwfd/thomson06SEGpwfd.pdf }
}



@CONFERENCE{thomson06SINBADppe,
  author = {Darren Thomson},
  title = {(P)SLIMPy: parallel extension},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {The parallel extensions to the SLIMpy environment enable pipe-based
	processing of large data sets in an MPI-based parallel environment.
	Parallel processing can be done by straightforward slicing of data,
	or by using an overlapping domain decomposition that requires communication
	between different processors. The principal aim of the parallel extensions
	is to leave abstract numerical algorithms (ANA's) and applications
	programmed for use in SLIMpy untouched when moving to parallel processing.
	The object-oriented functionality of Python makes this possible.},
  keywords = {SLIM, SINBAD, Presentation},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/21-Darren2.pdf}
}


@CONFERENCE{yarham06SEGcgrr,
  author = {Carson Yarham and Urs Boeniger and Felix Herrmann},
  title = {Curvelet-based ground roll removal},
  year = {2006},
  organization = {SEG},
  abstract = {We have effectively identified and removed ground roll through a two-
	step process. The first step is to identify the major components
	of the ground roll through various methods including multiscale separation,
	directional or frequency filtering or by any other method that identifies
	the ground roll. Given this estimate for ground roll, the recorded
	signal is separated during the second step through a block-coordinate
	relaxation method that seeks the sparsest set for weighted curvelet
	coefficients of the ground roll and the sought-after reflectivity.
	The combination of these two methods allows us to separate out the
	ground roll signal while preserving the reflector information. Since
	our method is iterative, we have control of the separation process.
	We successfully tested our algorithm on a real data set with a complex
	ground roll and reflector structure.},
  keywords = {SEG},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2006/yarham06SEGcgrr/yarham06SEGcgrr.pdf } 
}




%------------------------Haneet's Section ------------------ 2005-2001-------------------------------------------------------------------%      



@CONFERENCE{Beyreuther05SEGcot,
  author       = {Moritz Beyreuther and Jamin Cristall and Felix J. Herrmann},
  title        = {Computation of time-lapse differences with {3-D} directional frames},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2005},
  volume       = {24},
  number       = {1},
  pages        = {2488-2491},
  organization = {SEG},
  publisher    = {SEG},
  abstract     = {We present an alternative method of extracting production related
        differences from time-lapse seismic data sets. Our method is not
        based on the actual subtraction of the two data sets, risking the
        enhancement of noise and introduction of artifacts due to local phase
        rotation and slightly misaligned events. Rather, it mutes events
        of the monitor survey with respect to the baseline survey based on
        the magnitudes of coefficients in a sparse and local atomic decomposition.
        Our technique is demonstrated to be an effective tool for enhancing
        the time-lapse signal from surveys which have been cross-equalized.
        {\copyright}2005 Society of Exploration Geophysicists},
  doi          = {10.1190/1.2148227},
  keywords     = {SLIM},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2005/Beyreuther05SEGcot/Beyreuther05SEGcot.pdf },   
url2         = {http://dx.doi.org/10.1190/1.2148227}
}




@CONFERENCE{Hennenfent05SEGscd,
  author       = {Gilles Hennenfent and Felix J. Herrmann},
  title        = {Sparseness-constrained data continuation with frames: applications
                  to missing traces and aliased signals in {2/3-D}},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2005},
  volume       = {24},
  number       = {1},
  pages        = {2162-2165},
  organization = {SEG},
  publisher    = {SEG},
  abstract     = {We present a robust iterative sparseness-constrained interpolation
        algorithm using 2-/3-D curvelet frames and Fourier-like transforms
        that exploits continuity along reflectors in seismic data. By choosing
        generic transforms, we circumvent the necessity to make parametric
        assumptions (e.g. through linear/parabolic Radon or demigration)
        regarding the shape of events in seismic data. Simulation and real
        data examples for data with moderately sized gaps demonstrate that
        our algorithm provides interpolated traces that accurately reproduce
        the wavelet shape as well as the AVO behavior. Our method also shows
        good results for de-aliasing judged by the behavior of the ($f-k$)-spectrum
        before and after regularization. {\copyright}2005 Society of Exploration
        Geophysicists},
  doi          = {10.1190/1.2148142},
  keywords     = {SEG, SLIM},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2005/Hennenfent05SEGscd/Hennenfent05SEGscd.pdf },
presentation= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2005/Hennenfent05SEGscd/Hennenfent05SEGscd_pres.pdf  }
}




@CONFERENCE{Hennenfent05CSEGscs,
  author       = {Gilles Hennenfent and Felix J. Herrmann and R. Neelamani},
  title        = {Sparseness-constrained seismic deconvolution with curvelets},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2005},
  month        = {May},
  organization = {CSEG},
  publisher    = {CSEG},
  abstract     = {Continuity along reflectors in seismic images is used via Curvelet
        representation to stabilize the convolution operator inversion. The
        Curvelet transform is a new multiscale transform that provides sparse
        representations for images that comprise smooth objects separated
        by piece-wise smooth discontinuities (e.g. seismic images). Our iterative
        Curvelet-regularized deconvolution algorithm combines conjugate gradient-based
        inversion with noise regularization performed using non-linear Curvelet
        coefficient thresholding. The thresholding operation enhances the
        sparsity of Curvelet representations. We show on a synthetic example
        that our algorithm provides improved resolution and continuity along
        reflectors as well as reduced ringing effect compared to the iterative
        Wiener-based deconvolution approach.},
  keywords     = {SLIM},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2005/Hennenfent05CSEGscs/Hennenfent05CSEGscs.pdf},
presentation={https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2005/Hennenfent05CSEGscs/Hennenfent05CSEGscs_pres.pdf }

}




@CONFERENCE{Hennenfent05EAGEsdr,
  author       = {Gilles Hennenfent and R. Neelamani and Felix J. Herrmann},
  title        = {Seismic deconvolution revisited with curvelet frames},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2005},
  month        = {June},
  organization = {EAGE},
  publisher    = {EAGE},
  abstract     = {We propose an efficient iterative curvelet-regularized deconvolution
        algorithm that exploits continuity along reflectors in seismic images.
        Curvelets are a new multiscale transform that provides sparse representations
        for images (such as seismic images) that comprise smooth objects
        separated by piece-wise smooth discontinuities. Our technique combines
        conjugate gradient-based convolution operator inversion with noise
        regularization that is performed using non-linear curvelet coefficient
        shrinkage (thresholding). The shrinkage operation leverages the sparsity
        of curvelets representations. Simulations demonstrate that our algorithm
        provides improved resolution compared to the traditional Wiener-based
        deconvolution approach.},
  keywords     = {SLIM, EAGE},
url={https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2005/Hennenfent05EAGEsdr/Hennenfent05EAGEsdr.pdf },
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/hennenfent05eage_poster.pdf}
}




@CONFERENCE{Herrmann05CSEGnld,
  author       = {Felix J. Herrmann and Gilles Hennenfent},
  title        = {Non-linear data continuation with redundant frames},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2005},
  month        = {May},
  organization = {CSEG},
  publisher    = {CSEG},
  abstract     = {We propose an efficient iterative data interpolation method using
        continuity along reflectors in seismic images via curvelet and discrete
        cosine transforms. The curvelet transform is a new multiscale transform
        that provides sparse representations for images that comprise smooth
    The latter is motivated by the successful data continuation with
        the discrete Fourier transform. By choosing generic basis functions
        we circumvent the necessity to make parametric assumptions (e.g.
        through linear/parabolic Radon or demigration) regarding the shape
        of events in seismic data. Synthetic and real data examples demonstrate
        that our algorithm provides interpolated traces that accurately reproduce
        the wavelet shape as well as the AVO behavior along events in shot
        gathers.},
  keywords     = {SLIM},
url={https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2005/Herrmann05CSEGnlr/Herrmann05CSEGnld.pdf },
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2005/Herrmann05CSEGnlr/Herrmann05CSEGnld_pres.pdf}
}




@CONFERENCE{Herrmann05CSEGnlr,
  author       = {Felix J. Herrmann and Peyman P. Moghaddam},
  title        = {Non-linear regularization in seismic imaging},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2005},
  month        = {May},
  organization = {CSEG},
  publisher    = {CSEG},
  abstract     = {Two complementary solution strategies to the least-squares imaging
        problem with sparseness \& continuity continuity constraints are
        proposed. The applied formalism explores the sparseness of curvelets
        coefficients of the reflectivity and their invariance under the demigration-migration
        operator. We achieve the solution by jointly minimizing a weighted
        l1-norm on the curvelet coefficients and an anisotropic difussion
        or total variation norm on the imaged reflectivity model. The l1-norm
        exploits the sparsenss of the reflectivity in the curvelet domain
        whereas the anisotropic norm enhances the continuity along the reflections
        while removing artifacts residing in between reflectors. While the
        two optimization methods (convex versus non-convex) share the same
        type of regularization, they differ in flexibility how to handle
        additional constraints on the coefficients of the imaged reflectivity
        and in computational expense. A brief sketch of the theory is provided
        along with a number of synthetic examples.},
  keywords     = {SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2005/Herrmann05CSEGnlr/Herrmann05CSEGnlr.pdf},
presentation={https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2005/Herrmann05CSEGnlr/Herrmann05CSEGnlr_pres.pdf }
}




@CONFERENCE{Herrmann05EAGEosf,
  author       = {Felix J. Herrmann and Peyman P. Moghaddam and R. Kirlin},
  title        = {Optimization strategies for sparseness- and continuity-enhanced imaging:
                  theory},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2005},
  month        = {June},
  organization = {EAGE},
  publisher    = {EAGE},
  abstract     = {Two complementary solution strategies to the least-squares migration
        problem with sparseness- and continuity constraints are proposed.
        The applied formalism explores the sparseness of curvelets on the
        reflectivity and their invariance under the demigration-migration
        operator. Sparseness is enhanced by (approximately) minimizing a
        (weighted) l1-norm on the curvelet coefficients. Continuity along
        imaged reflectors is brought out by minimizing the anisotropic diffusion
        or total variation norm which penalizes variations along and in between
        reflectors. A brief sketch of the theory is provided as well as a
        number of synthetic examples. Technical details on the implementation
        of the optimization strategies are deferred to an accompanying paper:
        implementation.},
  keywords     = {SLIM, EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2005/Herrmann05EAGEosf/Herrmann05EAGEosf.pdf}
}




@CONFERENCE{Herrmann05EAGErcd,
  author       = {Felix J. Herrmann and Gilles Hennenfent},
  title        = {Robust curvelet-domain data continuation with sparseness constraints},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2005},
  month        = {June},
  organization = {EAGE},
  publisher    = {EAGE},
  abstract     = {A robust data interpolation method using curvelets frames is presented.
        The advantage of this method is that curvelets arguably provide an
        optimal sparse representation for solutions of wave equations with
        smooth coefficients. As such curvelets frames circum- vent {\textendash}
        besides the assumption of caustic-free data {\textendash} the necessity
        to make parametric assumptions (e.g. through linear/parabolic Radon
        or demigration) regarding the shape of events in seismic data. A
        brief sketch of the theory is provided as well as a number of examples
        on synthetic and real data.},
  keywords     = {SLIM, EAGE},
url ={https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2005/Herrmann05EAGErcd/Herrmann05EAGErcd.pdf },
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2005/Herrmann05EAGErcd/Herrmann05EAGErcd.pdf}
}




@CONFERENCE{Herrmann05EAGErcd1,
  author       = {Felix J. Herrmann and D. J. Verschuur},
  title        = {Robust curvelet-domain primary-multiple separation with sparseness
                  constraints},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2005},
  month        = {June},
  organization = {EAGE},
  publisher    = {EAGE},
  abstract     = {A non-linear primary-multiple separation method using curvelets frames
        is presented. The advantage of this method is that curvelets arguably
        provide an optimal sparse representation for both primaries and multiples.
        As such curvelets frames are ideal candidates to separate primaries
        from multiples given inaccurate predictions for these two data components.
        The method derives its robustness regarding the presence of noise;
        errors in the prediction and missing data from the curvelet frame{\textquoteright}s
        ability (i) to represent both signal components with a limited number
        of multi-scale and directional basis functions; (ii) to separate
        the components on the basis of differences in location, orientation
        and scales and (iii) to minimize correlations between the coefficients
        of the two components. A brief sketch of the theory is provided as
        well as a number of examples on synthetic and real data.},
  keywords     = {SLIM, EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2005/Herrmann05EAGErcd1/Herrmann05EAGErcd1.pdf}
}




@CONFERENCE{Beyreuther04EAGEcdo,
  author       = {Moritz Beyreuther and Felix J. Herrmann and Jamin Cristall},
  title        = {Curvelet denoising of {4-D} seismic},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2004},
  month        = {June},
  organization = {EAGE},
  publisher    = {EAGE},
  abstract     = {With burgeoning world demand and a limited rate of discovery of new
        reserves, there is increasing impetus upon the industry to optimize
        recovery from already existing fields. 4D, or time-lapse, seismic
        imaging is an emerging technology that holds great promise to better
        monitor and optimise reservoir production. The basic idea behind
        4D seismic is that when multiple 3D surveys are acquired at separate
        calendar times over a producing field, the reservoir geology will
        not change from survey to survey but the state of the reservoir fluids
        will change. Thus, taking the difference between two 3D surveys should
        remove the static geologic contribution to the data and isolate the
        time- varying fluid flow component. However, a major challenge in
        4D seismic is that acquisition and processing differences between
        3D surveys often overshadow the changes caused by fluid flow. This
        problem is compounded when 4D effects are sought to be derived from
        vintage 3D data sets that were not originally acquired with 4D in
        mind. The goal of this study is to remove the acquisition and imaging
        artefacts from a 4D seismic difference cube using Curveket processing techniques.},
  keywords     = {SLIM},
  url          = {http://slim.eos.ubc.ca/~felix/public/EAGE4D2004.pdf},
  url2         = {https://circle.ubc.ca/bitstream/handle/2429/453/EAGE4D2004.pdf?sequence=1}
}




@CONFERENCE{Cristall04CSEGcpa,
  author       = {Jamin Cristall and Moritz Beyreuther and Felix J. Herrmann},
  title        = {Curvelet processing and imaging: {4-D} adaptive subtraction},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2004},
  month        = {May},
  organization = {CSEG},
  publisher    = {CSEG},
  abstract     = {With burgeoning world demand and a limited rate of discovery of new
        reserves, there is increasing impetus upon the industry to optimize
        recovery from already existing fields. 4D, or time-lapse, seismic
        imaging holds great promise to better monitor and optimise reservoir
        production. The basic idea behind 4D seismic is that when multiple
        3D surveys are acquired at separate calendar times over a producing
        field, the reservoir geology will not change from survey to survey
        but the state of the reservoir fluids will change. Thus, taking the
        difference between two 3D surveys should remove the static geologic
        contribution to the data and isolate the time-varying fluid flow
        component. However, a major challenge in 4D seismic is that acquisition
        and processing differences between 3D surveys often overshadow the
        changes caused by fluid flow. This problem is compounded when 4D
        effects are sought to be derived from legacy 3D data sets that were
        not originally acquired with 4D in mind. The goal of this study is
        to remove the acquisition and imaging artefacts from a 4D seismic
        difference cube using Curvelet processing techniques.},
  keywords     = {SLIM},
  url          = {http://www.cseg.ca/conventions/abstracts/2004/2004abstracts/059S0201-Cristall_J_Curvelet_4D.pdf}
}




@CONFERENCE{Hennenfent04SEGtta,
  author       = {Gilles Hennenfent and Felix J. Herrmann},
  title        = {Three-term amplitude-versus-offset ({AVO}) inversion revisited by
                  curvelet and wavelet transforms},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2004},
  volume       = {23},
  number       = {1},
  pages        = {211-214},
  organization = {SEG},
  publisher    = {SEG},
  abstract     = {We present a new method to stabilize the three-term AVO inversion
        using Curvelet and Wavelet transforms. Curvelets are basis functions
        that effectively represent otherwise smooth objects having discontinuities
        along smooth curves. The applied formalism explores them to make
        the most of the continuity along reflectors in seismic images. Combined
        with Wavelets, Curvelets are used to denoise the data by penalizing
        high frequencies and small contributions in the AVO-cube. This approach
        is based on the idea that rapid amplitude changes along the ray-parameter
        axis are most likely due to noise. The AVO-inverse problem is linearized,
        formulated and solved for all (x, z) at once. Using densities and
        velocities of the Marmousi model to define the fluctuations in the
        elastic properties, the performance of the proposed method is studied
        and compared with the smoothing along the ray-parameter direction
        only. We show that our method better approximates the true data after
        the denoising step, especially when noise level increases. {\copyright}2004
        Society of Exploration Geophysicists},
  doi          = {10.1190/1.1851201},
  keywords     = {SLIM},
url= { https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Hennenfent04SEGtta/Hennenfent04SEGtta.pdf },
presentation= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Hennenfent04SEGtta/Hennenfent04SEGtta_pres.pdf }
}




@CONFERENCE{Herrmann04SEGcbn,
  author       = {Felix J. Herrmann and Peyman P. Moghaddam},
  year         = {2004},
  volume       = {23},
title={Curvelet-based non-linear adaptive subtraction with sparseness constraints},
  number       = {1},
  pages        = {1977-1980},
  organization = {SEG},
  publisher    = {SEG},
  abstract     = {In this paper an overview is given on the application of directional
        basis functions, known under the name Curvelets/Contourlets, to various
        aspects of seismic processing and imaging, which involve adaptive
        subtraction. Key concepts in the approach are the use of directional
        basis functions that localize in both domains (e.g. space and angle);
        non-linear estimation, which corresponds to localized muting on the
        coefficients, possibly supplemented by constrained optimization.
        We will discuss applications that include multiple, ground-roll removal
        and migration denoising. {\copyright}2004 Society of Exploration
        Geophysicists},
  doi          = {10.1190/1.1851181},
  keywords     = {SLIM},
presentation={https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Herrmann04SEGcbn/Herrmann04SEGcbn_pres.pdf  },
url={https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Herrmann04SEGcbn/Herrmann04SEGcbn.pdf },
  url2          = {http://slim.eos.ubc.ca/~felix/public/SEGAD2004.pdf}
}




@CONFERENCE{Herrmann04EAGEcdp,
  author       = {Felix J. Herrmann and Peyman P. Moghaddam},
  title        = {Curvelet-domain preconditioned 'wave-equation' depth-migration with sparseness and illumination constraints},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2004},
  month        = {June},
  organization = {EAGE},
  publisher    = {EAGE},
  abstract     = {A non-linear edge-preserving solution to the least-squares migration
        problem with sparseness constraints is introduced. The applied formalism
        explores Curvelets as basis functions that, by virtue of their sparseness
        and locality, not only allow for a reduction of the dimensionality
        of the imaging problem but which also naturally lead to a non-linear
        solution with significantly improved signal- to-noise ratio. Additional
        conditions on the image are imposed by solving a constrained optimization
        problem on the estimated Curvelet coefficients initialized by thresholding.
        This optimization is designed to also restore the amplitudes by (approximately)
        inverting the normal operator, which is like-wise the (de)-migration
        operators, almost diagonalized by the Curvelet transform.},
  keywords     = {SLIM},
  url          = {http://slim.eos.ubc.ca/Publications/Public/TechReports/herrmann04mpw.pdf}
}






@CONFERENCE{Herrmann04EAGEcdl,
  author       = {Felix J. Herrmann and Peyman P. Moghaddam},
  title        = {Curvelet-domain least-squares migration with sparseness constraints},
  year         = {2004},
  month        = {May},
  organization = {EAGE},
  publisher    = {EAGE},
  abstract     = {A non-linear edge-preserving solution to the least-squares migration
        problem with sparseness constraints is introduced. The applied formalism
        explores Curvelets as basis functions that, by virtue of their sparseness
        and locality, not only allow for a reduction of the dimensionality
        of the imaging problem but which also naturally lead to a non-linear
        solution with significantly improved signal-to-noise ratio. Additional
        conditions on the image are imposed by solving a constrained optimization
        problem on the estimated Curvelet coefficients initialized by thresholding.
        This optimization is designed to also restore the amplitudes by (approximately)
        inverting the normal operator, which is like-wise the (de)-migration
        operators, almost diagonalized by the Curvelet transform.},
  keywords     = {SLIM, EAGE},
url={https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2004/Herrmann04EAGEcdl/Herrmann04EAGEcdl.pdf },
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage04/herrmann04cdl.pdf}
}




@CONFERENCE{Herrmann04SEGcdm,
  author       = {Felix J. Herrmann and D. J. Verschuur},
  title        = {Curvelet-domain multiple elimination with sparseness constraints},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2004},
  volume       = {23},
  number       = {1},
  pages        = {1333-1336},
  organization = {SEG},
  publisher    = {SEG},
  abstract     = {Predictive multiple suppression methods consist of two main steps:
        a prediction step, in which multiples are predicted from the seismic
        data, and a subtraction step, in which the predicted multiples are
        matched with the true multiples in the data. The last step appears
        crucial in practice: an incorrect adaptive subtraction method will
        cause multiples to be sub-optimally subtracted or primaries being
        distorted, or both. Therefore, we propose a new domain for separation
        of primaries and multiples via the Curvelet transform. This transform
        maps the data into almost orthogonal localized events with a directional
        and spatial-temporal component. The multiples are suppressed by thresholding
        the input data at those Curvelet components where the predicted multiples
        have large amplitudes. In this way the more traditional filtering
        of predicted multiples to fit the input data is avoided. An initial
        field data example shows a considerable improvement in multiple suppression.
        {\copyright}2004 Society of Exploration Geophysicists},
  doi          = {10.1190/1.1851110},
  keywords     = {SLIM},
  presentation ={https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Herrmann04SEGcdm/Herrmann04SEGcdm_pres.pdf },
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Herrmann04SEGcdm/Herrmann04SEGcdm.pdf  },
  url2          = {http://slim.eos.ubc.ca/~felix/public/SEGM2004.pdf}
}




@CONFERENCE{Herrmann04CSEGcia,
  author       = {Felix J. Herrmann},
  title        = {Curvelet imaging and processing: an overview},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2004},
  month        = {May},
  organization = {CSEG},
  publisher    = {CSEG},
  abstract     = {In this paper an overview is given on the application of directional
        basis functions, known under the name Curvelets/Contourlets, to various
        aspects of seismic processing and imaging. Key conceps in the approach
        are the use of (i) directional basis functions that localize in both
        domains (e.g. space and angle); (ii) non-linear estimation, which
        corresponds to localized muting on the coefficients, possibly supplemented
        by constrained optimization (iii) invariance of the basis functions
        under the imaging operators. We will discuss applications that include
        multiple and ground roll removal; sparseness-constrained least-squares
        migration and the computation of 4-D difference cubes.},
  keywords     = {SLIM},
  url          = {http://slim.eos.ubc.ca/~felix/public/Herrmann_F_Curvelet_overview.doc}
}




@CONFERENCE{Herrmann04CSEGcia1,
  author       = {Felix J. Herrmann and D. J. Verschuur},
  title        = {Curvelet imaging and processing: adaptive multiple elimination},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2004},
  month        = {May},
  organization = {CSEG},
  publisher    = {CSEG},
  abstract     = {Predictive multiple suppression methods consist of two main steps:
        a prediction step, in which multiples are predicted from the seismic
        data, and a subtraction step, in which the predicted multiples are
        matched with the true multiples in the data. The last step appears
        crucial in practice: an incorrect adaptive subtraction method will
        cause multiples to be sub-optimally subtracted or primaries being
        distorted, or both. Therefore, we propose a new domain for separation
        of primaries and multiples via the Curvelet transform. This transform
        maps the data into almost orthogonal localized events with a directional
        and spatial-temporal component. The multiples are suppressed by thresholding
        the input data at those Curvelet components where the predicted multiples
        have large amplitudes. In this way the more traditional filtering
        of predicted multiples to fit the input data is avoided. An initial
        field data example shows a considerable improvement in multiple suppression.},
  keywords     = {SLIM},
  url          = {http://slim.eos.ubc.ca/~felix/public/Herrmann_F_Curvelet_multiple.doc}
}




@CONFERENCE{Herrmann04CSEGcia2,
  author       = {Felix J. Herrmann and Peyman P. Moghaddam},
  title        = {Curvelet imaging and processing: sparseness-constrained least-squares
        migration},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2004},
  month        = {May},
  organization = {CSEG},
  publisher    = {CSEG},
  abstract     = {A non-linear edge-preserving solution to the least-squares migration
        problem with sparseness constraints is introduced. The applied formalism
        explores Curvelets as basis functions that, by virtue of their sparseness
        and locality, not only allow for a reduction of the dimensionality
        of the imaging problem but which also naturally lead to a non-linear
        solution with significantly improved signal-to-noise ratio. Additional
        conditions on the image are imposed by solving a constrained optimization
        problem on the estimated Curvelet coefficients initialized by thresholding.
        This optimization is designed to also restore the amplitudes by (approximately)
        inverting the normal operator, which is, like-wise to the (de)-migration
        operators, almost diagonalized by the Curvelet transform.},
  keywords     = {SLIM},
  url          = {http://slim.eos.ubc.ca/~felix/public/Herrmann_F_Curvelet_imaging.doc}
}




@CONFERENCE{Herrmann04EAGEsop,
  author       = {Felix J. Herrmann and D. J. Verschuur},
  title        = {Separation of primaries and multiples by non-linear estimation in
        the curvelet domain},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2004},
  month        = {June},
  organization = {EAGE},
  publisher    = {EAGE},
  abstract     = {Predictive multiple suppression methods consist of two main steps:
        a prediction step, in which multiples are predicted from the seismic
        data, and a subtraction step, in which the predicted multiples are
        matched with the true multiples in the data. The last step appears
        crucial in practice: an incorrect adaptive subtraction method will
        cause multiples to be sub-optimally subtracted or primaries being
        distorted, or both. Therefore, we propose a new domain for separation
        of primaries and multiples via the Curvelet transform. This transform
        maps the data into almost orthogonal localized events with a directional
        and spatial-temporal component. The multiples are suppressed by thresholding
        the input data at those Curvelet components where the predicted multiples
        have large amplitudes. In this way the more traditional filtering
        of predicted multiples to fit the input data is avoided. An initial
        field data example shows a considerable improvement in multiple suppression.},
  keywords     = {SLIM},
  url          = {http://slim.eos.ubc.ca/~felix/public/EAGEM2004.pdf}
}




@CONFERENCE{Moghaddam04SEGmpw,
  author       = {Peyman P. Moghaddam and Felix J. Herrmann},
  title        = {Migration preconditioning with curvelets},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2004},
  volume       = {23},
  number       = {1},
  pages        = {2204-2207},
  organization = {SEG},
  publisher    = {SEG},
  abstract     = {In this paper, the property of Curvelet transforms for preconditioning
        the migration and normal operators is investigated. These operators
        belong to the class of Fourier integral operators and pseudo-differential
        operator, respectively. The effect of this pre-conditioner is shown
        in term of improvement of sparsity, convergence rate, number of iteration
        for the Krylov-subspace solver and clustering of singular(eigen)
        values. The migration operator, which we employed in this work is
        the common-offset Kirchoff-Born migration. {\copyright}2004 Society
        of Exploration Geophysicists},
  doi          = {10.1190/1.1845213},
  keywords     = {SLIM, SEG},
presentation= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Moghaddam04SEGmpw/Moghaddam04SEGmpw_pres.pdf },
  url          = {http://slim.eos.ubc.ca/~felix/public/SEGLAN2004.pdf}
}




@CONFERENCE{Yarham04CSEGcpa,
  author       = {Carson Yarham and Daniel Trad and Felix J. Herrmann},
  title        = {Curvelet processing and imaging: adaptive ground roll removal},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2004},
  month        = {May},
  organization = {CSEG},
  publisher    = {CSEG},
  abstract     = {In this paper we present examples of ground roll attenuation for synthetic
        and real data gathers by using Contourlet and Curvelet transforms.
        These non-separable wavelet transforms are locoalized both (x,t)-
        and (k,f)-domains and allow for adaptive seperation of signal and
        ground roll. Both linear and non-linear filtering are discussed using
        the unique properties of these basis that allow for simultaneous
        localization in the both domains. Eventhough, the linear filtering
        techniques are encouraging the true added value of these basis-function
        techniques becomes apparent when we use these decompositions to adaptively
        substract modeled ground roll from data using a non-linear thesholding
        procedure. We show real and synthetic examples and the results suggest
        that these directional-selective basis functions provide a usefull
        tool for the removal of coherent noise such as ground roll.},
  keywords     = {SLIM},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2004/Yarham04CSEGcpa/yarham04csegcpa.pdf}
}


@conference{Yarham04CSEGgrr,
author={Carson Yarham and  Felix J. Herrmann and Danial Trad},
title={Ground Roll Removal Using Non-Separable Wavelet Transforms },
year={2004},
presentation={https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2004/Yarham04CSEGgrr/Yarham04CSEGgrr.pdf }
}


@CONFERENCE{Herrmann03SEGoiw,
  author       = {Felix J. Herrmann},
  title        = {"Optimal" imaging with curvelets},
  booktitle    = {2003 SEG},
  year         = {2003},
  keywords     = {Presentation, SEG, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg03/herrmann03oiw.pdf}
}




@CONFERENCE{Herrmann03SPIEmsa,
  author       = {Felix J. Herrmann},
  title        = {Multifractional splines: application to seismic imaging},
  booktitle    = {Proceedings of SPIE Technical Conference on Wavelets: Applications
        in Signal and Image Processing X},
  year         = {2003},
  editor       = {Michael A. Unser and Akram Aldroubi and Andrew F. Laine},
  volume       = {5207},
  pages        = {240-258},
  organization = {SPIE},
  publisher    = {SPIE},
  abstract     = {Seismic imaging commits itself to locating singularities in the elastic
        properties of the Earth{\textquoteright}s subsurface. Using the high-frequency
        ray-Born approximation for scattering from non-intersecting smooth
        interfaces, seismic data can be represented by a generalized Radon
        transform mapping the singularities in the medium to seismic data.
        Even though seismic data are bandwidth limited, signatures of the
        singularities in the medium carry through this transform and its
        inverse and this mapping property presents us with the possibility
        to develop new imaging techniques that preserve and characterize
        the singularities from incomplete, bandwidth-limited and noisy data.
        In this paper we propose a non-adaptive Curvelet/Contourlet technique
        to image and preserve the singularities and a data-adaptive Matching
        Pursuit method to characterize these imaged singularities by Multi-fractional
        Splines. This first technique borrows from the ideas within the Wavelet-Vaguelette/Quasi-SVD
        approach. We use the almost diagonalization of the scattering operator
        to approximately compensate for (i) the coloring of the noise and
        hence facilitate estimation; (ii) the normal operator itself. Results
        of applying these techniques to seismic imaging are encouraging although
        many open fundamental questions remain.},
  keywords     = {SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/herrmann03msa.pdf},
  url          = {http://www.eos.ubc.ca/~felix/Preprint/SPIE03DEF.pdf}

}




@CONFERENCE{Herrmann01EAGEsas,
  author       = {Felix J. Herrmann},
  title        = {Scaling and seismic reflectivity: implications of scaling on {AVO}},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2001},
  month        = {June},
  organization = {EAGE},
  publisher    = {EAGE},
  abstract     = {AVO analysis of seismic data is based on the assumption that transitions
        in the earth consist of jump discontinuities only. Generalization
        of these transitions to more realistic transitions shows a drastic
        change in observed AVO behavior, especially for the large angles
        currently attained by increasing cable lengths. We propose a simple
ities. After renormalization, the inverted
        fluctuations regain their relative magnitudes which, due to the scaling,
        may have been significantly distorted.},
  keywords     = {SLIM},
}

@MASTERSTHESIS{lin08THccl,
  author = {Tim T.Y. Lin},
  title = {Compressed computation of large-scale wavefield extrapolation in
	inhomogeneous medium},
  year = {2008},
  type = {masters},
  month = {April},
  abstract = {In this work an explicit algorithm for the extrapolation of one-way
	wavefields is proposed which combines recent developments in information
	theory and theoretical signal processing with the physics of wave
	propagation. Because of excessive memory requirements, explicit formulations
	for wave propagation have proven to be a challenge in 3-D. By using
	ideas from {\textquoteleft}{\textquoteleft}compressed sensing{\textquoteright}{\textquoteright},
	we are able to formulate the (inverse) wavefield extrapolation problem
	on small subsets of the data volume, thereby reducing the size of
	the operators. Compressed sensing entails a new paradigm for signal
	recovery that provides conditions under which signals can be recovered
	from incomplete samplings by \emph{nonlinear} recovery methods that
	promote sparsity of the to-be-recovered signal. According to this
	theory, signals can successfully be recovered when the measurement
	basis is \emph{incoherent} with the representation in which the wavefield
	is sparse. In this new approach, the eigenfunctions of the Helmholtz
	operator are recognized as a basis that is incoherent with sparsity
	transforms that are known to compress seismic wavefields. By casting
	the wavefield extrapolation problem in this framework, wavefields
	can successfully be extrapolated in the modal domain, despite evanescent
	wave modes. The degree to which the wavefield can be recovered depends
	on the number of missing (evanescent) wave modes and on the complexity
	of the wavefield. A proof of principle for the {\textquoteleft}{\textquoteleft}compressed
	sensing{\textquoteright}{\textquoteright} method is given for inverse
	wavefield extrapolation in 2-D. The results show that our method
	is stable, has reduced dip limitations and handles evanescent waves
	in inverse extrapolation.},
  keywords = {BSc, SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/Theses/2008/timlin08bsch.pdf}
}
% This file was created with JabRef 2.6.
% Encoding: MacRoman
@MASTERSTHESIS{almatar10THesd,
  author = {Mufeed AlMatar},
  title = {Estimation of Surface-free Data by Curvelet-domain Matched Filtering
	and Sparse Inversion},
  school = {University of British Columbia},
  year = {2010},
  type = {masters},
  abstract = {A recent robust multiple-elimination technique, based on the underlying
	principle that relates primary impulse response to total upgoing
	wavefield, tries to change the paradigm that sees surface-related
	multiples as noise that needs to be removed from the data prior to
	imaging. This technique, estimation of primaries by sparse inversion
	(EPSI), (van Groenestijn and Verschuur, 2009; Lin and Herrmann, 2009),
	proposes an inversion procedure during which the source function
	and surface- free impulse response are directly calculated from the
	upgoing wavefield using an alternating optimization procedure. EPSI
	hinges on a delicate interplay between surface-related multiples
	and pri- maries. Finite aperture and other imperfections may violate
	this relationship. In this thesis, we investigate how to make EPSI
	more robust by incorporating curvelet- domain matching in its formulation.
	Compared to surface-related multiple removal (SRME), where curvelet-domain
	matching was used successfully, incorporating this step has the additional
	advantage that matches multiples to multiples rather than predicated
	multiples to total data as in SRME.},
  keywords = {MSc},
  url = {http://slim.eos.ubc.ca/Publications/public/theses/2010/AlMatarThesis.pdf}
}

@MASTERSTHESIS{alhashim09THsdp,
  author = {Fadhel Abbas Alhashim},
  title = {Seismic Data Processing with the Parallel Windowed Curvelet Transform
	Seismic Data Processing with the Parallel Windowed Curvelet Transform},
  school = {University of British Columbia},
  year = {2009},
  type = {masters},
  abstract = {The process of obtaining high quality seismic images is very challenging
	when exploring new areas that have high complexities. The to be processed
	seismic data comes from the field noisy and commonly incomplete.
	Recently, major advances were accomplished in the area of coherent
	noise removal, for example, Surface Related Multiple Elimination
	(SRME). Predictive multiple elimination methods, such as SRME, consist
	of two steps: The first step is the prediction step, in this step
	multiples are predicted from the seismic data. The second step is
	the separation step in which primary reflection and surface related
	multiples are separated, this involves predicted multiples from the
	first step to be matched
	with the true multiples in the data and eventually removed . A recent
	robust Bayesian wavefield separation method have been recently introduced
	to improve on the separation by matching methods. This method utilizes
	the effectiveness of using the multi scale and multi angular curvelet
	transform in processing seismic images. The method produced excellent
	results and improved multiple removal. A considerable problem in
	the seismic processing field is the fact that seismic data are large
	and require a correspondingly large memory size and processing time.
	The fact that curvelets are redundant also increases the need for
	large memory to process seismic data. In this thesis we propose a
	parallel approach based windowing operator that divides large seismic
	data into smaller more managable datasets that can fit in memory
	so that it is possible to apply the Bayesian separation pro- cess
	in parallel with minimal harm to the image quality and data integrity.
	However, by dividing the data, we introduce discontinuities. We take
	these discontinuities into account and compare two ways that different
	windows may communicate. The first method is to communicate edge
	information at only two steps, namely, data scattering and gathering
	processes while applying the multiple separation on each window separately.
	The second method is to define our windowing operator as a global
	operator, which exchanges window edge information at each forward
	and inverse curvelet transform. We discuss the trade off between
	the two methods trying to minimize complexity and I/O time spent
	in the process. We test our windowing operator on a seismic denoising
	problem and then apply the windowing operator on our sparse-domain
	Bayesian primary- multiple separation.},
  keywords = {MSc},
  URL = {http://slim.eos.ubc.ca/Publications/public/theses/2009/alhashim09sdp.pdf},
  presentation = {http://slim.eos.ubc.ca/Publications/public/presentations/2009/alhashim09sdp1.pdf}
}

@MASTERSTHESIS{kumar09THins,
  author = {Vishal Kumar},
  title = {Incoherent noise suppression and deconvolution using curvelet-domain
	sparsity},
  school = {University of British Columbia},
  year = {2009},
  type = {masters},
  abstract = {Curvelets are a recently introduced transform domain that belongs
	to a family of multiscale and also multidirectional data expansions.
	As such, curvelets can be applied to resolution of the issues of
	complicated seismic wavefronts. We make use of this multiscale, multidirectional
	and hence sparsifying ability of the curvelet transform to suppress
	incoherent noise from crustal data where the signal-to-noise ratio
	is low and to develop an improved deconvolution procedure. Incoherent
	noise present in seismic reflection data corrupts the quality of
	the signal and can often lead to misinterpretation. The curvelet
	domain lends itself particularly well for denoising because coherent
	seismic energy maps to a relatively small number of significant curvelet
	coefficents.},
  keywords = {MSc},
  url = {http://slim.eos.ubc.ca/Publications/Public/Theses/2009/kumar09ins.pdf}
}

@MASTERSTHESIS{lebed08THssr,
  author = {Evgeniy Lebed},
  title = {Sparse Signal Recovery in a Transform Domain},
  school = {The University of British Columbia},
  year = {2008},
  type = {masters},
  month = {August},
  abstract = {The ability to efficiently and sparsely represent seismic data is
	becoming an increasingly important problem in geophysics. Over the
	last thirty years many transforms such as wavelets, curvelets, contourlets,
	surfacelets, shear- lets, and many other types of x-lets
	have been developed. Such transform were leveraged to resolve this
	issue of sparse representations. In this work we compare the properties
	of four of these commonly used transforms, namely the shift-invariant
	wavelets, complex wavelets, curvelets and surfacelets. We also explore
	the performance of these transforms for the problem of recov- ering
	seismic wavefields from incomplete measurements.},
  keywords = {MSc},
  url = {http://slim.eos.ubc.ca/Publications/Public/Theses/2008/lebed08msc.pdf}
}

@MASTERSTHESIS{maysami08THlcs,
  author = {Mohammad Maysami},
  title = {Lithology constraints from seismic waveforms: application to opal-A
	to opal-CT transition},
  school = {The University of British Columbia},
  year = {2008},
  type = {masters},
  address = {Vancouver, BC Canada},
  abstract = {In this work, we present a new method for seismic waveform characterization,
	which is aimed at extracting detailed litho-stratigraphical information
	from seismic data. We attempt to estimate the lithological attributes
	from seismic data according to our parametric representation of stratigraphical
	horizons, where the parameter values provide us with a direct link
	to nature of lithological transitions. We test our method on a seismic
	dataset with a strong diagenetic transition (opal-A to opal-CT transition).
	Given some information from cutting samples of well, we use a percolation-based
	model to construct the elastic profile of lithological transitions.
	Our goal is to match parametric representation for the diagenetic
	transition in both real data and synthetic data given by these elastic
	profiles. This match may be interpreted as a well-seismic tie, which
	reveals lithological information about stratigraphical horizons.},
  keywords = {MSc},
  url = {http://slim.eos.ubc.ca/Publications/Public/Theses/2008/maysami08msc.pdf}
}

@MASTERSTHESIS{yarham08THsgs,
  author = {Carson Yarham},
  title = {Seismic ground-roll separation using sparsity promoting l1 minimization},
  school = {The University of British Columbia},
  year = {2008},
  address = {Vancouver, BC Canada},
  month = {May},
  abstract = {The removal of coherent noise generated by surface waves in land based
	seismic is a prerequisite to imaging the subsurface. These surface
	waves, termed as ground roll, overlay important reflector information
	in both the t-x and f-k domains. Standard techniques of ground roll
	removal commonly alter reflector information as a consequence of
	the ground roll removal. We propose the combined use of the curvelet
	domain as a sparsifying basis in which to perform signal separation
	techniques that can preserve reflector informa- tion while increasing
	ground roll removal. We examine two signal separation techniques,
	a block-coordinate relaxation method and a Bayesian separation method.
	The derivations and background for both methods are presented and
	the parameter sensitivity is examined. Both methods are shown to
	be effective in certain situations regarding synthetic data and erroneous
	surface wave predictions. The block-coordinate relaxation method
	is shown to have ma jor weaknesses when dealing with seismic signal
	separation in the pres- ence of noise and with the production of
	artifacts and reflector degradation. The Bayesian separation method
	is shown to improve overall separation for both seismic and real
	data. The Bayesian separation scheme is used on a real data set with
	a surface wave prediction containing reflector information. It is
	shown to improve the signal separation by recovering reflector information
	while improving the surface wave removal. The abstract contains a
	separate real data example where both the block-coordinate relaxation
	method and the Bayesian separation method are compared. },
  url= {http://slim.eos.ubc.ca/Publications/Public/Theses/2008/cyarham08msc.pdf},
  keywords = {MSc}
}

@MASTERSTHESIS{dupuis05THssc,
  author = {Catherine Dupuis},
  title = {Seismic singularity characterization with redundant dictionaries},
  school = {The University of British Columbia},
  year = {2005},
  type = {masters},
  address = {Vancouver, BC Canada},
  abstract = {We consider seismic signals as a superposition of waveforms parameterized
	by their fractional- orders. Each waveform models the reflection
	of a seismic wave at a particular transition between two lithological
	layers in the subsurface. The location of the waveforms in the seismic
	signal corresponds to the depth of the transitions in the subsurface,
	whereas their fractional-order constitutes a measure of the sharpness
	of the transitions. By considering fractional-order tran- sitions,
	we generalize the zero-order transition model of the conventional
	deconvolution problem, and aim at capturing the different types of
	transitions. The goal is to delineate and characterize transitions
	from seismic signals by recovering the locations and fractional-orders
	of its corre- sponding waveforms. This problem has received increasing
	interest, and several methods have been proposed, including multi-
	and monoscale analysis based on Mallat{\textquoteright}s wavelet
	transform modulus maxima, and seismic atomic decomposition. We propose
	a new method based on a two-step approach, which divides the initial
	problem of delineating and characterizing transitions over the whole
	seismic signal, into two easier sub- problems. The algorithm first
	partitions the seismic signal into its ma jor components, and then
	estimates the fractional-orders and locations of each component.
	Both steps are based on the sparse decomposition of seismic signals
	in overcomplete dictionaries of waveforms parameter- ized by their
	fractional-orders, and involve  1 minimizations solved by an iterative
	thresholding algorithm. We present the method and show numerical
	results on both synthetic and real data.},
  keywords = {MSc},
  url = {http://slim.eos.ubc.ca/Publications/Public/Theses/2005/dupuis05msc.pdf}
}
% This file was created with JabRef 2.6.
% Encoding: MacRoman

@techreport{herrmann10SEGerc,
  author = {Felix J. Herrmann},
  title = {Empirical recovery conditions for seismic sampling},
  year = {2010},
  organization = {SEG},
  abstract = {In this paper, we offer an alternative sampling method leveraging
        recent insights from compressive sensing towards seismic acquisition
        and processing for data that are traditionally considered to be undersampled.
        The main outcome of this approach is a new technology where acquisition
        and processing related costs are no longer determined by overly stringent
        sampling criteria, such as Nyquist. At the heart of our approach
        lies randomized incoherent sampling that breaks subsampling related
        interferences by turning them into harmless noise, which we subsequently
        remove by promoting transform-domain sparsity. Now, costs no longer
        grow with resolution and dimensionality of the survey area, but instead depend on transform-domain sparsity only. Our contribution is twofold. First, we demon- strate by means
        of carefully designed numerical experiments that compressive sensing
        can successfully be adapted to seismic acquisition. Second, we show
        that accurate recovery can be accomplished for compressively sampled
        data volumes sizes that exceed the size of conventional transform-domain
        data volumes by only a small factor. Because compressive sens- ing
        combines transformation and encoding by a single linear encoding
        step, this technology is directly applicable to acqui- sition and
        to dimensionality reduction during processing. In either case, sampling,
        storage, and processing costs scale with transform-domain sparsity.},
  keywords = {SEG},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/Publications/Public/Conferences/SEG/2010/herrmann10SEGerc/herrmann10SEGerc.pdf }

}


@techreport{almatar10SEGesfd,
  author = {Mufeed H. AlMatar and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Estimation of surface-free data by curvelet-domain matched filtering and sparse inversion},
year = {2010},
organization = {SEG},
  abstract = {Matching seismic wavefields and images lies at the heart of many pre-post-processing steps part of seismic imaging- whether one is matching predicted
        wavefield components, such as multiples, to the actual to-be-separated
        wavefield compo- nents present in the data or whether one is aiming
        to restore migration amplitudes by scaling, using an image-to-remigrated-
        image matching procedure to calculate the scaling coefficients. The
        success of these wavefield matching procedures depends on our ability
        to (i) control possible overfitting, which may lead to accidental
        removal of energy or to inaccurate image-amplitude corrections, (ii)
        handle data or images with nonunique dips, and (iii) apply subsequent
        wavefield separations or migraton amplitude corrections stably. In
        this paper, we show that the curvelet transform allows us to address
        all these issues by im- posing smoothness in phase space, by using
        their capability to handle conflicting dips, and by leveraging their
        ability to represent seismic data and images sparsely. This latter
        property renders curvelet-domain sparsity promotion an effective
        prior.},
 keywords = {SEG},
url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/almatar10SEGesfd/alma
tar10SEGesfd.pdf }


}


@TECHREPORT{vandenberg10TRsol,
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  title = {Sparse optimization with least-squares constraints},
  institution = {Department of Computer Science},
  year = {2010},
  type = {Tech. Rep.},
  number = {TR-2010-02},
  address = {University of British Columbia, Vancouver},
  month = {January},
  abstract = {The use of convex optimization for the recovery of sparse signals
	from incomplete or compressed data is now common practice. Motivated
	by the success of basis pursuit in recovering sparse vectors, new
	formulations have been proposed that take advantage of different
	types of sparsity. In this paper we propose an efficient algorithm
	for solving a general class of sparsifying formulations. For several
	common types of sparsity we provide applications, along with details
	on how to apply the algorithm, and experimental results.},
  url = {http://www.cs.ubc.ca/~mpf/papers/vdBergFriedlander11.pdf}
}

@TECHREPORT{vandenberg09TRter,
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  title = {Theoretical and empirical results for recovery from multiple measurements},
  institution = {Department of Computer Science},
  year = {2009},
  type = {Tech. Rep.},
  number = {TR-2009-7},
  address = {University of British Columbia, Vancouver},
  month = {September},
  abstract = {The joint-sparse recovery problem aims to recover, from sets of compressed
	measurements, unknown sparse matrices with nonzero entries restricted
	to a subset of rows. This is an extension of the single-measurement-vector
	(SMV) problem widely studied in compressed sensing. We analyze the
	recovery properties for two types of recovery algorithms. First,
	we show that recovery using sum-of-norm minimization cannot exceed
	the uniform recovery rate of sequential SMV using L1 minimization,
	and that there are problems that can be solved with one approach
	but not with the other. Second, we analyze the performance of the
	ReMBo algorithm [M. Mishali and Y. Eldar, IEEE Trans. Sig. Proc.,
	56 (2008)] in combination with L1 minimization, and show how recovery
	improves as more measurements are taken. From this analysis it follows
	that having more measurements than number of nonzero rows does not
	improve the potential theoretical recovery rate.},
  url = {http://www.cs.ubc.ca/~mpf/papers/vdBergFriedlander09.pdf}
}

@TECHREPORT{vandenberg07TRipr,
  author = {Ewout {van den} Berg and Michael P. Friedlander},
  title = {In pursuit of a root},
  institution = {Department of Computer Science},
  year = {2007},
  type = {Tech. Rep.},
  number = {TR-2007-19},
  address = {University of British Columbia, Vancouver},
  month = {June},
  abstract = {The basis pursuit technique is used to find a minimum one-norm solution
	of an un- derdetermined least-squares problem. Basis pursuit denoise
	fits the least-squares problem only approximately, and a single parameter
	determines a curve that traces the trade-off between the least-squares
	fit and the one-norm of the solution. We show that the function that
	describes this curve is convex and continuously differ- entiable
	over all points of interest. The dual solution of a least-squares
	problem with an explicit one-norm constraint gives function and derivative
	information needed for a root-finding method. As a result, we can
	compute arbitrary points on this curve. Numerical experiments demonstrate
	that our method, which relies on only matrix-vector operations, scales
	well to large problems.},
  url = {http://www.optimization-online.org/DB_HTML/2007/06/1708.html}
}

@TECHREPORT{vandenberg07TRsat,
  author = {E. van den Berg and Michael P. Friedlander and Gilles Hennenfent
	and Felix J. Herrmann and Rayan Saab and Ozgur Yilmaz},
  title = {Sparco: a testing framework for sparse reconstruction},
  institution = {UBC Computer Science Department},
  year = {2007},
  number = {TR-2007-20},
  abstract = {Sparco is a framework for testing and benchmarking algorithms for
	sparse reconstruction. It includes a large collection of sparse reconstruction
	problems drawn from the imaging, compressed sensing, and geophysics
	literature. Sparco is also a framework for implementing new test
	problems and can be used as a tool for reproducible research. Sparco
	is implemented entirely in Matlab, and is released as open-source
	software under the GNU Public License.},
  keywords = {SLIM, Sparco},
  presentation = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Ewout_Spar.pdf},
  url = {http://www.cs.ubc.ca/labs/scl/sparco/downloads.php?filename=sparco.pdf}
}

@TECHREPORT{erlangga08TRoam,
  author = {Yogi A. Erlangga and Reinhard Nabben},
  title = {On a multilevel projection Krylov method for the Helmholtz equation
	preconditioned by shifted Laplacian},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2008},
  number = {TR-2008-2},
  abstract = {In [Erlangga and Nabben, SIAM J. Sci. Comput. (2007), to appear],
	a multilevel Krylov method is proposed to solve linear systems with
	symmetric and nonsymmetric matrix of coefficients. This multilevel
	method is developed based on shifting (or projecting) some small
	eigen- values to the largest eigenvalue, leading to a more favorable
	spectrum for convergence acceleration of a Krylov subspace method.
	Such a projection is insensitive with respect to the approximation
	of the small eigenvalues to be projected, which for a particular
	choice of deflation subspaces is equivalent to solving a coarse-grid
	problem analogue to multigrid. Different from multigrid, in the multilevel
	Krylov method, however, the coarse-grid problem is solved by a Krylov
	method, whose convergence rate is further accelerated by applying
	projection to the coarse-grid system. A recursive application of
	projection and coarse-grid solve by a Krylov iterative method then
	leads to the multilevel Krylov method. The method has been successfully
	applied to 2D convection-diffusion problems for which a standard
	multigrid method fails to converge. In this paper, we extend this
	multilevel Krylov method to indefinite linear systems arising from
	a discretization of the Helmholtz equation, preconditioned by shifted
	Laplacian as introduced by [Erlangga, Oosterlee and Vuik, SIAM J.
	Sci. Comput. 27(2006), pp. 1471–1492]. Since in this case projection
	must be applied to the preconditioned system AM?1, the coarse-grid
	matrices are approximated by a product of some low dimension matrices
	associated with A and M. Within the Krylov iteration and projection
	step in each coarse-grid solve, a multigrid iteration is used to
	approximately invert the preconditioner. Hence, a multigrid-multilevel
	Krylov method results. Numerical results are given for high wavenumbers
	and show the effectiveness of the method for solving Helmholtz problems.
	Not only can the convergence be made almost independent of grid size
	h, but also only mildly independent of wavenumber k.},
  journal = {Elec. Trans. Numer. Anal.},
  keywords = {SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/Journals/Yogi/erlangga-nabben-helmholtz.pdf}
}







@TECHREPORT{hennenfent10TRnct,
  author = {Gilles Hennenfent and Lloyd Fenelon and Felix J. Herrmann},
  title = {Nonequispaced curvelet transform for seismic data reconstruction:
	a sparsity-promoting approach},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2010},
  number = {TR-2010-2},
  abstract = {Seismic data are typically irregularly sampled along spatial axes.
	This irregular sampling may adversely affect some key steps, e.g.,
	multiple prediction/attenuation or imaging, in the processing workflow.
	To overcome this problem almost every large dataset is regularized
	and/or interpolated. Our contribution is twofold. Firstly, we extend
	our earlier work on the nonequispaced fast discrete curvelet transform
	(NFDCT) and introduce a second generation of the transform. This
	new generation differs from the previous one by the approach taken
	to compute accurate curvelet coefficients from irregularly sampled
	data. The first generation relies on accurate Fourier coefficients
	obtained by an $\ell_2$-regularized inversion of the nonequispaced
	fast Fourier transform, while the second is based on a direct, $\ell_1$
	-regularized inversion of the operator that links curvelet coefficients
	to irregular data. Also, by construction, the NFDCT second generation
	is lossless, unlike the NFDCT first generation. This property is
	particularly attractive for processing irregularly sampled seismic
	data in the curvelet domain and bringing them back to their irregular
	recording locations with high fidelity. Secondly, we combine the
	NFDCT second generation with the standard fast discrete curvelet
	transform (FDCT) to form a new curvelet-based method, coined nonequispaced
	curvelet reconstruction with sparsity-promoting inversion (NCRSI),
	for the regularization and interpolation of irregularly sampled data.
	We demonstrate that, for a pure regularization problem, the reconstruction
	is very accurate. The signal-to-reconstruction error ratio is, in
	our example, above 40 dB. We also conduct combined interpolation
	and regularization experiments. The reconstructions for synthetic
	data are accurate, particularly when the recording locations are
	optimally jittered. The reconstruction in our real data example shows
	amplitudes along the main wavefronts smoothly varying with no obvious
	acquisition imprint; a result very competitive with results from
	other reconstruction methods overall.},
  url = {http://slim.eos.ubc.ca/Publications/Private/Journals/hennenfent2010nct.pdf}
}

@TECHREPORT{hennenfent08TRori,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {One-norm regularized inversion: learning from the Pareto curve},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2008},
  number = {TR-2008-5},
  abstract = {Geophysical inverse problems typically involve a trade off between
	data misfit and some prior. Pareto curves trace the optimal trade
	off between these two competing aims. These curves are commonly used
	in problems with two-norm priors where they are plotted on a log-log
	scale and are known as L-curves. For other priors, such as the sparsity-promoting
	one norm, Pareto curves remain relatively unexplored. First, we show
	how these curves provide an objective criterion to gauge how robust
	one-norm solvers are when they are limited by a maximum number of
	matrix-vector products that they can perform. Second, we use Pareto
	curves and their properties to define and compute one-norm compressibilities.
	We argue this notion is key to understand one-norm regularized inversion.
	Third, we illustrate the correlation between the one-norm compressibility
	and the performance of Fourier and curvelet reconstructions with
	sparsity promoting inversion.},
  keywords = {SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/TechReports/hennenfent08seg.pdf}
}

@TECHREPORT{herrmann10TRrsa,
  author = {Felix J. Herrmann},
  title = {Randomized sampling and sparsity: getting more information from fewer
	samples},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2010},
  number = {TR-2010-1},
  abstract = {Many seismic exploration techniques rely on the collection of massive
	data volumes that are subsequently mined for information during processing.
	While this approach has been extremely successful in the past, current
	efforts toward higher- resolution images in increasingly complicated
	regions of the Earth continue to reveal fundamental shortcomings
	in our workflows. Chiefly amongst these is the so-called {\textquoteleft}{\textquoteleft}curse
	of dimensionality{\textquoteright}{\textquoteright} exemplified by
	Nyquist{\textquoteright}s sampling criterion, which disproportionately
	strains current acquisition and processing systems as the size and
	desired resolution of our survey areas continues to increase. In
	this paper, we offer an alternative sampling method leveraging recent
	insights from compressive sensing towards seismic acquisition and
	processing for data that are traditionally considered to be undersampled.
	The main outcome of this approach is a new technology where acquisition
	and processing related costs are no longer determined by overly stringent
	sampling criteria, such as Nyquist. At the heart of our approach
	lies randomized incoherent sampling that breaks subsampling related
	interferences by turning them into harmless noise, which we subsequently
	remove by promoting transform-domain sparsity. Now, costs no longer
	grow significantly with resolution and dimensionality of the survey
	area, but instead depend on transform-domain sparsity only. Our contribution
	is twofold. First, we demonstrate by means of carefully designed
	numerical experiments that compressive sensing can successfully be
	adapted to seismic exploration. Second, we show that accurate recovery
	can be accomplished for compressively sampled data volumes sizes
	that exceed the size of conventional transform-domain data volumes
	by only a small factor. Because compressive sensing combines transformation
	and encoding by a single linear encoding step, this technology is
	directly applicable to acquisition and to dimensionality reduction
	during processing. In either case, sampling, storage, and processing
	costs scale with transform-domain sparsity. We illustrate this principle
	by means of number of case studies.},
  url = {http://slim.eos.ubc.ca/Publications/Public/Journals/herrmann2010rsa.pdf}
}


@TECHREPORT{herrmann08TRcmf,
  author = {Felix J. Herrmann and Peyman P. Moghaddam and Deli Wang},
  title = {Curvelet-domain matched filtering},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2008},
  number = {TR-2008-6.},
  abstract = {Matching seismic wavefields and images lies at the heart of many pre-/post-processing
	steps part of seismic imaging--- whether one is matching predicted
	wavefield components, such as multiples, to the actual to-be-separated
	wavefield components present in the data or whether one is aiming
	to restore migration amplitudes by scaling, using an image-to-remigrated-image
	matching procedure to calculate the scaling coefficients. The success
	of these wavefield matching procedures depends on our ability to
	(i) control possible overfitting, which may lead to accidental removal
	of energy or to inaccurate image-amplitude corrections, (ii) handle
	data or images with nonunique dips, and (iii) apply subsequent wavefield
	separations or migraton amplitude corrections stably. In this paper,
	we show that the curvelet transform allows us to address all these
	issues by imposing smoothness in phase space, by using their capability
	to handle conflicting dips, and by leveraging their ability to represent
	seismic data and images sparsely. This latter property renders curvelet-domain
	sparsity promotion an effective prior.},
  url = {http://slim.eos.ubc.ca/Publications/Public/TechReports/herrmann08segmat.pdf}
}


@TECHREPORT{lebed08TRhgg,
  author = {Evgeniy Lebed and Felix J. Herrmann},
  title = {A hitchhiker's guide to the galaxy of transform-domain sparsification},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2008},
  number = {TR-2008-4},
  abstract = {The ability to efficiently and sparsely represent seismic data is
	becoming an increasingly important problem in geophysics. Over the
	last decade many transforms such as wavelets, curervelets, contourlets,
	surfacelets, shearlets, and many other types of 'x-lets' have been
	developed to try to resolve this issue. In this abstract we compare
	the properties of four of these commonly used transforms, namely
	the shift-invariant wavelets, complex wavelets, curvelets and surfacelets.
	We also briefly explore the performance of these transforms for the
	problem of recovering seismic wavefields from incomplete measurements.},
  keywords = {SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/TechReports/lebed08seg.pdf}
}

@TECHREPORT{tang09TRdtr,
  author = {Gang Tang and Reza Shahidi and Jianwei Ma},
  title = {Design of two-dimensional randomized sampling schemes for curvelet-based
	sparsity-promoting seismic data recovery},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2009},
  number = {TR-2009-03},
  abstract = {The tasks of sampling, compression and reconstruction are very common
	and often necessary in seismic data processing due to the large size
	of seismic data. Curvelet-based Recovery by Sparsity-promoting Inversion,
	motivated by the newly developed theory of compressive sensing, is
	among the best recovery strategies for seismic data. The incomplete
	data input to this curvelet-based recovery is determined by randomized
	sampling of the original complete data. Unlike usual regular undersampling,
	randomized sampling can convert aliases to easy-to-eliminate noise,
	thus facilitating the process of reconstruction of the complete data
	from the incomplete data. Randomized sampling methods such as jittered
	sampling have been developed in the past that are suitable for curvelet-based
	recovery, however most have only been applied to sampling in one
	dimension. Considering that seismic datasets are usually higher dimensional
	and extremely large, in the present paper, we extend the 1D version
	of jittered sampling to two dimensions, both with underlying Cartesian
	and hexagonal grids. We also study separable and non-separable two
	dimensional jittered sampling, the former referring to the Kronecker
	product of two one-dimensional jittered samplings. These different
	categories of jittered sampling are compared against one another
	in terms of signal-to-noise ratio and visual quality, from which
	we find that jittered hexagonal sampling is better than jittered
	Cartesian sampling, while fully non-separable jittered sampling is
	better than separable sampling. Because in the image processing and
	computer graphics literature, sampling patterns with blue-noise spectra
	are found to be ideal to avoid aliasing, we also introduce two other
	randomized sampling methods, possessing sampling spectra with beneficial
	blue noise characteristics, Poisson Disk sampling and Farthest Point
	sampling. We compare these methods, and apply the introduced sampling
	methodologies to higher dimensional curvelet-based reconstruction.
	These sampling schemes are shown to lead to better results from CRSI
	compared to the other more traditional sampling protocols, e.g. regular
	subsampling.},
  url = {http://slim.eos.ubc.ca/Publications/Private/Journals/tang09dtr.pdf}
}

@TECHREPORT{wang08TRbss,
  author = {Deli Wang and Rayan Saab and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Bayesian-signal separation by sparsity promotion: application to
	primary-multiple separation},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2008},
  number = {TR-2008-1},
  month = {January},
  abstract = {Successful removal of coherent noise sources greatly determines the
	quality of seismic imaging. Major advances were made in this direction,
	e.g., Surface-Related Multiple Elimination (SRME) and interferometric
	ground-roll removal. Still, moderate phase, timing, amplitude errors
	and clutter in the predicted signal components can be detrimental.
	Adopting a Bayesian approach along with the assumption of approximate
	curvelet-domain independence of the to-be-separated signal components,
	we construct an iterative algorithm that takes the predictions produced
	by for example SRME as input and separates these components in a
	robust fashion. In addition, the proposed algorithm controls the
	energy mismatch between the separated and predicted components. Such
	a control, which was lacking in earlier curvelet-domain formulations,
	produces improved results for primary-multiple separation on both
	synthetic and real data.},
  keywords = {signal separation, SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Private/Journals/wang08bayes/paper_html/paper.html}
}

% This file was created with JabRef 2.6.
% Encoding: MacRoman

@unpublished{Friedlander11TRhdm,
  author = {Michael P. Friedlander and Mark Schmidt},
  title = {Hybrid deterministic-stochastic methods for data fitting},
  institution = {Department of Computer Science},
  year = {2011},
  address = {University of British Columbia, Vancouver},
  month = {04/2011},
  abstract = {Many structured data-fitting applications require the solution of
        an optimization problem involving a sum over a potentially large
        number of measurements. Incremental gradient algorithms (both deterministic
        and randomized) offer inexpensive iterations by sampling only subsets
        of the terms in the sum. These methods can make great progress initially,
        but often slow as they approach a solution. In contrast, full gradient
        methods achieve steady convergence at the expense of evaluating the
        full objective and gradient on each iteration. We explore hybrid
        methods that exhibit the benefits of both approaches. Rate of convergence
        analysis and numerical experiments illustrate the potential for the
        approach.},
  publisher = {Department of Computer Science},
  url = {http://www.cs.ubc.ca/~mpf/papers/FriedlanderSchmidt2011.pdf}
}


@unpublished{Herrmann11TRLlsqIm,
  author = {Felix J. Herrmann and Xiang Li},
  title = {Efficient least-squares imaging with sparsity promotion and compressive
        sensing},
  year = {2011},
  notes = { TR-2011-03},
  address = {University of British Columbia, Vancouver},
  month = {08/2011},
  abstract = {Seismic imaging is a linearized inversion problem relying on the minimization
        of a least-squares misfit functional as a function of the medium
        perturbation. The success of this procedure hinges on our ability
        to handle large systems of equations√ëwhose size grows exponentially
        with the demand for higher reso- lution images in more and more complicated
        areas√ëand our ability to invert these systems given a limited amount
        of computational resources. To overcome this √ícurse of dimensionality√ì
        in problem size and computational complexity, we propose a combination
        of randomized dimensionality-reduction and divide-and- conquer techniques.
        This approach allows us to take advantage of sophisticated sparsity-promoting
        solvers that work on a series of smaller subproblems each in- volving
        a small randomized subset of data. These subsets correspond to artificial
        simultaneous-source experiments made of random superpositions of
        sequential- source experiments. By changing these subsets after each
        subproblem is solved, we are able to attain an inversion quality
        that is competitive while requiring fewer computational, and possibly,
        fewer acquisition resources. Application of this con- cept to a controlled
        series of experiments showed the validity of our approach and the
        relationship between its efficiency√ëby reducing the number of sources
        and hence the number of wave-equation solves√ëand the image quality.
        Application of our dimensionality-reduction methodology with sparsity
        promotion to a com- plicated synthetic with well-constrained structure
        also yields excellent results underlining the importance of sparsity
        promotion.},
  url = {http://slim.eos.ubc.ca/Publications/private/Journals/leastsquaresimag.pdf}
}


@unpublished{Herrmann11TRfcd,
  author = {Felix J. Herrmann and Michael P. Friedlander and Ozgur Yilmaz},
  title = {Fighting the curse of dimensionality: compressive sensing in exploration
        seismology},
  year = {2011},
  address = {University of British Columbia, Vancouver},
  month = {08/2011},
  abstract = {Many seismic exploration techniques rely on the collection of massive
        data volumes that are mined for infor- mation during processing.
        This approach has been extremely successful, but current efforts
        toward higher-resolution images in increasingly complicated regions
        of the Earth continue to reveal fundamental shortcomings in our typical
        workflows. The √ícurse of dimensionality√ì is the main roadblock, and
        is exemplified by Nyquist√ïs sampling criterion, which disproportionately
        strains current acquisition and processing systems as the size and
        desired resolution of our survey areas continues to increase. We
        offer an alternative sampling strategy that leverages recent insights
        from compressive sensing towards seismic acquisition and processing
        for data that are traditionally considered to be undersampled. The
        main outcome of this approach is a new technology where acquisition
        and processing related costs are no longer determined by overly stringent
        sampling criteria. Compressive sensing is a novel nonlinear sampling
        paradigm, effective for acquiring signals that have a sparse representation
        in some transform domain. We review basic facts about this new sampling
        paradigm that revolutionized various areas of signal processing,
        and illustrate how it can be successfully exploited in various problems
        in seismic exploration to effectively fight the curse of dimensionality.},
  url = {http://slim.eos.ubc.ca/Publications/Private/Journals/sigprocmag.pdf}
}


@unpublished{Mansour11TRwmmw,
  author = {Hassan Mansour and Ozgur Yilmaz.},
  title = {Weighted -$\ell_1$ minimization with multiple weighting sets},
  year = {2011},
  type = {Tech. Rep.},
  address = {University of British Columbia, Vancouver},
  month = {09/2011},
notes= {TR-2011-07 },
  abstract = {In this paper, we study the support In this paper, we study the support
        recovery conditions of weighted -$\ell_1$ minimization for signal
        reconstruction from compressed sensing measurements when multiple
        support estimate sets with different accuracy are available. We identify
        a class of signals for which the recovered vector from -$\ell_1$
        minimization provides an accurate support estimate. We then derive
        stability and robustness guarantees for the weighted -$\ell_1$ minimization
        problem with more than one support estimate. We show that applying
        a smaller weight to support estimate that enjoy higher accuracy improves
        the recovery conditions compared with the case of a single support
        estimate and the case with standard, i.e., non-weighted,-$\ell_1$
        minimization. Our theoretical results are supported by numerical
        simulations on synthetic signals and real audio signals.},
  url = {http://slim.eos.ubc.ca/Publications/private/Journals/MansourYilmaz2011.pdf }
}




@unpublished{Mansour11TRssma,
  author = {Hassan Mansour and Haneet Wason and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Simultaneous-source marine acquisition with compressive sampling
        matrices},
  year = {2011},
notes= { TR-2011-04 },
  type = {Tech. Rep.},
  address = {University of British Columbia, Vancouver},
  month = {08/2011},
  abstract = {Seismic data acquisition in marine environments is a costly process
        that compels the adoption of simultaneous-source acquisition - an
        emerging technology that is stimu- lating both geophysical research
        and commercial efforts. In this paper, we discuss the properties
        of randomized simultaneous acquisition matrices and demonstrate that
        sparsity-promoting recovery improves the quality of the reconstructed
        seismic data volumes. Leveraging established findings from the field
        of compressive sensing, we demonstrate that the choice of the sparsifying
        transform that is incoherent with the compressive sampling matrix
        can significantly impact the reconstruction quality. Si- multaneous
        marine acquisition calls for the development of a new set of design
        principles and post-processing tools. We propose to use random time
        dithering where sequential acquisition with a single airgun is replaced
        by continuous acquisition with multiple airguns firing at random
        times and at random locations. We then demonstrate that the resulting
        compressive sampling matrix is incoherent with the curvelet transform
        and the combined measurement matrix exhibits better isometry properties
        than other transform bases such as a non-localized multidimensional
        Fourier transform. We il- lustrate our results with simulations of
        simultaneous-source marine acquisition using periodic and randomized
        time dithering.},
  url = {http://slim.eos.ubc.ca/Publications/Private/Journals/simmarineacq.pdf}
}



@unpublished{VanLeeuwen11TRfwiwse,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {Fast waveform inversion without source encoding},
  year = {2011},
  type = {Tech. Rep.},
notes = {TR-2011-06 },
  address = {University of British Columbia, Vancouver},
  month = {09/2011},
  abstract = {Randomized source encoding has recently been proposed as a way to
        dramatically reduce the costs of full waveform inversion. The main
        idea is to replace all sequential sources by a small number of simultaneous
        sources. This introduces random crosstalk in the model updates and
        special stochastic optimization strategies are required to deal with
        this. Two problems arise with this approach: i) source encoding can
        only be applied to fixed-spread acquisition setups, and ii) stochastic
        optimization methods tend to converge very slowly, relying on averaging
        to get rid of the cross-talk. Although the slow convergence is partly
        offset by the low iteration cost, we show that conventional optimization
        strategies are bound to outperform stochastic methods in the long
        run. In this paper we argue that we don¬øt need randomized source
        encoding to reap the benefits of stochastic optimization and we review
        an optimization strategy that combines the benefits of both conventional
        and stochastic optimization. The method uses a gradually increasing
        batch of sources. Thus, iterations are very cheap initially and this
        allows the method to make fast progress in the beginning. As the
        batch size grows, the method behaves like conventional optimization,
        allowing for fast convergence. Numerical examples suggest that the
        stochastic and hybrid method perform equally well with and without
        source encoding and that the hybrid method outperforms both conventional
        and stochastic optimization. The method does not rely on source encoding
        techniques and can thus be applied to non fixed-spread data.},
  url = {http://slim.eos.ubc.ca/Publications/private/Journals/TR201106.pdf }
}

@unpublished{Li11TRfrfwi,
  author = {Xiang Li and Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix
        J. Herrmann},
  title = {Fast randomized full-waveform inversion with compressive sensing},
  year = {2011},
notes = {TR-2011-12 },
  address = {University of British Columbia, Vancouver},
  month = {10/2011},
  abstract = { Wave-equation based seismic inversion can be formulated as a nonlinear
        inverse problem where the medium properties are obtained via minimization
        of a least- squares misfit functional. The demand for higher resolution
        models in more geologically complex areas drives the need to develop
        techniques that explore the special structure of full-waveform inversion
        to reduce the computational burden and to regularize the inverse
        problem. We meet these goals by using ideas from compressive sensing
        and stochastic optimization to design a novel Gauss-Newton method,
        where the updates are computed from random subsets of the data via
        curvelet-domain sparsity promotion. Application of this idea to a
        realistic synthetic shows improved results compared to quasi-Newton
        methods, which require passes through all data. Two different subset
        sampling strategies are considered: randomized source encoding, and
        drawing sequential shots firing at random source locations from marine
        data with missing near and far offsets. In both cases, we obtain
        excellent inversion results compared to conventional methods at reduced
        computational costs. },
  url = {http://slim.eos.ubc.ca/Publications/private/Journals/LiAravkinLeeuwenHerrmann.pdf }
}





@unpublished{Aravkin11TRridr,
  author = {Aleksandr Y. Aravkin and Michael P. Friedlander and Felix J. Herrmann and Tristan
        van Leeuwen},
  title = {Robust inversion, dimensionality reduction, and randomized sampling},
  year = {2011},
notes = {TR-2011-13},  
address = {University of British Columbia, Vancouver},
  month = {11/2011},
  abstract = {We consider a class of inverse problems in which the forward model
        is the solution operator to linear ODEs or PDEs. This class admits
        several dimensionality-reduction techniques based on data averaging
        or sampling, which are especially useful for large-scale problems.
        We survey these approaches and their connection to stochastic optimization.
        The data-averaging approach is only viable, however, for a least-squares
        misfit, which is sensitive to outliers in the data and artifacts
        unexplained by the forward model. This motivates us to propose a
        robust formulation based on the Student's t-distribution of the error.
        We demonstrate how the corresponding penalty function, together with
        the sampling approach, can obtain good results for a large-scale
        seismic inverse problem with 50% corrupted data.},
  url = {http://www.optimization-online.org/DB_FILE/2011/11/3243.pdf}
}



@unpublished{aravkin12ICASSProbustb,
author={Aleksandr Y. Aravkin and Michael P. Friedlander and Tristan van Leeuwen },
Keywords={ICASSP},
Organization={ICASSP},
Title={ Robust inversion via semistochastic dimensionality reduction},
Year={2011},
Month={10/2011},
notes = {Submitted to ICASSP 2012, 9/27/2011 },
Abstract={In this paper, we offer an alternative sampling method leveraging recent insights from compressive sensing towards seismic acquisition and processing for data that are traditionally considered to be undersampled. The main outcome of this approach is a new technology where acquisition and processing related costs are no longer determined by overly stringent sampling criteria, such as Nyquist. At the heart of our approach lies randomized incoherent sampling that breaks subsampling related interferences by turning them into harmless noise, which we subsequently remove by promoting transform-domain sparsity. Now, costs no longer grow with resolution and dimensionality of the survey area, but instead depend on transform-domain sparsity only. Our contribution is twofold. First, we demonstrate by means of carefully designed numerical experiments that compressive sensing can successfully be adapted to seismic acquisition. Second, we show that accurate recovery can be accomplished for compressively sampled data volumes sizes that exceed the size of conventional transform-domain data volumes by only a small factor. Because compressive sensing combines transformation and encoding by a single linear encoding step, this technology is directly applicable to acquisition and to dimensionality reduction during processing. In either case, sampling, storage, and processing costs scale with transform-domain sparsity. We illustrate this principle by means of number of case studies. },
URL= {http://slim.eos.ubc.ca/Publications/Private/Journals/ICASSP/AravkinFriedlanderLeeuwen.pdf }
}

@unpublished{Aravkin12ICASSPfastseis,
Abstract = {Seismic imaging can be formulated as a linear inverse problem where a medium perturbation is obtained via minimization of a least-squares misfit functional. The demand for higher resolution images in more geophysically complex areas drives the need to develop techniques that handle problems of tremendous size with limited computational resources. While seismic imaging is amenable to dimensionality reduction techniques that collapse the data volume into a smaller set of ‚Äúsuper-shots‚Äù, these techniques break down for complex acquisition geometries such as marine acquisition, where sources and receivers move during acquisition. To meet these challenges, we propose a novel method that combines sparsity-promoting (SP) solvers with random sub- set selection of sequential shots, yielding a SP algorithm that only ever sees a small portion of the full data, enabling its application to very large-scale problems. Application of this technique yields excellent results for a complicated synthetic, which underscores the robustness of sparsity promotion and its suitability for seismic imaging.},
Author = {Aleksandr Y. Aravkin and Xiang Li and Felix J. Herrmann },
Keywords = {ICASSP},
Organization = {ICASSP},
Publisher = {},
Title = {Fast seismic imaging for marine data},
Date-Added = {2011-09-27 15:00:00 -0700},
Year = {2011},
Month={09/2011},
url={http://slim.eos.ubc.ca/Publications/Private/Journals/ICASSP/AravkinLiHerrmann.pdf }
}


@unpublished{Mansour12ICASSadapt,
Abstract = {In this paper, we propose an adaptive compressed sensing scheme that utilizes a support estimate to focus the measurements on the large valued coefficients of a compressible signal. We embed a ‚Äúsparse-filtering‚Äù stage into the measure- ment matrix by weighting down the contribution of signal coefficients that are outside the support estimate. We present an application which can benefit from the proposed sampling scheme, namely, video compressive acquisition. We demonstrate that our proposed adaptive CS scheme results in a significant improvement in reconstruction quality compared with standard CS as well as adaptive recovery using weighted $\ell$1 minimization.},
Author = {Hassan Mansour and Ozgur Yilmaz},
Keywords = {ICASSP},
Organization = {ICASSP},
Publisher = {},
Title = {Adaptive compressed sensing for video acquisition},
Date-Added = {2011-09-27 15:00:00 -0700},
Year = {2011},
Month={09/2011},
url={http://slim.eos.ubc.ca/Publications/Private/Journals/ICASSP/MansourYilmazICASSPaCS.pdf }

}


@unpublished{Mansour12ICASSsupport,
Abstract = {In this paper, we propose a support driven reweighted $\ell$_Ôøø1 minimization algorithm (SDRL1) that solves a sequence of weighted $\ell$_Ôøø1 problems and relies on the support estimate accu- racy. Our SDRL1 algorithm is related to the IRL1 algorithm proposed by Cande`s, Wakin, and Boyd. We demonstrate that it is sufficient to find support estimates with good accuracy and apply constant weights instead of using the inverse coefficient magnitudes to achieve gains similar to those of IRL1. We then prove that given a support estimate with sufficient accuracy, if the signal decays according to a specific rate, the solution to the weighted Ôøø1 minimization problem results in a support estimate with higher accuracy than the initial estimate. We also show that under certain conditions, it is possible to achieve higher estimate accuracy when the inter- section of support estimates is considered. We demonstrate the performance of SDRL1 through numerical simulations and compare it with that of IRL1 and standard Ôøø$\ell$_Ôøø1 minimization.},
Author = {Hassan Mansour and Ozgur Yilmaz},
Keywords = {ICASSP},
Organization = {ICASSP},
Publisher = {},
Title = {Support driven reweighted $\ell_1$ minimization},
Date-Added = {2011-09-27 15:00:00 -0700},
Year = {2011},
Month={09/2011},
url={http://slim.eos.ubc.ca/Publications/Private/Journals/ICASSP/MansourYilmazICASSPwL1.pdf }
}


@unpublished{haber10TRemp,
  author = {Eldad Haber and Matthias Chung and Felix J. Herrmann},
  title = {An effective method for parameter estimation with PDE constraints
        with multiple right hand sides},
  institution = {UBC-Earth and Ocean Sciences Department},
  year = {2010},
  number = {TR-2010-4},
  abstract = {Many parameter estimation problems involve with a parameter-dependent
        PDEs with multiple right hand sides. The computational cost and memory
        requirements of such problems increases linearly with the number
        of right hand sides. For many applications this is the main bottleneck
        of the computation. In this paper we show that problems with multiple
        right hand sides can be reformulated as stochastic optimization problems
        that are much cheaper to solve. We discuss the solution methodology
        and use the direct current resistivity and seismic tomography as
        model problems to show the effectiveness of our approach.},
  keywords = {SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/Journals/Haber2010emp.pdf}
}


% This file was created with JabRef 2.6.
% Encoding: MacRoman

% This file was created with JabRef 2.6.
% Encoding: MacRoman

@article{mansour11TRrcs,
  author = {Michael P. Friedlander and Hassan Mansour and Rayan Saab and Ozgur Yilmaz},
  title = {Recovering compressively sampled signals using partial support information},
  institution = {Department of Computer Science},
  year = {2011},
  address = {University of British Columbia, Vancouver},
  month = {07/2011},
  abstract = {We study recovery conditions of weighted $\ell_1$ minimization for
        signal reconstruction from compressed sensing measurements when partial
        support information is available. We show that if at least 50\% of
        the (partial) support information is accurate, then weighted $\ell_1$
        minimization is stable and robust under weaker sufficient conditions
        than the analogous conditions for standard $\ell_1$ minimization.
        Moreover, weighted $\ell_1$ minimization provides better upper bounds
        on the reconstruction error in terms of the measurement noise and
        the compressibility of the signal to be recovered. We illustrate
        our results with extensive numerical experiments on synthetic data
        and real audio and video signals.},
  url = {http://slim/Publications/Public/Journals/mansour2011.pdf},
notes= {tech report }
}




@ARTICLE{hennenfent06CiSEsdn,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Seismic Denoising with Nonuniformly Sampled Curvelets},
  journal = {Computing in Science \& Engineering},
  year = {2006},
  volume = {8 Issue:3},
  pages = {16 - 25},
  month = {May-June 2006},
  abstract = {The authors present an extension of the fast discrete curvelet transform
	(FDCT) to nonuniformly sampled data. This extension not only restores
	curvelet compression rates for nonuniformly sampled data but also
	removes noise and maps the data to a regular grid.},
  doi = {10.1109/MCSE.2006.49},
  keywords = {CiSE},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Journals/CiSE/2006/hennenfent06CiSEsdn/hennenfent06CiSEsdn.pdf } 
}




@ARTICLE{BergFriedlander:2008,
  author = {E. van den Berg and Michael P. Friedlander},
  title = {Probing the Pareto frontier for basis pursuit solutions},
  journal = {SIAM Journal on Scientific Computing},
  year = {2008},
  volume = {31},
  pages = {890-912},
  number = {2},
  abstract = {The basis pursuit problem seeks a minimum one-norm solution of an
	underdetermined least-squares problem. Basis pursuit denoise (BPDN)
	fits the least-squares problem only approximately, and a single parameter
	determines a curve that traces the optimal trade-off between the
	least-squares fit and the one-norm of the solution. We prove that
	this curve is convex and continuously differentiable over all points
	of interest, and show that it gives an explicit relationship to two
	other optimization problems closely related to BPDN. We describe
	a root-finding algorithm for finding arbitrary points on this curve;
	the algorithm is suitable for problems that are large scale and for
	those that are in the complex domain. At each iteration, a spectral
	gradient-projection method approximately minimizes a least-squares
	problem with an explicit one-norm constraint. Only matrix-vector
	operations are required. The primal-dual solution of this problem
	gives function and derivative information needed for the root-finding
	method. Numerical experiments on a comprehensive set of test problems
	demonstrate that the method scales well to large problems.},
  doi = {10.1137/080714488},
  file = {890:http\://link.aip.org/link/?SCE/31/890:PDF},
  keywords = {basis pursuit, convex program, duality, Newton{\textquoteright}s method,
	one-norm regularization, projected gradient, root-finding, sparse
	solutions},
  publisher = {SIAM}
}

@ARTICLE{vandenberg08gsv,
  author = {Ewout van den Berg and Mark Schmidt and Michael P. Friedlander and K.
	Murphy},
  title = {Group sparsity via linear-time projection},
  year = {2008},
  abstract = {We present an efficient spectral projected-gradient algorithm for
	optimization subject to a group one-norm constraint. Our approach
	is based on a novel linear-time algorithm for Euclidean projection
	onto the one- and group one-norm constraints. Numerical experiments
	on large data sets suggest that the proposed method is substantially
	more efficient and scalable than existing methods.},
  keywords = {SLIM},
  url = {http://www.optimization-online.org/DB_FILE/2008/07/2056.pdf}
}

@ARTICLE{bernabe04JGRpas,
  author = {Y. Bernab{\'e} and U. Mok and B. Evans and Felix J. Herrmann},
  title = {Permeability and storativity of binary mixtures of high-and low-porosity
	materials},
  journal = {Journal of Geophysical Research},
  year = {2004},
  volume = {109},
  pages = {B12207},
  abstract = {As a first step toward determining the mixing laws for the transport
	properties of rocks, we prepared binary mixtures of high- and low-permeability
	materials by isostatically hot-pressing mixtures of fine powders
	of calcite and quartz. The resulting rocks were marbles containing
	varying concentrations of dispersed quartz grains. Pores were present
	throughout the rock, but the largest ones were preferentially associated
	with the quartz particles, leading us to characterize the material
	as being composed of two phases, one with high permeability and the
	second with low permeability. We measured the permeability and storativity
	of these materials using the oscillating flow technique, while systematically
	varying the effective pressure and the period and amplitude of the
	input fluid oscillation. Control measurements performed using the
	steady state flow and pulse decay techniques agreed well with the
	oscillating flow tests. The hydraulic properties of the marbles were
	highly sensitive to the volume fraction of the high-permeability
	phase (directly related to the quartz content). Below a critical
	quartz content, slightly less than 20 wt \%, the high-permeability
	volume elements were disconnected, and the overall permeability was
	low. Above the critical quartz content the high-permeability volume
	elements formed throughgoing paths, and permeability increased sharply.
	We numerically simulated fluid flow through binary materials and
	found that permeability approximately obeys a percolation-based mixing
	law, consistent with the measured permeability of the calcite-quartz
	aggregates.},
  doi = {10.1029/2004JB00311},
  keywords = {permeability, porosity, SLIM},
url= { https://www.slim.eos.ubc.ca/Publications/Public/Journals/Journal of Geophysical Research/bernabe04JGRpas/bernabe04JGRpas.pdf }
}

@ARTICLE{Erlangga07ETNAoam,
  author = {Yogi A. Erlangga and R. Nabben},
  title = {On multilevel projection Krylov method for the Helmholtz equation
	preconditioned by shifted Laplacian},
  journal = {Elec. Trans. Numer. Anal.},
  year = {2008},
  volume = {31},
  pages ={203-234},
  abstract = {In [Erlangga and Nabben, SIAM J. Sci. Comput. (2007), to appear],
	a multilevel Krylov method is proposed to solve linear systems with
	symmetric and nonsymmetric matrix of coefficients. This multilevel
	method is developed based on shifting (or pro jecting) some small
	eigen- values to the largest eigenvalue, leading to a more favorable
	spectrum for convergence acceleration of a Krylov subspace method.
	Such a pro jection is insensitive with respect to the approximation
	of the small eigenvalues to be pro jected, which for a particular
	choice of deflation subspaces is equivalent to solving a coarse-grid
	problem analogue to multigrid. Different from multigrid, in the multilevel
	Krylov method, however, the coarse-grid problem is solved by a Krylov
	method, whose convergence rate is further accelerated by applying
	pro jection to the coarse-grid system. A recursive application of
	pro jection and coarse-grid solve by a Krylov iterative method then
	leads to the multilevel Krylov method. The method has been successfully
	applied to 2D convection-diffusion problems for which a standard
	multigrid method fails to converge. In this paper, we extend this
	multilevel Krylov method to indefinite linear systems arising from
	a discretization of the Helmholtz equation, preconditioned by shifted
	Laplacian as introduced by [Erlangga, Oosterlee and Vuik, SIAM J.
	Sci. Comput. 27(2006), pp. 1471{\textendash}1492]. Since in this
	case pro jection must be applied to the preconditioned system AM
	- 1 , the coarse-grid matrices are approximated by a product of some
	low dimension matrices associated with A and M . Within the Krylov
	iteration and pro jection step in each coarse-grid solve, a multigrid
	iteration is used to approximately invert the preconditioner. Hence,
	a multigrid-multilevel Krylov method results. Numerical results are
	given for high wavenumbers and show the effectiveness of the method
	for solving Helmholtz problems. Not only can the convergence be made
	almost independent of grid size h, but also only mildly independent
	of wavenumber k},
url= { https://www.slim.eos.ubc.ca/Publications/Public/Journals/ETNA/2007/Erlangga07ETNAoam/Erlangga07ETNAoam.pdf }

}

@ARTICLE{fomel07rce,
  author = {S. Fomel and Gilles Hennenfent},
  title = {Reproducible computational experiments using {SC}ons},
  journal = {IEEE International Conference on Acoustics, Speech and Signal Processing
	(ICASSP)},
  year = {2007},
  volume = {4},
  pages = {IV-1257-IV-1260},
  month = {April},
  abstract = {SCons (from software construction) is a well-known open-source program
	designed primarily for building software. In this paper, we describe
	our method of extending SCons for managing data processing flows
	and reproducible computational experiments. We demonstrate our usage
	of SCons with a simple example.},
  bdsk-url-1 = {http://dx.doi.org/10.1109/ICASSP.2007.367305},
  bdsk-url-2 = {http://lcav.epfl.ch/reproducible_research/ICASSP07/FomelH07.pdf},
  bdsk-url-3 = {http://www.ee.columbia.edu/~dpwe/LabROSA/proceeds/icassp/2007/pdfs/0401257.pdf},
  date-added = {2008-05-22 11:42:36 -0700},
  date-modified = {2008-08-14 13:52:16 -0700},
  doi = {10.1109/ICASSP.2007.367305},
  issn = {1520-6149},
  keywords = {SLIM},
  pdf = {http://lcav.epfl.ch/reproducible_research/ICASSP07/FomelH07.pdf},
  url = {http://dx.doi.org/10.1109/ICASSP.2007.367305}
}

@ARTICLE{friedlander07dtd,
  author = {Michael P. Friedlander and M. A. Saunders},
  title = {Discussion: The Dantzig Selector: Statistical estimation when p is
	much larger then n},
  journal = {The Annals of Statistics},
  year = {2007},
  volume = {35},
  pages = {2385-2391},
  number = {6},
  doi = {10.1214/009053607000000479},
  keywords = {dantzig, SLIM, statistics},
  url = {http://www.cs.ubc.ca/~mpf/downloads/FriedlanderSaunders08.pdf}
}

@ARTICLE{friedlander07ero,
  author = {Michael P. Friedlander and P. Tseng},
  title = {Exact Regularization of Convex Programs},
  journal = {SIAM J. Optim},
  year = {2007},
  volume = {18},
  pages = {1326-1350},
  number = {4},
  abstract = {The regularization of a convex program is exact if all solutions of
	the regularized problem are also solutions of the original problem
	for all values of the regularization parameter below some positive
	threshold. For a general convex program, we show that the regularization
	is exact if and only if a certain selection problem has a Lagrange
	multiplier. Moreover, the regularization parameter threshold is inversely
	related to the Lagrange multiplier. We use this result to generalize
	an exact regularization result of Ferris and Mangasarian [Appl. Math.
	Optim., 23(1991), pp. 266{\textendash}273] involving a linearized
	selection problem. We also use it to derive necessary and sufficient
	conditions for exact penalization, similar to those obtained by Bertsekas
	[Math. Programming, 9(1975), pp. 87{\textendash}99] and by Bertsekas,
	Nedi , Ozdaglar [Convex Analysis and Optimization, Athena Scientific,
	Belmont, MA, 2003]. When the regularization is not exact, we derive
	error bounds on the distance from the regularized solution to the
	original solution set. We also show that existence of a {\textquoteleft}{\textquoteleft}weak
	sharp minimum{\textquoteright}{\textquoteright} is in some sense
	close to being necessary for exact regularization. We illustrate
	the main result with numerical experiments on the l1 regularization
	of benchmark (degenerate) linear programs and semidefinite/second-order
	cone programs. The experiments demonstrate the usefulness of l1 regularization
	in finding sparse solutions.},
  doi = {10.1137/060675320},
  keywords = {SLIM},
  url = {http://www.cs.ubc.ca/~mpf/index.php?q=cpreg.pdf}
}

@ARTICLE{hennenfent08GEOnii,
  author = {Gilles Hennenfent and Ewout van den Berg and Michael P. Friedlander
	and Felix J. Herrmann},
  title = {New insights into one-norm solvers from the {P}areto curve},
  journal = {Geophysics},
  year = {2008},
  volume = {73},
  number = {4},
  month = {July-August},
  abstract = {Geophysical inverse problems typically involve a trade off between
	data misfit and some prior. Pareto curves trace the optimal trade
	off between these two competing aims. These curves are commonly used
	in problems with two-norm priors where they are plotted on a log-log
	scale and are known as L-curves. For other priors, such as the sparsity-promoting
	one norm, Pareto curves remain relatively unexplored. We show how
	these curves lead to new insights in one-norm regularization. First,
	we confirm the theoretical properties of smoothness and convexity
	of these curves from a stylized and a geophysical example. Second,
	we exploit these crucial properties to approximate the Pareto curve
	for a large-scale problem. Third, we show how Pareto curves provide
	an objective criterion to gauge how different one-norm solvers advance
	towards the solution.},
  keywords = {Pareto, SLIM, Geophysics},
url= { https://www.slim.eos.ubc.ca/Publications/Public/Journals/Geophysics/2008/hennenfent08GEOnii/hennenfent08GEOnii.pdf }
}

@ARTICLE{hennenfent10nct,
  author = {Gilles Hennenfent and Lloyd Fenelon and Felix J. Herrmann},
  title = {Nonequispaced curvelet transform for seismic data reconstruction:
	A sparsity-promoting approach},
  journal = {Geophysics},
  year = {2010},
  volume = {75},
  pages = {WB203-WB210},
  number = {6},
  abstract = {We extend our earlier work on the nonequispaced fast discrete curvelet
	transform (NFDCT) and introduce a second generation of the transform.
	This new generation differs from the previous one by the approach
	taken to compute accurate curvelet coefficients from irregularly
	sampled data. The first generation relies on accurate Fourier coefficients
	obtained by an l2-regularized inversion of the nonequispaced fast
	Fourier transform (FFT) whereas the second is based on a direct l1-regularized
	inversion of the operator that links curvelet coefficients to irregular
	data. Also, by construction the second generation NFDCT is lossless
	unlike the first generation NFDCT. This property is particularly
	attractive for processing irregularly sampled seismic data in the
	curvelet domain and bringing them back to their irregular record-ing
	locations with high fidelity. Secondly, we combine the second generation
	NFDCT with the standard fast discrete curvelet transform (FDCT) to
	form a new curvelet-based method, coined nonequispaced curvelet reconstruction
	with sparsity-promoting inversion (NCRSI) for the regularization
	and interpolation of irregularly sampled data. We demonstrate that
	for a pure regularization problem the reconstruction is very accurate.
	The signal-to-reconstruction error ratio in our example is above
	40 dB. We also conduct combined interpolation and regularization
	experiments. The reconstructions for synthetic data are accurate,
	particularly when the recording locations are optimally jittered.
	The reconstruction in our real data example shows amplitudes along
	the main wavefronts smoothly varying with limited acquisition imprint.},
  doi = {10.1190/1.3494032},
  keywords = {curvelet transforms, data acquisition, geophysical techniques, seismology},
  publisher = {SEG},
  url = {http://link.aip.org/link/?GPY/75/WB203/1}
}

@ARTICLE{hennenfent:WB203,
  author = {Gilles Hennenfent and Lloyd Fenelon and Felix J. Herrmann},
  title = {Nonequispaced curvelet transform for seismic data reconstruction:
	A sparsity-promoting approach},
  journal = {Geophysics},
  year = {2010},
  volume = {75},
  pages = {WB203-WB210},
  number = {6},
  abstract = {Seismic data are typically irregularly sampled along spatial axes.
	This irregular sampling may adversely a?ect some key steps, e.g.,
	multiple prediction/attenuation or imaging, in the processing workﬂow.
	To overcome this problem almost every large dataset is regularized
	and/or interpolated. Our contribution is twofold. Firstly, we extend
	our earlier work on the nonequispaced fast discrete curvelet transform
	(NFDCT) and introduce a second generation of the transform. This
	new generation di?ers from the previous one by the approach taken
	to compute accurate curvelet coe?cients from irregularly sampled
	data. The ﬁrst generation relies on accurate Fourier coe?cients obtained
	by an ?2 -regularized inversion of the nonequispaced fast Fourier
	transform, while the second is based on a direct, ?1 -regularized
	inversion of the operator that links curvelet coe?cients to irregular
	data. Also, by construction, the NFDCT second generation is lossless,
	unlike the NFDCT ﬁrst generation. This property is particularly attractive
	for processing irregularly sampled seismic data in the curvelet domain
	and bringing them back to their irregular recording locations with
	high ﬁdelity. Secondly, we combine the NFDCT second generation with
	the standard fast discrete curvelet transform (FDCT) to form a new
	curvelet-based method, coined nonequispaced curvelet reconstruction
	with sparsity-promoting inversion (NCRSI), for the regularization
	and interpolation of irregularly sampled data. We demonstrate that,
	for a pure regularization problem, the reconstruction is very accurate.
	The signal-to-reconstruction error ratio is, in our example, above
	40 dB. We also conduct combined interpolation and regularization
	experiments. The reconstructions for synthetic data are accurate,
	particularly when the recording locations are optimally jittered.
	The reconstruction in our real data example shows amplitudes along
	the main wavefronts smoothly varying with no obvious acquisition
	imprint; a result very competitive with results from other reconstruction
	methods overall.},
  doi = {10.1190/1.3494032},
  keywords = {curvelet transforms; data acquisition; geophysical techniques; seismology},
  publisher = {SEG},
  url = {http://link.aip.org/link/?GPY/75/WB203/1}
}

@ARTICLE{hennenfent08GEOsdw,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Simply denoise: wavefield reconstruction via jittered undersampling},
  journal = {Geophysics},
  year = {2008},
  volume = {73},
  number = {3},
  month = {May-June},
  abstract = {In this paper, we present a new discrete undersampling scheme designed
	to favor wavefield reconstruction by sparsity-promoting inversion
	with transform elements that are localized in the Fourier domain.
	Our work is motivated by empirical observations in the seismic community,
	corroborated by recent results from compressive sampling, which indicate
	favorable (wavefield) reconstructions from random as opposed to regular
	undersampling. As predicted by theory, random undersampling renders
	coherent aliases into harmless incoherent random noise, effectively
	turning the interpolation problem into a much simpler denoising problem.
	A practical requirement of wavefield reconstruction with localized
	sparsifying transforms is the control on the maximum gap size. Unfortunately,
	random undersampling does not provide such a control and the main
	purpose of this paper is to introduce a sampling scheme, coined jittered
	undersampling, that shares the benefits of random sampling, while
	offering control on the maximum gap size. Our contribution of jittered
	sub-Nyquist sampling proves to be key in the formulation of a versatile
	wavefield sparsity-promoting recovery scheme that follows the principles
	of compressive sampling. After studying the behavior of the jittered
	undersampling scheme in the Fourier domain, its performance is studied
	for curvelet recovery by sparsity-promoting inversion (CRSI). Our
	findings on synthetic and real seismic data indicate an improvement
	of several decibels over recovery from regularly-undersampled data
	for the same amount of data collected.},
  doi = {10.1190/1.2841038},
  keywords = {sampling, Geophysics, SLIM},
  publisher = {SEG},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Journals/Geophysics/2008/hennenfent08GEOsdw/hennenfent08GEOsdw.pdf }
}

@ARTICLE{hennenfent06sdw,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Seismic denoising with non-uniformly sampled curvelets},
  journal = {Computing in Science and Engineering},
  year = {2006},
  volume = {8},
  number = {3},
  month = {May-June},
  abstract = {The authors present an extension of the fast discrete curvelet transform
	(FDCT) to nonuniformly sampled data. This extension not only restores
	curvelet compression rates for nonuniformly sampled data but also
	removes noise and maps the data to a regular grid.},
  doi = {10.1109/MCSE.2006.49},
  keywords = {curvelet transform, SLIM},
  publisher = {IEEE},
  url = {http://slim.eos.ubc.ca/Publications/Public/Journals/hennenfent06cdw.pdf}
}

@ARTICLE{herrmann10rsg,
  author = {Felix J. Herrmann},
  title = {Randomized sampling and sparsity: Getting more information from fewer
	samples},
  journal = {Geophysics},
  year = {2010},
  volume = {75},
  pages = {WB173-WB187},
  number = {6},
  abstract = {Many seismic exploration techniques rely on the collection of massive
	data volumes that are subsequently mined for information during processing.
	Although this approach has been extremely successful in the past,
	current efforts toward higher-resolution images in increasingly complicated
	regions of the earth continue to reveal fundamental shortcomings
	in our workflows. Chiefly among these is the so-called {\textquotedblleft}curse
	of dimensionality{\textquotedblright} exemplified by Nyquist{\textquoteright}s
	sampling criterion, which disproportionately strains current acquisition
	and processing systems as the size and desired resolution of our
	survey areas continue to increase. We offer an alternative sampling
	method leveraging recent insights from compressive sensing toward
	seismic acquisition and processing for data that are traditionally
	considered to be undersampled. The main outcome of this approach
	is a new technology where acquisition and processing related costs
	are no longer determined by overly stringent sampling criteria, such
	as Nyquist. At the heart of our approach lies randomized incoherent
	sampling that breaks subsampling related interferences by turning
	them into harmless noise, which we subsequently remove by promoting
	transform-domain sparsity. Now, costs no longer grow significantly
	with resolution and dimensionality of the survey area, but instead
	depend only on transform-domain sparsity. Our contribution is twofold.
	First, we demonstrate by means of carefully designed numerical experiments
	that compressive sensing can successfully be adapted to seismic exploration.
	Second, we show that accurate recovery can be accomplished for compressively
	sampled data volumes sizes that exceed the size of conventional transform-domain
	data volumes by only a small factor. Because compressive sensing
	combines transformation and encoding by a single linear encoding
	step, this technology is directly applicable to acquisition and to
	dimensionality reduction during processing. In either case, sampling,
	storage, and processing costs scale with transform-domain sparsity.
	We illustrate this principle by means of number of case studies.},
  doi = {10.1190/1.3506147},
  keywords = {data acquisition, geophysical techniques, Nyquist criterion, sampling
	methods, seismology},
  publisher = {SEG},
  url = {http://link.aip.org/link/?GPY/75/WB173/1}
}

@ARTICLE{herrmann05sdb,
  author = {Felix J. Herrmann},
  title = {Seismic deconvolution by atomic decomposition: a parametric approach
	with sparseness constraints},
  journal = {Integrated Computer-Aided Engineering},
  year = {2005},
  volume = {12},
  pages = {69-90},
  number = {1},
  month = {January},
  abstract = {In this paper an alternative approach to the blind seismic deconvolution
	problem is presented that aims for two goals namely recovering the
	location and relative strength of seismic reflectors, possibly with
	super-localization, as well as obtaining detailed parametric characterizations
	for the reflectors. We hope to accomplish these goals by decomposing
	seismic data into a redundant dictionary of parameterized waveforms
	designed to closely match the properties of reflection events associated
	with sedimentary records. In particular, our method allows for highly
	intermittent non-Gaussian records yielding a reflectivity that can
	no longer be described by a stationary random process or by a spike
	train. Instead, we propose a reflector parameterization that not
	only recovers the reflector{\textquoteright}s location and relative
	strength but which also captures reflector attributes such as its
	local scaling, sharpness and instantaneous phase-delay. The first
	set of parameters delineates the stratigraphy whereas the second
	provides information on the lithology. As a consequence of the redundant
	parameterization, finding the matching waveforms from the dictionary
	involves the solution of an ill-posed problem. Two complementary
	sparseness-imposing methods Matching and Basis Pursuit are compared
	for our dictionary and applied to seismic data.},
  address = {Amsterdam, The Netherlands, The Netherlands},
  issn = {1069-2509},
  keywords = {deconvolution, SLIM},
  publisher = {IOS Press},
  url = {http://slim.eos.ubc.ca/~felix/public/RobinsonSub.pdf}
}



@ARTICLE{herrmann04ssa,
  author = {Felix J. Herrmann and Y. Bernab\'e },
  title = {Seismic singularities at upper-mantle phase transitions: a site percolation
	model},
  journal = {Geophysical Journal International},
  year = {2004},
  volume = {159},
  pages = {949-960},
  number = {3},
  abstract = {Mineralogical phase transitions are usually invoked to account for
	the sharpness of globally observed upper-mantle seismic discontinuities.
	We propose a percolation-based model for the elastic properties of
	the phase mixture in the coexistence regions associated with these
	transitions. The major consequence of the model is that the elastic
	moduli (but not the density) display a singularity at the percolation
	threshold of the high-pressure phase. This model not only explains
	the sharp but continuous change in seismic velocities across the
	phase transition, but also predicts its abruptness and scale invariance,
	which are characterized by a non-integral scale exponent. Using the
	receiver-function approach and new, powerful signal-processing techniques,
	we quantitatively determine the singularity exponent from recordings
	of converted seismic waves at two Australian stations (CAN and WRAB).
	Using the estimated values, we construct velocity{\textendash}depth
	profiles across the singularities and verify that the calculated
	converted waveforms match the observations under CAN. Finally, we
	point out a series of additional predictions that may provide new
	insights into the physics and fine structure of the upper-mantle
	transition zone.},
  doi = {10.1111/j.1365-246X.2004.02464.x},
  keywords = {percolation, SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/Journals/herrmann04ssa.pdf}
}

@ARTICLE{friedlander11hybrid,
  author = {MICHAEL P. FRIEDLANDER AND MARK SCHMIDT},
  title = {HYBRID DETERMINISTIC-STOCHASTIC METHODS FOR DATA FITTING},
  year = {2011},
  abstract = {Many structured data-fitting applications require the solution of
	an optimization problem involving a sum over a potentially large
	number of measurements. Incremental gradient algorithms (both deterministic
	and randomized) offer inexpensive iterations by sampling only subsets
	of the terms in the sum. These methods can make great progress initially,
	but often slow as they approach a solution. In contrast, full gradient
	methods achieve steady convergence at the expense of evaluating the
	full objective and gradient on each iteration. We explore hybrid
	methods that exhibit the benefits of both approaches. Rate of convergence
	analysis and numerical experiments illustrate the potential for the
	approach.},
journal= {CoRR },
url= {https://www.slim.eos.ubc.ca/Publications/Public/Journals/CoRR/2011/friedlander11hybrid  }

}




@ARTICLE{herrmann07nlp,
  author = {Felix J. Herrmann and U. Boeniger and D. J. Verschuur},
  title = {Non-linear primary-multiple separation with directional curvelet
	frames},
  journal = {Geophysical Journal International},
  year = {2007},
  volume = {170},
  pages = {781-799},
  number = {2},
  abstract = {Predictive multiple suppression methods consist of two main steps:
	a prediction step, during which multiples are predicted from seismic
	data, and a primary-multiple separation step, during which the predicted
	multiples are {\textquoteright}matched{\textquoteright} with the
	true multiples in the data and subsequently removed. This second
	separation step, which we will call the estimation step, is crucial
	in practice: an incorrect separation will cause residual multiple
	energy in the result or may lead to a distortion of the primaries,
	or both. To reduce these adverse effects, a new transformed-domain
	method is proposed where primaries and multiples are separated rather
	than matched. This separation is carried out on the basis of differences
	in the multiscale and multidirectional characteristics of these two
	signal components. Our method uses the curvelet transform, which
	maps multidimensional data volumes into almost orthogonal localized
	multidimensional prototype waveforms that vary in directional and
	spatio-temporal content. Primaries-only and multiples-only signal
	components are recovered from the total data volume by a non-linear
	optimization scheme that is stable under noisy input data. During
	the optimization, the two signal components are separated by enhancing
	sparseness (through weighted l1-norms) in the transformed domain
	subject to fitting the observed data as the sum of the separated
	components to within a user-defined tolerance level. Whenever, during
	the optimization, the estimates for the primaries in the transformed
	domain correlate with the predictions for the multiples, the recovery
	of the coefficients for the estimated primaries will be suppressed
	while for regions where the correlation is small the method seeks
	the sparsest set of coefficients that represent the estimation for
	the primaries. Our algorithm does not seek a matched filter and as
	such it differs fundamentally from traditional adaptive subtraction
	methods. The method derives its stability from the sparseness obtained
	by a non-parametric (i.e. not depending on a parametrized physical
	model) multiscale and multidirectional overcomplete signal representation.
	This sparsity serves as prior information and allows for a Bayesian
	interpretation of our method during which the log-likelihood function
	is minimized while the two signal components are assumed to be given
	by a superposition of prototype waveforms, drawn independently from
	a probability function that is weighted by the predicted primaries
	and multiples. In this paper, the predictions are based on the data-driven
	surface-related multiple elimination method. Synthetic and field
	data examples show a clean separation leading to a considerable improvement
	in multiple suppression compared to the conventional method of adaptive
	matched filtering. This improved separation translates into an improved
	stack.},
  doi = {10.1111/j.1365-246X.2007.03360.x},
  keywords = {signal separation, SLIM},
url= { https://www.slim.eos.ubc.ca/Publications/Public/Journals/Geophysical Journal International/2007/herrmann07nlp/herrmann07nlp.pdf  } ,
  url2 = {http://onlinelibrary.wiley.com/doi/10.1111/j.1365-246X.2007.03360.x/abstract;jsessionid=956C674A489BB3A13D2D9D7F87D68FE7.d02t03}
}

@ARTICLE{herrmann09cbm,
  author = {Felix J. Herrmann and Cody R. Brown and Yogi A. Erlangga and Peyman
	P. Moghaddam},
  title = {Curvelet-based migration preconditioning and scaling},
  journal = {Geophysics},
  year = {2009},
  volume = {74},
  pages = {A41},
  abstract = {The extremely large size of typical seismic imaging problems has been
	one of the major stumbling blocks for iterative techniques to attain
	accurate migration amplitudes. These iterative methods are important
	because they complement theoretical approaches that are hampered
	by difficulties to control problems such as finite-acquisition aperture,
	source-receiver frequency response, and directivity. To solve these
	problems, we apply preconditioning, which significantly improves
	convergence of least-squares migration. We discuss different levels
	of preconditioning that range from corrections for the order of the
	migration operator to corrections for spherical spreading, and position
	and reflector-dip dependent amplitude errors. While the first two
	corrections correspond to simple scalings in the Fourier and physical
	domain, the third correction requires phase-space (space spanned
	by location and dip) scaling, which we carry out with curvelets.
	We show that our combined preconditioner leads to a significant improvement
	of the convergence of least-squares {\textquoteleft}wave-equation{\textquoteright}
	migration on a line from the SEG AA{\textquoteright} salt model.},
  keywords = {migration},
  url = {http://slim.eos.ubc.ca/Publications/Public/TechReports/herrmann08cmp-r.pdf}
}

@ARTICLE{herrmann09csf,
  author = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title = {Compressive simultaneous full-waveform simulation},
  journal = {Geophysics},
  year = {2009},
  volume = {74},
  pages = {A35},
  abstract = {The fact that computational complexity of wavefield simulation is
	proportional to the size of the discretized model and acquisition
	geometry, and not to the complexity of the simulated wavefield, is
	a major impediment within seismic imaging. By turning simulation
	into a compressive sensing problem{\textendash}-where simulated data
	is recovered from a relatively small number of independent simultaneous
	sources{\textendash}-we remove this impediment by showing that compressively
	sampling a simulation is equivalent to compressively sampling the
	sources, followed by solving a reduced system. As in compressive
	sensing, this allows for a reduction in sampling rate and hence in
	simulation costs. We demonstrate this principle for the time-harmonic
	Helmholtz solver. The solution is computed by inverting the reduced
	system, followed by a recovery of the full wavefield with a sparsity
	promoting program. Depending on the wavefield{\textquoteright}s sparsity,
	this approach can lead to significant cost reductions, in particular
	when combined with the implicit preconditioned Helmholtz solver,
	which is known to converge even for decreasing mesh sizes and increasing
	angular frequencies. These properties make our scheme a viable alternative
	to explicit time-domain finite-differences.},
  keywords = {full-waveform},
  url = {http://slim.eos.ubc.ca/Publications/Public/TechReports/herrmann08csf-r.pdf}
}

@ARTICLE{herrmann08nps,
  author = {Felix J. Herrmann and Gilles Hennenfent},
  title = {Non-parametric seismic data recovery with curvelet frames},
  journal = {Geophysical Journal International},
  year = {2008},
  volume = {173},
  pages = {233-248},
  month = {04/2011},
  abstract = {Seismic data recovery from data with missing traces on otherwise regular
	acquisition grids forms a crucial step in the seismic processing
	flow. For instance, unsuccessful recovery leads to imaging artifacts
	and to erroneous predictions for the multiples, adversely affecting
	the performance of multiple elimination. A non-parametric transform-based
	recovery method is presented that exploits the compression of seismic
	data volumes by recently developed curvelet frames. The elements
	of this transform are multidimensional and directional and locally
	resem- ble wavefronts present in the data, which leads to a compressible
	representation for seismic data. This compression enables us to formulate
	a new curvelet-based seismic data recovery algorithm through sparsity-promoting
	inversion. The concept of sparsity-promoting inversion is in itself
	not new to geophysics. However, the recent insights from the field
	of {\textquoteleft}compressed sensing{\textquoteright} are new since
	they clearly identify the three main ingredients that go into a successful
	formulation of a re- covery problem, namely a sparsifying transform,
	a sampling strategy that subdues coherent aliases and a sparsity-promoting
	program that recovers the largest entries of the curvelet-domain
	vector while explaining the measurements. These concepts are illustrated
	with a stylized experiment that stresses the importance of the degree
	of compression by the sparsifying transform. With these findings,
	a curvelet-based recovery algorithms is developed, which recovers
	seismic wavefields from seismic data volumes with large percentages
	of traces missing. During this construction, we benefit from the
	main three ingredients of compressive sampling, namely the curvelet
	compression of seismic data, the existence of a favorable sam- pling
	scheme and the formulation of a large-scale sparsity-promoting solver
	based on a cooling method. The recovery performs well on synthetic
	as well as real data and performs better by virtue of the sparsifying
	property of curvelets. Our results are applicable to other areas
	such as global seismology.},
  doi = {10.1111/j.1365-246X.2007.03698.x},
  keywords = {curvelet transform, reconstruction, SLIM},
url= { https://www.slim.eos.ubc.ca/Publications/Public/Journals/Geophysical Journal International/2008/herrmann08nps/herrmann08nps.pdf }
}

@ARTICLE{herrmann08sac,
  author = {Felix J. Herrmann and Peyman P. Moghaddam and C. C. Stolk},
  title = {Sparsity- and continuity-promoting seismic image recovery with curvelet
	frames},
  journal = {Applied and Computational Harmonic Analysis},
  year = {2008},
  volume = {24},
  pages = {150-173},
  number = {2},
  month = {03/2011},
  abstract = {A nonlinear singularity-preserving solution to seismic image recovery
	with sparseness and continuity constraints is proposed. We observe
	that curvelets, as a directional frame expan- sion, lead to sparsity
	of seismic images and exhibit invariance under the normal operator
	of the linearized imaging problem. Based on this observation we derive
	a method for stable recovery of the migration amplitudes from noisy
	data. The method corrects the amplitudes during a post-processing
	step after migration, such that the main additional cost is one ap-
	plication of the normal operator, i.e. a modeling followed by a migration.
	Asymptotically this normal operator corresponds to a pseudodifferential
	operator, for which a convenient diagonal approximation in the curvelet
	domain is derived, including a bound for its error and a method for
	the estimation of the diagonal from a compound operator consisting
	of discrete implementations for the scattering operator and its adjoint
	the migration opera- tor. The solution is formulated as a nonlinear
	optimization problem where sparsity in the curvelet domain as well
	as continuity along the imaged reflectors are jointly promoted. To
	enhance sparsity, the l1 -norm on the curvelet coefficients is minimized,
	while continuity is promoted by minimizing an anisotropic diffusion
	norm on the image. The performance of the recovery scheme is evaluated
	with a time-reversed {\textquoteright}wave-equation{\textquoteright}
	migration code on synthetic datasets, including the complex SEG/EAGE
	AA salt model.},
  doi = {10.1016/j.acha.2007.06.007},
  keywords = {curvelet transform, imaging, SLIM}
}

@ARTICLE{herrmann08GEOcbs,
  author = {Felix J. Herrmann and Deli Wang and Gilles Hennenfent and Peyman
	P. Moghaddam},
  title = {Curvelet-based seismic data processing: a multiscale and nonlinear
	approach},
  journal = {Geophysics},
  year = {2008},
  volume = {73},
  pages = {A1-A5},
  number = {1},
  abstract = {Mitigating missing data, multiples, and erroneous migration amplitudes
	are key factors that determine image quality. Curvelets, little {\textquoteleft}{\textquoteleft}plane
	waves,{\textquoteright}{\textquoteright} complete with oscillations
	in one direction and smoothness in the other directions, sparsify
	a property we leverage explicitly with sparsity promotion. With this
	principle, we recover seismic data with high fidelity from a small
	subset (20\%) of randomly selected traces. Similarly, sparsity leads
	to a natural decorrelation and hence to a robust curvelet-domain
	primary-multiple separation for North Sea data. Finally, sparsity
	helps to recover migration amplitudes from noisy data. With these
	examples, we show that exploiting the curvelet{\textquoteright}s
	ability to sparsify wavefrontlike features is powerful, and our results
	are a clear indication of the broad applicability of this transform
	to exploration seismology. {\copyright}2008 Society of Exploration
	Geophysicists},
  doi = {10.1190/1.2799517},
  keywords = {curvelet transform, SLIM},
  publisher = {SEG},
url= { https://www.slim.eos.ubc.ca/Publications/Public/Journals/Geophysics/2008/herrmann08GEOcbs/herrmann08GEOcbs.pdf } 
}


@ARTICLE{herrmann08GEOacd,
  author = {Felix J. Herrmann and Deli Wang and D. J. Verschuur},
  title = {Adaptive curvelet-domain primary-multiple separation},
  journal = {Geophysics},
  year = {2008},
  volume = {73},
  pages = {A17-A21},
  number = {3},
  abstract = {In many exploration areas, successful separation of primaries and
	multiples greatly determines the quality of seismic imaging. Despite
	major advances made by surface-related multiple elimination (SRME),
	amplitude errors in the predicted multiples remain a problem. When
	these errors vary for each type of multiple in different ways (as
	a function of offset, time, and dip), they pose a serious challenge
	for conventional least-squares matching and for the recently introduced
	separation by curvelet-domain thresholding. We propose a data-adaptive
	method that corrects amplitude errors, which vary smoothly as a function
	of location, scale (frequency band), and angle. With this method,
	the amplitudes can be corrected by an elementwise curvelet-domain
	scaling of the predicted multiples. We show that this scaling leads
	to successful estimation of primaries, despite amplitude, sign, timing,
	and phase errors in the predicted multiples. Our results on synthetic
	and real data show distinct improvements over conventional least-squares
	matching in terms of better suppression of multiple energy and high-frequency
	clutter and better recovery of estimated primaries. {\copyright}2008
	Society of Exploration Geophysicists},
  doi = {10.1190/1.2904986},
  keywords = {Geophysics, SLIM},
  publisher = {SEG},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Journals/Geophysics/2008/herrmann08GEOacd/herrmann08GEOacd.pdf }
}

@ARTICLE{VanLeeuwen2010,
  author = {Tristan van Leeuwen and Aleksandr Y. Aravkin and  Felix J. Herrmann},
  title = {Seismic waveform inversion by stochastic optimization},
  journal = {International Journal of Geophysics},
  year = {2011},
  volume = {doi:10.1155/2011/689041},
  abstract = {We explore the use of stochastic optimization methods for seismic waveform inversion. The basic principle of such methods is to randomly draw a batch of realizations of a given misfit function and goes back to the 1950s. The ultimate goal of such an approach is to dramatically reduce the computational cost involved in evaluating the misfit. Following earlier work, we introduce the stochasticity in waveform inversion problem in a rigorous way via a technique called randomized trace estimation. We then review theoretical results that underlie recent developments in the use of stochastic methods for waveform inversion. We present numerical experiments to illustrate the behavior of different types of stochastic optimization methods and investigate the sensitivity to the batch size and the noise level in the data. We find that it is possible to reproduce results that are qualitatively similar to the solution of the full problem with modest batch sizes, even on noisy data. Each iteration of the corresponding stochastic methods requires an order of magnitude fewer PDE solves than a comparable deterministic method applied to the full problem, which may lead to an order of magnitude speedup for waveform inversion in practice.},
  note = {http://www.hindawi.com/journals/ijgp/2011/689041/cta/}
}

@ARTICLE{lin07cwe,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Compressed wavefield extrapolation},
  journal = {Geophysics},
  year = {2007},
  volume = {72},
  pages = {SM77-SM93},
  number = {5},
  abstract = {An explicit algorithm for the extrapolation of one-way wavefields
	is proposed that combines recent developments in information theory
	and theoretical signal processing with the physics of wave propagation.
	Because of excessive memory requirements, explicit formulations for
	wave propagation have proven to be a challenge in 3D. By using ideas
	from compressed sensing, we are able to formulate the (inverse) wavefield
	extrapolation problem on small subsets of the data volume, thereby
	reducing the size of the operators. Compressed sensing entails a
	new paradigm for signal recovery that provides conditions under which
	signals can be recovered from incomplete samplings by nonlinear recovery
	methods that promote sparsity of the to-be-recovered signal. According
	to this theory, signals can be successfully recovered when the measurement
	basis is incoherent with the representa-tion in which the wavefield
	is sparse. In this new approach, the eigenfunctions of the Helmholtz
	operator are recognized as a basis that is incoherent with curvelets
	that are known to compress seismic wavefields. By casting the wavefield
	extrapolation problem in this framework, wavefields can be successfully
	extrapolated in the modal domain, despite evanescent wave modes.
	The degree to which the wavefield can be recovered depends on the
	number of missing (evanescent) wavemodes and on the complexity of
	the wavefield. A proof of principle for the compressed sensing method
	is given for inverse wavefield extrapolation in 2D, together with
	a pathway to 3D during which the multiscale and multiangular properties
	of curvelets, in relation to the Helmholz operator, are exploited.
	The results show that our method is stable, has reduced dip limitations,
	and handles evanescent waves in inverse extrapolation. {\copyright}2007
	Society of Exploration Geophysicists},
  doi = {10.1190/1.2750716},
  keywords = {SLIM, wave propagation},
  publisher = {SEG},
url= { https://www.slim.eos.ubc.ca/Publications/Public/Journals/Geophysics/2007/lin07cwe/lin07cwe.pdf }
}

@ARTICLE{saab08srb,
  author = {Rayan Saab and Ozgur Yilmaz},
  title = {Sparse Recovery by Non-Convex Optimization - Instance Optimality},
  journal = {Applied and Computational Harmonic Analysis},
  year = {2010},
  volume = {29},
  number = {1},
  pages = {30-48},
  abstract = {In this note, we address the theoretical properties of $Œî_p$, a class
	of compressed sensing decoders that rely on $l^p$ minimization with
	$p {\i}n (0, 1)$ to recover estimates of sparse and compressible
	signals from incomplete and inaccurate measurements. In particular,
	we extend the results of Cand{\textquoteleft}es, Romberg and Tao
	[3] and Wojtaszczyk [30] regarding the decoder $Œî_1$, based on $\ell^1$
	minimization, to $Œî p$ with $p {\i}n (0, 1)$. Our results are two-fold.
	First, we show that under certain sufficient conditions that are
	weaker than the analogous sufficient conditions for $Œî_1$ the decoders
	$Œî_p$ are robust to noise and stable in the sense that they are
	$(2, p)$ instance optimal. Second, we extend the results of Wojtaszczyk
	to show that, like $Œî_1$, the decoders $Œî_p$ are (2, 2) instance
	optimal in probability provided the measurement matrix is drawn from
	an appropriate distribution. While the extension of the results of
	[3] to the setting where $p {\i}n (0, 1)$ is straightforward, the
	extension of the instance optimality in probability result of [30]
	is non-trivial. In particular, we need to prove that the $LQ_1$ property,
	introduced in [30], and shown to hold for Gaussian matrices and matrices
	whose columns are drawn uniformly from the sphere, generalizes to
	an $LQ_p$ property for the same classes of matrices. Our proof is
	based on a result by Gordon and Kalton [18] about the Banach-Mazur
	distances of p-convex bodies to their convex hulls.},
  keywords = {non-convex},
  url = {http://dx.doi.org/10.1016/j.acha.2009.08.002}
}

@ARTICLE{wang08GEObws,
  author = {Deli Wang and Rayan Saab and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Bayesian wavefield separation by transform-domain sparsity promotion},
  journal = {Geophysics},
  year = {2008},
  volume = {73},
  number = {5},
  pages = {1-6},
  month = {July},
  abstract = {Successful removal of coherent noise sources greatly determines the
	quality of seismic imag- ing. Ma jor advances were made in this direction,
	e.g., Surface-Related Multiple Elimination (SRME) and interferometric
	ground-roll removal. Still, moderate phase, timing, amplitude errors
	and clutter in the predicted signal components can be detrimental.
	Adopting a Bayesian approach along with the assumption of approximate
	curvelet-domain independence of the to-be-separated signal components,
	we construct an iterative algorithm that takes the predictions produced
	by for example SRME as input and separates these components in a
	robust fashion. In addition, the proposed algorithm controls the
	energy mismatch between the separated and predicted components. Such
	a control, which was lacking in earlier curvelet-domain formulations,
	produces improved results for primary-multiple separation on both
	synthetic and real data.},
  doi = {10.1190/1.2952571},
  keywords = {curvelet transform, SLIM, Geophysics},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Journals/Geophysics/2008/wang08GEObws/wang08GEObws.pdf }
}
% This file was created with JabRef 2.6.
% Encoding: MacRoman


@MANUAL{hennenfent08MNrap,
  title = {Repro: a Python package for automating reproducible research
	in scientific computing},
  author = {Gilles Hennenfent and Sean Ross-Ross},
  month = {August},
  year = {2008},
  abstract = {repro is a Python package for automating reproducible research in
	scientific computing. Repro works in combination with SCons, a next-generation
	build tool. The package is freely available over the Internet. Downloading
	and installation instructions are provided in this gui de. The repro
	package is documented in various ways (many comments in source code,
	this guide{\textendash}-written using repro itself!{\textendash}-and
	a reference guide ). In this user{\textquoteright}s guide, we present
	a few pedagogical examples that uses Matlab, Python, Seismic Unix
	(SU), and Madagascar. We also include demo pa pers. These papers
	are written in LaTeX and compiled using repro. The figures they
	contain are automatically generated from the source codes prov ided.
	In that sense, the demo papers are a model of self-contained documents
	that are fully reproducible. The repro package is largely inspired
	by some parts of Madagascar, a geophysical software package for reproducible
	research. However, the repro package is intended for a broad audience
	co ming from a wide spectrum of interest areas.},
  keywords = {SLIM},
  url = {http://repro.sourceforge.net/Site/Home.html}
}

@MANUAL{rossross08MNsai,
  title = {{SLIMPy: a python interface for unix-pipe based coordinate-free
	scientific computing}},
  author = {Sean Ross-Ross and Henryk Modzelewski and Felix J. Herrmann},
  month = {July},
  year = {2008},
  abstract = {SLIMpy is a Python interface that exposes the functionality of seismic
	data processing packages, such as MADAGASCAR, through oper ator overloading.
	SLIMpy provides a concrete coordinate-free implementation of classes
	for out-of-core linear (implicit matrix-vector), and element -wise
	operations, including calculation of norms and other basic vector
	operations. The library is intended to provide the user with an abstract
	sc ripting language to program iterative algorithms from numerical
	linear algebra. These algorithms require repeated evaluation of operators
	that were initially designed to be run as part of batch-oriented
	processing flows. The current implementation supports a plugin for
	Madagascar{\textquoteright}s out-of-core UNIX pipe-based applications
	and is extenable to pipe-based collections of programs such as Seismic
	Un*x, SEPLib, and FreeUSP. To optimize perform ance, SLIMpy uses
	an Abstract Syntax Tree that parses the algorithm and optimizes the
	pipes.},
  url = {http://slim.eos.ubc.ca/SLIMpy/}
}

@MANUAL{rossross07MNsda,
  title = {{SLIMpy} development and programming interface for seismic processing},
  author = {Sean Ross-Ross and Henryk Modzelewski and Cody R. Brown and Felix
	J. Herrmann},
  year = {2007},
  abstract = {Inverse problems in (exploration) seismology are known for their large
	to very large scale. For instance, certain sparsity-promoting inversion
	techniques involve vectors that easily exceed unknowns while seismic
	imaging involves the construction and application of matrix-free
	discretized operators where single matrix-vector evaluations may
	require hours, days or even weeks on large compute clusters. For
	these reasons, software development in this field has remained the
	domain of highly technical codes programmed in low-level languages
	with little eye for easy development, code reuse and integration
	with (nonlinear) programs that solve inverse problems.Following ideas
	from the Symes{\textquoteright} Rice Vector Library and Bartlett{\textquoteright}s
	C++ object-oriented interface, Thyra, and Reduction/Transformation
	operators (both part of the Trilinos software package), we developed
	a software-development environment based on overloading. This environment
	provides a pathway from in-core prototype development to out-of-core
	and MPI {\textquoteright}production{\textquoteright} code with a
	high level of code reuse. This code reuse is accomplished by integrating
	the out-of-core and MPI functionality into the dynamic object-oriented
	programming language Python. This integration is implemented through
	operator overloading and allows for the development of a coordinate-free
	solver framework that (i) promotes code reuse; (ii) analyses the
	statements in an abstract syntax tree and (iii) generates executable
	statements. In the current implementation, we developed an interface
	to generate executable statements for the out-of-core unix-pipe based
	(seismic) processing package RSF-Madagascar (rsf.sf.net). The modular
	design allows for interfaces to other seismic processing packages
	and to in-core Python packages such as numpy. So far, the implementation
	overloads linear operators and element-wise reduction/transformation
	operators. We are planning extensions towards nonlinear operators
	and integration with existing (parallel) solver frameworks such as
	Trilinos.},
  keywords = {SLIM, software},
  url = {http://slim.eos.ubc.ca/SLIMpy}
}

@MANUAL{vandenberg07MNsat,
  title = {SPARCO: A toolbox for testing sparse reconstruction algorithms},
  author = {Ewout van den Berg and Michael P. Friedlander},
  month = {October},
  year = {2007},
  abstract = {Sparco is a suite of problems for testing and benchmarking algorithms
	for sparse signal reconstruction. It is also an environment for creating
	new test problems, and a suite of standard linear operators is provided
	from which new problems can be assembled. Sparco is implement ed
	entirely in Matlab and is self contained. (A few optional test problems
	are based on the CurveLab toolbox, which can be installed separately.)
	At the core of the sparse recovery problem is the linear system $Ax+r=b$,
	where $A$ is an $m$-by-$n$ linear operator and the $m$-vector $b$
	is the observed signal. The goal is to find a sparse $n$-vector $x$
	such that $r$ is small in norm.},
  keywords = {SLIM},
  url = {http://www.cs.ubc.ca/labs/scl/sparco/}
}



@PHDTHESIS{moghaddam10phd,
  author = {Peyman P. Moghaddam},
  title = {Curvelet-Based Migration Amplitude Recovery},
  school = {The University of British Columbia},
  year = {2010},
  type = {phd},
  address = {Vancouver, BC Canada},
  month = {May},
  abstract = {Migration can accurately locate reflectors in the earth but in most
	cases fails to correctly resolve their amplitude. This might lead
	to mis-interpretation of the nature of reflector. In this thesis,
	I introduced a method to accurately recover the amplitude of the
	seismic reflector. This method relies on a new transform-based recovery
	that exploits the expression of seismic images by the recently developed
	curvelet transform. The elements of this transform, called curvelets,
	are multi-dimensional, multi-scale, and multi-directional. They also
	remain approximately invariant under the imaging operator. I exploit
	these properties of the curvelets to introduce a method called Curvelet
	Match Filtering (CMF) for recovering the seismic amplitude in presence
	of noise in both migrated image and data. I detail the method and
	illustrate its performance on synthetic dataset. I also extend CMF
	formulation to other geophysical applications and present results
	on multiple removal. In addition of that, I investigate preconditioning
	of the migration which results to rapid convergence rate of the iterative
	method using migration.},
  url = {http://slim.eos.ubc.ca/Publications/Public/Theses/2010/moghaddam10phd.pdf}
}


@PHDTHESIS{hennenfent08phd,
  author = {Gilles Hennenfent},
  title = {Sampling and reconstruction of seismic wavefields in the curvelet
	domain},
  school = {The University of British Columbia},
  year = {2008},
  type = {phd},
  address = {Vancouver, BC Canada},
  month = {May},
  abstract = {Wavefield reconstruction is a crucial step in the seismic processing
	flow. For instance, unsuccessful interpolation leads to erroneous
	multiple predictions that adversely affect the performance of multiple
	elimination, and to imaging artifacts. We present a new non-parametric
	transform-based reconstruction method that exploits the compression
	of seismic data by the recently developed curvelet transform. The
	elements of this transform, called curvelets, are multi-dimensional,
	multi-scale, and multi-directional. They locally resemble wavefronts
	present in the data, which leads to a compressible representation
	for seismic data. This compression enables us to formulate a new
	curvelet-based seismic data recovery algorithm through sparsity-promoting
	inversion (CRSI). The concept of sparsity-promoting inversion is
	in itself not new to geophysics. However, the recent insights from
	the field of {\textquoteleft}{\textquoteleft}compressed sensing{\textquoteright}{\textquoteright}
	are new since they clearly identify the three main ingredients that
	go into a successful formulation of a reconstruction problem, namely
	a sparsifying transform, a sub-Nyquist sampling strategy that subdues
	coherent aliases in the sparsifying domain, and a data-consistent
	sparsity-promoting program. After a brief overview of the curvelet
	transform and our seismic-oriented extension to the fast discrete
	curvelet transform, we detail the CRSI formulation and illustrate
	its performance on synthetic and real datasets. Then, we introduce
	a sub-Nyquist sampling scheme, termed jittered undersampling, and
	show that, for the same amount of data acquired, jittered data are
	best interpolated using CRSI compared to regular or random undersampled
	data. We also discuss the large-scale one-norm solver involved in
	CRSI. Finally, we extend CRSI formulation to other geophysical applications
	and present results on multiple removal and migration-amplitude recovery.},
  keywords = {curvelet transform, reconstruction, SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/Theses/2008/hennenfent08phd.pdf}
}

% This file was created with JabRef 2.6.
% Encoding: MacRoman


@ARTICLE{berkhout97eom,
  author = {A. J. Berkhout and D. J. Verschuur},
  title = {Estimation of multiple scattering by iterative inversion, Part {I}:
	Theoretical considerations},
  journal = {Geophysics},
  year = {1997},
  volume = {62},
  pages = {1586-1595},
  number = {5},
  abstract = {A review has been given of the surface-related multiple problem by
	making use of the so-called feedback model. From the resulting equations
	it has been concluded that the proposed solution does not require
	any properties of the subsurface. However, source-detector and reflectivity
	properties of the surface need be specified. Those properties have
	been quantified in a surface operator and this operator is estimated
	as part of the multiple removal problem. The surface-related multiple
	removal algorithm has been formulated in terms of a Neumann series
	and in terms of an iterative equation. The Neumann formulation requires
	a nonlinear optimization process for the surface operator; while
	the iterative formulation needs a number of linear optimizations.
	The iterative formulation also has the advantage that it can be integrated
	easily with another multiple removal method. An algorithm for the
	removal of internal multiples has been proposed as well. This algorithm
	is an extension of the surface-related method. Removal of internal
	multiples requires knowledge of the macro velocity model between
	the surface and the upper boundary of the multiple generating layer.
	In part II (also published in this issue) the success of the proposed
	algorithms has been demonstrated on numerical experiments and field
	data examples. {\copyright}1997 Society of Exploration Geophysicists},
  bdsk-url-1 = {http://link.aip.org/link/?GPY/62/1586/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.1444261},
  date-added = {2008-05-07 18:38:50 -0700},
  date-modified = {2008-08-14 14:46:15 -0700},
  doi = {10.1190/1.1444261},
  issue = {5},
  keywords = {SRME},
  pdf = {http://link.aip.org/link/?GPY/62/1586/1},
  publisher = {SEG},
  url = {http://link.aip.org/link/?GPY/62/1586/1}
}

@BOOK{biondo063ds,
  title = {3-{D} seismic imaging},
  publisher = {SEG},
  year = {2006},
  author = {B. L. Biondi},
  number = {14},
  series = {Investigations in Geophysics},
  date-added = {2008-05-08 15:25:18 -0700},
  date-modified = {2008-05-20 19:45:00 -0700},
  issue = {14},
  keywords = {imaging}
}

@ARTICLE{cordoba78wpa,
  author = {A. C\'ordoba and C. Fefferman},
  title = {Wave packets and {F}ourier integral operators},
  journal = {Communications in Partial Differential Equations},
  year = {1978},
  volume = {3},
  pages = {979-1005},
  number = {11},
  bdsk-url-1 = {http://dx.doi.org/10.1080/03605307808820083},
  date-added = {2008-05-07 11:53:23 -0700},
  date-modified = {2008-05-20 11:48:08 -0700},
  doi = {10.1080/03605307808820083},
  issue = {11},
  keywords = {wave packets, FIO},
  publisher = {Taylor \& Francis}
}

@PHDTHESIS{candes98rta,
  author = {E. J. Cand\`es},
  title = {Ridgelets: theory and applications},
  school = {Stanford University},
  year = {1998},
  address = {Stanford, CA},
  bdsk-url-1 = {http://www.acm.caltech.edu/~emmanuel/papers/Thesis.ps.gz},
  date-added = {2008-05-27 18:24:11 -0700},
  date-modified = {2008-05-27 18:26:14 -0700},
  keywords = {ridgelet transform}
}

@ARTICLE{candes05tcr,
  author = {E. J. Cand\`es and L. Demanet},
  title = {The curvelet representation of wave propagators is optimally sparse},
  journal = {Communications on Pure and Applied Mathematics},
  year = {2005},
  volume = {58},
  pages = {1472-1528},
  number = {11},
  abstract = {This paper argues that curvelets provide a powerful tool for representing
	very general linear symmetric systems of hyperbolic differential
	equations. Curvelets are a recently developed multiscale system [10,
	7] in which the elements are highly anisotropic at fine scales, with
	effective support shaped according to the parabolic scaling principle
	width ‚âà length^2 at fine scales. We prove that for a wide class
	of linear hyperbolic differential equations, the curvelet representation
	of the solution operator is both optimally sparse and well organized.
	* It is sparse in the sense that the matrix entries decay nearly
	exponentially fast (i.e. faster than any negative polynomial), *
	and well-organized in the sense that the very few nonnegligible entries
	occur near a few shifted diagonals. Indeed, we show that the wave
	group maps each curvelet onto a sum of curvelet-like waveforms whose
	locations and orientations are obtained by following the different
	Hamiltonian flows---hence the diagonal shifts in the curvelet representation.
	A physical interpretation of this result is that curvelets may be
	viewed as coherent waveforms with enough frequency localization so
	that they behave like waves but at the same time, with enough spatial
	localization so that they simultaneously behave like particles. },
  bdsk-url-1 = {http://www.acm.caltech.edu/~emmanuel/papers/CurveletsWaves.pdf},
  date-added = {2008-05-07 11:10:43 -0700},
  date-modified = {2008-08-14 14:57:23 -0700},
  doi = {10.1002/cpa.20078},
  issue = {11},
  keywords = {curvelet transform, FIO},
  pdf = {http://www.acm.caltech.edu/~emmanuel/papers/CurveletsWaves.pdf}
}

@ARTICLE{candes06fdc,
  author = {E. J. Cand\`es and L. Demanet and D. L. Donoho and L. Ying},
  title = {Fast discrete curvelet transforms},
  journal = {Multiscale Modeling and Simulation},
  year = {2006},
  volume = {5},
  pages = {861-899},
  number = {3},
  abstract = {This paper describes two digital implementations of a new mathematical
	transform, namely, the second generation curvelet transform [12,
	10] in two and three dimensions. The first digital transformation
	is based on unequally-spaced fast Fourier transforms (USFFT) while
	the second is based on the wrapping of specially selected Fourier
	samples. The two implementations essentially differ by the choice
	of spatial grid used to translate curvelets at each scale and angle.
	Both digital transformations return a table of digital curvelet coefficients
	indexed by a scale parameter, an orientation parameter, and a spatial
	location parameter. And both implementations are fast in the sense
	that they run in O(n^2 log n) flops for n by n Cartesian arrays;
	in addition, they are also invertible, with rapid inversion algorithms
	of about the same complexity. Our digital transformations improve
	upon earlier implementations---based upon the first generation of
	curvelets---in the sense that they are conceptually simpler, faster
	and far less redundant. The software CurveLab, which implements both
	transforms presented in this paper, is available at http://www.curvelet.org.
	},
  bdsk-url-1 = {http://dx.doi.org/10.1137/05064182X},
  bdsk-url-2 = {http://www.acm.caltech.edu/~emmanuel/papers/FDCT.pdf},
  date-added = {2008-05-06 19:34:41 -0700},
  date-modified = {2008-08-14 14:58:30 -0700},
  doi = {10.1137/05064182X},
  issue = {3},
  keywords = {curvelet transform},
  pdf = {http://www.acm.caltech.edu/~emmanuel/papers/FDCT.pdf},
  publisher = {SIAM}
}

@INCOLLECTION{candes00cas,
  author = {E. J. Cand\`es and D. L. Donoho},
  title = {Curvelets: a surprisingly effective nonadaptive representation of
	objects with edges},
  booktitle = {Curve and surface fitting},
  publisher = {Vanderbilt University Press},
  year = {2000},
  editor = {A. Cohen, C. Rahut, and L. L. Schumaker},
  pages = {105-120},
  address = {Nashville, TN},
  abstract = {It is widely believed that to efficiently represent an otherwise smooth
	ob ject with discontinuities along edges, one must use an adaptive
	representation that in some sense `tracks' the shape of the discontinuity
	set. This folk-belief --- some would say folk-theorem --- is incorrect.
	At the very least, the possible quantitative advantage of such adaptation
	is vastly smaller than commonly believed. We have recently constructed
	a tight frame of curvelets which provides stable, efficient, and
	near-optimal representation of otherwise smooth ob jects having discontinuities
	along smooth curves. By applying naive thresholding to the curvelet
	transform of such an ob ject, one can form m-term approximations
	with rate of L2 approximation rivaling the rate obtainable by complex
	adaptive schemes which attempt to `track' the discontinuity set.
	In this article we explain the basic issues of efficient m-term approximation,
	the construction of efficient adaptive representation, the construction
	of the curvelet frame, and a crude analysis of the performance of
	curvelet schemes. },
  bdsk-url-1 = {http://www.acm.caltech.edu/~emmanuel/papers/Curvelet-SMStyle.pdf},
  date-added = {2008-05-26 17:48:55 -0700},
  date-modified = {2008-08-14 15:26:58 -0700},
  keywords = {curvelet transform}
}

@ARTICLE{candes05cct,
  author = {E. J. Cand\`es and D. L. Donoho},
  title = {Continuous curvelet transform: {I.} Resolution of the wavefront set},
  journal = {Applied and Computational Harmonic Analysis},
  year = {2005},
  volume = {19},
  pages = {162-197},
  number = {2},
  month = {September},
  bdsk-url-1 = {http://dx.doi.org/10.1016/j.acha.2005.02.003},
  date-added = {2008-05-26 18:21:22 -0700},
  date-modified = {2008-05-26 18:23:57 -0700},
  issue = {2},
  keywords = {curvelet transform}
}

@ARTICLE{candes05cct1,
  author = {E. J. Cand\`es and D. L. Donoho},
  title = {Continuous curvelet transform: {II.} Discretization and frames},
  journal = {Applied and Computational Harmonic Analysis},
  year = {2005},
  volume = {19},
  pages = {198-222},
  number = {2},
  month = {September},
  bdsk-url-1 = {http://dx.doi.org/10.1016/j.acha.2005.02.004},
  date-added = {2008-05-26 18:23:17 -0700},
  date-modified = {2008-05-26 18:24:36 -0700},
  issue = {2},
  keywords = {curvelet transform}
}

@ARTICLE{candes04ntf,
  author = {E. J. Cand\`es and D. L. Donoho},
  title = {New tight frames of curvelets and optimal representations of objects
	with piecewise-{C}$^2$ singularities},
  journal = {Communications on Pure and Applied Mathematics},
  year = {2004},
  volume = {57},
  pages = {219-266},
  number = {2},
  bdsk-url-1 = {http://dx.doi.org/10.1002/cpa.10116},
  bdsk-url-2 = {http://www.acm.caltech.edu/~emmanuel/papers/CurveEdges.pdf},
  date-added = {2008-05-07 11:47:59 -0700},
  date-modified = {2008-08-14 14:46:59 -0700},
  doi = {10.1002/cpa.10116},
  issue = {2},
  keywords = {curvelet transform},
  pdf = {http://www.acm.caltech.edu/~emmanuel/papers/CurveEdges.pdf}
}

@ARTICLE{chauris08sdm,
  author = {H. Chauris and T. Nguyen},
  title = {Seismic demigration/migration in the curvelet domain},
  journal = {Geophysics},
  year = {2008},
  volume = {73},
  pages = {S35-S46},
  number = {2},
  abstract = {Curvelets can represent local plane waves. They efficiently decompose
	seismic images and possibly imaging operators. We study how curvelets
	are distorted after demigration followed by migration in a different
	velocity model. We show that for small local velocity perturbations,
	the demigration/migration is reduced to a simple morphing of the
	initial curvelet. The derivation of the expected curvature of the
	curvelets shows that it is easier to sparsify the demigration/migration
	operator than the migration operator. An application on a 2D synthetic
	data set, generated in a smooth heterogeneous velocity model and
	with a complex reflectivity, demonstrates the usefulness of curvelets
	to predict what a migrated image would become in a locally different
	velocity model without the need for remigrating the full input data
	set. Curvelets are thus well suited to study the sensitivity of a
	prestack depth-migrated image with respect to the heterogeneous velocity
	model used for migration. {\copyright}2008 Society of Exploration
	Geophysicists},
  bdsk-url-1 = {http://link.aip.org/link/?GPY/73/S35/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.2831933},
  date-added = {2008-05-07 14:48:33 -0700},
  date-modified = {2008-08-14 14:59:04 -0700},
  doi = {10.1190/1.2831933},
  issue = {2},
  keywords = {curvelet transform, imaging},
  pdf = {http://link.aip.org/link/?GPY/73/S35/1},
  publisher = {SEG}
}

@BOOK{claerbout92esa,
  title = {Earth soundings analysis: processing versus inversion},
  publisher = {Blackwell Scientific Publications},
  year = {1992},
  author = {J. F. Claerbout},
  address = {Boston},
  bdsk-url-1 = {http://sepwww.stanford.edu/sep/prof/pvi.pdf},
  date-added = {2008-05-06 19:27:28 -0700},
  date-modified = {2008-05-07 11:44:19 -0700},
  keywords = {PEF},
  pdf = {http://sepwww.stanford.edu/sep/prof/pvi.pdf}
}

@ARTICLE{claerbout71tau,
  author = {J. F. Claerbout},
  title = {Toward a unified theory of reflector mapping},
  journal = {Geophysics},
  year = {1971},
  volume = {36},
  pages = {467-481},
  number = {3},
  abstract = {Schemes for seismic mapping of reflectors in the presence of an arbitrary
	velocity model, dipping and curved reflectors, diffractions, ghosts,
	surface elevation variations, and multiple reflections are reviewed
	and reduced to a single formula involving up and downgoing waves.
	The mapping formula may be implemented without undue complexity by
	means of difference approximations to the relativistic Schroedinger
	equation. {\copyright}1971 Society of Exploration Geophysicists},
  bdsk-url-1 = {http://link.aip.org/link/?GPY/36/467/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.1440185},
  date-added = {2008-05-08 14:59:36 -0700},
  date-modified = {2008-08-14 14:59:35 -0700},
  doi = {10.1190/1.1440185},
  issue = {3},
  keywords = {WEM, imaging},
  pdf = {http://link.aip.org/link/?GPY/36/467/1},
  publisher = {SEG}
}

@ARTICLE{daubechies04ait,
  author = {I. Daubechies and M. Defrise and C. {De Mol}},
  title = {An iterative thresholding algorithm for linear inverse problems with
	a sparsity constraint},
  journal = {Communications on Pure and Applied Mathematics},
  year = {2004},
  volume = {57},
  pages = {1413-1457},
  number = {11},
  abstract = {We consider linear inverse problems where the solution is assumed
	to have a sparse expansion on an arbitrary preassigned orthonormal
	basis. We prove that replacing the usual quadratic regularizing penalties
	by weighted p-penalties on the coefficients of such expansions, with
	1 p 2, still regularizes the problem. Use of such p-penalized problems
	with p < 2 is often advocated when one expects the underlying ideal
	noiseless solution to have a sparse expansion with respect to the
	basis under consideration. To compute the corresponding regularized
	solutions, we analyze an iterative algorithm that amounts to a Landweber
	iteration with thresholding (or nonlinear shrinkage) applied at each
	iteration step. We prove that this algorithm converges in norm. {\copyright}
	2004 Wiley Periodicals, Inc.},
  bdsk-url-1 = {http://dx.doi.org/10.1002/cpa.20042},
  date-added = {2008-05-20 13:58:17 -0700},
  date-modified = {2008-08-14 15:01:17 -0700},
  issue = {11},
  pdf = {http://dx.doi.org/10.1002/cpa.20042},
  refer1 = {10.1002/cpa.20042}
}

@ARTICLE{do2002can,
  author = {M. N. Do and M. Vetterli},
  title = {Contourlets: a new directional multiresolution image representation},
  journal = {Proceedings. 2002 International Conference on Image Processing.},
  year = {2002},
  volume = {1},
  abstract = {We propose a new scheme, named contourlet, that provides a flexible
	multiresolution, local and directional image expansion. The contourlet
	transform is realized efficiently via a double iterated filter bank
	structure. Furthermore, it can be designed to satisfy the anisotropy
	scaling relation for curves, and thus offers a fast and structured
	curvelet-like decomposition. As a result, the contourlet transform
	provides a sparse representation for two-dimensional piecewise smooth
	signals resembling images. Finally, we show some numerical experiments
	demonstrating the potential of contourlets in several image processing
	tasks.},
  bdsk-url-1 = {http://dx.doi.org/10.1109/ICIP.2002.1038034},
  date-added = {2008-05-07 11:58:00 -0700},
  date-modified = {2008-08-14 15:01:55 -0700},
  doi = {10.1109/ICIP.2002.1038034},
  keywords = {contourlet transform}
}

@TECHREPORT{donoho99dct,
  author = {D. L. Donoho and M. R. Duncan},
  title = {Digital curvelet transform: strategy, implementation, and experiments},
  institution = {Stanford Statistics Department},
  year = {1999},
  month = {November},
  bdsk-url-1 = {http://citeseer.ist.psu.edu/rd/44392127,300178,1,0.25,Download/http://citeseer.ist.psu.edu/cache/papers/cs/15527/http:zSzzSzwww-stat.stanford.eduzSz~donohozSzReportszSz1999zSzDCvT.pdf/donoho99digital.pdf},
  date-added = {2008-05-26 17:33:51 -0700},
  date-modified = {2008-05-26 17:35:32 -0700},
  keywords = {curvelet transform}
}

@ARTICLE{douma07los,
  author = {H. Douma and M. V. de Hoop},
  title = {Leading-order seismic imaging using curvelets},
  journal = {Geophysics},
  year = {2007},
  volume = {72},
  pages = {S231-S248},
  number = {6},
  abstract = {Curvelets are plausible candidates for simultaneous compression of
	seismic data, their images, and the imaging operator itself. We show
	that with curvelets, the leading-order approximation (in angular
	frequency, horizontal wavenumber, and migrated location) to common-offset
	(CO) Kirchhoff depth migration becomes a simple transformation of
	coordinates of curvelets in the data, combined with amplitude scaling.
	This transformation is calculated using map migration, which employs
	the local slopes from the curvelet decomposition of the data. Because
	the data can be compressed using curvelets, the transformation needs
	to be calculated for relatively few curvelets only. Numerical examples
	for homogeneous media show that using the leading-order approximation
	only provides a good approximation to CO migration for moderate propagation
	times. As the traveltime increases and rays diverge beyond the spatial
	support of a curvelet; however, the leading-order approximation is
	no longer accurate enough. This shows the need for correction beyond
	leading order, even for homogeneous media. {\copyright}2007 Society
	of Exploration Geophysicists},
  bdsk-url-1 = {http://link.aip.org/link/?GPY/72/S231/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.2785047},
  date-added = {2008-05-07 14:35:47 -0700},
  date-modified = {2008-08-14 15:02:25 -0700},
  doi = {10.1190/1.2785047},
  issue = {6},
  keywords = {curvelet transform, imaging},
  pdf = {http://link.aip.org/link/?GPY/72/S231/1},
  publisher = {SEG}
}

@INCOLLECTION{feichtinger94tap,
  author = {H. G. Feichtinger and K. Grochenig},
  title = {Theory and practice of irregular sampling},
  booktitle = {Wavelets: mathematics and applications},
  publisher = {CRC Press},
  year = {1994},
  editor = {J. J. Benedetto and M. Frazier},
  series = {Studies in Advanced Mathematics},
  chapter = {8},
  pages = {305-363},
  address = {Boca Raton, FL},
  bdsk-url-1 = {http://www.univie.ac.at/nuhag-php/bibtex/open_files/fegr94_fgthpra.pdf},
  date-added = {2008-05-20 17:10:18 -0700},
  date-modified = {2008-05-20 17:24:38 -0700},
  keywords = {sampling},
  pdf = {http://www.univie.ac.at/nuhag-php/bibtex/open_files/fegr94_fgthpra.pdf}
}

@MISC{fenelon08msc,
  author = {Lloyd Fenelon},
  title = {Nonequispaced discrete curvelet transform for seismic data reconstruction},
  howpublished = {BSc thesis, Ecole Nationale Superieure De Physique de Strasbourg},
  month = {August},
  year = {2008},
  abstract = {Physical constraints during seismic acquisitions lead to incomplete
	seismic datasets. Curvelet Reconstruction with Sparsity promoting
	Inversion (CRSI) is one of the most efficient interpolation method
	available to recover complete datasets from data with missing traces.
	The method uses in its definition the curvelet transform which is
	well suited to process seismic data. However, its main shortcoming
	is to not be able to provide an accurate result if the data are acquired
	at irregular positions. This come from the curvelet transform implementation
	which cannot handle this type of data. In this thesis the implementation
	of the curvelet transform is modified to offer the possibility to
	CRSI to give better representation of seismic data for high quality
	seismic imaging. },
  bdsk-url-1 = {http://slim.eos.ubc.ca/Publications/Public/Theses/2008/fenelon08msc.pdf},
  date-added = {2008-09-03 16:18:08 -0700},
  date-modified = {2008-09-03 16:25:10 -0700},
  keywords = {SLIM, BSc},
  pdf = {http://slim.eos.ubc.ca/Publications/Public/Theses/2008/fenelon08msc.pdf}
}

@MISC{fomel07mos,
  author = {S. Fomel and P. Sava},
  title = {{MADAGASCAR}: open-source software package for geophysical data processing
	and reproducible numerical experiments},
  year = {2007},
  abstract = {is an open-source software package for geophysical data analysis and
	reproducible numerical experiments. Its mission is to provide -a
	convenient and powerful environment -a convenient technology transfer
	tool for researchers working with digital image and data processing.
	The technology developed using the Madagascar project management
	system is transferred in the form of recorded processing histories,
	which become "computational recipes" to be verified, exchanged, and
	modified by users of the system.},
  bdsk-url-1 = {http://rsf.sf.net},
  date-added = {2008-06-26 15:31:10 -0700},
  date-modified = {2008-08-14 15:31:44 -0700},
  keywords = {software},
  url = {http://rsf.sf.net}
}

@ARTICLE{guo07osm,
  author = {K. Guo and D. Labate},
  title = {Optimally sparse multidimensional representation using shearlets},
  journal = {Journal of Mathematical Analysis},
  year = {2007},
  volume = {39},
  pages = {298-318},
  number = {1},
  bdsk-url-1 = {http://www4.ncsu.edu/~dlabate/shear_GL.pdf},
  bdsk-url-2 = {http://dx.doi.org/10.1137/060649781},
  date-added = {2008-05-07 12:03:03 -0700},
  date-modified = {2008-05-08 10:28:30 -0700},
  doi = {10.1137/060649781},
  issue = {1},
  keywords = {shearlet transform},
  pdf = {http://www4.ncsu.edu/~dlabate/shear_GL.pdf},
  publisher = {SIAM}
}

@ARTICLE{hampson86ivs,
  author = {D. Hampson},
  title = {Inverse Velocity Stacking for Multiple Elimination},
  journal = {Journal of the Canadian Society of Exploration Geophysicists},
  year = {1986},
  volume = {22},
  pages = {44-45},
  number = {1},
  bdsk-url-1 = {http://www.cseg.ca/publications/journal/1986_12/1986_Hampson_D_inverse_velocity_stacking.pdf},
  date-added = {2008-05-06 19:09:45 -0700},
  date-modified = {2008-05-07 11:44:52 -0700},
  issue = {1},
  keywords = {Radon transform},
  pdf = {http://www.cseg.ca/publications/journal/1986_12/1986_Hampson_D_inverse_velocity_stacking.pdf},
  publisher = {CSEG}
}

@ARTICLE{hindriks00ro3,
  author = {K. Hindriks and A. J. W. Duijndam},
  title = {Reconstruction of {3-D} seismic signals irregularly sampled along
	two spatial coordinates},
  journal = {Geophysics},
  year = {2000},
  volume = {65},
  pages = {253-263},
  number = {1},
  abstract = {Seismic signals are often irregularly sampled along spatial coordinates,
	leading to suboptimal processing and imaging results. Least-squares
	estimation of Fourier components is used for the reconstruction of
	band-limited seismic signals that are irregularly sampled along two
	spatial coordinates. A simple and efficient diagonal weighting scheme,
	based on the areas surrounding the spatial samples, takes the properties
	of the noise (signal outside the bandwidth) into account in an approximate
	sense. Diagonal stabilization based on the energies of the signal
	and the noise ensures robust estimation. Reconstruction by temporal
	frequency component allows the specification of varying bandwidth
	in two dimensions, depending on the minimum apparent velocity. This
	parameterization improves the reconstruction capability for lower
	temporal frequencies. The shape of the spatial aperture affects the
	method of sampling the Fourier domain. Taking into account this property,
	a larger bandwidth can be recovered. The properties of the least-squares
	estimator allow a very efficient implementation which, when using
	a conjugate gradient algorithm, requires a modest number of 2-D fast
	Fourier transforms per temporal frequency. The method shows signicant
	improvement over the conventionally used binning and stacking method
	on both synthetic and real data. The method can be applied to any
	subset of seismic data with two varying spatial coordinates. {\copyright}2000
	Society of Exploration Geophysicists},
  bdsk-url-1 = {http://link.aip.org/link/?GPY/65/253/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.1444716},
  date-added = {2008-05-20 16:12:37 -0700},
  date-modified = {2008-08-14 15:05:01 -0700},
  doi = {10.1190/1.1444716},
  issue = {1},
  keywords = {reconstruction},
  pdf = {http://link.aip.org/link/?GPY/65/253/1},
  publisher = {SEG}
}

@PHDTHESIS{kunis06nff,
  author = {S. Kunis},
  title = {Nonequispaced {FFT}: generalisation and inversion},
  school = {L\"ubeck university},
  year = {2006},
  bdsk-url-1 = {http://www-user.tu-chemnitz.de/~skunis/paper/KunisDiss.pdf},
  date-added = {2008-05-07 18:51:16 -0700},
  date-modified = {2008-05-20 11:49:04 -0700},
  keywords = {NFFT},
  pdf = {http://www-user.tu-chemnitz.de/~skunis/paper/KunisDiss.pdf}
}

@ARTICLE{lu07mdf,
  author = {Y. M. Lu and M. N. Do},
  title = {Multidimensional directional filter banks and surfacelets},
  journal = {IEEE Transactions on Image Processing},
  year = {2007},
  volume = {16},
  pages = {918-931},
  number = {4},
  month = {April},
  abstract = {In 1992, Bamberger and Smith proposed the directional filter bank
	(DFB) for an efficient directional decomposition of 2-D signals.
	Due to the nonseparable nature of the system, extending the DFB to
	higher dimensions while still retaining its attractive features is
	a challenging and previously unsolved problem. We propose a new family
	of filter banks, named NDFB, that can achieve the directional decomposition
	of arbitrary N-dimensional (Nges2) signals with a simple and efficient
	tree-structured construction. In 3-D, the ideal passbands of the
	proposed NDFB are rectangular-based pyramids radiating out from the
	origin at different orientations and tiling the entire frequency
	space. The proposed NDFB achieves perfect reconstruction via an iterated
	filter bank with a redundancy factor of N in N-D. The angular resolution
	of the proposed NDFB can be iteratively refined by invoking more
	levels of decomposition through a simple expansion rule. By combining
	the NDFB with a new multiscale pyramid, we propose the surfacelet
	transform, which can be used to efficiently capture and represent
	surface-like singularities in multidimensional data},
  bdsk-url-1 = {http://dx.doi.org/10.1109/TIP.2007.891785},
  date-added = {2008-05-07 12:19:48 -0700},
  date-modified = {2008-08-14 15:05:31 -0700},
  doi = {10.1109/TIP.2007.891785},
  issn = {1057-7149},
  issue = {4},
  keywords = {surfacelet transform},
  publisher = {IEEE}
}

@BOOK{mallat99awt,
  title = {A Wavelet Tour of Signal Processing, Second Edition},
  publisher = {Academic Press},
  year = {1999},
  author = {S. Mallat},
  month = {September},
  date-added = {2008-05-22 16:32:31 -0700},
  date-modified = {2008-05-22 16:33:57 -0700},
  howpublished = {Hardcover},
  isbn = {012466606X},
  keywords = {wavelet transform}
}

@CONFERENCE{morton98fsr,
  author = {S. A. Morton and C. C. Ober},
  title = {Faster shot-record depth migrations using phase encoding},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {1998},
  volume = {17},
  number = {1},
  pages = {1131-1134},
  publisher = {SEG},
  bdsk-url-1 = {http://link.aip.org/link/?SGA/17/1131/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.1820088},
  date-added = {2008-05-27 16:44:01 -0700},
  date-modified = {2008-05-27 16:45:21 -0700},
  doi = {10.1190/1.1820088},
  issue = {1},
  pdf = {http://link.aip.org/link/?SGA/17/1131/1}
}

@ARTICLE{paige82lsq,
  author = {C. C. Paige and M. A. Saunders},
  title = {{LSQR}: an algorithm for sparse linear equations and sparse least
	squares},
  journal = {Transactions on Mathematical Software},
  year = {1982},
  volume = {8},
  pages = {43-71},
  number = {1},
  address = {New York, NY, USA},
  bdsk-url-1 = {http://doi.acm.org/10.1145/355984.355989},
  date-added = {2008-05-20 14:00:44 -0700},
  date-modified = {2008-05-20 19:47:37 -0700},
  doi = {http://doi.acm.org/10.1145/355984.355989},
  issn = {0098-3500},
  issue = {1},
  keywords = {LSQR},
  publisher = {ACM}
}

@INCOLLECTION{potts01mst,
  author = {D. Potts and G. Steidl and M. Tasche},
  title = {Fast {F}ourier transforms for nonequispaced data: a tutorial},
  booktitle = {Modern sampling theory: mathematics and applications},
  publisher = {Birkhauser},
  year = {2001},
  editor = {J. J. Benedetto and P. Ferreira},
  chapter = {12},
  pages = {249-274},
  abstract = {In this section, we consider approximate methods for the fast computiation
	of multivariate discrete Fourier transforms for nonequispaced data
	(NDFT) in the time domain and in the frequency domain. In particular,
	we are interested in the approximation error as function of arithmetic
	complexity of the algorithm. We discuss the robustness of NDFT-algorithms
	with respect to roundoff errors and apply NDFT-algorithms for the
	fast computation of Bessel transforms.},
  bdsk-url-1 = {http://www.tu-chemnitz.de/~potts/paper/ndft.pdf},
  date-added = {2008-05-07 18:44:29 -0700},
  date-modified = {2008-08-14 15:28:37 -0700},
  keywords = {NFFT},
  pdf = {http://www.tu-chemnitz.de/~potts/paper/ndft.pdf}
}

@ARTICLE{romero00peo,
  author = {L. A. Romero and D. C. Ghiglia and C. C. Ober and S. A. Morton},
  title = {Phase encoding of shot records in prestack migration},
  journal = {Geophysics},
  year = {2000},
  volume = {65},
  pages = {426-436},
  number = {2},
  abstract = {Frequency-domain shot-record migration can produce higher quality
	images than Kirchhoff migration but typically at a greater cost.
	The computing cost of shot-record migration is the product of the
	number of shots in the survey and the expense of each individual
	migration. Many attempts to reduce this cost have focused on the
	speed of the individual migrations, trying to achieve a better trade-off
	between accuracy and speed. Another approach is to reduce the number
	of migrations. We investigate the simultaneous migration of shot
	records using frequency-domain shot-record migration algorithms.
	The difficulty with this approach is the production of so-called
	crossterms between unrelated shot and receiver wavefields, which
	generate unwanted artifacts or noise in the final image. To reduce
	these artifacts and obtain an image comparable in quality to the
	single-shot-per-migration result, we have introduced a process called
	phase encoding, which shifts or disperses these crossterms. The process
	of phase encoding thus allows one to trade S/N ratio for the speed
	of migrating the entire survey. Several encoding functions and two
	application strategies have been tested. The first strategy, combining
	multiple shots per migration and using each shot only once, reduces
	computation in direct relation to the number of shots combined. The
	second strategy, performing multiple migrations of all the shots
	in the survey, provides a means to reduce the crossterm noise by
	stacking the resulting images. The additional noise in both strategies
	may be tolerated if it is no stronger than the inherent seismic noise
	in the migrated image and if the final image is achieved with less
	cost. {\copyright}2000 Society of Exploration Geophysicists},
  bdsk-url-1 = {http://link.aip.org/link/?GPY/65/426/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.1444737},
  date-added = {2008-05-27 16:42:50 -0700},
  date-modified = {2008-08-14 15:07:08 -0700},
  doi = {10.1190/1.1444737},
  issue = {2},
  pdf = {http://link.aip.org/link/?GPY/65/426/1},
  publisher = {SEG}
}

@ARTICLE{sacchi98iae,
  author = {M. D. Sacchi and T. J. Ulrych and C. J. Walker},
  title = {Interpolation and extrapolation using a high-resolution discrete
	{F}ourier transform},
  journal = {IEEE Transactions on Signal Processing},
  year = {1998},
  volume = {46},
  pages = {31-38},
  number = {1},
  abstract = {We present an iterative nonparametric approach to spectral estimation
	that is particularly suitable for estimation of line spectra. This
	approach minimizes a cost function derived from Bayes' theorem. The
	method is suitable for line spectra since a ``long tailed'' distribution
	is used to model the prior distribution of spectral amplitudes. An
	important aspect of this method is that since the data themselves
	are used as constraints, phase information can also be recovered
	and used to extend the data outside the original window. The objective
	function is formulated in terms of hyperpa- rameters that control
	the degree of fit and spectral resolution. Noise rejection can also
	be achieved by truncating the number of iterations. Spectral resolution
	and extrapolation length are controlled by a single parameter. When
	this parameter is large compared with the spectral powers, the algorithm
	leads to zero extrapolation of the data, and the estimated Fourier
	transform yields the periodogram. When the data are sampled at a
	constant rate, the algorithm uses one Levinson recursion per iteration.
	For irregular sampling (unevenly sampled and/or gapped data), the
	algorithm uses one Cholesky decomposition per iteration. The performance
	of the algorithm is illustrated with three different problems that
	frequently arise in geophysical data processing: 1) harmonic retrieval
	from a time series contaminated with noise; 2) linear event detection
	from a finite aperture array of receivers [which, in fact, is an
	extension of 1)], 3) interpolation/extrapolation of gapped data.
	The performance of the algorithm as a spectral estimator is tested
	with the Kay and Marple data set. It is shown that the achieved resolution
	is comparable with parametric methods but with more accurate representation
	of the relative power in the spectral lines. },
  bdsk-url-1 = {http://www-geo.phys.ualberta.ca/saig/papers/Sacchi_Ulrych_Walker_IEEE_98.pdf},
  date-added = {2008-05-06 19:18:50 -0700},
  date-modified = {2008-08-14 15:08:37 -0700},
  doi = {10.1109/78.651165},
  issue = {1},
  keywords = {Fourier transform, reconstruction},
  pdf = {http://www-geo.phys.ualberta.ca/saig/papers/Sacchi_Ulrych_Walker_IEEE_98.pdf},
  publisher = {IEEE}
}

@PHDTHESIS{schonewille00phd,
  author = {M. A. Schonewille},
  title = {Fourier reconstruction of irregularly sampled seismic data},
  school = {Delft University of Technology},
  year = {2000},
  address = {Delft, The Netherlands},
  month = {November},
  date-added = {2008-05-06 19:03:35 -0700},
  date-modified = {2008-05-09 14:43:57 -0700},
  keywords = {Fourier transform, reconstruction},
  rating = {0},
  read = {Yes}
}

@ARTICLE{smith98ahs,
  author = {H. Smith},
  title = {A Hardy space for {F}ourier integral operators},
  journal = {Journal of Geometric Analysis},
  year = {1998},
  volume = {8},
  pages = {629-653},
  number = {4},
  date-added = {2008-05-07 12:25:03 -0700},
  date-modified = {2008-08-14 15:09:47 -0700},
  issue = {4},
  keywords = {FIO}
}

@BOOK{snieder93giu,
  title = {Global inversions using normal mode and long-period surface waves},
  publisher = {Chapman and Hall},
  year = {1993},
  author = {R. Snieder},
  date-added = {2008-05-20 17:16:42 -0700},
  date-modified = {2008-05-20 17:19:44 -0700},
  keywords = {sampling}
}

@ARTICLE{spitz91sti,
  author = {S. Spitz},
  title = {Seismic trace interpolation in the {FX} domain},
  journal = {Geophysics},
  year = {1991},
  volume = {56},
  pages = {785-794},
  number = {6},
  abstract = {Interpolation of seismic traces is an effective means of improving
	migration when the data set exhibits spatial aliasing. A major difficulty
	of standard interpolation methods is that they depend on the degree
	of reliability with which the various geological events can be separated.
	In this respect, a multichannel interpolation method is described
	which requires neither a priori knowledge of the directions of lateral
	coherence of the events, nor estimation of these directions. The
	method is based on the fact that linear events present in a section
	made of equally spaced traces may be interpolated exactly, regardless
	of the original spatial interval, without any attempt to determine
	their true dips. The predictability of linear events in the f-x domain
	allows the missing traces to be expressed as the output of a linear
	system, the input of which consists of the recorded traces. The interpolation
	operator is obtained by solving a set of linear equations whose coefficients
	depend only on the spectrum of the spatial prediction filter defined
	by the recorded traces. Synthetic examples show that this method
	is insensitive to random noise and that it correctly handles curvatures
	and lateral amplitude variations. Assessment of the method with a
	real data set shows that the interpolation yields an improved migrated
	section. {\copyright}1991 Society of Exploration Geophysicists},
  bdsk-url-1 = {http://dx.doi.org/10.1190/1.1443096},
  date-added = {2008-05-06 19:29:12 -0700},
  date-modified = {2008-08-14 15:18:16 -0700},
  doi = {10.1190/1.1443096},
  issue = {6},
  keywords = {PEF},
  publisher = {SEG}
}

@ARTICLE{starck02tct,
  author = {J.-L. Starck and E. J. Cand\`es and D. L. Donoho},
  title = {The curvelet transform for image denoising},
  journal = {IEEE Transactions on Image Processing},
  year = {2002},
  volume = {11},
  pages = {670-684},
  number = {6},
  month = {June},
  abstract = {We describe approximate digital implementations of two new mathematical
	transforms, namely, the ridgelet transform and the curvelet transform.
	Our implementations offer exact reconstruction, stability against
	perturbations, ease of implementation, and low computational complexity.
	A central tool is Fourier-domain computation of an approximate digital
	Radon transform. We introduce a very simple interpolation in the
	Fourier space which takes Cartesian samples and yields samples on
	a rectopolar grid, which is a pseudo-polar sampling set based on
	a concentric squares geometry. Despite the crudeness of our interpolation,
	the visual performance is surprisingly good. Our ridgelet transform
	applies to the Radon transform a special overcomplete wavelet pyramid
	whose wavelets have compact support in the frequency domain. Our
	curvelet transform uses our ridgelet transform as a component step,
	and implements curvelet subbands using a filter bank of a&grave;
	trous wavelet filters. Our philosophy throughout is that transforms
	should be overcomplete, rather than critically sampled. We apply
	these digital transforms to the denoising of some standard images
	embedded in white noise. In the tests reported here, simple thresholding
	of the curvelet coefficients is very competitive with "state of the
	art" techniques based on wavelets, including thresholding of decimated
	or undecimated wavelet transforms and also including tree-based Bayesian
	posterior mean methods. Moreover, the curvelet reconstructions exhibit
	higher perceptual quality than wavelet-based reconstructions, offering
	visually sharper images and, in particular, higher quality recovery
	of edges and of faint linear and curvilinear features. Existing theory
	for curvelet and ridgelet transforms suggests that these new approaches
	can outperform wavelet methods in certain image reconstruction problems.
	The empirical results reported here are in encouraging agreement},
  bdsk-url-1 = {http://dx.doi.org/10.1109/TIP.2002.1014998},
  bdsk-url-2 = {http://ieeexplore.ieee.org/iel5/83/21845/01014998.pdf},
  date-added = {2008-05-26 17:38:14 -0700},
  date-modified = {2008-08-14 15:19:16 -0700},
  doi = {10.1109/TIP.2002.1014998},
  issn = {1057-7149},
  issue = {6},
  keywords = {curvelet transform},
  publisher = {IEEE}
}

@ARTICLE{symes07rtm,
  author = {W. W. Symes},
  title = {Reverse time migration with optimal checkpointing},
  journal = {Geophysics},
  year = {2007},
  volume = {72},
  pages = {SM213-SM221},
  number = {5},
  abstract = {Reverse time migration (RTM) requires that fields computed in forward
	time be accessed in reverse order. Such out-of-order access, to recursively
	computed fields, requires that some part of the recursion history
	be stored (checkpointed), with the remainder computed by repeating
	parts of the forward computation. Optimal checkpointing algorithms
	choose checkpoints in such a way that the total storage is minimized
	for a prescribed level of excess computation, or vice versa. Optimal
	checkpointing dramatically reduces the storage required by RTM, compared
	to that needed for nonoptimal implementations, at the price of a
	small increase in computation. This paper describes optimal checkpointing
	in a form which applies both to RTM and other applications of the
	adjoint state method, such as construction of velocity updates from
	prestack wave equation migration. {\copyright}2007 Society of Exploration
	Geophysicists},
  bdsk-url-1 = {http://link.aip.org/link/?GPY/72/SM213/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.2742686},
  date-added = {2008-05-08 14:42:11 -0700},
  date-modified = {2008-08-14 15:19:43 -0700},
  doi = {10.1190/1.2742686},
  issue = {5},
  keywords = {RTM, imaging},
  pdf = {http://link.aip.org/link/?GPY/72/SM213/1},
  publisher = {SEG}
}

@ARTICLE{thorson85vsa,
  author = {J. R. Thorson and J. F. Claerbout},
  title = {Velocity-stack and slant-stack stochastic inversion},
  journal = {Geophysics},
  year = {1985},
  volume = {50},
  pages = {2727-2741},
  number = {12},
  abstract = {Normal moveout (NMO) and stacking, an important step in analysis of
	reflection seismic data, involves summation of seismic data over
	paths represented by a family of hyperbolic curves. This summation
	process is a linear transformation and maps the data into what might
	be called a velocity space: a two-dimensional set of points indexed
	by time and velocity. Examination of data in velocity space is used
	for analysis of subsurface velocities and filtering of undesired
	coherent events (e.g., multiples), but the filtering step is useful
	only if an approximate inverse to the NMO and stack operation is
	available. One way to effect velocity filtering is to use the operator
	LT (defined as NMO and stacking) and its adjoint L as a transform
	pair, but this leads to unacceptable filtered output. Designing a
	better estimated inverse to L than LT is a generalization of the
	inversion problem of computerized tomography: deconvolving out the
	point-spread function after back projection. The inversion process
	is complicated by missing data, because surface seismic data are
	recorded only within a finite spatial aperture on the Earth's surface.
	Our approach to solving the problem of an ill-conditioned or nonunique
	inverse L--1, brought on by missing data, is to design a stochastic
	inverse to L. Starting from a maximum a posteriori (MAP) estimator,
	a system of equations can be set up in which a priori information
	is incorporated into a sparseness measure: the output of the stochastic
	inverse is forced to be locally focused, in order to obtain the best
	possible resolution in velocity space. The size of the resulting
	nonlinear system of equations is immense, but using a few iterations
	with a gradient descent algorithm is adequate to obtain a reasonable
	solution. This theory may also be applied to other large, sparse
	linear operators. The stochastic inverse of the slant-stack operator
	(a particular form of the Radon transform), can be developed in a
	parallel manner, and will yield an accurate slant-stack inverse pair.
	{\copyright}1985 Society of Exploration Geophysicists},
  bdsk-url-1 = {http://dx.doi.org/10.1190/1.1441893},
  date-added = {2008-05-06 19:06:15 -0700},
  date-modified = {2008-08-14 15:20:19 -0700},
  doi = {10.1190/1.1441893},
  issue = {12},
  keywords = {Radon transform},
  publisher = {SEG}
}

@ARTICLE{trad03lvo,
  author = {D. Trad and T. J. Ulrych and M. D. Sacchi},
  title = {Latest views of the sparse {R}adon transform},
  journal = {Geophysics},
  year = {2003},
  volume = {68},
  pages = {386-399},
  number = {1},
  abstract = {The Radon transform (RT) suffers from the typical problems of loss
	of resolution and aliasing that arise as a consequence of incomplete
	information, including limited aperture and discretization. Sparseness
	in the Radon domain is a valid and useful criterion for supplying
	this missing information, equivalent somehow to assuming smooth amplitude
	variation in the transition between known and unknown (missing) data.
	Applying this constraint while honoring the data can become a serious
	challenge for routine seismic processing because of the very limited
	processing time available, in general, per common midpoint. To develop
	methods that are robust, easy to use and flexible to adapt to different
	problems we have to pay attention to a variety of algorithms, operator
	design, and estimation of the hyperparameters that are responsible
	for the regularization of the solution.In this paper, we discuss
	fast implementations for several varieties of RT in the time and
	frequency domains. An iterative conjugate gradient algorithm with
	fast Fourier transform multiplication is used in all cases. To preserve
	the important property of iterative subspace methods of regularizing
	the solution by the number of iterations, the model weights are incorporated
	into the operators. This turns out to be of particular importance,
	and it can be understood in terms of the singular vectors of the
	weighted transform. The iterative algorithm is stopped according
	to a general cross validation criterion for subspaces. We apply this
	idea to several known implementations and compare results in order
	to better understand differences between, and merits of, these algorithms.
	{\copyright}2003 Society of Exploration Geophysicists},
  bdsk-url-1 = {http://link.aip.org/link/?GPY/68/386/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.1543224},
  date-added = {2008-05-07 19:03:39 -0700},
  date-modified = {2008-08-14 15:20:56 -0700},
  doi = {10.1190/1.1543224},
  issue = {1},
  keywords = {Radon transform},
  pdf = {http://link.aip.org/link/?GPY/68/386/1},
  publisher = {SEG}
}

@ARTICLE{verschuur97eom,
  author = {D. J. Verschuur and A. J. Berkhout},
  title = {Estimation of multiple scattering by iterative inversion, Part {II}:
	Practical aspects and examples},
  journal = {Geophysics},
  year = {1997},
  volume = {62},
  pages = {1596-1611},
  number = {5},
  abstract = {A surface-related multiple-elimination method can be formulated as
	an iterative procedure: the output of one iteration step is used
	as input for the next iteration step (part I of this paper). In this
	paper (part II) it is shown that the procedure can be made very efficient
	if a good initial estimate of the multiple-free data set can be provided
	in the first iteration, and in many situations, the Radon-based multiple-elimination
	method may provide such an estimate. It is also shown that for each
	iteration, the inverse source wavelet can be accurately estimated
	by a linear (least-squares) inversion process. Optionally, source
	and detector variations and directivity effects can be included,
	although the examples are given without these options. The iterative
	multiple elimination process, together with the source wavelet estimation,
	are illustrated with numerical experiments as well as with field
	data examples. The results show that the surface-related multiple-elimination
	process is very effective in time gates where the moveout properties
	of primaries and multiples are very similar (generally deep data),
	as well as for situations with a complex multiple-generating system.
	{\copyright}1997 Society of Exploration Geophysicists},
  bdsk-url-1 = {http://link.aip.org/link/?GPY/62/1596/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.1444262},
  date-added = {2008-05-07 18:40:45 -0700},
  date-modified = {2008-08-14 15:21:18 -0700},
  doi = {10.1190/1.1444262},
  issue = {5},
  keywords = {SRME},
  pdf = {http://link.aip.org/link/?GPY/62/1596/1},
  publisher = {SEG}
}

@ARTICLE{xu05aft,
  author = {S. Xu and Y. Zhang and D. Pham and G. Lambar\'{e}},
  title = {Antileakage {F}ourier transform for seismic data regularization},
  journal = {Geophysics},
  year = {2005},
  volume = {70},
  pages = {V87-V95},
  number = {4},
  abstract = {Seismic data regularization, which spatially transforms irregularly
	sampled acquired data to regularly sampled data, is a long-standing
	problem in seismic data processing. Data regularization can be implemented
	using Fourier theory by using a method that estimates the spatial
	frequency content on an irregularly sampled grid. The data can then
	be reconstructed on any desired grid. Difficulties arise from the
	nonorthogonality of the global Fourier basis functions on an irregular
	grid, which results in the problem of "spectral leakage": energy
	from one Fourier coefficient leaks onto others. We investigate the
	nonorthogonality of the Fourier basis on an irregularly sampled grid
	and propose a technique called "antileakage Fourier transform" to
	overcome the spectral leakage. In the antileakage Fourier transform,
	we first solve for the most energetic Fourier coefficient, assuming
	that it causes the most severe leakage. To attenuate all aliases
	and the leakage of this component onto other Fourier coefficients,
	the data component corresponding to this most energetic Fourier coefficient
	is subtracted from the original input on the irregular grid. We then
	use this new input to solve for the next Fourier coefficient, repeating
	the procedure until all Fourier coefficients are estimated. This
	procedure is equivalent to "reorthogonalizing" the global Fourier
	basis on an irregularly sampled grid. We demonstrate the robustness
	and effectiveness of this technique with successful applications
	to both synthetic and real data examples. {\copyright}2005 Society
	of Exploration Geophysicists},
  bdsk-url-1 = {http://link.aip.org/link/?GPY/70/V87/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.1993713},
  date-added = {2008-05-09 17:43:47 -0700},
  date-modified = {2008-08-14 15:21:45 -0700},
  doi = {10.1190/1.1993713},
  issue = {4},
  keywords = {Fourier transform, reconstruction},
  pdf = {http://link.aip.org/link/?GPY/70/V87/1},
  publisher = {SEG}
}

@ARTICLE{ying053dd,
  author = {L. Ying and L. Demanet and E. J. Cand\`es},
  title = {{3-D} discrete curvelet transform},
  journal = {Proceedings SPIE wavelets XI, San Diego},
  year = {2005},
  volume = {5914},
  pages = {344-354},
  month = {January},
  abstract = {In this paper, we present the first 3D discrete curvelet transform.
	This transform is an extension to the 2D transform described in Candes
	et al..1 The resulting curvelet frame preserves the important properties,
	such as parabolic scaling, tightness and sparse representation for
	singularities of codimension one. We describe three different implementations:
	in-core, out-of-core and MPI-based parallel implementations. Numerical
	results verify the desired properties of the 3D curvelets and demonstrate
	the efficiency of our implementations. },
  bdsk-url-1 = {http://dx.doi.org/10.1117/12.616205},
  date-added = {2008-05-07 14:14:59 -0700},
  date-modified = {2008-08-14 15:21:59 -0700},
  doi = {10.1117/12.616205},
  keywords = {curvelet transform}
}

@PHDTHESIS{zwartjes05phd,
  author = {P. M. Zwartjes},
  title = {Fourier reconstruction with sparse inversion},
  school = {Delft University of Technology},
  year = {2005},
  address = {Delft, The Netherlands},
  month = {December},
  date-added = {2008-05-06 18:58:35 -0700},
  date-modified = {2008-05-09 14:44:04 -0700},
  keywords = {Fourier transform, reconstruction},
  rating = {0},
  read = {Yes}
}

