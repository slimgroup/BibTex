% This file was created with JabRef 2.9.
% Encoding: MacRoman

%-----2016-----%

@TECHREPORT{herrmann2016SLBors,
  author = {Felix J. Herrmann},
  title = {Overview research at the {SINBAD Consortium}},
  year = {2016},
  month = {03},
  number = {TR-EOAS-2016-1},
  institution = {UBC},
  keywords = {presentation, SLIM, private},
  note = {Presented at a seminar at Schlumberger Gould, Cambridge on March 17, 2016.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/TechReport/2016/SLB/herrmann2016SLBors/herrmann2016SLBors.pdf}
}


%-----2015-----%

@TECHREPORT{kumar2015EAGElse,
  author = {Rajiv Kumar and Ning Tu and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Least-squares extended imaging with surface-related multiples},
  year = {2015},
  month = {01},
  number = {TR-EOAS-2015-1},
  institution = {UBC},
  abstract = {Common image gathers are used in building velocity
                  models, inverting for anisotropy parameters, and
                  analyzing reservoir attributes. Typically, only
                  primary reflections are used to form image gathers
                  as multiples can cause artifacts that interfere with
                  the events of interest. However, it has been shown
                  that multiples can actually provide extra
                  illumination of the subsurface, especially for
                  delineating the near-surface features. In this
                  paper, we aim to form common image gathers directly
                  from the data with surface related multiples by
                  applying concepts that have been used to
                  successfully deal with surface-related multiples in
                  imaging. We achieve this by effectively inverting an
                  extended migration operator. This results in
                  extended images with better near-surface
                  illumination that are free of artifacts that can
                  hamper velocity analysis. In addition, being able to
                  generate extended images directly from the total
                  data avoids the need for (time-consuming)
                  pre-processing. Synthetic examples on a layered
                  model show that the proposed formulation is
                  promising.},
  keywords = {surface-related multiples, image gathers},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2015/kumar2015EAGElse/kumar2015EAGElse.html}
}


@TECHREPORT{kumar2015EAGEtjm,
  author = {Rajiv Kumar and Haneet Wason and Felix J. Herrmann},
  title = {Time-jittered marine acquisition: low-rank v/s sparsity},
  year = {2015},
  month = {01},
  number = {TR-EOAS-2015-2},
  institution = {UBC},
  abstract = {Time-jittered simultaneous marine acquisition has been
                  recognized as an economic way of improving the
                  spatial sampling, and speedup acquisition, where a
                  single (or multiple) source vessel fires at --
                  jittered source locations and instances in time. It
                  has been shown in the past that this problem can be
                  setup as a -- compressed sensing problem, where
                  conventional seismic data is reconstructed from
                  blended data via a sparsity-promoting optimization
                  formulation. While the recovery quality of deblended
                  data is very good, the recovery process is
                  computationally very expensive. In this paper, we
                  present a computationally efficient
                  rank-minimization algorithm to deblend the seismic
                  data. The proposed algorithm is suitable for
                  large-scale seismic data, since it avoids SVD
                  computations and uses a low-rank factorized
                  formulation instead. Results are illustrated with
                  simulations of time-jittered marine acquisition,
                  which translates to jittered source locations for a
                  given speed of the source vessel, for a single
                  source vessel with two airgun arrays.},
  keywords = {deblending, low-rank},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2015/kumar2015EAGEtjm/kumar2015EAGEtjm.html}
}


@TECHREPORT{lago2015EAGEtrg,
  author = {Rafael Lago and Felix J. Herrmann},
  title = {Towards a robust geometric multigrid scheme for {Helmholtz} equation},
  year = {2015},
  month = {01},
  number = {TR-EOAS-2015-3},
  institution = {UBC},
  abstract = {We discuss an improvement of existing multigrid
                  techniques for the solution of the time harmonic
                  wave equation targeting application to seismic
                  inversion and imaging, using non-traditional
                  smoothing and coarse correction techniques, namely
                  the CGMN and CRMN methods. We aim at developing a
                  multigrid scheme to be used as a preconditioner for
                  FGMRES showing less sensibility to changes in the
                  discretization of the operator. We compare this
                  multigrid scheme with recent developments in the
                  multigrid field obtaining very satisfactory
                  results. Our numerical experiments using SEG/EAGE
                  Overthrust velocity model showing not only more
                  robustness when switching from a basic 7 points
                  stencil to a more compact 27 points stencil, but
                  also a considerable reduction in the number of
                  preconditioning steps required to attain
                  convergence, a result encouraging further
                  investigation.},
  keywords = {Helmholtz, multigrid, CGMN, CRMN},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2015/lago2015EAGEtrg/lago2015EAGEtrg.pdf}
}


@TECHREPORT{peters2015EAGErwi,
  author = {Bas Peters and Brendan Smithyman and Felix J. Herrmann},
  title = {Regularizing waveform inversion by projection onto intersections of convex sets},
  year = {2015},
  month = {01},
  number = {TR-EOAS-2015-4},
  institution = {UBC},
  abstract = {A framework is proposed for regularizing the waveform
                  inversion problem by projections onto intersections
                  of convex sets. Multiple pieces of prior information
                  about the geology are represented by multiple convex
                  sets, for example limits on the velocity or minimum
                  smoothness conditions on the model. The data-misfit
                  is then minimized, such that the estimated model is
                  always in the intersection of the convex
                  sets. Therefore, it is clear what properties the
                  estimated model will have at each iteration. This
                  approach does not require any quadratic penalties to
                  be used and thus avoids the known problems and
                  limitations of those types of penalties. It is shown
                  that by formulating waveform inversion as a
                  constrained problem, regularization ideas such as
                  Tikhonov regularization and gradient filtering can
                  be incorporated into one framework. The algorithm is
                  generally applicable, in the sense that it works
                  with any (differentiable) objective function,
                  several gradient and quasi-Newton based solvers and
                  does not require significant additional
                  computation. The method is demonstrated on the
                  inversion of very noisy synthetic data and vertical
                  seismic profiling field data.},
  keywords = {waveform inversion, regularization, convex sets},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2015/peters2015EAGErwi/peters2015EAGErwi.html}
}


@TECHREPORT{esser2015tvwri,
  author = {Ernie Esser and Lluís Guasch and Tristan van Leeuwen and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Total variation regularization strategies in full waveform inversion for improving robustness to noise, limited data and poor initializations},
  year = {2015},
  month = {06},
  number = {TR-EOAS-2015-5},
  institution = {UBC},
  abstract = {We propose an extended full waveform inversion
                  formulation that includes convex constraints on the
                  model. In particular, we show how to simultaneously
                  constrain the total variation of the slowness
                  squared while enforcing bound constraints to keep it
                  within a physically realistic range. Synthetic
                  experiments show that including total variation
                  regularization can improve the recovery of a high
                  velocity perturbation to a smooth background model,
                  removing artifacts caused by noise and limited data.
                  Total variation-like constraints can make the
                  inversion results significantly more robust to a
                  poor initial model, leading to reasonable results in
                  some cases where unconstrained variants of the
                  method completely fail. Numerical results are
                  presented for portions of the SEG/EAGE salt model
                  and the 2004 BP velocity
                  benchmark. ***Disclaimer.*** *This technical report
                  is ongoing work (and posted as is except for the
                  addition of another author) of the late John "Ernie"
                  Esser (May 19, 1980 - March 8, 2015), who passed
                  away under tragic circumstances. We will work hard
                  to finalize and submit this work to the peer-review
                  literature.* Felix J. Herrmann},
  keywords = {Wavefield Reconstruction Inversion, total-variation, hinge loss, cones constraints},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2015/esser2015tvwri/esser2015tvwri.html}
}


@TECHREPORT{witte2015TRoam,
  author = {Philipp A. Witte and Mathias Louboutin and Felix J. Herrmann},
  title = {Overview on anisotropic modeling and inversion},
  year = {2015},
  month = {08},
  number = {TR-EOAS-2015-6},
  institution = {UBC},
  abstract = {This note provides an overview on strategies for
                  modeling and inversion with the anisotropic wave
                  equation. Since linear and non-linear inversion
                  methods like least squares RTM and Full Waveform
                  Inversion depend on matching observed field data
                  with synthetically modelled data, accounting for
                  anisotropy effects is necessary in order to
                  accurately match waveforms at long offsets and
                  propagation times. In this note, the two main
                  strategies for anisotropic modelling by solving
                  either a pseudo acoustic wave equation or a pure
                  quasi-P-wave equation are discussed and an inversion
                  workflow using the pure quasi-P-wave equation is
                  provided. In particular, we derive the exact adjoint
                  of the anisotropic forward modelling and jacobian
                  operator and give a detailled describtion of their
                  implementation. The anistropic FWI workflow is
                  tested on a sythetic data example.},
  keywords = {full waveform inversion, anisotropy, modeling, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/TechReport/2015/witte2015TRoam/witte2015TRoam.html}
}


@TECHREPORT{peters2015SEGrwi,
  author = {Bas Peters and Zhilong Fang and Brendan Smithyman and Felix J. Herrmann},
  title = {Regularizing waveform inversion by projections onto convex sets --- application to the {2D} {Chevron} 2014 synthetic blind-test dataset},
  year = {2015},
  month = {06},
  number = {TR-EOAS-2015-7},
  institution = {UBC},
  abstract = {A framework is proposed for regularizing the waveform
                  inversion problem by projections onto intersections
                  of convex sets. Multiple pieces of prior information
                  about the geology are represented by multiple convex
                  sets, for example limits on the velocity or minimum
                  smoothness conditions on the model. The data-misfit
                  is then minimized, such that the estimated model is
                  always in the intersection of the convex
                  sets. Therefore, it is clear what properties the
                  estimated model will have at each iteration. This
                  approach does not require any quadratic penalties to
                  be used and thus avoids the known problems and
                  limitations of those types of penalties. It is shown
                  that by formulating waveform inversion as a
                  constrained problem, regularization ideas such as
                  Tikhonov regularization and gradient filtering can
                  be incorporated into one framework. The algorithm is
                  generally applicable, in the sense that it works
                  with any (differentiable) objective function and
                  does not require significant additional
                  computation. The method is demonstrated on the
                  inversion of the 2D marine isotropic elastic
                  synthetic seismic benchmark by Chevron using an
                  acoustic modeling code. To highlight the effect of
                  the projections, we apply no data pre-processing.},
  keywords = {SEG, Waveform inversion, regularization, projection, blind-test, Wavefield Reconstruction Inversion},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2015/peters2015SEGrwi/peters2015SEGrwi.html}
}


%-----2014-----%

@TECHREPORT{esser2014sln,
  author = {Ernie Esser},
  title = {Some lifting notes},
  year = {2014},
  month = {02},
  number = {TR-EOAS-2014-1},
  institution = {UBC},
  keywords = {lifting, nonconvex quadratic problems, convex semidefinite programming, low rank},
  note = {written on February 15, 2014},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2014/esser2014sln/esser2014sln.pdf}
}


@TECHREPORT{herrmann2014pmpde,
  author = {Felix J. Herrmann and Tristan van Leeuwen},
  title = {A penalty method for {PDE}-constrained optimization},
  institution = {UBC},
  year = {2014},
  month = {10},
  day = {30},
  number = {WO 2014/172787},
  type = {Patent},
  yearfiled = {2014},
  monthfiled = {04},
  dayfiled = {23},
  abstract = {The invention is directed to a computer-implemented
                  method for obtaining a physical model having
                  physical model parameters wherein solutions to one
                  or more partial-differential-equations (PDE's) are
                  calculated ans wherein (i) an appropriate initial
                  model is selected, (ii) setup a system of equations
                  referred to as the data-augmented PDE for the field,
                  comprising of the discretized PDE, the sampling
                  operator, the source function and the observed data,
                  and solve the data-augmented PDE in a suitable
                  manner to obtain a field that both satisfies the PDE
                  and fits the data to some degree, (iii) setup a
                  system of equations by using the PDE, the source
                  function and the field obtained in step (ii) and
                  solve this system of equations in a suitable manner
                  to obtain an update of the physical model parameters
                  and repeat steps (ii)-(iii) until a predetermined
                  stopping criterion is met.},
  keywords = {penalty method, optimization, patent},
  note = {(International publication date 30 October 2014. International publication number WO 2014/172787.)},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Patents/2014/herrmann2014pmpde/herrmann2014pmpde_WO2014172787.pdf},
  url2 = {http://patentscope.wipo.int/search/en/WO2014172787}
}


@TECHREPORT{slim2014NSERCapp,
  author = {Felix J. Herrmann},
  title = {{NSERC} 2014 {DNOISE} application},
  year = {2014},
  number = {TR-EOAS-2014-7},
  institution = {UBC},
  abstract = {This current proposal describes a comprehensive
                  five-year continuation of our research project in
                  dynamic nonlinear optimization for imaging in
                  seismic exploration (DNOISE). DNOISE III—Exploration
                  Seismology in the Petascale Age builds on the proven
                  track record of our multidisciplinary research team
                  that conducts transformative research in the fields
                  of seismic-data acquisition, processing, and
                  wave-equation based inversion. The overarching goals
                  of the DNOISE series of projects can be simply
                  summarized as: ``How to image more deeply and with
                  more detail?" and ``How to do more with less data?"
                  Also, to help overcome the current substantial
                  challenges in the oil and gas industry, we maintain
                  this focus with more specific follow-up questions
                  such as: ``How can we control costs and remove
                  acquisition-related artifacts in 3D (time-lapse)
                  seismic data sets?" and ``How can we replace
                  conventional seismic data processing with
                  wave-equation based inversion, control computational
                  costs, assess uncertainties, extract reservoir
                  information and remove sensitivity to starting
                  models?" To answer these questions, we have
                  assembled an expanded cross-disciplinary research
                  team with backgrounds in scientific computing (SC),
                  machine learning (ML), compressive sensing (CS),
                  hardware design, and computational and observational
                  exploration seismology (ES). With this team, we will
                  continue to drive innovations in ES by utilizing our
                  unparalleled access to high-performance computing
                  (HPC), our expertise and experience in CS and
                  wave-equation based inversion (WEI) and our proven
                  abilities in incorporating our research findings
                  into practical scalable software of our inversion
                  solutions.},
  keywords = {NSERC, DNOISE, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/TechReport/NSERC/2014/DNOISEIII/crd.html}
}

@TECHREPORT{tschannen2014tdl,
  author = {Valentin Tschannen and Zhilong Fang and Felix J. Herrmann},
  title = {Time domain least squares migration and dimensionality reduction},
  institution = {UBC},
  year = {2014},
  month = {06},
  number = {TR-EOAS-2014-9},
  abstract = {Least-squares seismic migration (LSM) is a wave equation based linearized inversion problem relying on the minimization of a least-squares misfit function, with respect to the medium perturbation, between recorded and modeled wavefields. Today’s challenges in Hydrocarbon ex- ploration are to build high resolution images of more and more complicated geological reservoirs, which requires to handle very large systems of equations. The extreme size of the problem com- bined with the fact that it is ill-conditioned make LSM not yet feasible for industrial purposes. To overcome this "curse of dimensionality", dimension reduction and divide-and-conquer tech- niques that aim to decrease the computation time and the required memory, while conserving the image quality, have recently been developed. By borrowing ideas from stochastic optimiza- tion and compressive sensing, the imaging problem is reformulated as an L1-regularized, sparsity promoted LSM. The idea is to take advantage of the compressibility of the model perturbation in the curvelet domain and to work on series of smaller subproblems each involving a small ran- domized subset of data. We try two different subset sampling strategies, artificial randomized simultaneous sources experiments ("supershots") and drawing sequential shots firing at random source locations. These subsets are changed after each subproblem is solved. In both cases we obtain good migration results at significantly reduced computational cost. Application of these methods to a complicated synthetic model yields to encouraging results, underlining the usefulness of sparsity promotion and randomization in time stepping formulation.
},
  keywords = {Wave equation migration, sparsity promotion, compressive sensing, stochastic optimization},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2014/tschannen2014tdl/tschannen2014tdl.pdf}
}

@TECHREPORT{esser2014SEGsgp,
  author = {Ernie Esser and Tristan van Leeuwen and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {A scaled gradient projection method for total variation regularized full waveform inversion},
  year = {2014},
  month = {04},
  number = {TR-EOAS-2014-2},
  institution = {UBC},
  abstract = {We propose an extended full waveform inversion
                  formulation that includes convex constraints on the
                  model. In particular, we show how to simultaneously
                  constrain the total variation of the slowness
                  squared while enforcing bound constraints to keep it
                  within a physically realistic range. Synthetic
                  experiments show that including total variation
                  regularization can improve the recovery of a high
                  velocity perturbation to a smooth background model.},
  keywords = {full waveform inversion, convex constraints, total variation regularization},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2014/esser2014SEGsgp/esser2014SEGsgp.html}
}


@TECHREPORT{zfang2014SEGsqn,
  author = {Zhilong Fang and Felix J. Herrmann},
  title = {A stochastic quasi-Newton {McMC} method for uncertainty quantification of full-waveform inversion},
  year = {2014},
  month = {04},
  number = {TR-EOAS-2014-6},  
  institution = {UBC},
  abstract = {In this work we propose a stochastic quasi-Newton Markov
                  chain Monte Carlo (McMC) method to quantify the
                  uncertainty of full-waveform inversion (FWI). We
                  formulate the uncertainty quantification problem in
                  the framework of the Bayesian inference, which
                  formulates the posterior probability as the
                  conditional probability of the model given the
                  observed data. The Metropolis-Hasting algorithm is
                  used to generate samples satisfying the posterior
                  probability density function (pdf) to quantify the
                  uncertainty. However it suffers from the challenge
                  to construct a proposal distribution that
                  simultaneously provides a good representation of the
                  true posterior pdf and is easy to manipulate. To
                  address this challenge, we propose a stochastic
                  quasi-Newton McMC method, which relies on the fact
                  that the Hessian of the deterministic problem is
                  equivalent to the inverse of the covariance matrix
                  of the posterior pdf. The l-BFGS (limited-memory
                  Broyden–Fletcher–Goldfarb–Shanno) Hessian is used to
                  approximate the inverse of the covariance matrix
                  efficiently, and the randomized source sub-sampling
                  strategy is used to reduce the computational cost of
                  evaluating the posterior pdf and constructing the
                  l-BFGS Hessian. Numerical experiments show the
                  capability of this stochastic quasi-Newton McMC
                  method to quantify the uncertainty of FWI with a
                  considerable low cost.},
  keywords = {FWI, uncertainty quantification, quasi-Newton, McMC},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2014/zfang2014SEGsqn/zfang2014SEGsqn.html}
}


@TECHREPORT{wang2014SEGfwi,
  author = {Rongrong Wang and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Full waveform inversion with interferometric measurements},
  year = {2014},
  month = {04},
  number = {TR-EOAS-2014-5},
  institution = {UBC},
  abstract = {In this note, we design new misfit functions for
                  full-waveform inversion by using interferometric
                  measurements to reduce sensitivity to phase
                  errors. Though established within a completely
                  different setting from the linear case, we obtain a
                  similar observation: the interferometry can improve
                  robustness under certain modeling errors. Moreover,
                  in order to deal with errors on both source and
                  receiver sides, we propose a higher order
                  interferometry, which, as a generalization of the
                  usual definition, involves the cross correlation of
                  four traces. A proof of principle simulations is
                  included on a stylized example.},
  keywords = {FWI},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2014/wang2014SEGfwi/wang2014SEGfwi.html}
}


@TECHREPORT{kumar2014SEGmcu,
  author = {Rajiv Kumar and Oscar Lopez and Ernie Esser and Felix J. Herrmann},
  title = {Matrix completion on unstructured grids : 2-D seismic data regularization and interpolation},
  year = {2014},
  month = {04},
  number = {TR-EOAS-2014-3},
  institution = {UBC},
  abstract = {Seismic data interpolation via rank-minimization
                  techniques has been recently introduced in the
                  seismic community. All the existing
                  rank-minimization techniques assume the recording
                  locations to be on a regular grid, e.g. sampled
                  periodically, but seismic data are typically
                  irregularly sampled along spatial axes. Other than
                  the irregularity of the sampled grid, we often have
                  missing data. In this paper, we study the effect of
                  grid irregularity to conduct matrix completion on a
                  regular grid for unstructured data. We propose an
                  improvement of existing rank-minimization techniques
                  to do regularization. We also demonstrate that we
                  can perform seismic data regularization and
                  interpolation simultaneously. We illustrate the
                  advantages of the modification using a real seismic
                  line from the Gulf of Suez to obtain high quality
                  results for regularization and interpolation, a key
                  application in exploration geophysics.},
  keywords = {regularization, interpolation, matrix completion, NFFT},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2014/kumar2014SEGmcu/kumar2014SEGmcu.html}
}


@TECHREPORT{smithyman2014SEGjfw,
  author = {Brendan R. Smithyman and Bas Peters and Bryan DeVault and Felix J. Herrmann},
  title = {Joint full-waveform inversion of on-land surface and {VSP} data from the {Permian} {Basin}},
  year = {2014},
  month = {04},
  number = {TR-EOAS-2014-4},
  institution = {UBC},
  abstract = {Full-waveform Inversion is applied to generate a
                  high-resolution model of P-wave velocity for a site
                  in the Permian Basin, Texas, USA. This investigation
                  jointly inverts seismic waveforms from a surface 3-D
                  vibroseis surface seismic survey and a co-located
                  3-D Vertical Seismic Profiling (VSP) survey, which
                  shared common source Vibration Points (VPs). The
                  resulting velocity model captures features that were
                  not resolvable by conventional migration velocity
                  analysis.},
  keywords = {full-waveform inversion, seismic, land, vibroseis, downhole receivers},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2014/smithyman2014SEGjfw/smithyman2014SEGjfw.html}
}


@TECHREPORT{slim2014NSERCpr,
  author = {Felix J. Herrmann},
  title = {{NSERC} 2014 {DNOISE} progress report},
  year = {2014},
  number = {TR-EOAS-2014-8},
  institution = {UBC},
  abstract = {As we entered the second half of the DNOISE II project,
                  we are happy to report that we have made significant
                  progress on several fronts. Firstly, our work on
                  seismic data acquisition with compressive sensing is
                  becoming widely recognized. For instance,
                  ConocoPhilips ran a highly successful field trial on
                  Marine acquisition with compressive sensing and
                  obtained significant improvements compared to
                  standard production (see figure below). Moreover,
                  one of the main outcomes of this year’s EAGE
                  workshop was that industry is ready to adapt
                  randomized sampling as a new acquisition
                  paradigm. Needless to say this is a big success for
                  what we have been trying to accomplish with DNOISE
                  II. Finally, we have made a breakthrough in the
                  application of randomized sampling in 4-D seismic,
                  which is receiving a lot of interest from
                  industry. Secondly, our work on large-scale
                  optimization in the context of wave-equation based
                  inversion is also increasingly widely adapted. For
                  instance, our batching techniques are making the
                  difference between making a loss or profit for a
                  large contractor company active in the area of
                  full-waveform inversion. We also continued to make
                  progress in exciting new directions that go beyond
                  sparsity promotion and which allow us to exploit
                  other types of structure within the data, such as
                  low-rank for matrices or hierarchical Tucker formats
                  for tensors. Application of these techniques show
                  excellent results and in certain cases, such as
                  source separation problems with small dithering,
                  show significant improvements over transform-domain
                  methods. Thirdly, we continued to make significant
                  progress in wave-equation based inversion. We
                  extended our new penalty-based formulation now
                  called Wavefield Reconstruction Inversion/Imaging to
                  include total-variation regularization and density
                  variations. We also continued to make progress on
                  multiples, imaging with multiples and 3-D
                  full-waveform inversion. Statoil is the latest
                  company to join and we have several other companies
                  that have shown a keen interest. We also received
                  substantial in-kind contributions including a
                  license to WesternGeco’s iOmega and HPC equipment
                  discounts. After many years of support BP decided
                  unfortunately to no longer support SINBAD quoting
                  financial headwind related to the Deep horizon
                  disaster. On a more positive note, we are extremely
                  happy to report major progress on our efforts to
                  secure access to high-performance compute, including
                  renewed funding from NSERC and our involvement in
                  the International Inversion Initiative in Brazil. 9
                  peer-reviewed journal publications have resulted
                  from our work within the reporting period, with a
                  further 6 submitted, and DNOISE members disseminated
                  the results of our research at 49 major national and
                  international conference presentations. On the HQP
                  training side, 4 MSc students have recently
                  graduated, with one obtaining a position with CGG
                  Calgary, and we added 4 postdocs and 3 PhD students
                  to our team in September 2014, greatly increasing
                  our research capacity. As can be seen from the
                  report below, we are well on schedule and on certain
                  topics well beyond the milestones included in the
                  original proposal. With the purchase of the new
                  cluster we expect to see a surge of activity in
                  extending our algorithms to 3D. With this increased
                  capacity, we continue to be in an excellent position
                  to make fundamental contributions to the fields of
                  seismic data acquisition, processing, and
                  wave-equation based inversion. In the sections
                  below, we give a detailed overview of the research
                  and publication activities of the different members
                  of the group and how these relate to the objectives
                  of the grant, to industrial uptake, and to
                  outreach. Unless stated otherwise the students and
                  PDFs are (co)-supervised by the PI. We refer to the
                  publications section 4.0 for a complete list of our
                  presentations, conference proceedings, and journal
                  publications. We also refer to our mindmap, which
                  clearly establishes connections between the
                  different research topics we have embarked upon as
                  part of the DNOISE II project.},
  keywords = {NSERC, DNOISE, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/TechReport/NSERC/2014/Progress_Report_2014.html}
}


%-----2013-----%

@TECHREPORT{li2013EAGEwebmplijsp,
  author = {Xiang Li and Felix J. Herrmann},
  title = {Wave-equation based multi-parameter linearized inversion with joint-sparsity promotion},
  year = {2013},
  number = {TR-EOAS-2013-1},  
  institution = {UBC},
  abstract = {The successful application of linearized inversion is
                  affected by the prohibitive size of the data,
                  computational resources required, and how accurately
                  the model parameters reflects the real Earth
                  properties. The issue of data size and computational
                  resources can be addressed by combining ideas from
                  sparsity promoting and stochastic optimization,
                  which can allow us to invert model perturbation with
                  a small subset of the data, yielding a few PDE
                  solves for the inversion. In this abstract, we are
                  aiming at addressing the issue of accuracy of model
                  parameters by inverting density and velocity
                  simultaneously rather than only using velocity. As a
                  matter of face, the effects of density and velocity
                  variations towards the wavefield are very similar,
                  which will cause energy leakage between density and
                  velocity images. To overcome this issue, we proposed
                  a incoherence enhanced method that can reduce the
                  similarity between the effect of density and
                  velocity. Moreover, the location of structural
                  variations in velocity and density are often
                  overlapped in geological setting, thus in this
                  abstract, we also exploit this property with
                  joint-sparsity promoting to further improve the
                  imaging result.},
  keywords = {linearized inversion, incoherence enhancement, joint-sparsity},
  month = {01},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2013/li2013EAGEwebmplijsp/li2013EAGEwebmplijsp.pdf}
}


@TECHREPORT{kumar2013ICMLlr,
  author = {Aleksandr Y. Aravkin and Rajiv Kumar and Hassan Mansour and Ben Recht and Felix J. Herrmann},
  title = {An {SVD}-free {Pareto} curve approach to rank minimization},
  year = {2013},
  number = {TR-EOAS-2013-2},  
  institution = {UBC},
  abstract = {Recent SVD-free matrix factorization formulations have
                  enabled rank optimization for extremely large-scale
                  systems (millions of rows and columns). In this
                  paper, we consider rank-regularized formulations
                  that only require a target data-fitting error level,
                  and propose an algorithm for the corresponding
                  problem. We illustrate the advantages of the new
                  approach using the Netflix problem, and use it to
                  obtain high quality results for seismic trace
                  interpolation, a key application in exploration
                  geophysics. We show that factor rank can be easily
                  adjusted as the inversion proceeds, and propose a
                  weighted extension that allows known subspace
                  information to improve the results of matrix
                  completion formulations. Using these methods, we
                  obtain high-quality reconstructions for large scale
                  seismic interpolation problems with real data.},
  keywords = {interpolation, low rank},
  month = {02},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2013/kumar2013ICMLlr/kumar2013ICMLlr.pdf}
}


@TECHREPORT{oghenekohwo2013SEGtlswrs,
  author = {Felix Oghenekohwo and Felix J. Herrmann},
  title = {Time-lapse seismics with randomized sampling},
  year = {2013},
  number = {TR-EOAS-2013-3},  
  institution = {UBC},
  abstract = {In time-lapse or 4D seismics, repeatability of the
                  acquisition is a very crucial step, as we do not
                  want spurious events that are not there. In this
                  paper, we propose an approach which avoids any
                  requirement to repeat the surveys, by using
                  randomized sampling technique which allows us to be
                  more efficient in the acquisition. Our method
                  applies to sampling data using ocean bottom nodes
                  (OBN) as receivers. We test the efficacy of our
                  proposed randomized acquisition geometry for
                  time-lapse survey on two different models. In the
                  first example, model properties does not change with
                  time, while in the second example, model exhibit a
                  time-lapse effect which may be caused by the
                  migration of fluid within the reservoir. We perform
                  two types of randomized sampling - uniform
                  randomized sampling and jittered sampling to
                  visualize the effects of non-repeatability in
                  time-lapse survey. We observe that jittered
                  randomized sampling is a more efficient method
                  compared to randomized sampling, due to it's
                  requirement to control the maximum spacing between
                  the receivers. The results are presented, in the
                  image space, as a least-squares migration of the
                  model perturbation and they are shown for a subset
                  of a synthetic model - the Marmousi model},
  keywords = {acquisition, time-lapse, migration},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2013/oghenekohwo2013SEGtlswrs/oghenekohwo2013SEGtlswrs.pdf}
}


@TECHREPORT{petrenko2013SEGsaoc,
  author = {Art Petrenko and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Software acceleration of {CARP}, an iterative linear solver and preconditioner},
  year = {2013},
  number = {TR-EOAS-2013-4},  
  institution = {UBC},
  abstract = {We present the results of software optimization of a
                  row-wise preconditioner (Component Averaged Row
                  Projections) for the method of conjugate gradients,
                  which is used to solve the diagonally banded
                  Helmholtz system representing frequency domain,
                  isotropic acoustic seismic wave simulation. We
                  demonstrate that in our application, a
                  preconditioner bound to one processor core and
                  accessing memory contiguously reduces execution time
                  by 7\% for matrices having on the order of 108
                  non-zeros. For reference we note that our C
                  implementation is over 80 times faster than the
                  corresponding code written for a high-level
                  numerical analysis language.},
  keywords = {Helmholtz equation, Kaczmarz, software, wave propagation, frequency-domain},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2013/petrenko2013SEGsaoc/petrenko2013SEGsaoc.pdf}
}


@TECHREPORT{slim2013NSERCpr,
  author = {Felix J. Herrmann},
  title = {{NSERC} 2013 {DNOISE} progress report},
  year = {2013},
  number = {TR-EOAS-2013-5},  
  institution = {UBC},
  abstract = {As we enter the second half of the DNOISE II project, we
                  are happy to report that we have made significant
                  progress on several fronts. Firstly, our work on
                  seismic data acquisition with compressive sensing is
                  becoming widely recognized, reflected in adaptations
                  of this technology by industry and in this year’s
                  SEG Karcher award, which went to Gilles Hennenfent,
                  who was one of the researchers who started working
                  in this area in our group. As this report shows, we
                  continued to make progress on this topic with
                  numerous presentations, publications, and software
                  releases. Secondly, our work on large-scale
                  optimization is also widely adapted and instrumental
                  to the different research areas on the grant. In
                  particular, we are excited about new directions that
                  go beyond sparsity promotion and which allow us to
                  exploit other types of structure within the data,
                  such as low-rank. Over the near future, we expect to
                  see a body of new research based on these findings
                  touching acquisition as well as the wave-equation
                  based inversion aspects of our research
                  program. Thirdly, we are also very happy to report
                  that we continued to make substantial progress in
                  wave-equation base inversion. In particular, we
                  would like to mention successes in the areas of
                  acceleration of sparsity-promoting imaging with
                  source estimation and multiples and in theoretical
                  as well as practical aspects of full-waveform
                  inversion. We derived a highly practical and
                  economic formulation of 3-D FWI and we also came up
                  with a complete new formulation of FWI, which
                  mitigates issues related to cycle skipping. Finally,
                  we made a lot of progress applying our algorithm to
                  industrial datasets, which has been well received by
                  industry. Our findings show that FWI is still an
                  immature technology calling for more theoretical
                  input and for the development of practical
                  workflows. Over the last year our work cumulated in
                  14 peer-reviewed journal publications, 5 submitted
                  journal publications, 13 (+ 9) extended abstracts,
                  32 talks at international conferences, and 6
                  software packages. Finally, we are happy to report
                  that we have been joined by several new companies,
                  namely, ION Geophysical, CGG, and Woodside. At this
                  midpoint of the Grant, we are also happy to report
                  that we are well on schedule to meet the milestones
                  included in the original proposal. Given our wide
                  range of expertise and our plans to replace our
                  compute cluster, we continue to be in an excellent
                  position to make fundamental contributions to the
                  fields of seismic data acquisition, processing, and
                  wave-equation based inversion.},
  keywords = {NSERC, DNOISE, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/TechReport/NSERC/2013/Progress_Report_2013.pdf}
}


@TECHREPORT{vanLeeuwen2013Penalty2,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {A penalty method for {PDE}-constrained optimization},
  year = {2013},
  number = {TR-EOAS-2013-6},  
  institution = {UBC},
  abstract = {We present a method for solving PDE constrained
                  optimization problems based on a penalty
                  formulation. This method aims to combine advantages
                  of both full-space and reduced methods by exploiting
                  a large search-space (consisting of both control and
                  state variables) while allowing for an efficient
                  implementation that avoids storing and updating the
                  state-variables. This leads to a method that has
                  roughly the same per-iteration complexity as
                  conventional reduced approaches while dening an
                  objective that is less non-linear in the control
                  variable by implicitly relaxing the constraint. We
                  apply the method to a seismic inverse problem where
                  it leads to a particularly ecient implementation
                  when compared to a conventional reduced approach as
                  it avoids the use of adjoint
                  state-variables. Numerical examples illustrate the
                  approach and suggest that the proposed formulation
                  can indeed mitigate some of the well-known problems
                  with local minima in the seismic inverse problem.},
  keywords = {waveform inversion, optimization, private},
  month = {04},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/TechReport/2013/vanLeeuwen2013Penalty2/vanLeeuwen2013Penalty2.pdf}
}


%-----2012-----%

@TECHREPORT{rajiv2012SEGFRM,
  author = {Rajiv Kumar and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Fast methods for rank minimization with applications to seismic-data interpolation},
  institution = {Department of Earth and Ocean Sciences},
  year = {2012},
  number = {TR-EOAS-2012-3},
  address = {University of British Columbia, Vancouver},
  abstract = {Rank penalizing techniques are an important direction in
                  seismic inverse problems, since they allow improved
                  recovery by exploiting low-rank structure. A major
                  downside of current state of the art techniques is
                  their reliance on the SVD of seismic data
                  structures, which can be prohibitively
                  expensive. Fortunately, recent work allows us to
                  circumvent this problem by working with matrix
                  factorizations. We review a novel approach to rank
                  penalization, and successfully apply it to the
                  seismic interpolation problem by exploiting the
                  low-rank structure of seismic data in the
                  midpoint-offset domain. Experiments for the recovery
                  of 2D monochromatic data matrices and seismic lines
                  represented as 3D volumes support the feasibility
                  and potential of the new approach.},
  keywords = {rank, optimization, seismic data interpolation},
  month = {04},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2012/rajiv2012SEGFRM/rajiv2012SEGFRM.pdf}
}


@TECHREPORT{slim2012NSERCpr,
  author = {Felix J. Herrmann},
  title = {{NSERC} 2012 {DNOISE} progress report},
  year = {2012},
  number = {TR-EOAS-2012-4},
  institution = {UBC},
  keywords = {NSERC, DNOISE, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/TechReport/NSERC/2012/Progress_Report_2012.pdf}
}


@TECHREPORT{vanleeuwen2012CGMN,
  author = {Tristan van Leeuwen},
  title = {Fourier analysis of the {CGMN} method for solving the {Helmholtz} equation},
  year = {2012},
  number = {TR-EOAS-2012-1},
  institution = {Department of Earth, Ocean and Atmospheric Sciences},
  address = {The University of British Columbia, Vancouver},
  abstract = {The Helmholtz equation arises in many applications, such
                  as seismic and medical imaging. These application
                  are characterized by the need to propagate many
                  wavelengths through an inhomogeneous medium. The
                  typical size of the problems in 3D applications
                  precludes the use of direct factorization to solve
                  the equation and hence iterative methods are used in
                  practice. For higher wavenumbers, the system becomes
                  increasingly indefinite and thus good
                  preconditioners need to be constructed. In this note
                  we consider an accelerated Kazcmarz method (CGMN)
                  and present an expression for the resulting
                  iteration matrix. This iteration matrix can be used
                  to analyze the convergence of the CGMN method. In
                  particular, we present a Fourier analysis for the
                  method applied to the 1D Helmholtz equation. This
                  analysis suggests an optimal choice of the
                  relaxation parameter. Finally, we present some
                  numerical experiments.},
  keywords = {Helmholtz equation, modelling},
  url = {http://arxiv.org/abs/1210.2644},
}


@TECHREPORT{vanleeuwen2012SEGparallel,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {A parallel, object-oriented framework for frequency-domain wavefield imaging and inversion.},
  institution = {Department of Earth and Ocean Sciences},
  year = {2012},
  number = {TR-EOAS-2012-2},
  address = {University of British Columbia, Vancouver},
  abstract = {We present a parallel object-oriented matrix-free
                  framework for frequency-domain seismic modeling,
                  imaging and inversion. The key aspects of the
                  framework are its modularity and level of
                  abstraction, which allows us to write code that
                  reflects the underlying mathematical structure and
                  develop unit-tests that guarantee the fidelity of
                  the code. By overloading standard linear-algebra
                  operations, such as matrix-vector multiplications,
                  we can use standard optimization packages to work
                  with our code without any modification. This leads
                  to a scalable testbed on which new methods can be
                  rapidly prototyped and tested on medium-sized 2D
                  problems. Although our current implementation uses
                  (parallel) Matlab, all of these design principles
                  can also be met by using lower-level languages which
                  is important when we want to scale to realistic 3D
                  problems. We present some numerical examples on
                  synthetic data.},
  keywords = {modeling, imaging, inversion, SEG},
  month = {04},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2012/vanleeuwen2012SEGparallel/vanleeuwen2012SEGparallel.pdf}
}

  
@TECHREPORT{vanleeuwen2012smii,
  author = {Tristan van Leeuwen},
  title = {A parallel matrix-free framework for frequency-domain seismic modelling, imaging and inversion in Matlab},
  year = {2012},
  number = {TR-EOAS-2012-5},
  abstract = {I present a parallel matrix-free framework for
                  frequency-domain seismic modeling, imaging and
                  inversion. The framework provides basic building
                  blocks for designing and testing optimization-based
                  formulations of both linear and non-linear seismic
                  in- verse problems. By overloading standard
                  linear-algebra operations, such as matrix-vector
                  multiplications, standard optimization packages can
                  be used to work with the code without any
                  modification. This leads to a scalable testbed on
                  which new methods can be rapidly prototyped and
                  tested on medium-sized 2D problems. I present some
                  numerical examples on both linear and non-linear
                  seismic inverse problems.},
  keywords = {seismic imaging, optimization, Matlab, object-oriented programming},
  month = {07},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2012/vanleeuwen2012smii/vanleeuwen2012smii.pdf}
}


%-----2011-----%

@TECHREPORT{slim2011NSERCpr,
  author = {Felix J. Herrmann},
  title = {{NSERC} 2011 {DNOISE} progress report},
  year = {2011},
  number = {TR-EOAS-2011-1},
  institution = {UBC},
  abstract = {The main thrust of the DNOISE project is focused on the
                  following researth themes: [1] seismic acquisition
                  design and recovery from incomplete data with the
                  goal to reduce acquisition costs while increasing
                  the spatial bandwidth and aperture of seismic data;
                  [2] Removal of the 'surface nonlinearity' by
                  simultaneous estimation of the source signature and
                  the surface-free Green's function by inverting the
                  surface-related multiple prediction operator; [3]
                  Reduction of the computational complexity of
                  full-waveform inversion (FWI) by randomized
                  dimensionality reduction; [4] "Convexification" of
                  FWI to remove or at least diminish the adverse
                  effects of non-uniqueness that has plagued FWI since
                  its inception; The first three themes are directed
                  towards removing major impediments faced by FWI
                  related to the costs of acquiring data, the
                  computational costs of processing and inverting
                  data, and to issues with source calibration and
                  surface-related multiples. The final theme is more
                  'blue sky' and tries to incorporate ideas from
                  migration-velocity analysis into the formulation of
                  full-waveform inversion. Aside from these themes, we
                  will continue to work on seismic data acquisition
                  schemes that favor sparsity-promoting recovery and
                  on the development of large-scale solvers using
                  recent developments in convex and stochastic
                  optimization.},
  keywords = {NSERC, DNOISE, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/TechReport/NSERC/2011/nserc-2011-dnoise-progress-report.pdf}
}


%-----2010-----%

@TECHREPORT{almatar10SEGesfd,
  author = {Mufeed H. AlMatar and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Estimation of surface-free data by curvelet-domain matched filtering and sparse inversion},
  institution = {Department of Earth and Ocean Sciences},
  year = {2010},
  number = {TR-EOAS-2010-1},
  address = {University of British Columbia, Vancouver},
  abstract = {Matching seismic wavefields and images lies at the heart
                  of many pre-post-processing steps part of seismic
                  imaging- whether one is matching predicted wavefield
                  components, such as multiples, to the actual
                  to-be-separated wavefield components present in the
                  data or whether one is aiming to restore migration
                  amplitudes by scaling, using an image-to-remigrated-
                  image matching procedure to calculate the scaling
                  coefficients. The success of these wavefield
                  matching procedures depends on our ability to (i)
                  control possible overfitting, which may lead to
                  accidental removal of energy or to inaccurate
                  image-amplitude corrections, (ii) handle data or
                  images with nonunique dips, and (iii) apply
                  subsequent wavefield separations or migraton
                  amplitude corrections stably. In this paper, we show
                  that the curvelet transform allows us to address all
                  these issues by im- posing smoothness in phase
                  space, by using their capability to handle
                  conflicting dips, and by leveraging their ability to
                  represent seismic data and images sparsely. This
                  latter property renders curvelet-domain sparsity
                  promotion an effective prior.},
  keywords = {SEG},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/almatar10SEGesfd/almatar10SEGesfd.pdf}
}


@TECHREPORT{herrmann2010SEGerc,
  author = {Felix J. Herrmann},
  title = {Empirical recovery conditions for seismic sampling},
  institution = {Department of Earth and Ocean Sciences, UBC},
  year = {2010},
  number = {TR-EOAS-2010-2},
  abstract = {In this paper, we offer an alternative sampling method
                  leveraging recent insights from compressive sensing
                  towards seismic acquisition and processing for data
                  that are traditionally considered to be
                  undersampled. The main outcome of this approach is a
                  new technology where acquisition and processing
                  related costs are no longer determined by overly
                  stringent sampling criteria, such as Nyquist. At the
                  heart of our approach lies randomized incoherent
                  sampling that breaks subsampling related
                  interferences by turning them into harmless noise,
                  which we subsequently remove by promoting
                  transform-domain sparsity. Now, costs no longer grow
                  with resolution and dimensionality of the survey
                  area, but instead depend on transform-domain
                  sparsity only. Our contribution is twofold. First,
                  we demonstrate by means of carefully designed
                  numerical experiments that compressive sensing can
                  successfully be adapted to seismic
                  acquisition. Second, we show that accurate recovery
                  can be accomplished for compressively sampled data
                  volumes sizes that exceed the size of conventional
                  transform-domain data volumes by only a small
                  factor. Because compressive sensing combines
                  transformation and encoding by a single linear
                  encoding step, this technology is directly
                  applicable to acquisition and to dimensionality
                  reduction during processing. In either case,
                  sampling, storage, and processing costs scale with
                  transform-domain sparsity.},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/herrmann10SEGerc/herrmann10SEGerc.pdf}
}


@TECHREPORT{slim2010NSERCapp,
  author = {Felix J. Herrmann},
  title = {{NSERC} 2010 {DNOISE} application},
  year = {2010},
  number = {TR-EOAS-2010-3},
  institution = {UBC},
  abstract = {DNOISE II: Dynamic Nonlinear Optimization for Imaging in
                  Seismic Exploration is a multidisciplinary research
                  project that involves faculty from the Mathematics,
                  Computer Science, and Earth and Ocean Sciences
                  Departments at the University of British Columbia.
                  DNOISE II constitutes a transformative research
                  program towards a new paradigm in seismic
                  exploration where the acquisition- and
                  processing-related costs are no longer determined by
                  the survey area and discretization but by
                  transform-domain sparsity of the final result. In
                  this approach, we rid ourselves from the
                  confinements of conventional overly stringent
                  sampling criteria that call for regular sampling
                  with sequentiual sources at Nyquist rates. By
                  adapting the principles of compressive sensing,
                  DNOISE II promotes a ground-up formulation for
                  seismic imaging where adverse subsampling-related
                  artifacts are removed by intelligent
                  simultaneous-acquisition design and recovery by
                  transform-domain sparsity promotion. This
                  development---in conjunction with our track records
                  in sparse recovery and time-harmonic Helmholtz
                  solvers---puts us in an unique position to deliver
                  on fundamental breakthroughs in the development and
                  implementation of the next-generation of processing,
                  imaging, and full-waveform inversion solutions.},
  keywords = {NSERC, DNOISE, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/TechReport/NSERC/2010/nserc-2010-dnoise-application.pdf}
}


%-----2009-----%

@TECHREPORT{tang09TRdtr,
  author = {Gang Tang and Reza Shahidi and Jianwei Ma},
  title = {Design of two-dimensional randomized sampling schemes for curvelet-based sparsity-promoting seismic data recovery},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2009},
  number = {TR-EOAS-2009-3},
  abstract = {The tasks of sampling, compression and reconstruction
                  are very common and often necessary in seismic data
                  processing due to the large size of seismic
                  data. Curvelet-based Recovery by Sparsity-promoting
                  Inversion, motivated by the newly developed theory
                  of compressive sensing, is among the best recovery
                  strategies for seismic data. The incomplete data
                  input to this curvelet-based recovery is determined
                  by randomized sampling of the original complete
                  data. Unlike usual regular undersampling, randomized
                  sampling can convert aliases to easy-to-eliminate
                  noise, thus facilitating the process of
                  reconstruction of the complete data from the
                  incomplete data. Randomized sampling methods such as
                  jittered sampling have been developed in the past
                  that are suitable for curvelet-based recovery,
                  however most have only been applied to sampling in
                  one dimension. Considering that seismic datasets are
                  usually higher dimensional and extremely large, in
                  the present paper, we extend the 1D version of
                  jittered sampling to two dimensions, both with
                  underlying Cartesian and hexagonal grids. We also
                  study separable and non-separable two dimensional
                  jittered sampling, the former referring to the
                  Kronecker product of two one-dimensional jittered
                  samplings. These different categories of jittered
                  sampling are compared against one another in terms
                  of signal-to-noise ratio and visual quality, from
                  which we find that jittered hexagonal sampling is
                  better than jittered Cartesian sampling, while fully
                  non-separable jittered sampling is better than
                  separable sampling. Because in the image processing
                  and computer graphics literature, sampling patterns
                  with blue-noise spectra are found to be ideal to
                  avoid aliasing, we also introduce two other
                  randomized sampling methods, possessing sampling
                  spectra with beneficial blue noise characteristics,
                  Poisson Disk sampling and Farthest Point
                  sampling. We compare these methods, and apply the
                  introduced sampling methodologies to higher
                  dimensional curvelet-based reconstruction. These
                  sampling schemes are shown to lead to better results
                  from CRSI compared to the other more traditional
                  sampling protocols, e.g. regular subsampling.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Journals/2009/tang09TRdtr/tang09TRdtr.pdf}
}


%-----2008-----%

@TECHREPORT{hennenfent08TRori,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {One-norm regularized inversion: learning from the {Pareto} curve},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2008},
  number = {TR-EOAS-2008-5},
  abstract = {Geophysical inverse problems typically involve a trade
                  off between data misfit and some prior. Pareto
                  curves trace the optimal trade off between these two
                  competing aims. These curves are commonly used in
                  problems with two-norm priors where they are plotted
                  on a log-log scale and are known as L-curves. For
                  other priors, such as the sparsity-promoting one
                  norm, Pareto curves remain relatively
                  unexplored. First, we show how these curves provide
                  an objective criterion to gauge how robust one-norm
                  solvers are when they are limited by a maximum
                  number of matrix-vector products that they can
                  perform. Second, we use Pareto curves and their
                  properties to define and compute one-norm
                  compressibilities. We argue this notion is key to
                  understand one-norm regularized inversion. Third, we
                  illustrate the correlation between the one-norm
                  compressibility and the performance of Fourier and
                  curvelet reconstructions with sparsity promoting
                  inversion.},
  keywords = {SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2008/hennenfent08TRori/hennenfent08TRori.pdf}
}


@TECHREPORT{lebed08TRhgg,
  author = {Evgeniy Lebed and Felix J. Herrmann},
  title = {A hitchhiker's guide to the galaxy of transform-domain sparsification},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2008},
  number = {TR-EOAS-2008-4},
  abstract = {The ability to efficiently and sparsely represent
                  seismic data is becoming an increasingly important
                  problem in geophysics. Over the last decade many
                  transforms such as wavelets, curvelets, contourlets,
                  surfacelets, shearlets, and many other types of
                  'x-lets' have been developed to try to resolve this
                  issue. In this abstract we compare the properties of
                  four of these commonly used transforms, namely the
                  shift-invariant wavelets, complex wavelets,
                  curvelets and surfacelets. We also briefly explore
                  the performance of these transforms for the problem
                  of recovering seismic wavefields from incomplete
                  measurements.},
  keywords = {SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2008/lebed08TRhgg/lebed08TRhgg.pdf}
}


@TECHREPORT{vandenberg08gsv,
  author = {Ewout {van den Berg} and Mark Schmidt and Michael P. Friedlander and K. Murphy},
  title = {Group sparsity via linear-time projection},
  year = {2008},
  number = {TR-2008-09},
  month = {06},
  abstract = {We present an efficient spectral projected-gradient
                  algorithm for optimization subject to a group
                  one-norm constraint. Our approach is based on a
                  novel linear-time algorithm for Euclidean projection
                  onto the one- and group one-norm
                  constraints. Numerical experiments on large data
                  sets suggest that the proposed method is
                  substantially more efficient and scalable than
                  existing methods.},
  institution = {UBC - Department of Computer Science},
  keywords = {SLIM, optimization},
  url = {http://www.optimization-online.org/DB_FILE/2008/07/2056.pdf}
}


%-----2007-----%

@TECHREPORT{vandenberg07TRipr,
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  title = {In pursuit of a root},
  institution = {Department of Computer Science},
  year = {2007},
  month = {06},
  number = {TR-EOAS-2007-19},
  address = {University of British Columbia, Vancouver},
  abstract = {The basis pursuit technique is used to find a minimum
                  one-norm solution of an underdetermined
                  least-squares problem. Basis pursuit denoise fits
                  the least-squares problem only approximately, and a
                  single parameter determines a curve that traces the
                  trade-off between the least-squares fit and the
                  one-norm of the solution. We show that the function
                  that describes this curve is convex and continuously
                  differentiable over all points of interest. The dual
                  solution of a least-squares problem with an explicit
                  one-norm constraint gives function and derivative
                  information needed for a root-finding method. As a
                  result, we can compute arbitrary points on this
                  curve. Numerical experiments demonstrate that our
                  method, which relies on only matrix-vector
                  operations, scales well to large problems.},
  url = {http://www.optimization-online.org/DB_HTML/2007/06/1708.html}
}

