% This file was created with JabRef 2.9.
% Encoding: MacRoman

%-----2021-----%

@UNPUBLISHED{ren2021AGUsvi,
  author = {Yuxiao Ren and Philipp A. Witte and Ali Siahkoohi and Mathias Louboutin and Ziyi Yin and Felix J. Herrmann},
  title = {Seismic Velocity Inversion and Uncertainty Quantification Using Conditional Normalizing Flows},
  year = {2021},
  month = {08},
  abstract = {In this work, we use a conditional normalizing flow (CNF) to address the seismic velocity inversion problem. Considering the large dimension difference between seismic data and velocity model, we reduce the data dimension by calculating its reverse time. After that, we train the CNF on pairs of migrated data and velocity. During inference, given a new seismic data, feeding the corresponding migrated image into the trained CNF will lead to posterior samples of the velocity inversion distribution. In addition, uncertainty quantification of the inverted results can be achieved by statistical metrics like mean and standard deviation. In our numerical example, the implementation is based on open-sourced software InvertibleNetworks.jl (Witte et al., 2021), JUDI.jl (Witte et al., 2019) and Devito (Louboutin et al., 2019). The training dataset are built based on the SEG/EAGE Overthrust model. For an unseen seismic data, the posterior samples of inversion results given by the trained CNF can be considered as good estimates of the true velocity. Especially, judging from the metrics like MAE, MSE, PSNR, SSIM, et al., the posterior mean is usually closer to the true velocity and the standard deviation indicates that the velocity value is more reliable within the subsurface layers than that on layer edges. Moreover, the inverted results, either the posterior samples or posterior mean, can be used as an initial model in the subsequent FWI for a more accurate result.},
  keywords = {Machine Learning, Normalizing Flows, Inversion, Uncertainity Quantification},
  note = {Submitted to AGU 2021},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2021/ren2021AGUsvi/ren2021AGUsvi.pdf},
  software = {https://github.com/slimgroup/INN_Velocity-Migration}
}

@UNPUBLISHED{louboutin2021NIPSmcte,
  author = {Mathias Louboutin and Ali Siahkoohi and Rongrong Wang and Felix J. Herrmann},
  title = {Low-memory stochastic backpropagation with multi-channel randomized trace estimation},
  year = {2021},
  month = {06},
  abstract = {Thanks to the combination of state-of-the-art accelerators and highly
optimized open software frameworks, there has been tremendous progress in
the performance of deep neural networks. While these developments have
been responsible for many breakthroughs, progress towards solving large-scale
problems, such as video encoding and semantic segmentation in 3D,
is hampered because access to on-premise memory is often limited.
Instead of relying on (optimal) checkpointing or invertibility of the
network layers---to recover the activations during backpropagation---we
propose to approximate the gradient of convolutional layers in neural
networks with a multi-channel randomized trace estimation technique. Compared
to other methods, this approach is simple, amenable to analyses, and leads to a
greatly reduced memory footprint. Even though the randomized trace
estimation introduces stochasticity during training, we argue that this
is of little consequence as long as the induced errors are of the same
order as errors in the gradient due to the use of stochastic gradient
descent. We discuss the performance of networks trained with stochastic
backpropagation and how the error can be controlled while maximizing
memory usage and minimizing computational overhead.},
  keywords = {HPC, Machine Learning, Randomized Linear Algebra, Convolutions, Low Memory},
  note = {Submitted to NeurIPS 2021},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2021/louboutin2021NIPSmcte/louboutin2021NIPSmcte.pdf},
  software = {https://github.com/slimgroup/XConv}
}

%-----2020-----%

@UNPUBLISHED{lopez2020gsb,
  author = {Oscar Lopez and Rajiv Kumar and Nick Moldoveanu and Felix J. Herrmann},
  title = {Graph Spectrum Based Seismic Survey Design},
  year = {2020},
  month = {12},
  abstract = {Randomized sampling techniques have become increasingly useful in seismic data acquisition and processing, allowing practitioners to achieve dense wavefield reconstruction from a substantially reduced number of field samples. However, typical designs studied in the low-rank matrix recovery and compressive sensing literature are difficult to achieve by standard industry hardware. For practical purposes, a compromise between stochastic and realizable samples is needed. In this paper, we propose a deterministic and computationally cheap tool to alleviate randomized acquisition design, prior to survey deployment and large-scale optimization. We consider universal and deterministic matrix completion results in the context of seismology, where a bipartite graph representation of the source-receiver layout allows for the respective spectral gap to act as a quality metric for wavefield reconstruction. We provide realistic survey design scenarios to demonstrate the utility of the spectral gap for successful seismic data acquisition via low-rank and sparse signal recovery.},
  keywords = {acquisition, compressed sensing, data reconstruction, survey design, algorithm},
  note = {Submitted to Geophysics},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/lopez2020gsb/lopez2020gsb.pdf}
}

@UNPUBLISHED{sharan2020lsh,
  author = {Shashin Sharan and Yijun Zhang and Oscar Lopez and Felix J. Herrmann},
  title = {Large scale high-frequency wavefield reconstruction with recursively weighted matrix factorizations},
  year = {2020},
  month = {10},
  abstract = {Acquiring seismic data on a regular periodic fine grid is challenging. By exploiting the low-rank approximation property of fully sampled seismic data in some transform domain, low-rank matrix completion offers a scalable way to reconstruct seismic data on a regular periodic fine grid from coarsely randomly sampled data acquired in the field. While wavefield reconstruction have been applied successfully at the lower end of the spectrum, its performance deteriorates at the higher frequencies where the low-rank assumption no longer holds rendering this type of wavefield reconstruction ineffective in situations where high resolution images are desired. We overcome this shortcoming by exploiting similarities between adjacent frequency slices explicitly. During low-rank matrix factorization, these similarities translate to alignment of subspaces of the factors, a notion we propose to employ as we reconstruct monochromatic frequency slices recursively starting at the low frequencies. While this idea is relatively simple in its core, to turn this recent insight into a successful scalable wavefield reconstruction scheme for 3D seismic requires a number of important steps. First, we need to move the weighting matrices, which encapsulate the prior information from adjacent frequency slices, from the objective to the data misfit constraint. This move considerably improves the performance of the weighted low-rank matrix factorization on which our wavefield reconstructions is based. Secondly, we introduce approximations that allow us to decouple computations on a row-by-row and column-by-column basis, which in turn allow to parallelize the alternating optimization on which our low-rank factorization relies. The combination of weighting and decoupling leads to a computationally feasible full-azimuth wavefield reconstruction scheme that scales to industry-scale problem sizes. We demonstrate the performance of the proposed parallel algorithm on a 2D field data and on a 3D synthetic dataset. In both cases our approach produces high-fidelity broadband wavefield reconstructions from severely subsampled data.},
  keywords = {5D reconstruction, compressed sensing, frequency-domain, parallel, signal processing},
  note = {Submitted to Geophysics},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/sharan2020lsh/sharan2020lsh.html}
}

@UNPUBLISHED{rizzuti2020dfw,
  author = {Gabrio Rizzuti and Mathias Louboutin and Rongrong Wang and Felix J. Herrmann},
  title = {A dual formulation of wavefield reconstruction inversion for
large-scale seismic inversion},
  year = {2020},
  month = {10},
  abstract = {Most of the seismic inversion techniques currently proposed focus
on robustness with respect to the background model choice or inaccurate
physical modeling assumptions, but are not apt to large-scale 3D
applications. On the other hand, methods that are computationally feasible
for industrial problems, such as full waveform inversion, are notoriously
bogged down by local minima and require adequate starting models. We propose
a novel solution that is both scalable and less sensitive to starting model
or inaccurate physics when compared to full waveform inversion. The method is
based on a dual (Lagrangian) reformulation of the classical wavefield
reconstruction inversion, whose robustness with respect to local minima is
well documented in the literature. However, it is not suited to 3D, as it
leverages expensive frequency-domain solvers for the wave equation. The
proposed reformulation allows the deployment of state-of-the-art time-domain
finite-difference methods, and is computationally mature for industrial scale
problems.},
  keywords = {3D, Full-waveform inversion, Wave equation, Finite-difference},
  note = {Submitted to Geophysics},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/rizzuti2020dfw/rizzuti2020dfw.html}
}

@UNPUBLISHED{kukreja2020lcc,
  author = {Navjot Kukreja and Jan Hueckelheim and Mathias Louboutin and John Washbourne and Paul H. J. Kelly and Gerard J. Gorman},
  title = {Lossy Checkpoint Compression in Full Waveform Inversion},
  year = {2020},
  month = {9},
  abstract = {This paper proposes a new method that combines check-pointing
methods with error-controlled lossy compression for large-scale
high-performance Full-Waveform Inversion (FWI), an inverse problem commonly
used in geophysical exploration. This combination can significantly reduce
data movement, allowing a reduction in run time as well as peak memory. In
the Exascale computing era, frequent data transfer (e.g., memory bandwidth,
PCIe bandwidth for GPUs, or network) is the performance bottleneck rather
than the peak FLOPS of the processing unit. Like many other adjoint-based
optimization problems, FWI is costly in terms of the number of floating-point
operations, large memory footprint during backpropagation, and data transfer
overheads. Past work for adjoint methods has developed checkpointing methods
that reduce the peak memory requirements during backpropagation at the cost
of additional floating-point computations. Combining this traditional
checkpointing with error-controlled lossy compression, we explore the
three-way tradeoff between memory, precision, and time to solution. We
investigate how approximation errors introduced by lossy compression of the
forward solution impact the objective function gradient and final inverted
solution. Empirical results from these numerical experiments indicate that
high lossy-compression rates (compression factors ranging up to 100) have a
relatively minor impact on convergence rates and the quality of the final
solution.},
  keywords = {Lossy compression, Full waveform inversion, checkpointing, memory},
  note = {Submitted to GMD},
  url = {https://arxiv.org/pdf/2009.12623.pdf}
}
