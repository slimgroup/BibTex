% This file was created with JabRef 2.9.
% Encoding: MacRoman

%-----2015-----%

@UNPUBLISHED{bougher2015CSEGpsu,
  author = {Ben Bougher},
  title = {Prediction of stratigraphic units from spectral co-occurance coefficients of well logs},
  year = {2015},
  abstract = {Well logging is the process of making physical
                  measurements down bore holes in order to
                  characterize geological and structural
                  properties. Logs are visually interpreted and
                  correlated to classify regions that are similar in
                  structure, a process that can be modelled with
                  machine learning. This project applies supervised
                  learning methods to labelled well logs from the
                  Trenton Black River data set in order to classify
                  major stratigraphic units. Spectral co-occurance
                  coefficients were used for feature extraction, and a
                  k-nearest-neighbours approach was used for
                  classification. This novel approach was applied to
                  real field data in a high-impact domain, yielding
                  promising results for future research.},
  keywords = {CSEG, scattering transform, well logs, machine learning},
  note = {(to be presented at the CSEG conference)},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2015/bougher2015CSEGpsu/bougher2015CSEGpsu.pdf}
}


@UNPUBLISHED{dasilva2015EAGEogt,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Off the grid tensor completion for seismic data interpolation},
  year = {2015},
  abstract = {The practical realities of acquiring seismic data in a
                  realistic survey are often at odds with the
                  stringent requirements of Shannon-Nyquist-based
                  sampling theory. The unpredictable movement of the
                  ocean`s currents can be detrimental in acquiring
                  exactly equally-spaced samples while sampling at
                  Nyquist-rates are expensive, given the huge
                  dimensionality and size of the data volume. Recent
                  work in matrix and tensor completion for seismic
                  data interpolation aim to alleviate such stringent
                  Nyquist-based sampling requirements but are
                  fundamentally posed on a regularly-spaced grid. In
                  this work, we extend our previous results in using
                  the so-called Hierarchical Tucker (HT) tensor format
                  for recovering seismic data to the irregularly
                  sampled case. We introduce an interpolation operator
                  that resamples our tensor from a regular grid (in
                  which we impose our low-rank constraints) to our
                  irregular sampling grid. Our framework is very
                  flexible and efficient, depending primarily on the
                  computational costs of this operator. We demonstrate
                  the superiority of this approach on a realistic BG
                  data set compared to using low-rank tensor methods
                  that merely use binning.},
  keywords = {EAGE, hierarchical tucker, structured tensor, tensor interpolation, off the grid, irregular sampling},
  note = {(to be presented at the EAGE Conference)},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2015/dasilva2015EAGEogt/dasilva2015EAGEogt.html}
}


@UNPUBLISHED{esser2015EAGElcs,
  author = {Ernie Esser and Tim T.Y. Lin and Rongrong Wang and Felix J. Herrmann},
  title = {A lifted $\ell_1/\ell_2$ constraint for sparse blind deconvolution},
  year = {2015},
  abstract = {We propose a modification to a sparsity constraint based
                  on the ratio of $\ell_1$ and $\ell_2$ norms for
                  solving blind seismic deconvolution problems in
                  which the data consist of linear convolutions of
                  different sparse reflectivities with the same source
                  wavelet. We also extend the approach to the
                  Estimation of Primaries by Sparse Inversion (EPSI)
                  model, which includes surface related multiples.
                  Minimizing the ratio of $\ell_1$ and $\ell_2$ norms
                  has been previously shown to promote sparsity in a
                  variety of applications including blind
                  deconvolution. Most existing implementations are
                  heuristic or require smoothing the $\ell_1/\ell_2$
                  penalty. Lifted versions of $\ell_1/\ell_2$
                  constraints have also been proposed but are
                  challenging to implement. Inspired by the lifting
                  approach, we propose to split the sparse signals
                  into positive and negative components and apply an
                  $\ell_1/\ell_2$ constraint to the difference,
                  thereby obtaining a constraint that is easy to
                  implement without smoothing the $\ell_1$ or $\ell_2$
                  norms. We show that a method of multipliers
                  implementation of the resulting model can recover
                  source wavelets that are not necessarily minimum
                  phase and approximately reconstruct the sparse
                  reflectivities. Numerical experiments demonstrate
                  robustness to the initialization as well as to noise
                  in the data.},
  keywords = {EAGE, blind deconvolution, EPSI},
  note = {(to be presented at the EAGE Conference)},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2015/esser2015EAGElcs/esser2015EAGElcs.html}
}


@UNPUBLISHED{esser2015SEGasd,
  author = {Ernie Esser and Lluís Guasch and Tristan van Leeuwen and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Automatic salt delineation --- {Wavefield} {Reconstruction} {Inversion} with convex constraints},
  year = {2015},
  date_submitted = {04/01/2015},
  abstract = {We extend full-waveform inversion by Wavefield
                  Reconstruction Inversion by including convex
                  constraints on the model. Contrary to the
                  conventional adjoint-state formulations, Wavefield
                  Reconstruction Inversion has the advantage that the
                  Gauss-Newton Hessian is well approximated by a
                  diagonal scaling, which allows us to add convex
                  constraints, such as the box- and the
                  edge-preserving total-variation constraint, on the
                  square slowness without incurring significant
                  increases in computational costs. As the examples
                  demonstrate, including these constraints yields far
                  superior results in complex geological areas that
                  contain high-velocity high-contrast bodies
                  (e.g. salt or basalt). Without these convex
                  constraints, adjoint-state and Wavefield
                  Reconstruction Inversion get trapped in local minima
                  for poor starting models.},
  keywords = {SEG, full-waveform inversion, convex constraints, total-variation norm, salt, private},
  note = {(submitted to the SEG conference)},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2015/esser2015SEGasd/esser2015SEGasd.html}
}


@UNPUBLISHED{fang2015EAGEsew,
  author = {Zhilong Fang and Felix J. Herrmann},
  title = {Source estimation for {Wavefield} {Reconstruction} {Inversion}},
  year = {2015},
  abstract = {Wavefield reconstruction inversion is a new approach to
                  waveform based inversion that helps overcome the
                  `cycle skipping' problem. However, like most
                  waveform based inversion methods, wavefield
                  reconstruction inversion also requires good source
                  wavelets. Without correct source wavelets,
                  wavefields cannot be reconstructed correctly and the
                  velocity model cannot be updated correctly
                  neither. In this work, we propose a source
                  estimation method for wavefield reconstruction
                  inversion based on the variable projection method.
                  In this method, we reconstruct wavefields and
                  estimate source wavelets simultaneously by solving
                  an extended least-squares problem, which contains
                  source wavelets. This approach does not increase the
                  computational cost compared to conventional
                  wavefield reconstruction inversion. Numerical
                  results illustrates with our source estimation
                  method we are able to recover source wavelets and
                  obtain inversion results that are comparable to
                  results obtained with true source wavelets.},
  keywords = {EAGE, source estimation, WRI},
  note = {(to be presented at the EAGE Conference)},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2015/fang2015EAGEsew/fang2015EAGEsew.html}
}


@UNPUBLISHED{fang2015EAGEuqw,
  author = {Zhilong Fang and Chia Ying Lee and Curt Da Silva and Felix J. Herrmann and Rachel Kuske},
  title = {Uncertainty quantification for {Wavefield} {Reconstruction} {Inversion}},
  year = {2015},
  abstract = {In this work, we propose a method to quantify the
                  uncertainty of wavefield reconstruction inversion
                  under the framework of Bayesian inference. Unlike
                  the conventional method using the wave equation as
                  the forward mapping, we involve the wave equation
                  misfit in the posterior distribution and propose a
                  new posterior distribution. The negative
                  log-likelihood of the new distribution is less
                  oscillatory than that of the conventional posterior
                  distribution, and its Gauss-Newton Hessian is a
                  diagonal matrix that can be generated without any
                  additional computational cost. We use the diagonal
                  Gauss-Newton Hessian to derive an approximate
                  Gaussian distribution at the maximum likelihood
                  point to quantify the uncertainty. This method makes
                  the uncertainty quantification for WRI
                  computationally tractable and is able to provide
                  reasonable uncertainty analysis based on our
                  numerical results.},
  keywords = {EAGE, UQ, WRI, FWI},
  note = {(to be presented at the EAGE Conference)},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2015/fang2015EAGEuqw/fang2015EAGEuqw.html}
}


@UNPUBLISHED{herrmann2015EAGEfom,
  author = {Felix J. Herrmann and Ning Tu and Ernie Esser},
  title = {Fast "online" migration with {Compressive} {Sensing}},
  year = {2015},
  abstract = {We present a novel adaptation of a recently developed
                  relatively simple iterative algorithm to solve
                  large-scale sparsity-promoting optimization
                  problems. Our algorithm is particularly suitable to
                  large-scale geophysical inversion problems, such as
                  sparse least-squares reverse-time migration or
                  Kirchoff migration since it allows for a tradeoff
                  between parallel computations, memory allocation,
                  and turnaround times, by working on subsets of the
                  data with different sizes. Comparison of the
                  proposed method for sparse least-squares imaging
                  shows a performance that rivals and even exceeds the
                  performance of state-of-the art one-norm solvers
                  that are able to carry out least-squares migration
                  at the cost of a single migration with all data.},
  keywords = {EAGE, LSRTM},
  note = {(to be presented at the EAGE Conference)},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2015/herrmann2015EAGEfom/herrmann2015EAGEfom.html}
}


@UNPUBLISHED{kumar2015CSEGlse,
  author = {Rajiv Kumar and Ning Tu and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Least-squares extended imaging with surface-related multiples},
  year = {2015},
  abstract = {Common image gathers are used in building velocity
                  models, inverting anisotropy parameters, and
                  analyzing reservoir attributes. Often primary
                  reflections are used to form image gathers and
                  multiples are typically attenuated in processing to
                  remove strong coherent artifacts generated by
                  multiples that interfere with the imaged
                  reflectors. However, researchers have shown that, if
                  cor- rectly used, multiples can actually provide
                  extra illumination of the subsurface in seismic
                  imaging, especially for delineating the near-surface
                  features. In this work, we borrow ideas from
                  literatures on imaging with surface-related
                  multiples, and apply these ideas to extended
                  imaging. This way we save the massive computation
                  cost in separating multiples from the data before
                  using them during the formation of image
                  gathers. Also, we mitigate the strong coherent
                  artifacts generated by multiples which can send the
                  migration velocity analysis type algorithms in wrong
                  direction. Synthetic examples on a three-layer model
                  show the efficacy of the proposed formulation.},
  keywords = {CSEG, image-gather, surface-related multiples},
  note = {(to be presented at the CSEG conference)},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2015/kumar2015CSEGlse/kumar2015CSEGlse.pdf}
}


@UNPUBLISHED{kumar2015EAGEmcu,
  author = {Rajiv Kumar and Oscar Lopez and Ernie Esser and Felix J. Herrmann},
  title = {Matrix completion on unstructured grids : {2-D} seismic data regularization and interpolation},
  year = {2015},
  abstract = {Seismic data interpolation via rank-minimization
                  techniques has been recently introduced in the
                  seismic community. All the existing
                  rank-minimization techniques assume the recording
                  locations to be on a regular grid, e.g., sampled
                  periodically, but seismic data are typically
                  irregularly sampled along spatial axes. Other than
                  the irregularity of the sampled grid, we often have
                  missing data. In this paper, we study the effect of
                  grid irregularity to conduct matrix completion on a
                  regular grid for unstructured data. We propose an
                  improvement of existing rank-minimization techniques
                  to do regularization. We also demonstrate that we
                  can perform seismic data regularization and
                  interpolation simultaneously. We illustrate the
                  advantages of the modification using a real seismic
                  line from the Gulf of Suez to obtain high quality
                  results for regularization and interpolation, a key
                  application in exploration geophysics.},
  keywords = {EAGE, regularization, interpolation, matrix completion, NFFT},
  note = {(to be presented at the EAGE Conference)},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2015/kumar2015EAGEmcu/kumar2015EAGEmcu.html}
}


@UNPUBLISHED{kumar2015sss,
  author = {Rajiv Kumar and Haneet Wason and Felix J. Herrmann},
  title = {Source separation for simultaneous towed-streamer marine acquisition---a compressed sensing approach},
  year = {2015},
  institution = {UBC},
  abstract = {Simultaneous marine acquisition is an economic way to
                  sample seismic data and speedup acquisition, wherein
                  single and/or multiple source vessels fire sources
                  at near-simultaneous or slightly random times,
                  resulting in overlapping shot records. The current
                  paradigm for simultaneous towed-streamer marine
                  acquisition incorporates "low-variability" in source
                  firing times---i.e., $\leq$ 1 or 2 seconds, since
                  both the sources and receivers are moving. This
                  results in low degree of randomness in simultaneous
                  data, which is challenging to separate (into its
                  constituent sources) using compressed sensing based
                  separation techniques since randomization is the key
                  to successful recovery via compressed sensing. In
                  this paper, we address the challenge of source
                  separation for simultaneous towed-streamer
                  acquisitions via two compressed sensing based
                  approaches---i.e., sparsity-promotion and
                  rank-minimization. We illustrate the performance of
                  both the sparsity-promotion and rank-minimization
                  based techniques by simulating two simultaneous
                  towed-streamer acquisition scenarios---i.e.,
                  over/under and simultaneous long offset. We observe
                  that the proposed approaches give good and
                  comparable recovery qualities of the separated
                  sources, but the rank-minimization technique is
                  relatively faster and memory efficient. We also
                  compare these two techniques with the NMO-based
                  median filtering type approach.},
  keywords = {simultaneous marine acquisition, source separation, sparsity, rank, private},
  note = {Submitted to Geophysics on February 16.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2015/kumar2015sss/kumar2015sss.html}
}


@UNPUBLISHED{lopez2015EAGErma,
  author = {Oscar Lopez and Rajiv Kumar and Felix J. Herrmann},
  title = {Rank minimization via alternating optimization: seismic data interpolation},
  year = {2015},
  abstract = {Low-rank matrix completion techniques have recently
                  become an effective tool for seismic trace
                  interpolation problems. In this talk, we consider an
                  alternating optimization scheme for nuclear norm
                  minimization and discuss the applications to large
                  scale wave field reconstruction. By adopting a
                  factorization approach to the rank minimization
                  problem we write our low-rank matrix in bi-linear
                  form, and modify this workflow by alternating our
                  optimization to handle a single matrix factor at a
                  time. This allows for a more tractable procedure
                  that can robustly handle large scale, highly
                  oscillatory and critically subsampled seismic data
                  sets. We demonstrate the potential of this approach
                  with several numerical experiments on a seismic line
                  from the Nelson 2D data set and a frequency slice
                  from the Gulf of Mexico data set.},
  keywords = {EAGE, matrix completion, low-rank, interpolation},
  note = {(to be presented at the EAGE Conference)},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2015/lopez2015EAGErma/lopez2015EAGErma.html}
}


@UNPUBLISHED{louboutin2015SEGtcs,
  author = {Mathias Louboutin and Felix J. Herrmann},
  title = {Time compressively sampled full-waveform inversion with stochastic optimization},
  year = {2015},
  date_submitted = {04/01/2015},
  abstract = {Time-domain Full-Waveform Inversion (FWI) aims to image
                  the subsurface of the earth accurately from field
                  recorded data and can be solved via the reduced
                  adjoint-state method. However, this method requires
                  access to the forward and adjoint wavefields that
                  are meet when computing gradient updates. The
                  challenge here is that the adjoint wavefield is
                  computed in reverse order during time stepping and
                  therefore requires storage or other type of
                  mitigation because storing the full time history of
                  the forward wavefield is too expensive in realistic
                  3D settings. To overcome this challenge, we propose
                  an approximate adjoint-state method where the
                  wavefields are subsampled randomly, which
                  drastically the amount of storage needed. By using
                  techniques from stochastic optimization, we control
                  the errors induced by the subsampling. Examples of
                  the proposed technique on a synthetic but realistic
                  2D model show that the subsampling-related artifacts
                  can be reduced significantly by changing the
                  sampling for each source after each model
                  update. Combination of this gradient approximation
                  with a quasi-Newton method shows virtually artifact
                  free inversion results requiring only 5% of storage
                  compared to saving the history at Nyquist. In
                  addition, we avoid having to recompute the
                  wavefields as is required by checkpointing.},
  keywords = {SEG, Full-waveform inversion, Acoustic, Subsampling, Time-domain, Inversion, Stochastic optimization, private},
  note = {(submitted to the SEG conference)},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2015/louboutin2015SEGtcs/louboutin2015SEGtcs.html}
}


@UNPUBLISHED{oghenekohwo2015CSEGctl,
  author = {Felix Oghenekohwo and Felix J. Herrmann},
  title = {Compressive time-lapse seismic data processing using shared information},
  year = {2015},
  abstract = {Time-lapse images void of acquisition and processing
                  artifacts can provide more useful information about
                  subsurface changes compared to those with
                  acquisition footprints and other unwanted
                  anomalies. Although, several pre-processing
                  techniques are being developed and used to mitigate
                  these unwanted artifacts, these operations can be
                  very expensive, challenging and data
                  dependent. Migration, as a processing tool, using a
                  sparsity constraint has been shown to reduce
                  artifacts drastically but little is known about the
                  significance for compressed time-lapse seismic
                  data. Leveraging ideas from distributed compressed
                  sensing, and motivated by our earlier work on
                  recovery of densely sampled time-lapse data from
                  compressively sampled measurements, we present a
                  sparsity-constrained migration for time-lapse data
                  that uses a common component shared by the baseline
                  and monitor data. Our algorithm tested on a
                  synthetic example highlights the advantages of
                  exploiting the common information, compared to ad
                  hoc methods that involve parallel processing of the
                  time-lapse data before differencing.},
  keywords = {CSEG, time-lapse},
  note = {(to be presented at the CSEG conference)},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2015/oghenekohwo2015CSEGctl/oghenekohwo2015CSEGctl.pdf}
}


@UNPUBLISHED{oghenekohwo2015EAGEuci,
  author = {Felix Oghenekohwo and Rajiv Kumar and Ernie Esser and Felix J. Herrmann},
  title = {Using common information in compressive time-lapse full-waveform inversion},
  year = {2015},
  abstract = {The use of time-lapse seismic data to monitor changes in
                  the subsurface has become standard practice in
                  industry. In addition, full-waveform inversion has
                  also been extended to time-lapse seismic to obtain
                  useful time-lapse information. The computational
                  cost of this method are becoming more pronounced as
                  the volume of data increases. Therefore, it is
                  necessary to develop fast inversion algorithms that
                  can also give improved time-lapse results. Rather
                  than following existing joint inversion algorithms,
                  we are motivated by a joint recovery model which
                  exploits the common information among the baseline
                  and monitor data. We propose a joint inversion
                  framework, leveraging ideas from distributed
                  compressive sensing and the modified Gauss-Newton
                  method for full-waveform inversion, by using the
                  shared information in the time-lapse data. Our
                  results on a realistic synthetic example highlight
                  the benefits of our joint inversion approach over a
                  parallel inversion method that does not exploit the
                  shared information. Preliminary results also
                  indicate that our formulation can address time-lapse
                  data with inconsistent acquisition geometries.},
  keywords = {EAGE, time-lapse, FWI},
  note = {(to be presented at the EAGE Conference)},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2015/oghenekohwo2015EAGEuci/oghenekohwo2015EAGEuci.html}
}


@UNPUBLISHED{oghenekohwo2015PIMSntc,
  author = {Felix Oghenekohwo and Felix J. Herrmann},
  title = {A new take on compressive time-lapse seismic acquisition, imaging and inversion},
  year = {2015},
  abstract = {Compressive sensing (CS), a sampling paradigm that is
                  changing how data is acquired and processed in many
                  fields, has attracted tremendous attention in recent
                  years. In exploration seismology, particularly in 3D
                  seismic technology, there have been published work
                  on the application of CS to seismic data
                  acquisition, processing and inversion. However, very
                  little is known about the implications of CS in
                  time-lapse (4D) seismic. Leveraging ideas from
                  distributed CS where more than one signal is
                  acquired and processed, as in 4D seismic, we present
                  a joint recovery algorithm for time-lapse seismic
                  data acquisition, imaging and inversion. The
                  algorithm exploits the common information in the
                  data sets (baseline and monitor) as part of an
                  inversion procedure. Application of our algorithm to
                  realistic synthetic examples highlight its
                  advantages over other conventional approaches for
                  processing time-lapse seismic data.},
  keywords = {PIMS, workshop, time-lapse, private},
  note = {to be presented at the PIMS Workshop on Advances in Seismic Imaging and Inversion, University of Alberta, Edmonton}
}


@UNPUBLISHED{peters2015AIPwri,
  author = {Bas Peters},
  title = {Wavefield-reconstruction inversion},
  year = {2015},
  date_submitted = {04/17/2015},
  abstract = {Wavefield Reconstruction Inversion is a method for
                  PDE-constrained optimization, which revolves around
                  the estimation of fields using the PDE as well as
                  the observed data in a least-squares sense. The
                  method is quadratic penalty based, which offers some
                  interesting possibilities for the construction of
                  algorithms, compared to the Lagrangian form. One of
                  the main benefits of the method is when the initial
                  guess is far from the global minimizer.
                  Reduced-space and full-space algorithms are
                  discussed, including illustrative examples. The
                  method was developed with seismic applications in
                  mind, but applies to other PDE-constrained
                  optimization problems as well.},
  keywords = {AIP, Waveform reconstruction inversion, penalty method, optimization, full-waveform inversion, private},
  note = {to be presented at the AIP Conference in Helsinki, Finland}
}


@UNPUBLISHED{peters2015SEGrwi,
  author = {Bas Peters and Zhilong Fang and Brendan Smithyman and Felix J. Herrmann},
  title = {Regularizing waveform inversion by projections onto convex sets --- application to the {2D} {Chevron} 2014 synthetic blind-test dataset},
  year = {2015},
  date_submitted = {04/01/2015},
  abstract = {A framework is proposed for regularizing the waveform
                  inversion problem by projections onto intersections
                  of convex sets. Multiple pieces of prior information
                  about the geology are represented by multiple convex
                  sets, for example limits on the velocity or minimum
                  smoothness conditions on the model.  The data-misfit
                  is then minimized, such that the estimated model is
                  always in the intersection of the convex
                  sets. Therefore, it is clear what properties the
                  estimated model will have at each iteration. This
                  approach does not require any quadratic penalties to
                  be used and thus avoids the known problems and
                  limitations of those types of penalties. It is shown
                  that by formulating waveform inversion as a
                  constrained problem, regularization ideas such as
                  Tikhonov regularization and gradient filtering can
                  be incorporated into one framework. The algorithm is
                  generally applicable, in the sense that it works
                  with any (differentiable) objective function and
                  does not require significant additional
                  computation. The method is demonstrated on the
                  inversion of the 2D marine isotropic elastic
                  synthetic seismic benchmark by Chevron using an
                  acoustic modeling code. To highlight the effect of
                  the projections, we apply no data pre-processing.},
  keywords = {SEG, Waveform inversion, regularization, projection, blind-test,Wavefield Reconstruction Inversion, private},
  note = {(submitted to the SEG conference)},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2015/peters2015SEGrwi/peters2015SEGrwi.html}
}


@UNPUBLISHED{smithyman2015EAGEcwi,
  author = {Brendan Smithyman and Bas Peters and Felix J. Herrmann},
  title = {Constrained waveform inversion of colocated {VSP} and surface seismic data},
  year = {2015},
  abstract = {Constrained Full-Waveform Inversion (FWI) is applied to
                  produce a high-resolution velocity model from both
                  Vertical Seismic Profiling (VSP) and surface seismic
                  data. The case study comes from the Permian Basin in
                  Texas, USA. This dataset motivates and tests several
                  new developments in methodology that enable recovery
                  of model results that sit within multiple a priori
                  constraint sets. These constraints are imposed
                  through a Projected Quasi-Newton (PQN) approach,
                  wherein the projection set is the intersection of
                  physical property bounds and anisotropic wavenumber
                  filtering. This enables the method to recover
                  geologically-reasonable models while preserving the
                  fast model convergence offered by a quasi-Newton
                  optimization scheme like l-BFGS. In the Permian
                  Basin example, low-frequency data from both arrays
                  are inverted together and regularized by this
                  projection approach. Careful choice of the
                  constraint sets is possible without requiring
                  tradeoff parameters as in a quadratic penalty
                  approach to regularization. Multiple 2D FWI results
                  are combined to produce an interpolated 3D model
                  that is consistent with the models from migration
                  velocity analysis and VSP processing, while offering
                  improved resolution and illumination of features
                  from both datasets.},
  keywords = {EAGE, waveform inversion, VSP},
  note = {(to be presented at the EAGE Conference)},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2015/smithyman2015EAGEcwi/smithyman2015EAGEcwi.pdf}
}


@UNPUBLISHED{tu2015TLEfls,
  author = {Ning Tu and Felix J. Herrmann},
  title = {Fast least-squares imaging with surface-related multiples: application to a {North}-{Sea} data set},
  year = {2015},
  institution = {UBC},
  abstract = {In marine seismic acquisition, surface-related multiples
                  constitute a significant portion of the acquired
                  data. Typically, multiples are removed during
                  early-stage data-processing as they can lead to
                  phantom reflectors during migration that may result
                  in erroneous geological interpretations. However, if
                  properly dealt with, multiples can provide valuable
                  extra information and complement primaries in
                  illuminating the subsurface. In this article, we
                  demonstrate the limitation of the reverse-time
                  migration in imaging these multiples, and present an
                  alternative inversion procedure that is
                  computationally efficient, that jointly maps both
                  primaries and multiples to the true reflectors, and
                  where the source function is estimated on the
                  fly. As a result, we obtain high-quality, mostly
                  artifact-free, broad-band images where the imprint
                  of the source-function are partly removed at a
                  computationally affordable expense compared to the
                  combined costs of the wave-equation based
                  surface-related multiple elimination and the
                  reverse-time migration. We achieve all this by
                  including the total downgoing wavefields as areal
                  sources in the least-squares migration in
                  combination with curvelet-domain sparsity promotion.
                  We apply the proposed method to a shallow-water
                  marine data set from the North sea, which contains
                  abundant short-period surface-related multiples, and
                  show its efficacy in eliminating coherent imaging
                  artifacts associated with these multiples. We also
                  demonstrate the benefits of joint imaging of
                  primaries and multiples compared to imaging these
                  signal components separately.},
  keywords = {migration, multiples, inversion, least squares, field data, private},
  note = {Submitted to The Leading Edge on April 9.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2015/tu2015TLEfls/tu2015TLEfls.html}
}


@UNPUBLISHED{vanleeuwen2015EAGEafs,
  author = {Tristan van Leeuwen and Rajiv Kumar and Felix J. Herrmann},
  title = {Affordable full subsurface image volume---an application to {WEMVA}},
  year = {2015},
  abstract = {Common image gathers are used in building velocity
                  models, inverting for anisotropy parameters, and
                  analyzing reservoir attributes. In this paper, we
                  offer a new perspective on image gathers, where we
                  glean information from the image volume via
                  efficient matrix-vector products. The proposed
                  formulation make the computation of full subsurface
                  image volume feasible. We illustrate how this
                  matrix-vector product can be used to construct
                  objective functions for automatic MVA.},
  keywords = {EAGE, MVA, wave-equation, randomized trace estimation},
  note = {(to be presented at the EAGE conference)},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2015/vanleeuwen2015EAGEafs/vanleeuwen2015EAGEafs.pdf}
}


@UNPUBLISHED{vanleeuwen2015IPpmp,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {A penalty method for {PDE}-constrained optimization in inverse problems},
  year = {2015},
  institution = {UBC},
  abstract = {Many inverse and parameter estimation problems can be
                  written as PDE-constrained optimization
                  problems. The goal, then, is to infer the
                  parameters, typically coefficients of the PDE, from
                  partial measurements of the solutions of the PDE for
                  several right-hand-sides. Such PDE-constrained
                  problems can be solved by finding a stationary point
                  of the Lagrangian, which entails simultaneously
                  updating the parameters and the (adjoint) state
                  variables. For large-scale problems, such an
                  all-at-once approach is not feasible as it requires
                  storing all the state variables. In this case one
                  usually resorts to a reduced approach where the
                  constraints are explicitly eliminated (at each
                  iteration) by solving the PDEs. These two
                  approaches, and variations thereof, are the main
                  workhorses for solving PDE-constrained optimization
                  problems arising from inverse problems. In this
                  paper, we present an alternative method that aims to
                  combine the advantages of both approaches. Our
                  method is based on a quadratic penalty formulation
                  of the constrained optimization problem. By
                  eliminating the state variable, we develop an
                  efficient algorithm that has roughly the same
                  computational complexity as the conventional reduced
                  approach while exploiting a larger search
                  space. Numerical results show that this method
                  indeed reduces some of the non-linearity of the
                  problem and is less sensitive the initial iterate.},
  keywords = {penalty method, PDE, optimization, inverse problems, private},
  note = {Submitted to Inverse Problems on April 10.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2015/vanleeuwen2015IPpmp/vanleeuwen2015IPpmp.pdf}
}


@UNPUBLISHED{wason2015EAGEcsm,
  author = {Haneet Wason and Felix Oghenekohwo and Felix J. Herrmann},
  title = {Compressed sensing in {4-D marine}---recovery of dense time-lapse data from subsampled data without repetition},
  year = {2015},
  abstract = {We present an extension of our time-jittered marine
                  acquisition for time-lapse surveys by working on
                  more realistic field acquisition scenarios by
                  incorporating irregular spatial grids without
                  insisting on repeatability between the
                  surveys. Since we are always subsampled in both the
                  baseline and monitor surveys, we are interested in
                  recovering the densely sampled baseline and monitor,
                  and then the (complete) 4-D difference from
                  subsampled/incomplete baseline and monitor data.},
  keywords = {EAGE, simultaneous acquisition, time-lapse, off-the-grid, NFFT},
  note = {(to be presented at the EAGE Conference)},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2015/wason2015EAGEcsm/wason2015EAGEcsm.html}
}



%-----2014-----%

@UNPUBLISHED{kumar2014GEOPemc,
  author = {Rajiv Kumar and Curt Da Silva and Okan Akalin and Aleksandr Y. Aravkin and Hassan Mansour and Ben Recht and Felix J. Herrmann},
  title = {Efficient matrix completion for seismic data reconstruction},
  year = {2014},
  month = {08},
  institution = {UBC},
  abstract = {Despite recent developments in improved acquisition,
                  seismic data often remains undersampled along source
                  and/or receiver coordinates, resulting in incomplete
                  data for key applications such as migration and
                  multiple prediction requiring densely sampled,
                  alias-free wide azimuth data. When seismic data is
                  organized in monochromatic frequency slices,
                  missing-trace interpolation can be cast into a
                  matrix completion problem, where the low-rank
                  structure of seismic data in the appropriate domain
                  can be exploited to recover densely sampled data
                  volumes from data with missing entries. Current
                  approaches that exploit low-rank structure are based
                  on repeated singular value decompositions, which
                  become prohibitively expensive for large-scale
                  problems unless the data is partitioned and
                  processed in small windows. While computationally
                  manageable, our theory and experiments show degraded
                  results when the windows sizes become too small. To
                  overcome this problem, we carry out our
                  interpolations for each frequency independently
                  while working with the complete data in the
                  midpoint-offset domain instead of windowing. For
                  lateral varying geologies that are not too complex,
                  working in the midpoint-offset domain leads to
                  favorable rank minimization recovery because the
                  singular values decay faster while sampling-related
                  artifacts remain full rank. This combination of fast
                  decay and full-rank artifacts agrees with the
                  principles of the compressive sensing paradigm,
                  which is based on exploiting (low-rank) structure, a
                  sampling process that breaks this structure, and a
                  rank-minimizing optimization that restores the
                  signal's structure and interpolates the subsampled
                  data. To make our proposed method computationally
                  viable and practical, we introduce a
                  factorization-based approach that avoids computing
                  the singular values, and that therefore scales to
                  large seismic data problems as long as the factors
                  can be stored in memory. Tests on realistic two- and
                  three-dimensional seismic data show that our method
                  compares favorably, both in terms of computational
                  speed and recovery quality, to existing
                  curvelet-based and tensor-based techniques.},
  keywords = {interpolation, low-rank, private},
  note = {Submitted to Geophysics on August 8, 2014.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2014/kumar2014GEOPemc/kumar2014GEOPemc.pdf}
}


@UNPUBLISHED{oghenekohwo2014GEOPfrt,
  author = {Felix Oghenekohwo and Haneet Wason and Ernie Esser and Felix J. Herrmann},
  title = {Compressive 4D—economic time-lapse seismic with randomized subsampling and joint recovery},
  year = {2014},
  month = {10},
  institution = {UBC},
  abstract = {The current paradigm of time-lapse seismic relies on
                  dense sampling and repeatability amongst the
                  baseline and the monitor surveys. Recent results in
                  distributed compressive sensing allow us to come up
                  with a new economic sampling paradigm where the
                  vintages and time-lapse difference are recovered
                  from incomplete data. The combination of randomized
                  sampling, signal structure and correlations among
                  the vintages underlies this approach. In a somewhat
                  idealized setting where effects such as difference
                  in currents are ignored, and where we do not have
                  access to dense samplings of the baseline and/or
                  monitor surveys, we can get high quality recovery of
                  these vintages and time-lapse difference when there
                  is a small “overlap” in the surveys—i.e., where the
                  random samplings have partial statistical
                  dependence. Specifically, we find that the quality
                  of the vintages improves for decreasing overlap in
                  the surveys while the converse is true for the
                  time-lapse difference. Our setting differs from
                  conventional time-lapse acquisition because we do
                  not have access to dense samplings. Surveys with
                  partial overlapping randomized samplings lead to the
                  best trade-off between the recovery quality of the
                  vintages and the time-lapse signal. We confirm this
                  by a series of experiments.},
  keywords = {acquistion, time-lapse, marine, sampling, random, joint recovery method, private},
  note = {Submitted revision 1 to Geophysics on October 22, 2014},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2014/oghenekohwo2014GEOPfrt/oghenekohwo2014GEOPfrt.html}
}





%-----2013-----%

@UNPUBLISHED{ghadermarzy2013ncs,
  author = {Navid Ghadermarzy and Hassan Mansour and Ozgur Yilmaz},
  title = {Non-convex compressed sensing using partial support information},
  year = {2013},
  institution = {UBC},
  abstract = {In this paper we address the recovery conditions of
                  weighted $\ell_p$ minimization for signal reconstruction
                  from compressed sensing measurements when partial
                  support in- formation is available. We show that
                  weighted $\ell_p$ minimization with 0 < p < 1 is stable
                  and robust under weaker sufficient conditions
                  compared to weighted $\ell_1$ minimization. Moreover, the
                  sufficient recovery conditions of weighted $\ell_p$ are
                  weaker than those of regular $\ell_p$ minimization if at
                  least 50\% of the support estimate is accurate. We
                  also review some algorithms which exist to solve the
                  non-convex $\ell_p$ problem and illustrate our results
                  with numerical experiments.},
  keywords = {Compressed sensing, weighted $\ell_p$, nonconvex optimization, sparse reconstruction},
  month = {11},
  url = {http://arxiv.org/abs/1311.3773}
}

