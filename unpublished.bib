% This file was created with JabRef 2.9.
% Encoding: MacRoman

%-----2024-----%

@UNPUBLISHED{orozco2024IPaspire,
  author = {Rafael Orozco and Ali Siahkoohi and Mathias Louboutin and Felix J. Herrmann},
  title = {ASPIRE: Iterative Amortized Posterior Inference for Bayesian Inverse Problems},
  year = {2024},
  month = {5},
  abstract = {Due to their uncertainty quantification, Bayesian solutions to
inverse problems are the framework of choice in applications that are risk
averse. These benefits come at the cost of computations that are in general,
intractable. New advances in machine learning and variational inference (VI)
have lowered the computational barrier by learning from examples. Two VI
paradigms have emerged that represent different tradeoffs: amortized and
non-amortized. Amortized VI can produce fast results but due to generalizing
to many observed datasets it produces suboptimal inference results.
Non-amortized VI is slower at inference but finds better posterior
approximations since it is specialized towards a single observed dataset.
Current amortized VI techniques run into a sub-optimality wall that can not
be improved without more expressive neural networks or extra training data.
We present a solution that enables iterative improvement of amortized
posteriors that uses the same networks architectures and training data. The
benefits of our method requires extra computations but these remain frugal
since they are based on physics-hybrid methods and summary statistics.
Importantly, these computations remain mostly offline thus our method
maintains cheap and reusable online evaluation while bridging the
approximation gap these two paradigms. We denote our proposed method
ASPIRE - Amortized posteriors with Summaries that are
Physics-based and Iteratively REfined. We first validate our
method on a stylized problem with a known posterior then demonstrate its
practical use on a high-dimensional and nonlinear transcranial medical
imaging problem with ultrasound. Compared with the baseline and previous
methods from the literature our method stands out as an computationally
efficient and high-fidelity method for posterior inference.},
  keywords = {Normalizing flows, amortization gap, Bayesian inference, simulation-based inference, amortized variational inference, medical imaging},
  doi = {10.48550/arXiv.2405.05398}
}

@UNPUBLISHED{yin2024wiser,
  author = {Ziyi Yin and Rafael Orozco and Felix J. Herrmann},
  title = {WISER: multimodal variational inference for full-waveform inversion without dimensionality reduction},
  year = {2024},
  month = {5},
  abstract = {We present a semi-amortized variational inference framework designed for computationally feasible uncertainty quantification in 2D full-waveform inversion to explore the multimodal posterior distribution without dimensionality reduction. The framework is called WISER, short for full-Waveform variational Inference via Subsurface Extensions with Refinements. WISER leverages the power of generative artificial intelligence to perform approximate amortized inference that is low-cost albeit showing an amortization gap. This gap is closed through non-amortized refinements that make frugal use of acoustic wave physics. Case studies illustrate that WISER is capable of full-resolution, computationally feasible, and reliable uncertainty estimates of velocity models and imaged reflectivities.},
  keywords = {WISER, WISE, FWI, imaging, CIG, conditional normalizing flows, Bayesian inference, amortized variational inference, uncertainty quantification, deep learning, inverse problems, summary statistics, MVA},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2024/yin2024wiser/WISER.html},
  doi = {10.13140/RG.2.2.34906.15044}
}

%-----2023-----%

@UNPUBLISHED{orozco2023invnet,
  author = {Rafael Orozco and Philipp A. Witte and Mathias Louboutin and Ali Siahkoohi and Gabrio Rizzuti and Bas Peters and Felix J. Herrmann},
  title = {InvertibleNetworks.jl: A Julia package for scalable normalizing flows},
  year = {2023},
  month = {12},
  abstract = {InvertibleNetworks.jl is a Julia package designed for the scalable implementation of normalizing flows, a method for density estimation and sampling in high-dimensional distributions. This package excels in memory efficiency by leveraging the inherent invertibility of normalizing flows, which significantly reduces memory requirements during backpropagation compared to existing normalizing flow packages that rely on automatic differentiation frameworks. InvertibleNetworks.jl has been adapted for diverse applications, including seismic imaging, medical imaging, and CO2 monitoring, demonstrating its effectiveness in learning high-dimensional distributions.},
  keywords = {normalizing flows, software, conditional normalizing flows, Bayesian inference, uncertainty quantification, deep learning, inverse problems, memory, HPC, computing},
  doi = {10.48550/arXiv.2312.13480}
}