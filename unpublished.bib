% This file was created with JabRef 2.9.
% Encoding: MacRoman

%-----2022-----%

@UNPUBLISHED{grady2022SCtll,
  author = {Thomas J. Grady II and Rishi Khan and Mathias Louboutin and Ziyi Yin and Philipp A. Witte and Ranveer Chandra and Russell J. Hewett and Felix J. Herrmann},
  title = {Towards large-scale learned solvers for parametric PDEs with model-parallel Fourier neural operators},
  year = {2022},
  month = {04},
  abstract = {Fourier neural operators (FNOs) are a recently
introduced neural network architecture for learning solution
operators of partial differential equations (PDEs), which have
been shown to perform significantly better than comparable
approaches based on convolutional networks. Once trained,
FNOs can achieve speed-ups of multiple orders of magnitude
over conventional numerical PDE solvers. However, due to the
high dimensionality of their input data and network weights,
FNOs have so far only been applied to two-dimensional or
small three-dimensional problems. To remove this limited
problem-size barrier, we propose a model-parallel version of
FNOs based on domain-decomposition of both the input data
and network weights. We demonstrate that our model-parallel
FNO is able to predict time-varying PDE solutions of over
3.2 billion variables on Summit using up to 768 GPUs and
show an example of training a distributed FNO on the Azure
cloud for simulating multiphase CO2 dynamics in the Earth's
subsurface.},
  keywords = {Fourier neural operators, HPC, large-scale, CCS, deep learning},
  note = {Submitted},
  software = {https://github.com/slimgroup/dfno},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2022/grady2022SCtll/grady2022SCtll.pdf}
}

@UNPUBLISHED{siahkoohi2022EAGEweb,
  author = {Ali Siahkoohi and Rafael Orozco and Gabrio Rizzuti and Felix J. Herrmann},
  title = {Wave-equation based inversion with amortized variational Bayesian inference},
  year = {2022},
  month = {03},
  abstract = {Solving inverse problems involving measurement noise and modeling
errors requires regularization in order to avoid data overfit. Geophysical
inverse problems, in which the Earth's highly heterogeneous structure is
unknown, present a challenge in encoding prior knowledge through analytical
expressions. Our main contribution is a generative-model-based regularization
approach, robust to out-of-distribution data, which exploits the prior
knowledge embedded in existing data and model pairs. Utilizing an amortized
variational inference objective, a conditional normalizing flow (NF) is
pretrained on pairs of low- and high-fidelity migrated images in order to
achieve a low-fidelity approximation to the seismic imaging posterior
distribution for previously unseen data. The NF is used after pretraining to
reparameterize the unknown seismic image in an inversion scheme involving
physics-guided data misfit and a Gaussian prior on the NF latent variable.
Solving this optimization problem with respect to the latent variable enables
us to leverage the benefits of data-driven conditional priors whilst being
informed by physics and data. The numerical experiments demonstrate that the
proposed inversion scheme produces seismic images with limited artifacts when
dealing with noisy and out-of-distribution data.},
  keywords = {seismic imaging, normalizing flows, conditional priors},
  note = {Submitted},
  software = {https://github.com/slimgroup/ConditionalNFs4Imaging.jl},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2022/siahkoohi2022EAGEweb/abstract.html}
}

@UNPUBLISHED{zhang2022SEGass,
  author = {Yijun Zhang and Mathias Louboutin and Ali Siahkoohi and Ziyi Yin and Rajiv Kumar and Felix J. Herrmann},
  title = {A simulation-free seismic survey design by maximizing the spectral gap},
  year = {2022},
  month = {03},
  abstract = {Due to the tremendous cost of seismic data acquisition, methods have been developed to reduce the amount of data acquired by designing optimal missing trace reconstruction algorithms. These technologies are designed to record as little data as possible in the field, while providing accurate wavefield reconstruction in the areas of the survey that are not recorded. This is achieved by designing randomized subsampling masks that allow for accurate wavefield reconstruction via matrix completion methods. Motivated by these recent results, we propose a simulation-free seismic survey design that aims at improving the quality of a given randomized subsampling using a simulated annealing algorithm that iteratively increases the spectral gap of the subsampling mask, a property recently linked to the quality of the reconstruction. We demonstrate that our proposed method improves the data reconstruction quality for a fixed subsampling rate on a realistic synthetic dataset.},
  keywords = {acquisition, survey design, wavefield reconstruction, spectral gap, matrix factorization},
  note = {Submitted},
  software ={https://github.com/slimgroup/opt_spectral_gap},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2022/zhang2022SEGass/Yijun2022SEGass.html}
}

@UNPUBLISHED{yin2022SEGlci,
  author = {Ziyi Yin and Ali Siahkoohi and Mathias Louboutin and Felix J. Herrmann},
  title = {Learned coupled inversion for carbon sequestration monitoring and forecasting with Fourier neural operators},
  year = {2022},
  month = {03},
  abstract = {Seismic monitoring of carbon storage sequestration is a challenging problem involving both fluid-flow physics and wave physics. Additionally, monitoring usually requires the solvers for these physics to be coupled and differentiable to effectively invert for the subsurface properties of interest. To drastically reduce the computational cost, we introduce a learned coupled inversion framework based on the wave modeling operator, rock property conversion and a proxy fluid-flow simulator. We show that we can accurately use a Fourier neural operator as a proxy for the fluid-flow simulator for a fraction of the computational cost. We demonstrate the efficacy of our proposed method by means of a synthetic experiment. Finally, our framework is extended to carbon sequestration forecasting, where we effectively use the surrogate Fourier neural operator to forecast the CO2 plume in the future at near-zero additional cost.},
  keywords = {Fourier neural operators, CCS, multiphysics, machine learning, deep learning, time-lapse, inversion},
  note = {Submitted},
  software = {https://github.com/slimgroup/FNO4CO2},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2022/yin2022SEGlci/paper.html}
}

@UNPUBLISHED{louboutin2022SEGais,
  author = {Mathias Louboutin and Philipp A. Witte and Ali Siahkoohi and Gabrio Rizzuti and Ziyi Yin and Rafael Orozco and Felix J. Herrmann},
  title = {Accelerating innovation with software abstractions for scalable computational geophysics},
  year = {2022},
  month = {03},
  abstract = {We present the SLIM open-source software framework for
computational geophysics, and more generally, inverse problems based on the
wave-equation (e.g., medical ultrasound). We developed a software environment
aimed at scalable research and development by designing multiple layers of
abstractions. This environment allows the researchers to easily formulate
their problem in an abstract fashion, while still being able to exploit the
latest developments in high-performance computing. We illustrate and
demonstrate the benefits of our software design on many geophysical
applications, including seismic inversion and physics-informed machine
learning for geophysics(e.g., loop unrolled imaging, uncertainty
quantification), all while facilitating the integration of external software.},
  keywords = {Software, inversion, FWI, HPC, Devito, JUDI, Machine Learning, Uncertainty Quantification, CCS},
  note = {Submitted},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2022/louboutin2022SEGais/louboutin_seg22.html}
}

@UNPUBLISHED{siahkoohi2022SEGvcw,
  author = {Ali Siahkoohi and Mathias Louboutin and Felix J. Herrmann},
  title = {Velocity continuation with Fourier neural operators for accelerated uncertainty quantification},
  year = {2022},
  month = {03},
  abstract = {Seismic imaging is an ill-posed inverse problem that is challenged
by noisy data and modeling inaccuracies---due to errors in the background
squared-slowness model. Uncertainty quantification is essential for
determining how variability in the background models affects seismic imaging.
Due to the costs associated with the forward Born modeling operator as well
as the high dimensionality of seismic images, quantification of uncertainty
is computationally expensive. As such, the main contribution of this work is
a survey-specific Fourier neural operator surrogate to velocity continuation
that maps seismic images associated with one background model to another
virtually for free. While being trained with only 200 background and seismic
image pairs, this surrogate is able to accurately predict seismic images
associated with new background models, thus accelerating seismic imaging
uncertainty quantification. We support our method with a realistic data
example in which we quantify seismic imaging uncertainties using a Fourier
neural operator surrogate, illustrating how variations in background models
affect the position of reflectors in a seismic image.},
  keywords = {Fourier neural operators, Velocity continuation, Uncertainty quantification},
  note = {Submitted},
  software = {https://github.com/slimgroup/fno4vc},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2022/siahkoohi2022SEGvcw/abstract.html}
}

%-----2021-----%

@UNPUBLISHED{siahkoohi2021dbif,
  author = {Ali Siahkoohi and Gabrio Rizzuti and Felix J. Herrmann},
  title = {Deep Bayesian inference for seismic imaging with tasks},
  year = {2021},
  month = {10},
  abstract = {We propose to use techniques from Bayesian inference and deep
neural networks to translate uncertainty in seismic imaging to uncertainty in
tasks performed on the image, such as horizon tracking. Seismic imaging is an
ill-posed inverse problem because of unavoidable bandwidth and aperture
limitations, which that is hampered by the presence of noise and
linearization errors. Many regularization methods, such as transform-domain
sparsity promotion, have been designed to deal with the adverse effects of
these errors, however, these methods run the risk of biasing the solution and
do not provide information on uncertainty in the image space and how this
uncertainty impacts certain tasks on the image. A systematic approach is
proposed to translate uncertainty due to noise in the data to confidence
intervals of automatically tracked horizons in the image. The uncertainty is
characterized by a convolutional neural network (CNN) and to assess these
uncertainties, samples are drawn from the posterior distribution of the CNN
weights, used to parameterize the image. Compared to traditional priors, in
the literature it is argued that these CNNs introduce a flexible inductive
bias that is a surprisingly good fit for many diverse domains in imaging. The
method of stochastic gradient Langevin dynamics is employed to sample from
the posterior distribution. This method is designed to handle large scale
Bayesian inference problems with computationally expensive forward operators
as in seismic imaging. Aside from offering a robust alternative to maximum a
posteriori estimate that is prone to overfitting, access to these samples
allow us to translate uncertainty in the image, due to noise in the data, to
uncertainty on the tracked horizons. For instance, it admits estimates for
the pointwise standard deviation on the image and for confidence intervals on
its automatically tracked horizons.},
  keywords = {deep priors, seismic imaging, uncertainty quantification, horizon tracking},
  note = {Submitted to Geophysics},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2021/siahkoohi2021dbif/paper.html}
}

%-----2020-----%

@UNPUBLISHED{lopez2020gsb,
  author = {Oscar Lopez and Rajiv Kumar and Nick Moldoveanu and Felix J. Herrmann},
  title = {Graph Spectrum Based Seismic Survey Design},
  year = {2020},
  month = {12},
  abstract = {Randomized sampling techniques have become increasingly useful in seismic data acquisition and processing, allowing practitioners to achieve dense wavefield reconstruction from a substantially reduced number of field samples. However, typical designs studied in the low-rank matrix recovery and compressive sensing literature are difficult to achieve by standard industry hardware. For practical purposes, a compromise between stochastic and realizable samples is needed. In this paper, we propose a deterministic and computationally cheap tool to alleviate randomized acquisition design, prior to survey deployment and large-scale optimization. We consider universal and deterministic matrix completion results in the context of seismology, where a bipartite graph representation of the source-receiver layout allows for the respective spectral gap to act as a quality metric for wavefield reconstruction. We provide realistic survey design scenarios to demonstrate the utility of the spectral gap for successful seismic data acquisition via low-rank and sparse signal recovery.},
  keywords = {acquisition, compressed sensing, data reconstruction, survey design, algorithm},
  note = {Submitted to Geophysics},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/lopez2020gsb/lopez2020gsb.pdf}
}

@UNPUBLISHED{sharan2020lsh,
  author = {Shashin Sharan and Yijun Zhang and Oscar Lopez and Felix J. Herrmann},
  title = {Large scale high-frequency wavefield reconstruction with recursively weighted matrix factorizations},
  year = {2020},
  month = {10},
  abstract = {Acquiring seismic data on a regular periodic fine grid is challenging. By exploiting the low-rank approximation property of fully sampled seismic data in some transform domain, low-rank matrix completion offers a scalable way to reconstruct seismic data on a regular periodic fine grid from coarsely randomly sampled data acquired in the field. While wavefield reconstruction have been applied successfully at the lower end of the spectrum, its performance deteriorates at the higher frequencies where the low-rank assumption no longer holds rendering this type of wavefield reconstruction ineffective in situations where high resolution images are desired. We overcome this shortcoming by exploiting similarities between adjacent frequency slices explicitly. During low-rank matrix factorization, these similarities translate to alignment of subspaces of the factors, a notion we propose to employ as we reconstruct monochromatic frequency slices recursively starting at the low frequencies. While this idea is relatively simple in its core, to turn this recent insight into a successful scalable wavefield reconstruction scheme for 3D seismic requires a number of important steps. First, we need to move the weighting matrices, which encapsulate the prior information from adjacent frequency slices, from the objective to the data misfit constraint. This move considerably improves the performance of the weighted low-rank matrix factorization on which our wavefield reconstructions is based. Secondly, we introduce approximations that allow us to decouple computations on a row-by-row and column-by-column basis, which in turn allow to parallelize the alternating optimization on which our low-rank factorization relies. The combination of weighting and decoupling leads to a computationally feasible full-azimuth wavefield reconstruction scheme that scales to industry-scale problem sizes. We demonstrate the performance of the proposed parallel algorithm on a 2D field data and on a 3D synthetic dataset. In both cases our approach produces high-fidelity broadband wavefield reconstructions from severely subsampled data.},
  keywords = {5D reconstruction, compressed sensing, frequency-domain, parallel, signal processing},
  note = {Submitted to Geophysics},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/sharan2020lsh/sharan2020lsh.html}
}

@UNPUBLISHED{kukreja2020lcc,
  author = {Navjot Kukreja and Jan Hueckelheim and Mathias Louboutin and John Washbourne and Paul H. J. Kelly and Gerard J. Gorman},
  title = {Lossy Checkpoint Compression in Full Waveform Inversion},
  year = {2020},
  month = {9},
  abstract = {This paper proposes a new method that combines check-pointing
methods with error-controlled lossy compression for large-scale
high-performance Full-Waveform Inversion (FWI), an inverse problem commonly
used in geophysical exploration. This combination can significantly reduce
data movement, allowing a reduction in run time as well as peak memory. In
the Exascale computing era, frequent data transfer (e.g., memory bandwidth,
PCIe bandwidth for GPUs, or network) is the performance bottleneck rather
than the peak FLOPS of the processing unit. Like many other adjoint-based
optimization problems, FWI is costly in terms of the number of floating-point
operations, large memory footprint during backpropagation, and data transfer
overheads. Past work for adjoint methods has developed checkpointing methods
that reduce the peak memory requirements during backpropagation at the cost
of additional floating-point computations. Combining this traditional
checkpointing with error-controlled lossy compression, we explore the
three-way tradeoff between memory, precision, and time to solution. We
investigate how approximation errors introduced by lossy compression of the
forward solution impact the objective function gradient and final inverted
solution. Empirical results from these numerical experiments indicate that
high lossy-compression rates (compression factors ranging up to 100) have a
relatively minor impact on convergence rates and the quality of the final
solution.},
  keywords = {Lossy compression, Full waveform inversion, checkpointing, memory},
  note = {Submitted to GMD},
  url = {https://arxiv.org/pdf/2009.12623.pdf}
}
