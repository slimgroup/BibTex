% This file was created with JabRef 2.9.
% Encoding: MacRoman

%-----2022-----%

https://www.dropbox.com/home/EAS-Herrmann-TEAM-Ext/DoNotSync/WebServer/Publications/Public/Conferences/MIDL/2022

@UNPUBLISHED{orozco2022MIDLmei,
  author = {Rafael Orozco and Mathias Louboutin and Felix J. Herrmann},
  title = {Memory Efficient Invertible Neural Networks for 3D Photoacoustic Imaging},
  year = {2022},
  month = {04},
  abstract = {Photoacoustic imaging (PAI) can image high-resolution structures of
clinical interest such as vascularity in cancerous tumor monitoring. When
imaging human subjects, geometric restrictions force limited-view data
retrieval causing imaging artifacts. Iterative physical model based
approaches reduce artifacts but require prohibitively time consuming PDE
solves. Machine learning (ML) has accelerated PAI by combining physical
models and learned networks. However, the depth and overall power of ML
methods is limited by memory intensive training. We propose using invertible
neural networks (INNs) to alleviate memory pressure. We demonstrate INNs can
image 3D photoacoustic volumes in the setting of limited-view, noisy, and
subsampled data. The frugal constant memory usage of INNs enables us to train
an arbitrary depth of learned layers on a consumer GPU with 16GB RAM.},
  keywords = {Invertible Networks, Medical Imaging, Physics and Machine Learning Hybrid, Photoacoustic Imaging},
  note = {Submitted},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2022/orozco2022MIDLmei/midl_2022.html}
}

@UNPUBLISHED{grady2022SCtll,
  author = {Thomas J. Grady II and Rishi Khan and Mathias Louboutin and Ziyi Yin and Philipp A. Witte and Ranveer Chandra and Russell J. Hewett and Felix J. Herrmann},
  title = {Model-Parallel Fourier Neural Operators as Learned Surrogates for Large-Scale Parametric PDEs},
  year = {2022},
  month = {04},
  abstract = {Fourier neural operators (FNOs) are a recently introduced neural
network architecture for learning solution operators of partial differential
equations (PDEs), which have been shown to perform significantly better than
comparable deep learning approaches. Once trained, FNOs can achieve speed-ups
of multiple orders of magnitude over conventional numerical PDE solvers.
However, due to the high dimensionality of their input data and network
weights, FNOs have so far only been applied to two-dimensional or small
three-dimensional problems. To remove this limited problem-size barrier, we
propose a model-parallel version of FNOs based on domain-decomposition of
both the input data and network weights. We demonstrate that our
model-parallel FNO is able to predict time-varying PDE solutions of over 3.2
billion variables on Summit using up to 768 GPUs and show an example of
training a distributed FNO on the Azure cloud for simulating multiphase CO2
dynamics in the Earth's subsurface.},
  keywords = {Fourier neural operators, HPC, large-scale, CCS, deep learning, Operator Learning, Model Parallelism, Multiphase Flow},
  note = {Submitted},
  software = {https://github.com/slimgroup/dfno},
  url = {https://arxiv.org/pdf/2204.01205.pdf}
}

@UNPUBLISHED{siahkoohi2022EAGEweb,
  author = {Ali Siahkoohi and Rafael Orozco and Gabrio Rizzuti and Felix J. Herrmann},
  title = {Wave-equation based inversion with amortized variational Bayesian inference},
  year = {2022},
  month = {03},
  abstract = {Solving inverse problems involving measurement noise and modeling
errors requires regularization in order to avoid data overfit. Geophysical
inverse problems, in which the Earth's highly heterogeneous structure is
unknown, present a challenge in encoding prior knowledge through analytical
expressions. Our main contribution is a generative-model-based regularization
approach, robust to out-of-distribution data, which exploits the prior
knowledge embedded in existing data and model pairs. Utilizing an amortized
variational inference objective, a conditional normalizing flow (NF) is
pretrained on pairs of low- and high-fidelity migrated images in order to
achieve a low-fidelity approximation to the seismic imaging posterior
distribution for previously unseen data. The NF is used after pretraining to
reparameterize the unknown seismic image in an inversion scheme involving
physics-guided data misfit and a Gaussian prior on the NF latent variable.
Solving this optimization problem with respect to the latent variable enables
us to leverage the benefits of data-driven conditional priors whilst being
informed by physics and data. The numerical experiments demonstrate that the
proposed inversion scheme produces seismic images with limited artifacts when
dealing with noisy and out-of-distribution data.},
  keywords = {seismic imaging, normalizing flows, conditional priors},
  note = {Submitted},
  software = {https://github.com/slimgroup/ConditionalNFs4Imaging.jl},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2022/siahkoohi2022EAGEweb/abstract.html}
}

%-----2021-----%

@UNPUBLISHED{siahkoohi2021dbif,
  author = {Ali Siahkoohi and Gabrio Rizzuti and Felix J. Herrmann},
  title = {Deep Bayesian inference for seismic imaging with tasks},
  year = {2021},
  month = {10},
  abstract = {We propose to use techniques from Bayesian inference and deep
neural networks to translate uncertainty in seismic imaging to uncertainty in
tasks performed on the image, such as horizon tracking. Seismic imaging is an
ill-posed inverse problem because of unavoidable bandwidth and aperture
limitations, which that is hampered by the presence of noise and
linearization errors. Many regularization methods, such as transform-domain
sparsity promotion, have been designed to deal with the adverse effects of
these errors, however, these methods run the risk of biasing the solution and
do not provide information on uncertainty in the image space and how this
uncertainty impacts certain tasks on the image. A systematic approach is
proposed to translate uncertainty due to noise in the data to confidence
intervals of automatically tracked horizons in the image. The uncertainty is
characterized by a convolutional neural network (CNN) and to assess these
uncertainties, samples are drawn from the posterior distribution of the CNN
weights, used to parameterize the image. Compared to traditional priors, in
the literature it is argued that these CNNs introduce a flexible inductive
bias that is a surprisingly good fit for many diverse domains in imaging. The
method of stochastic gradient Langevin dynamics is employed to sample from
the posterior distribution. This method is designed to handle large scale
Bayesian inference problems with computationally expensive forward operators
as in seismic imaging. Aside from offering a robust alternative to maximum a
posteriori estimate that is prone to overfitting, access to these samples
allow us to translate uncertainty in the image, due to noise in the data, to
uncertainty on the tracked horizons. For instance, it admits estimates for
the pointwise standard deviation on the image and for confidence intervals on
its automatically tracked horizons.},
  keywords = {deep priors, seismic imaging, uncertainty quantification, horizon tracking},
  note = {Submitted to Geophysics},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2021/siahkoohi2021dbif/paper.html}
}

%-----2020-----%

@UNPUBLISHED{lopez2020gsb,
  author = {Oscar Lopez and Rajiv Kumar and Nick Moldoveanu and Felix J. Herrmann},
  title = {Graph Spectrum Based Seismic Survey Design},
  year = {2020},
  month = {12},
  abstract = {Randomized sampling techniques have become increasingly useful in seismic data acquisition and processing, allowing practitioners to achieve dense wavefield reconstruction from a substantially reduced number of field samples. However, typical designs studied in the low-rank matrix recovery and compressive sensing literature are difficult to achieve by standard industry hardware. For practical purposes, a compromise between stochastic and realizable samples is needed. In this paper, we propose a deterministic and computationally cheap tool to alleviate randomized acquisition design, prior to survey deployment and large-scale optimization. We consider universal and deterministic matrix completion results in the context of seismology, where a bipartite graph representation of the source-receiver layout allows for the respective spectral gap to act as a quality metric for wavefield reconstruction. We provide realistic survey design scenarios to demonstrate the utility of the spectral gap for successful seismic data acquisition via low-rank and sparse signal recovery.},
  keywords = {acquisition, compressed sensing, data reconstruction, survey design, algorithm},
  note = {Submitted to Geophysics},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/lopez2020gsb/lopez2020gsb.pdf}
}

@UNPUBLISHED{sharan2020lsh,
  author = {Shashin Sharan and Yijun Zhang and Oscar Lopez and Felix J. Herrmann},
  title = {Large scale high-frequency wavefield reconstruction with recursively weighted matrix factorizations},
  year = {2020},
  month = {10},
  abstract = {Acquiring seismic data on a regular periodic fine grid is challenging. By exploiting the low-rank approximation property of fully sampled seismic data in some transform domain, low-rank matrix completion offers a scalable way to reconstruct seismic data on a regular periodic fine grid from coarsely randomly sampled data acquired in the field. While wavefield reconstruction have been applied successfully at the lower end of the spectrum, its performance deteriorates at the higher frequencies where the low-rank assumption no longer holds rendering this type of wavefield reconstruction ineffective in situations where high resolution images are desired. We overcome this shortcoming by exploiting similarities between adjacent frequency slices explicitly. During low-rank matrix factorization, these similarities translate to alignment of subspaces of the factors, a notion we propose to employ as we reconstruct monochromatic frequency slices recursively starting at the low frequencies. While this idea is relatively simple in its core, to turn this recent insight into a successful scalable wavefield reconstruction scheme for 3D seismic requires a number of important steps. First, we need to move the weighting matrices, which encapsulate the prior information from adjacent frequency slices, from the objective to the data misfit constraint. This move considerably improves the performance of the weighted low-rank matrix factorization on which our wavefield reconstructions is based. Secondly, we introduce approximations that allow us to decouple computations on a row-by-row and column-by-column basis, which in turn allow to parallelize the alternating optimization on which our low-rank factorization relies. The combination of weighting and decoupling leads to a computationally feasible full-azimuth wavefield reconstruction scheme that scales to industry-scale problem sizes. We demonstrate the performance of the proposed parallel algorithm on a 2D field data and on a 3D synthetic dataset. In both cases our approach produces high-fidelity broadband wavefield reconstructions from severely subsampled data.},
  keywords = {5D reconstruction, compressed sensing, frequency-domain, parallel, signal processing},
  note = {Submitted to Geophysics},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/sharan2020lsh/sharan2020lsh.html}
}

@UNPUBLISHED{kukreja2020lcc,
  author = {Navjot Kukreja and Jan Hueckelheim and Mathias Louboutin and John Washbourne and Paul H. J. Kelly and Gerard J. Gorman},
  title = {Lossy Checkpoint Compression in Full Waveform Inversion},
  year = {2020},
  month = {9},
  abstract = {This paper proposes a new method that combines check-pointing
methods with error-controlled lossy compression for large-scale
high-performance Full-Waveform Inversion (FWI), an inverse problem commonly
used in geophysical exploration. This combination can significantly reduce
data movement, allowing a reduction in run time as well as peak memory. In
the Exascale computing era, frequent data transfer (e.g., memory bandwidth,
PCIe bandwidth for GPUs, or network) is the performance bottleneck rather
than the peak FLOPS of the processing unit. Like many other adjoint-based
optimization problems, FWI is costly in terms of the number of floating-point
operations, large memory footprint during backpropagation, and data transfer
overheads. Past work for adjoint methods has developed checkpointing methods
that reduce the peak memory requirements during backpropagation at the cost
of additional floating-point computations. Combining this traditional
checkpointing with error-controlled lossy compression, we explore the
three-way tradeoff between memory, precision, and time to solution. We
investigate how approximation errors introduced by lossy compression of the
forward solution impact the objective function gradient and final inverted
solution. Empirical results from these numerical experiments indicate that
high lossy-compression rates (compression factors ranging up to 100) have a
relatively minor impact on convergence rates and the quality of the final
solution.},
  keywords = {Lossy compression, Full waveform inversion, checkpointing, memory},
  note = {Submitted to GMD},
  url = {https://arxiv.org/pdf/2009.12623.pdf}
}
